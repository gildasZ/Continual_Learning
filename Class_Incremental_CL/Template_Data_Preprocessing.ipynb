{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *__Working on BTCUSD predictions with GRU model.__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *__Check first before starting__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Do 'pipenv install ipykernel' if you get error.\n",
    "print(\"Kernel is working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: C:\\Users\\gilda\\OneDrive\\Documents\\_NYCU\\MASTER_S_studies\\Master's Thesis\\LABORATORY\\_Global_Pytorch\\Continual_Learning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Change the working directory to the project root\n",
    "Working_directory = os.path.normpath(\"C:/Users/gilda/OneDrive/Documents/_NYCU/MASTER_S_studies/Master\\'s Thesis/LABORATORY/_Global_Pytorch/Continual_Learning\")\n",
    "os.chdir(Working_directory)\n",
    "print(f\"Working directory: {os.getcwd()}\")  # Prints the current working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **__All imports__**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_interactions import zoom_factory, panhandler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from ta import trend, momentum, volatility, volume\n",
    "import math\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from typing import Callable, Tuple\n",
    "import shutil\n",
    "import contextlib\n",
    "import traceback\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __**All functions (For data processing)**__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "def plot_with_matplotlib(data: pd.DataFrame, \n",
    "                         title: str, \n",
    "                         interactive: bool = False, \n",
    "                         save_path: str = None, \n",
    "                         show_plot: bool = True, \n",
    "                         save_matplotlib_object: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Plot data using Matplotlib, with optional interactivity using mpl-interactions.\n",
    "    \n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): The data to plot, must contain 'close' column.\n",
    "    - title (str): The title of the plot.\n",
    "    - interactive (bool): If True, enables interactive zoom and pan.\n",
    "    - save_path (Optional[str]): If provided, saves the plot to this path.\n",
    "    - show_plot (bool): If True, displays the plot. If False, skips display.\n",
    "    - save_matplotlib_object (Optional[str]): If provided, saves the Matplotlib figure object to this file path.\n",
    "    \"\"\"\n",
    "    if not all(col in data.columns for col in ['close']):\n",
    "        raise ValueError(\"The input DataFrame must contain 'close' column.\")\n",
    "\n",
    "    # Use the default Matplotlib color cycle for the line\n",
    "    default_blue = plt.rcParams['axes.prop_cycle'].by_key()['color'][0]\n",
    "    print(f\"Default blue: {default_blue}\")\n",
    "\n",
    "    # Explicit colors for trends\n",
    "    trend_colors = {\n",
    "        0: 'black',\n",
    "        1: 'yellow',\n",
    "        2: 'red',\n",
    "        3: 'green',\n",
    "        4: default_blue #'purple',\n",
    "    }\n",
    "    # unique_trends = [0, -25, -15, 15, 25]\n",
    "    # colormap = plt.cm.get_cmap('tab10', len(unique_trends))  # Choose 'tab10' or 'Set1' for distinct colors\n",
    "    # trend_colors = {trend: colormap(i) for i, trend in enumerate(unique_trends)}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot data as a single connected line, colored by trend\n",
    "    if 'trend' in data.columns:\n",
    "        legend_added = set() # Track which trends have already been added to the legend\n",
    "        prev_idx = data.index[0]\n",
    "        for idx, row in data.iterrows():\n",
    "            if idx != prev_idx:\n",
    "                trend_key = int(row['trend'])  # Convert trend value to int for lookup\n",
    "                label = f'Trend {trend_key}' if trend_key not in legend_added else None\n",
    "                ax.plot([prev_idx, idx], \n",
    "                        [data.loc[prev_idx, 'close'], row['close']],\n",
    "                        color=trend_colors[trend_key], \n",
    "                        linestyle='-', \n",
    "                        # marker='o', \n",
    "                        linewidth=1,\n",
    "                        label=label  # Add label only if it's not in the legend\n",
    "                )\n",
    "                legend_added.add(trend_key)  # Mark this trend as added to the legend\n",
    "            prev_idx = idx\n",
    "\n",
    "        ax.set_title(f\"{title} (Connected, Colored by Trend)\")\n",
    "    else:\n",
    "        # Default plot if no trend column exists\n",
    "        ax.plot(data.index, data['close'], label='Closing Price', linestyle='-', marker='o', \n",
    "                markersize=2, linewidth=1, color=default_blue, markerfacecolor='green', markeredgecolor='black')\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Closing Price (USD)')\n",
    "    \n",
    "    # Add a legend manually for trends\n",
    "    # for trend, color in trend_colors.items():\n",
    "    #     ax.plot([], [], color=color, label=f'Trend {trend}')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    \n",
    "    # Enable interactivity if requested\n",
    "    if interactive:\n",
    "        zoom_factory(ax)  # Enable zoom with mouse wheel\n",
    "        panhandler(fig)   # Enable panning with left-click\n",
    "        print(\"Interactive mode enabled. Use mouse wheel to zoom and left click to pan.\")\n",
    "\n",
    "    # Save the plot if a path is provided\n",
    "    if save_path:\n",
    "        fig.tight_layout()  # Ensures the layout is clean\n",
    "        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to: {save_path}\")\n",
    "\n",
    "    # Save the Matplotlib figure object\n",
    "    if save_matplotlib_object:\n",
    "        with open(save_matplotlib_object, 'wb') as f:\n",
    "            pickle.dump(fig, f)\n",
    "        print(f\"Matplotlib figure object saved to: {save_matplotlib_object}\")\n",
    "\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Plot display skipped.\")\n",
    "\n",
    "def load_and_show_pickle(pickle_file_path: str):\n",
    "    \"\"\"\n",
    "    Load a pickled Matplotlib figure object and display it with optional interactivity.\n",
    "\n",
    "    Parameters:\n",
    "    - pickle_file_path (str): Path to the pickled Matplotlib figure file.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the pickle file and load the figure\n",
    "        with open(pickle_file_path, \"rb\") as f:\n",
    "            loaded_fig = pickle.load(f)\n",
    "\n",
    "        print(f\"Figure successfully loaded and displayed from: {pickle_file_path}\")\n",
    "\n",
    "        # Use plt.show() to allow interactivity\n",
    "        plt.show(block=True)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {pickle_file_path}. Please check the path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the pickled figure: {e}\")\n",
    "\n",
    "# Save data to CSV\n",
    "def save_to_csv(df: pd.DataFrame, file_path: str):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the data to be saved.\n",
    "        file_path (str): The file path (including the file name) to save the CSV.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df.to_csv(file_path)\n",
    "    # df_to_save = df.copy()\n",
    "    # df_to_save[\"date\"] = df_to_save.index.strftime('%Y-%m-%d %H:%M:%S')  # Add formatted index as a column\n",
    "    \n",
    "    # Save the DataFrame to CSV\n",
    "    # df_to_save.to_csv(file_path)\n",
    "    print(f\"\\nSuccessfully saved data with moving average to CSV: \\n\\t{file_path}\\n\")\n",
    "\n",
    "def read_csv_file(file_path: str, preview_rows: int = 5, \n",
    "                  days_towards_end: int = None, \n",
    "                  days_from_start: int = None, description: str = \"\"):\n",
    "    \"\"\"\n",
    "    Reads a CSV file and returns a pandas DataFrame filtered by date range.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        preview_rows (int): Number of rows to preview (default is 5).\n",
    "        days_towards_end (int, optional): Number of days from the most recent date to retrieve data.\n",
    "        days_from_start (int, optional): Number of days from the oldest date of the filtered data to retrieve data.\n",
    "        description (str): A brief description of the dataset being loaded.\n",
    "                           Explanation:\n",
    "                           - To retrieve data from the **end**: Use `days_towards_end`.\n",
    "                           - To retrieve data from the **start of the filtered range**: Use `days_from_start`.\n",
    "                           - To retrieve data from the **middle**: Use both:\n",
    "                             For example, if `days_towards_end=100` and `days_from_start=50`,\n",
    "                             the function will first filter the last 100 days of the dataset,\n",
    "                             and then filter the first 50 days from this range.\n",
    "                             This results in data between the last 100th and the last 50th day.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The loaded and filtered data from the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if description:\n",
    "            print(f\"\\nDescription: {description}\")\n",
    "        print(f\"\\nFile path: {file_path}\")\n",
    "        \n",
    "        # Read the CSV file\n",
    "        data = pd.read_csv(file_path, parse_dates=['date'], index_col='date')\n",
    "        \n",
    "        # Filter by days towards the end\n",
    "        if days_towards_end is not None:\n",
    "            last_date = data.index.max()  # Get the most recent date in the dataset\n",
    "            end_cutoff_date = last_date - pd.Timedelta(days=days_towards_end)\n",
    "            data = data[data.index >= end_cutoff_date]\n",
    "            print(f\"\\nRetrieving data from the past {days_towards_end} days (from {end_cutoff_date.date()} onwards):\")\n",
    "        \n",
    "        # Filter by days from the start (from the filtered data)\n",
    "        if days_from_start is not None:\n",
    "            first_date = data.index.min()  # Get the earliest date in the filtered dataset\n",
    "            start_cutoff_date = first_date + pd.Timedelta(days=days_from_start)\n",
    "            data = data[data.index <= start_cutoff_date]\n",
    "            print(f\"\\nRetrieving the first {days_from_start} days from the filtered data (up to {start_cutoff_date.date()}):\")\n",
    "\n",
    "        if preview_rows:\n",
    "            # Print a preview of the data\n",
    "            print(f\"\\nPreview of the first {preview_rows} rows:\")\n",
    "            # print(data.head(preview_rows), '\\n')\n",
    "            display(data.head(preview_rows))\n",
    "            print()\n",
    "\n",
    "            print(f\"\\nPreview of the last {preview_rows} rows:\")\n",
    "            # print(data.tail(preview_rows), '\\n')\n",
    "            display(data.tail(preview_rows))\n",
    "            print()\n",
    "\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File not found. Please check the file path.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The file is empty.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error: The file could not be parsed. Please check the file format.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def downsample_minute_data(data: pd.DataFrame, n: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downsample minute data into N-minute intervals by retaining every Nth row.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The original DataFrame with a datetime index.\n",
    "        n (int): The number of minutes for the downsampling interval.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Downsampled DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\n========---> Downsampling the data! \\n\")\n",
    "    data = data.copy()\n",
    "    # Ensure the index is a DatetimeIndex\n",
    "    if not isinstance(data.index, pd.DatetimeIndex):\n",
    "        try:\n",
    "            data.index = pd.to_datetime(data.index)  # Convert to DatetimeIndex\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"The DataFrame index could not be converted to DatetimeIndex.\") from e\n",
    "\n",
    "    # Filter rows where the minute index modulo N is 0\n",
    "    downsampled_data = data[data.index.minute % n == 0]\n",
    "\n",
    "    return downsampled_data\n",
    "\n",
    "def add_indicators(data: pd.DataFrame, output_path: str, number_days: int = None, preview_rows: int = 5, freq: str = '1min') -> None:\n",
    "    \"\"\"\n",
    "    Adds technical indicators to a financial dataset, trims the dataset to the last N days, \n",
    "    and saves the enriched dataset to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input financial dataset. Must contain at least the following columns: \n",
    "                             'open', 'high', 'low', 'close', and 'volume'. The index should be a datetime index.\n",
    "        output_path (str): The file path where the enriched dataset will be saved as a CSV file.\n",
    "        number_days (int): The number of days to retain from the most recent date in the dataset.\n",
    "        preview_rows (int, optional): Number of rows to preview during various stages of processing. Default is 5.\n",
    "        freq (str): Frequency for the continuous time index. Default is '1min'.\n",
    "\n",
    "    Returns:\n",
    "        None: The function saves the processed dataset to a file and does not return a value.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"\\n========---> Adding indicators to the data!\")\n",
    "        if number_days is not None:\n",
    "            # Trim data to the last 190 days\n",
    "            last_date = data.index.max()  # Get the most recent date in the dataset\n",
    "            cutoff_date = last_date - pd.Timedelta(days=number_days)  # Calculate the cutoff date\n",
    "            data = data[data.index >= cutoff_date]  # Keep only rows within the last 190 days\n",
    "            print(f\"\\nData trimmed to the last {number_days} days from {cutoff_date} to {last_date}.\\n\")\n",
    "\n",
    "        # Sort data in ascending order by date for proper indicator calculations\n",
    "        data = data.sort_index(ascending=True)\n",
    "        print(f\"\\nPreview of the first {preview_rows} rows after reversing: \\n\")\n",
    "        display(data.head(preview_rows))\n",
    "\n",
    "        # Verify the presence of required columns\n",
    "        required_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "        if not all(col in data.columns for col in required_columns):\n",
    "            raise ValueError(\"Dataset is missing required columns: 'open', 'high', 'low', 'close', 'volume'.\")\n",
    "\n",
    "        #-----------------------------------------------------------------\n",
    "        # Get missing timestamps\n",
    "        missing_timestamps = pd.date_range(\n",
    "            start=data.index.min(), # Returns smallest/earliest/oldest date\n",
    "            end=data.index.max(),\n",
    "            freq=freq,  # Use 'min' for a frequency of 1 minute, '30s' for a frequency of 30 seconds\n",
    "            tz=data.index.tz,\n",
    "        ).difference(data.index)\n",
    "\n",
    "        # Generate a continuous time index (1-minute frequency)\n",
    "        full_time_index = pd.date_range(\n",
    "            start=data.index.min(),\n",
    "            end=data.index.max(),\n",
    "            freq=freq, \n",
    "            tz=data.index.tz,\n",
    "        )\n",
    "        index_name = data.index.name\n",
    "        \n",
    "        # Reindex the DataFrame to include all timestamps, introducing NaNs for missing timestamps\n",
    "        data = data.reindex(full_time_index)\n",
    "        data.index.name = index_name  # Restore index name after reindexing\n",
    "\n",
    "        # Fill missing data with the previous available value\n",
    "        data = data.ffill()  # Forward-fill missing values\n",
    "\n",
    "        # Check if there are missing timestamps\n",
    "        if missing_timestamps.empty:\n",
    "            print(\"\\nNo missing timestamps.\\n\")\n",
    "        else:\n",
    "            for timestamp in missing_timestamps:\n",
    "                print(f\"\\nMissing timestamp: {timestamp}\")\n",
    "                try:\n",
    "                    # Safely access the row after reindexing and forward-filling\n",
    "                    print(f\"Data at missing timestamp ({timestamp}):\")\n",
    "                    print(data.loc[timestamp], '\\n')\n",
    "                except KeyError:\n",
    "                    print(f\"No data available for timestamp {timestamp}\\n\")\n",
    "\n",
    "        # Get missing timestamps\n",
    "        missing_timestamps = pd.date_range(\n",
    "            start=data.index.min(), # Returns smallest/earliest/oldest date\n",
    "            end=data.index.max(),\n",
    "            freq=freq,  # Use 'min' for a frequency of 1 minute, '30s' for a frequency of 30 seconds\n",
    "            tz=data.index.tz,\n",
    "        ).difference(data.index)\n",
    "        print(f\"\\nMissing timestamps time: \\n{missing_timestamps}\\n\")\n",
    "        #-----------------------------------------------------------------\n",
    "\n",
    "\n",
    "        # Drop unnecessary columns if present\n",
    "        columns_to_drop = ['volume weighted average', 'barCount']  # Adjust if needed\n",
    "        data = data.drop(columns=[col for col in columns_to_drop if col in data.columns], errors='ignore')\n",
    "\n",
    "        # Add Technical Indicators\n",
    "\n",
    "        ## Trend Indicators\n",
    "        # Simple Moving Averages (SMA)\n",
    "        data['SMA_5'] = trend.sma_indicator(data['close'], window=5)\n",
    "        data['SMA_10'] = trend.sma_indicator(data['close'], window=10)\n",
    "        data['SMA_50'] = trend.sma_indicator(data['close'], window=50)\n",
    "\n",
    "        # Exponential Moving Average (EMA)\n",
    "        data['EMA_10'] = trend.ema_indicator(data['close'], window=10)\n",
    "\n",
    "        # Moving Average Convergence Divergence (MACD)\n",
    "        data['MACD'] = trend.macd(data['close'])\n",
    "        data['MACD_signal'] = trend.macd_signal(data['close'])\n",
    "\n",
    "        # Bollinger Bands\n",
    "        data['BB_upper'] = volatility.bollinger_hband(data['close'], window=20, window_dev=2)\n",
    "        data['BB_middle'] = volatility.bollinger_mavg(data['close'], window=20)\n",
    "        data['BB_lower'] = volatility.bollinger_lband(data['close'], window=20)\n",
    "\n",
    "        ## Momentum Indicators\n",
    "        # Relative Strength Index (RSI)\n",
    "        data['RSI_14'] = momentum.rsi(data['close'], window=14)\n",
    "\n",
    "        # Rate of Change (ROC)\n",
    "        data['ROC_10'] = momentum.roc(data['close'], window=10)\n",
    "\n",
    "        ## Volume Indicators\n",
    "        # On-Balance Volume (OBV)\n",
    "        data['OBV'] = volume.on_balance_volume(data['close'], data['volume'])\n",
    "\n",
    "        # Accumulation/Distribution Line (A/D Line)\n",
    "        data['AD_Line'] = volume.acc_dist_index(data['high'], data['low'], data['close'], data['volume'])\n",
    "\n",
    "        ## Volatility Indicators\n",
    "        # Average True Range (ATR)\n",
    "        data['ATR_14'] = volatility.average_true_range(data['high'], data['low'], data['close'], window=14)\n",
    "\n",
    "        # Bollinger Band Width (BBW)\n",
    "        data['BBW'] = (data['BB_upper'] - data['BB_lower']) / data['BB_middle']\n",
    "\n",
    "        # Adjust decimal precision as needed\n",
    "        data = data.round(decimals=9) \n",
    "\n",
    "        # Replace 0 values with NaN to allow forward-fill\n",
    "        data = data.replace(0, pd.NA)\n",
    "        \n",
    "        # Forward-fill to replace NaN (originally 0) with the previous row value\n",
    "        data = data.ffill()\n",
    "\n",
    "        # Drop rows with NaN values (optional, as technical indicators often result in NaN for initial (oldest dates) rows)\n",
    "        data = data.dropna()\n",
    "\n",
    "        # Sort the dataset back to descending order for final output\n",
    "        data = data.sort_index(ascending=False)\n",
    "\n",
    "        print(f\"\\nPreview of the first {preview_rows} rows with indicators: \\n\")\n",
    "        display(data.head(preview_rows))\n",
    "\n",
    "        print(f\"\\nPreview of the last {preview_rows} rows with indicators: \\n\")\n",
    "        display(data.tail(preview_rows))\n",
    "\n",
    "        # Save the enriched dataset\n",
    "        data.to_csv(output_path)\n",
    "        print(f\"\\nEnriched dataset saved to \\n\\t{output_path}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError adding indicators: {e}\\n\")\n",
    "        print(\"\\nDetailed traceback:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Function to calculate percentage changes\n",
    "def calculate_percentage_changes(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate percentage changes for a given dataset and return the result in ascending order.\n",
    "    The `real_close` column is added after the percentage changes calculation.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The input dataset with a datetime index and numeric values (e.g., closing prices).\n",
    "                              The DataFrame must already have a properly formatted datetime index.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the percentage changes, indexed by date in ascending order.\n",
    "                      The 'real_close' column is added, holding the original close prices.\n",
    "    \"\"\"\n",
    "    data = data.sort_index(ascending=True).copy()  # Ensure chronological order\n",
    "    data_pct_change = data.pct_change() # Calculate percentage changes\n",
    "    data_pct_change['real_close'] = data['close'] # Add the 'real_close' column (copy of original 'close' column)\n",
    "    return data_pct_change.dropna()  # Drop rows with NaN values resulting from pct_change()\n",
    "\n",
    "def calculate_log_returns_all_columns(data: pd.DataFrame, exclude_columns: list = [], dropna: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate log returns for all numeric columns in a pandas DataFrame,\n",
    "    excluding specified columns, and removing excluded columns from the returned DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Input DataFrame containing numeric data.\n",
    "        exclude_columns (list): List of columns to exclude from log return calculations and the result.\n",
    "        dropna (bool): Whether to drop rows with NaN values resulting from the calculation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with log returns for numeric columns,\n",
    "                      excluding specified columns.\n",
    "    \"\"\"\n",
    "    data = data.copy().drop(columns=exclude_columns)\n",
    "    columns_to_transform = data.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"columns_to_transform = \\n{columns_to_transform}, \\nlen(columns_to_transform) = {len(columns_to_transform)}\")\n",
    "\n",
    "    for col in columns_to_transform:\n",
    "        # Ensure no negative or zero values\n",
    "        if (data[col] <= 0).any():\n",
    "            raise ValueError(f\"Column '{col}' contains non-positive values. Log returns require strictly positive values.\")\n",
    "        data[col] = np.log(data[col] / data[col].shift(1))\n",
    "\n",
    "    # Optionally drop rows with NaN values\n",
    "    return data.dropna() if dropna else data\n",
    "\n",
    "# Prepare the data for the model by creating sequences of input features and output targets.\n",
    "def create_sequences(data: pd.DataFrame, \n",
    "                     input_length: int, \n",
    "                     output_length: int, \n",
    "                     sliding_interval: int = 60) -> tuple[np.ndarray, np.ndarray, pd.Index]:\n",
    "    \"\"\"\n",
    "    Generate input-output sequences with a sliding window.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing features for time-series modeling, indexed by 'date'.\n",
    "        input_length (int): Number of timesteps for input sequences (e.g., 2880 for 2 days).\n",
    "        output_length (int): Number of timesteps for output sequences (e.g., 1440 for 1 day).\n",
    "        sliding_interval (int): Interval to slide the window (e.g., 60 for 1 hour).\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray, pd.Index, list, list]:\n",
    "            - X: Input sequences of shape (num_sequences, input_length, num_features).\n",
    "            - y: Output sequences of shape (num_sequences, output_length, num_features).\n",
    "            - indices: A pandas Index object with the start dates of each input sequence for reference.\n",
    "            - end_start_X: The real end values for each input sequence.\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    # Preserve the date index for later reference\n",
    "    original_index = data.index\n",
    "    # print(original_index[:5])\n",
    "\n",
    "    # Reset index to allow slicing by integer position\n",
    "    data = data.reset_index(drop=True)  # Removes the 'date' column which was the index\n",
    "    print(\"\\nColumns after resetting index with drop=True: \\n\", data.columns)\n",
    "    close_column_index = data.columns.get_loc('close')\n",
    "    print(f\"'close' column index: {close_column_index}\")\n",
    "    num_columns = data.shape[1]\n",
    "    print(f\"Number of columns: {num_columns}\")\n",
    "    print('')\n",
    "    \n",
    "    X, y, indices = [], [], []\n",
    "    real_close_X = []\n",
    "    total_length = len(data)\n",
    "\n",
    "    for start in range(0, total_length - input_length - output_length + 1, sliding_interval):\n",
    "        end_input = start + input_length\n",
    "        end_output = end_input + output_length\n",
    "\n",
    "        # Slice input and output sequences, excluding 'real_close' column\n",
    "        X.append(data.iloc[start:end_input].drop(columns=['real_close']).values)\n",
    "        y.append(data.iloc[end_input:end_output].values)\n",
    "        \n",
    "        # Save the start date of the sequence\n",
    "        indices.append(original_index[start])\n",
    "\n",
    "        # Get the real 'close' value at the start of the input and output sequences\n",
    "        real_close_X.append(data.iloc[start:end_input]['real_close'])\n",
    "    \n",
    "    return np.array(X), np.array(y), pd.Index(indices), np.array(real_close_X)\n",
    "\n",
    "def created_sequences_2(data: pd.DataFrame, sequence_length: int = 60, sliding_interval: int = 60) -> list:\n",
    "    \"\"\"\n",
    "    Divide the dataset into sequences based on the sequence_length.\n",
    "    Each sequence must fully cover the window size.\n",
    "\n",
    "    Args:\n",
    "    - data (pd.DataFrame): The input DataFrame.\n",
    "    - sequence_length (int): The window size for sequences.\n",
    "\n",
    "    Returns:\n",
    "    - sequences (list): A list of sequences (as DataFrames).\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for i in range(0, len(data) - sequence_length + 1, sliding_interval):\n",
    "        # print(f\"Processing sequence starting at index: {i}\")\n",
    "        seq = data.iloc[i:i + sequence_length].copy()\n",
    "        sequences.append(seq)\n",
    "    # print(f\"Total sequences created: {len(sequences)}\")\n",
    "    return sequences\n",
    "\n",
    "def prepare_percentage_change_data(df: pd.DataFrame, \n",
    "                                   features: list, \n",
    "                                   target: str = 'close', \n",
    "                                   window_size: int = 50, \n",
    "                                   test_size: float = 0.25, \n",
    "                                   shuffle: bool = False) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Prepares percentage change stock data for training a sequence-based model.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The percentage change data.\n",
    "        features (list): List of column names to use as input features.\n",
    "        target (str): The column name to predict.\n",
    "        window_size (int): The size of the sliding window.\n",
    "        test_size (float): The proportion of the dataset to include in the test split.\n",
    "        shuffle (bool): Whether to shuffle the data before splitting.\n",
    "\n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test: Prepared train and test datasets.\n",
    "    \"\"\"\n",
    "    # Create sequences\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(df) - window_size):\n",
    "        sequences.append(df[features].iloc[i:i + window_size].values)\n",
    "        targets.append(df[target].iloc[i + window_size])  # Predict the target at the next step\n",
    "    \n",
    "    sequences = np.array(sequences)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sequences, targets, test_size=test_size, shuffle=shuffle)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def moving_average(data, window_size=5):\n",
    "    data = data.sort_index(ascending=True).copy()\n",
    "    return data.rolling(window=window_size).mean()\n",
    "\n",
    "def exponential_moving_average(data, span=5):\n",
    "    data = data.sort_index(ascending=True).copy()\n",
    "    return data.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "def gaussian_smoothing(data: pd.DataFrame, sigma=2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies Gaussian smoothing to numeric columns in a DataFrame and ensures the index is sorted in ascending order.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Input DataFrame.\n",
    "        sigma (float): Standard deviation for the Gaussian kernel. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with smoothed numeric columns and sorted index.\n",
    "    \"\"\"\n",
    "    # Sort the DataFrame by index in ascending order\n",
    "    data = data.sort_index(ascending=True).copy()\n",
    "    for column in data.columns:\n",
    "        if pd.api.types.is_numeric_dtype(data[column]):  # Only apply to numeric columns\n",
    "            data[column] = gaussian_filter1d(data[column].values, sigma=sigma)\n",
    "    return data\n",
    "\n",
    "def detect_trends(\n",
    "    dataframe: pd.DataFrame, \n",
    "    column: str = 'close', \n",
    "    lower_threshold: float = 0.10, \n",
    "    upper_threshold: float = 0.20\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes a DataFrame to calculate log returns and categorize trends based on thresholds.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): Input DataFrame containing the price data.\n",
    "        column (str): Column name for price data. Defaults to 'close'.\n",
    "        lower_threshold (float): Threshold for categorizing moderate trends. Defaults to 0.10.\n",
    "        upper_threshold (float): Threshold for categorizing strong trends. Defaults to 0.20.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with added columns:\n",
    "                      - 'log_return': Logarithmic returns of the specified column.\n",
    "                      - 'trend': Categorized trend values:\n",
    "                          - -25 for very strong negative trend\n",
    "                          - -15 for moderate negative trend\n",
    "                          - 0 for no trend\n",
    "                          - 15 for moderate positive trend\n",
    "                          - 25 for very strong positive trend\n",
    "    \"\"\"\n",
    "    # Copy to avoid modifying the original DataFrame\n",
    "    df = dataframe.copy()\n",
    "    \n",
    "    # Calculate log returns\n",
    "    df['log_return'] = np.log(df[column] / df[column].shift(1))\n",
    "    \n",
    "    # Function to categorize trends\n",
    "    def categorize_trend(log_return):\n",
    "        if log_return < -upper_threshold:\n",
    "            return -25\n",
    "        elif -upper_threshold <= log_return < -lower_threshold:\n",
    "            return -15\n",
    "        elif -lower_threshold <= log_return <= lower_threshold:\n",
    "            return 0\n",
    "        elif lower_threshold < log_return <= upper_threshold:\n",
    "            return 15\n",
    "        else:  # log_return > upper_threshold\n",
    "            return 25\n",
    "    \n",
    "    # Apply trend categorization\n",
    "    df['trend'] = df['log_return'].apply(categorize_trend)\n",
    "\n",
    "    # Drop NaN values caused by shift\n",
    "    return df.dropna()\n",
    "\n",
    "def detect_trends_2(\n",
    "    dataframe: pd.DataFrame, \n",
    "    column: str = 'close', \n",
    "    lower_threshold: float = 0.10, \n",
    "    upper_threshold: float = 0.20\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes a DataFrame to calculate log returns and categorize trends using interval bins.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): Input DataFrame containing the price data.\n",
    "        column (str): Column name for price data. Defaults to 'close'.\n",
    "        lower_threshold (float): Threshold for categorizing moderate trends. Defaults to 0.10.\n",
    "        upper_threshold (float): Threshold for categorizing strong trends. Defaults to 0.20.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with added columns:\n",
    "                      - 'log_return': Logarithmic returns of the specified column.\n",
    "                      - 'trend': Categorized trend values:\n",
    "                          - -25 for very strong negative trend\n",
    "                          - -15 for moderate negative trend\n",
    "                          - 0 for no trend\n",
    "                          - 15 for moderate positive trend\n",
    "                          - 25 for very strong positive trend\n",
    "    \"\"\"\n",
    "    # Copy to avoid modifying the original DataFrame\n",
    "    df = dataframe.copy()\n",
    "\n",
    "    # Calculate log returns\n",
    "    df['log_return'] = np.log(df[column] / df[column].shift(1))\n",
    "    \n",
    "    # Define bins and corresponding labels for trends\n",
    "    bins = [-np.inf, -upper_threshold, -lower_threshold, lower_threshold, upper_threshold, np.inf]\n",
    "    labels = [-25, -15, 0, 15, 25]\n",
    "    \n",
    "    # Categorize trends using pd.cut\n",
    "    df['trend'] = pd.cut(df['log_return'], bins=bins, labels=labels, right=True)\n",
    "    \n",
    "    # Replace NaN trends with 0 (default value for no trend)\n",
    "    df['trend'] = df['trend'].fillna(0).astype(int)\n",
    "\n",
    "    # Drop NaN values caused by shift\n",
    "    return df.dropna()\n",
    "\n",
    "def detect_trends_3(\n",
    "    dataframe: pd.DataFrame, \n",
    "    column: str = 'close', \n",
    "    lower_threshold: float = 0.001, \n",
    "    upper_threshold: float = 0.02,\n",
    "    reverse_steps: int = 7\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detects trends based on log return data provided in a specified column and categorizes them into different strength levels.\n",
    "\n",
    "    This function analyzes time-series data by evaluating cumulative trends in log return values provided in the input DataFrame. It uses three dictionaries (`dic1`, `dic2`, `dic3`) to track different phases of trends, handles multi-step reversals, and classifies trends dynamically based on cumulative product thresholds and specified thresholds for trend strengths.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): Input DataFrame containing log return data.\n",
    "        column (str): Column name containing log return values. Defaults to 'close'.\n",
    "        lower_threshold (float): Threshold for categorizing moderate trends. Defaults to 0.001.\n",
    "        upper_threshold (float): Threshold for categorizing strong trends. Defaults to 0.02.\n",
    "        reverse_steps (int): Number of consecutive steps to confirm a trend reversal. Defaults to 7.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with an added column:\n",
    "                    - 'trend': Categorized trend values based on the detected phases:\n",
    "                        - 0: No trend\n",
    "                        - 1: Moderate negative trend\n",
    "                        - 2: Very strong negative trend\n",
    "                        - 3: Moderate positive trend\n",
    "                        - 4: Very strong positive trend\n",
    "\n",
    "    Function Details:\n",
    "    1. **Input Assumption**:\n",
    "    - The input DataFrame already contains log return data in the specified column (`column`).\n",
    "\n",
    "    2. **Trend Tracking**:\n",
    "    - Uses dictionaries to monitor trends:\n",
    "        - `dic1`: Tracks the first phase of the trend.\n",
    "        - `dic2`: Tracks the second phase if a reversal occurs.\n",
    "        - `dic3`: Tracks the third phase if another reversal occurs.\n",
    "\n",
    "    3. **Cumulative Product**:\n",
    "    - Calculates the cumulative product of `(1 + log_return)` from the specified column to evaluate the strength of trends.\n",
    "\n",
    "    4. **Reversal Handling**:\n",
    "    - If a trend reversal persists beyond `reverse_steps`, labels are assigned based on the cumulative product tracked in `dic1`.\n",
    "    - Subsequent reversals are merged or labeled independently if conditions are met.\n",
    "\n",
    "    5. **Label Assignment**:\n",
    "    - Labels are dynamically assigned based on cumulative product thresholds for positive and negative trends:\n",
    "        - Positive trends are categorized as moderate (3) or strong (4).\n",
    "        - Negative trends are categorized as moderate (1) or strong (2).\n",
    "\n",
    "    6. **Edge Cases**:\n",
    "    - Properly handles scenarios where data points are insufficient for trend analysis or when trend phases overlap, ensuring all data points are labeled.\n",
    "    \"\"\"\n",
    "    # Copy to avoid modifying the original DataFrame\n",
    "    df = dataframe.copy()\n",
    "    df['trend'] = None  # Default value \n",
    "    \n",
    "    # print(\"\\n#-------------------- Working on 'trend' patterns -----------------------#\")\n",
    "    dic1, dic2, dic3 = None, None, None # Initialize trend tracking dictionaries\n",
    "    # dic1 = None # {'ids': [], 'last_sign': None, 'cumulative': 1.0}\n",
    "    \n",
    "    def assign_label(dictio_, lower_threshold, upper_threshold):\n",
    "        cumulative = dictio_['cumulative']\n",
    "        # print(f\"cumulative = {cumulative}\")\n",
    "        if cumulative > (1 + upper_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 4  # Very strong positive\n",
    "        elif (1 + lower_threshold) < cumulative <= (1 + upper_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 3  # Moderate positive\n",
    "        elif (1 - upper_threshold) < cumulative <= (1 - lower_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 1  # Moderate negative\n",
    "        elif cumulative <= (1 - upper_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 2  # Very strong negative\n",
    "        else:\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 0  # No trend\n",
    "    \n",
    "    #----------------------- For Loop -----------------------#\n",
    "    for idx, log_ret in enumerate(df[column]):\n",
    "        sign = 1 if log_ret > 0 else -1\n",
    "\n",
    "        if dic1 is None:  # Initialize dic1\n",
    "            # print(f\"\\nThis one time condition 'if loop' is running \\n\")\n",
    "            dic1 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "            continue\n",
    "        last_sign = dic1['last_sign']\n",
    "        if sign == last_sign and dic2 is None:  # Continue same trend\n",
    "            dic1['ids'].append(idx)\n",
    "            dic1['last_sign'] = sign\n",
    "            dic1['cumulative'] *= (1 + log_ret)\n",
    "            continue\n",
    "\n",
    "        # 1st Reversal occuring\n",
    "        if dic2 is None:  # Start dic2\n",
    "            dic2 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "            continue\n",
    "        last_sign = dic2['last_sign']\n",
    "        if sign == last_sign and dic3 is None:  # Continue same trend\n",
    "            dic2['ids'].append(idx)\n",
    "            dic2['last_sign'] = sign\n",
    "            dic2['cumulative'] *= (1 + log_ret)\n",
    "            if len(dic2['ids']) == reverse_steps:\n",
    "                assign_label(dic1, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "                # print(f\"dic1['cumulative'] = {dic1['cumulative']}, and dic1['ids'] = {dic1['ids']}\")\n",
    "                dic1 = dic2\n",
    "                dic2 = None\n",
    "                # print(f\"dic1 after trend reversal persisted and dic1 = dic2 = \\n{dic1}\")\n",
    "                # print(f\"dic2 after being reset: {dic2}\\n\")\n",
    "            continue\n",
    "\n",
    "        # 2nd Reversal occuring\n",
    "        if dic3 is None:  # Start dic3\n",
    "            dic3 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "            continue\n",
    "        last_sign = dic3['last_sign']\n",
    "        if sign == last_sign: # Continue same trend, there is no dic4 to check if is None\n",
    "            dic3['ids'].append(idx)\n",
    "            dic3['last_sign'] = sign\n",
    "            dic3['cumulative'] *= (1 + log_ret)\n",
    "            dic_prod = dic2['cumulative'] * dic3['cumulative']\n",
    "            # if (sign == 1 and dic1['cumulative'] * dic_prod > dic1['cumulative']) or (sign == -1 and dic1['cumulative'] * dic_prod < dic1['cumulative'])):\n",
    "            if (sign == 1 and dic_prod > 1) or (sign == -1 and dic_prod < 1): # More beautiful\n",
    "                # Merge dic1, dic2, and dic3\n",
    "                dic1['ids'] += dic2['ids'] + dic3['ids']\n",
    "                dic1['last_sign'] = dic3['last_sign']\n",
    "                dic1['cumulative'] *= dic2['cumulative'] * dic3['cumulative']\n",
    "                dic2, dic3 = None, None\n",
    "                continue\n",
    "\n",
    "            if len(dic3['ids']) == reverse_steps:      \n",
    "                assign_label(dic1, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "                assign_label(dic2, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "                dic1 = dic3\n",
    "                dic2, dic3 = None, None\n",
    "                # print(f\"dic2 after 2nd trend reversal didn't catch up fast enough, and now \\ndic1 = dic3 = {dic1}\")\n",
    "                # print(f\"dic3 and dic2 after being reset: {dic3}\\n\")\n",
    "            continue\n",
    "            \n",
    "        # 3rd Reversal occuring\n",
    "        assign_label(dic1, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "        # Reassign values\n",
    "        dic1 = dic2\n",
    "        dic2 = dic3\n",
    "        dic3 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "        # print(f\"There was a 3rd trend reversal, and now \\ndic1 = dic2 = {dic1}, \\ndic2 = dic3 = {dic2}\")\n",
    "        # print(f\"dic3 after being reset: {dic3}\\n\")\n",
    "\n",
    "    # Assign remaining labels\n",
    "    if dic1:\n",
    "        assign_label(dic1, lower_threshold, upper_threshold)\n",
    "    if dic2:\n",
    "        assign_label(dic2, lower_threshold, upper_threshold)\n",
    "    if dic3:\n",
    "        assign_label(dic3, lower_threshold, upper_threshold)\n",
    "    # print(\"\\n#-------------------- Returning 'trend' patterns ------------------------#\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def detect_trends_4(\n",
    "    dataframe: pd.DataFrame, \n",
    "    column: str = 'close', \n",
    "    lower_threshold: float = 0.001, \n",
    "    upper_threshold: float = 0.02,\n",
    "    reverse_steps: int = 7,\n",
    "    trends_to_keep: set = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detects trends based on log return data provided in a specified column and categorizes them into different strength levels.\n",
    "\n",
    "    This function analyzes time-series data by evaluating cumulative trends in log return values provided in the input DataFrame. \n",
    "    It uses three dictionaries (`dic1`, `dic2`, `dic3`) to track different phases of trends, handles multi-step reversals, and \n",
    "    classifies trends dynamically based on cumulative product thresholds and specified thresholds for trend strengths.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): Input DataFrame containing log return data.\n",
    "        column (str): Column name containing log return values. Defaults to 'close'.\n",
    "        lower_threshold (float): Threshold for categorizing moderate trends. Defaults to 0.001.\n",
    "        upper_threshold (float): Threshold for categorizing strong trends. Defaults to 0.02.\n",
    "        reverse_steps (int): Number of consecutive steps to confirm a trend reversal. Defaults to 7.\n",
    "        trends_to_keep (set): A set of trend categories to retain; others will be set to 0 (No Trend). Defaults to keeping all trends {0, 1, 2, 3, 4}.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with an added column:\n",
    "                    - 'trend': Categorized trend values based on the detected phases:\n",
    "                        - 0: No trend\n",
    "                        - 1: Moderate negative trend\n",
    "                        - 2: Very strong negative trend\n",
    "                        - 3: Moderate positive trend\n",
    "                        - 4: Very strong positive trend\n",
    "                      Any trends not included in `trends_to_keep` will be reset to 0.\n",
    "\n",
    "    Function Details:\n",
    "    1. **Input Assumption**:\n",
    "    - The input DataFrame already contains log return data in the specified column (`column`).\n",
    "\n",
    "    2. **Trend Tracking**:\n",
    "    - Uses dictionaries to monitor trends:\n",
    "        - `dic1`: Tracks the first phase of the trend.\n",
    "        - `dic2`: Tracks the second phase if a reversal occurs.\n",
    "        - `dic3`: Tracks the third phase if another reversal occurs.\n",
    "\n",
    "    3. **Cumulative Product**:\n",
    "    - Calculates the cumulative product of `(1 + log_return)` from the specified column to evaluate the strength of trends.\n",
    "\n",
    "    4. **Reversal Handling**:\n",
    "    - If a trend reversal persists beyond `reverse_steps`, labels are assigned based on the cumulative product tracked in `dic1`.\n",
    "    - Subsequent reversals are merged or labeled independently if conditions are met.\n",
    "\n",
    "    5. **Label Assignment**:\n",
    "    - Labels are dynamically assigned based on cumulative product thresholds for positive and negative trends:\n",
    "        - Positive trends are categorized as moderate (3) or strong (4).\n",
    "        - Negative trends are categorized as moderate (1) or strong (2).\n",
    "\n",
    "    6. **Trend Filtering**:\n",
    "    - After detecting trends, only those specified in `trends_to_keep` remain unchanged.\n",
    "    - Any trend category not included in `trends_to_keep` is reset to 0 (No Trend).\n",
    "\n",
    "    7. **Edge Cases**:\n",
    "    - Properly handles scenarios where data points are insufficient for trend analysis or when trend phases overlap, ensuring all data points are labeled.\n",
    "    \"\"\"\n",
    "    # Copy to avoid modifying the original DataFrame\n",
    "    df = dataframe.copy()\n",
    "    df['trend'] = None  # Default value \n",
    "    \n",
    "    # print(\"\\n#-------------------- Working on 'trend' patterns -----------------------#\")\n",
    "    dic1, dic2, dic3 = None, None, None # Initialize trend tracking dictionaries\n",
    "    # dic1 = None # {'ids': [], 'last_sign': None, 'cumulative': 1.0}\n",
    "    \n",
    "    def assign_label(dictio_, lower_threshold, upper_threshold):\n",
    "        cumulative = dictio_['cumulative']\n",
    "        # print(f\"cumulative = {cumulative}\")\n",
    "        if cumulative > (1 + upper_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 4  # Very strong positive\n",
    "        elif (1 + lower_threshold) < cumulative <= (1 + upper_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 3  # Moderate positive\n",
    "        elif (1 - upper_threshold) < cumulative <= (1 - lower_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 1  # Moderate negative\n",
    "        elif cumulative <= (1 - upper_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 2  # Very strong negative\n",
    "        else:\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 0  # No trend\n",
    "    \n",
    "    #----------------------- For Loop -----------------------#\n",
    "    for idx, log_ret in enumerate(df[column]):\n",
    "        sign = 1 if log_ret > 0 else -1\n",
    "\n",
    "        if dic1 is None:  # Initialize dic1\n",
    "            # print(f\"\\nThis one time condition 'if loop' is running \\n\")\n",
    "            dic1 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "            continue\n",
    "        last_sign = dic1['last_sign']\n",
    "        if sign == last_sign and dic2 is None:  # Continue same trend\n",
    "            dic1['ids'].append(idx)\n",
    "            dic1['last_sign'] = sign\n",
    "            dic1['cumulative'] *= (1 + log_ret)\n",
    "            continue\n",
    "\n",
    "        # 1st Reversal occuring\n",
    "        if dic2 is None:  # Start dic2\n",
    "            dic2 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "            continue\n",
    "        last_sign = dic2['last_sign']\n",
    "        if sign == last_sign and dic3 is None:  # Continue same trend\n",
    "            dic2['ids'].append(idx)\n",
    "            dic2['last_sign'] = sign\n",
    "            dic2['cumulative'] *= (1 + log_ret)\n",
    "            if len(dic2['ids']) == reverse_steps:\n",
    "                assign_label(dic1, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "                # print(f\"dic1['cumulative'] = {dic1['cumulative']}, and dic1['ids'] = {dic1['ids']}\")\n",
    "                dic1 = dic2\n",
    "                dic2 = None\n",
    "                # print(f\"dic1 after trend reversal persisted and dic1 = dic2 = \\n{dic1}\")\n",
    "                # print(f\"dic2 after being reset: {dic2}\\n\")\n",
    "            continue\n",
    "\n",
    "        # 2nd Reversal occuring\n",
    "        if dic3 is None:  # Start dic3\n",
    "            dic3 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "            continue\n",
    "        last_sign = dic3['last_sign']\n",
    "        if sign == last_sign: # Continue same trend, there is no dic4 to check if is None\n",
    "            dic3['ids'].append(idx)\n",
    "            dic3['last_sign'] = sign\n",
    "            dic3['cumulative'] *= (1 + log_ret)\n",
    "            dic_prod = dic2['cumulative'] * dic3['cumulative']\n",
    "            # if (sign == 1 and dic1['cumulative'] * dic_prod > dic1['cumulative']) or (sign == -1 and dic1['cumulative'] * dic_prod < dic1['cumulative'])):\n",
    "            if (sign == 1 and dic_prod > 1) or (sign == -1 and dic_prod < 1): # More beautiful\n",
    "                # Merge dic1, dic2, and dic3\n",
    "                dic1['ids'] += dic2['ids'] + dic3['ids']\n",
    "                dic1['last_sign'] = dic3['last_sign']\n",
    "                dic1['cumulative'] *= dic2['cumulative'] * dic3['cumulative']\n",
    "                dic2, dic3 = None, None\n",
    "                continue\n",
    "\n",
    "            if len(dic3['ids']) == reverse_steps:      \n",
    "                assign_label(dic1, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "                assign_label(dic2, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "                dic1 = dic3\n",
    "                dic2, dic3 = None, None\n",
    "                # print(f\"dic2 after 2nd trend reversal didn't catch up fast enough, and now \\ndic1 = dic3 = {dic1}\")\n",
    "                # print(f\"dic3 and dic2 after being reset: {dic3}\\n\")\n",
    "            continue\n",
    "            \n",
    "        # 3rd Reversal occuring\n",
    "        assign_label(dic1, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "        # Reassign values\n",
    "        dic1 = dic2\n",
    "        dic2 = dic3\n",
    "        dic3 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "        # print(f\"There was a 3rd trend reversal, and now \\ndic1 = dic2 = {dic1}, \\ndic2 = dic3 = {dic2}\")\n",
    "        # print(f\"dic3 after being reset: {dic3}\\n\")\n",
    "\n",
    "    # Assign remaining labels\n",
    "    if dic1:\n",
    "        assign_label(dic1, lower_threshold, upper_threshold)\n",
    "    if dic2:\n",
    "        assign_label(dic2, lower_threshold, upper_threshold)\n",
    "    if dic3:\n",
    "        assign_label(dic3, lower_threshold, upper_threshold)\n",
    "    # print(\"\\n#-------------------- Returning 'trend' patterns ------------------------#\")\n",
    "    \n",
    "    # Apply filtering: Keep only selected trends, set others to 0\n",
    "    df['trend'] = df['trend'].apply(lambda x: x if x in trends_to_keep else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def split_X_y(sequences: list[pd.DataFrame], \n",
    "              target_column: str = 'trend',\n",
    "              detect_trends_function: Callable[[pd.DataFrame, str, float, float, int, set], pd.DataFrame] = detect_trends_4, \n",
    "              column: str = 'close', \n",
    "              lower_threshold: float = 0.0009, \n",
    "              upper_threshold: float = 0.015,\n",
    "              reverse_steps: int = 7,\n",
    "              trends_to_keep: set = {0, 1, 2, 3, 4}) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Process sequences to generate features (X) and labels (y) while applying trend detection.\n",
    "\n",
    "    Args:\n",
    "    - sequences (list of pd.DataFrame): List of DataFrame sequences.\n",
    "    - lower_threshold (float): Lower threshold for trend detection.\n",
    "    - upper_threshold (float): Upper threshold for trend detection.\n",
    "    - reverse_steps (int): Steps to reverse trends in the sequence.\n",
    "    - target_column (str): Column name to use as the label (default: 'trend').\n",
    "    - detect_trends_function (Callable): Function for detecting trends, defaults to `detect_trends_4`.\n",
    "    - trends_to_keep (set): A set of trend categories to retain; others will be set to 0 (No Trend).\n",
    "\n",
    "    Returns:\n",
    "    - X (np.ndarray): Features array of shape (num_sequences, sequence_length, num_features).\n",
    "    - y (np.ndarray): Labels array of shape (num_sequences,).\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    # count = 0\n",
    "    for seq in sequences:\n",
    "        # Apply trend detection on the sequence\n",
    "        seq = detect_trends_function(seq, column=column, \n",
    "                                     lower_threshold=lower_threshold, \n",
    "                                     upper_threshold=upper_threshold, \n",
    "                                     reverse_steps=reverse_steps,\n",
    "                                     trends_to_keep=trends_to_keep)\n",
    "        # if count == 0:\n",
    "        #     count = 1\n",
    "        #     print(f\"\\nseq.head()\")\n",
    "        #     display(seq.head())\n",
    "        #     print()\n",
    "        # Extract features (X) and labels (y)\n",
    "        X.append(seq.drop(columns=[target_column]).values)  # All but the target column\n",
    "        y.append(seq[target_column].values)  # Target column as labels\n",
    "        \n",
    "    # return np.array(X, dtype=np.float32), np.array(y, dtype=np.int64)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def relabel_split_data(\n",
    "    data: pd.DataFrame, \n",
    "    detect_trends_function: Callable[[pd.DataFrame, str, float, float, int], pd.DataFrame], \n",
    "    column: str = 'close', \n",
    "    lower_threshold: float = 0.0009, \n",
    "    upper_threshold: float = 0.015,\n",
    "    reverse_steps: int = 7\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Relabels the start and end of a DataFrame based on trend consistency.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame to relabel.\n",
    "        detect_trends_function (Callable): A callable function to detect trends.\n",
    "        column (str): Column name for trend detection. Defaults to 'close'.\n",
    "        lower_threshold (float): Lower threshold for trend detection.\n",
    "        upper_threshold (float): Upper threshold for trend detection.\n",
    "        reverse_steps (int): Number of steps for reversal detection.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The relabeled DataFrame.\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "\n",
    "    # Get the start and end labels\n",
    "    start_label = data['trend'].iloc[0]\n",
    "    end_label = data['trend'].iloc[-1]\n",
    "\n",
    "    # Identify start segment\n",
    "    if start_label != 0:\n",
    "        start_segment = []\n",
    "        for idx, label in enumerate(data['trend']):\n",
    "            if label == start_label:\n",
    "                start_segment.append(idx)\n",
    "            else:\n",
    "                break\n",
    "        start_segment = data.iloc[start_segment][[column]].copy()\n",
    "        print(f\"Start segment's label is not 0, it is {start_label} and data: \")\n",
    "        display(data.loc[start_segment.index])\n",
    "        # Apply trend detection on the start and end segments and Update the 'trend' column for the start and end segments\n",
    "        if not start_segment.empty:\n",
    "            start_segment_relabel = detect_trends_function(start_segment, column, lower_threshold, upper_threshold, reverse_steps)\n",
    "            data.loc[start_segment.index, 'trend'] = start_segment_relabel['trend']\n",
    "        print(\"Start segment after new label added: \")\n",
    "        display(data.loc[start_segment.index])\n",
    "    else:\n",
    "        print(f\"The start segment has a label of '{start_label}', so no operation needed. \")\n",
    "\n",
    "    # Identify end segment\n",
    "    if end_label != 0:\n",
    "        end_segment = []\n",
    "        for idx in range(len(data) - 1, -1, -1):\n",
    "            if data['trend'].iloc[idx] == end_label:\n",
    "                end_segment.append(idx)\n",
    "            else:\n",
    "                break\n",
    "        end_segment = data.iloc[sorted(end_segment)][[column]].copy()\n",
    "        print(f\"End segment's label is not 0, it is {end_label} and data: \")\n",
    "        display(data.loc[end_segment.index])\n",
    "        # Apply trend detection on the start and end segments and Update the 'trend' column for the start and end segments\n",
    "        if not end_segment.empty:\n",
    "            end_segment_relabel = detect_trends_function(end_segment, column, lower_threshold, upper_threshold, reverse_steps)\n",
    "            data.loc[end_segment.index, 'trend'] = end_segment_relabel['trend']\n",
    "        print(\"End segment after new label added: \")\n",
    "        display(data.loc[end_segment.index])\n",
    "    else:\n",
    "        print(f\"The end segment has a label of '{end_label}', so no operation needed. \")\n",
    "\n",
    "    return data\n",
    "\n",
    "def process_and_return_splits(\n",
    "    with_indicators_file_path: str,\n",
    "    downsampled_data_minutes: int,\n",
    "    exclude_columns: list[str],\n",
    "    lower_threshold: float,\n",
    "    upper_threshold: float,\n",
    "    reverse_steps: int,\n",
    "    sequence_length: int,\n",
    "    sliding_interval: int,\n",
    "    trends_to_keep: set = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    ") -> tuple[\n",
    "    list[list[float]],  # X_train: List of sequences, each containing a list of features\n",
    "    list[list[int]],    # y_train: List of sequences, each containing a list of labels\n",
    "    list[list[float]],  # X_val: List of sequences, each containing a list of features\n",
    "    list[list[int]],    # y_val: List of sequences, each containing a list of labels\n",
    "    list[list[float]],  # X_test: List of sequences, each containing a list of features\n",
    "    list[list[int]]     # y_test: List of sequences, each containing a list of labels\n",
    "]:\n",
    "    \"\"\"\n",
    "    Processes time-series data from a CSV file and prepares it for machine learning.\n",
    "\n",
    "    This function performs the following steps:\n",
    "        1. Reads data from the specified CSV file and sorts it by date in descending order.\n",
    "        2. Optionally downsamples the data to a lower frequency (e.g., 5-minute intervals).\n",
    "        3. Applies Gaussian smoothing to reduce noise in the data.\n",
    "        4. Calculates log returns for all numeric columns, excluding specified columns.\n",
    "        5. Detects trends based on defined thresholds (`lower_threshold`, `upper_threshold`, and `reverse_steps`).\n",
    "        6. Filters trends to keep only those specified in `trends_to_keep`, setting others to 0 (No Trend).\n",
    "        7. Converts the processed data into sequences of a fixed length (`sequence_length`) with a sliding interval.\n",
    "        8. Splits the sequences into training (80%), validation (10%), and test (10%) sets.\n",
    "        9. Further splits the sequences into features (`X`) and labels (`y`) for supervised learning.\n",
    "\n",
    "    Args:\n",
    "        with_indicators_file_path (str): Path to the CSV file containing the time-series data.\n",
    "        downsampled_data_minutes (int): Frequency for downsampling the data (e.g., 1 for no downsampling).\n",
    "        exclude_columns (list[str]): List of column names to exclude from log return calculations.\n",
    "        lower_threshold (float): Lower threshold for trend detection.\n",
    "        upper_threshold (float): Upper threshold for trend detection.\n",
    "        reverse_steps (int): Number of steps for reversing trends in trend detection.\n",
    "        sequence_length (int): Length of sequences to create from the data.\n",
    "        sliding_interval (int): Interval for sliding the window when creating sequences.\n",
    "        trends_to_keep (set): A set of trend categories to retain; others will be set to 0 (No Trend). Defaults to keeping all trends {0, 1, 2, 3, 4}.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[list[float]], list[list[int]], list[list[float]], list[list[int]], list[list[float]], list[list[int]]]:\n",
    "            A tuple containing:\n",
    "            - X_train (list[list[float]]): Input sequences for training.\n",
    "            - y_train (list[list[int]]): Target sequences for training.\n",
    "            - X_val (list[list[float]]): Input sequences for validation.\n",
    "            - y_val (list[list[int]]): Target sequences for validation.\n",
    "            - X_test (list[list[float]]): Input sequences for testing.\n",
    "            - y_test (list[list[int]]): Target sequences for testing.\n",
    "\n",
    "    Example:\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = process_and_return_splits(\n",
    "            with_indicators_file_path=\"data.csv\",\n",
    "            downsampled_data_minutes=5,\n",
    "            exclude_columns=[\"volume\"],\n",
    "            lower_threshold=-0.05,\n",
    "            upper_threshold=0.05,\n",
    "            reverse_steps=3,\n",
    "            sequence_length=50,\n",
    "            sliding_interval=5,\n",
    "            trends_to_keep={1, 2, 3, 4}  # Only keep categorized trends, set others to 0\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    data_retrieved = read_csv_file(with_indicators_file_path, preview_rows=0) # 190 days of data\n",
    "    data_retrieved = data_retrieved.sort_index(ascending=False)\n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    if downsampled_data_minutes != 1:\n",
    "        print(\"Downsampling the data! \\n\")\n",
    "        data_retrieved = downsample_minute_data(data_retrieved, downsampled_data_minutes)\n",
    "    #---------------------------------------------------------------------------------------\n",
    "\n",
    "    # Get missing timestamps\n",
    "    missing_timestamps = pd.date_range(\n",
    "        start=data_retrieved.index.min(), # Returns smallest/earliest/oldest date\n",
    "        end=data_retrieved.index.max(),\n",
    "        freq='1min',  # Use 'min' for a frequency of 1 minute, '30s' for a frequency of 30 seconds\n",
    "        tz=data_retrieved.index.tz,\n",
    "    ).difference(data_retrieved.index)\n",
    "    print(f\"\\ndata_retrieved - Missing timestamps time: \\n{missing_timestamps}\") \n",
    "\n",
    "    data_gaussian = gaussian_smoothing(data_retrieved, sigma=7)\n",
    "\n",
    "    # Get missing timestamps\n",
    "    missing_timestamps = pd.date_range(\n",
    "        start=data_gaussian.index.min(), # Returns smallest/earliest/oldest date\n",
    "        end=data_gaussian.index.max(),\n",
    "        freq='1min',  # Use 'min' for a frequency of 1 minute, '30s' for a frequency of 30 seconds\n",
    "        tz=data_gaussian.index.tz,\n",
    "    ).difference(data_gaussian.index)\n",
    "    print(f\"\\ndata_gaussian - Missing timestamps time: \\n{missing_timestamps}\\n\")\n",
    "\n",
    "    data_log_return = calculate_log_returns_all_columns(data_gaussian, exclude_columns=exclude_columns)\n",
    "\n",
    "    # Get missing timestamps\n",
    "    missing_timestamps = pd.date_range(\n",
    "        start=data_log_return.index.min(), # Returns smallest/earliest/oldest date\n",
    "        end=data_log_return.index.max(),\n",
    "        freq='1min',  # Use 'min' for a frequency of 1 minute, '30s' for a frequency of 30 seconds\n",
    "        tz=data_log_return.index.tz,\n",
    "    ).difference(data_log_return.index)\n",
    "    print(f\"\\ndata_log_return - Missing timestamps time: \\n{missing_timestamps}\\n\") \n",
    "\n",
    "    # Check if there are missing timestamps\n",
    "    if missing_timestamps.empty:\n",
    "        print(\"No missing timestamps.\")\n",
    "    else:\n",
    "        for timestamp in missing_timestamps:\n",
    "            print(f\"\\nMissing timestamp: {timestamp}\")\n",
    "            \n",
    "            # Create a placeholder for the missing timestamp\n",
    "            if timestamp not in data_log_return.index:\n",
    "                print('Missing')\n",
    "            \n",
    "            # Get data before and after the missing timestamp\n",
    "            before_data = data_log_return[data_log_return.index < timestamp].tail(5)  # 5 data points before\n",
    "            after_data = data_log_return[data_log_return.index > timestamp].head(5)  # 5 data points after\n",
    "            \n",
    "            # Display surrounding data\n",
    "            if not before_data.empty:\n",
    "                print(\"\\nData before:\")\n",
    "                print(before_data)\n",
    "            else:\n",
    "                print(\"\\nNo data available before the missing timestamp.\")\n",
    "            \n",
    "            if not after_data.empty:\n",
    "                print(\"\\nData after:\")\n",
    "                print(after_data)\n",
    "            else:\n",
    "                print(\"\\nNo data available after the missing timestamp.\")\n",
    "\n",
    "    sequences = created_sequences_2(data_log_return, sequence_length, sliding_interval)\n",
    "\n",
    "    # Split sequences into training, validation, and test sets\n",
    "    train_size = int(len(sequences) * 0.8)\n",
    "    val_size = int(len(sequences) * 0.1)\n",
    "\n",
    "    train_sequences = sequences[:train_size]\n",
    "    val_sequences = sequences[train_size:train_size + val_size]\n",
    "    test_sequences = sequences[train_size + val_size:]\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Number of sequences:\n",
    "        - sequences[0].shape: {sequences[0].shape}\n",
    "        - Total sequences: {len(sequences)}\n",
    "        - Train sequences: {len(train_sequences)}\n",
    "        - Validation sequences: {len(val_sequences)}\n",
    "        - Test sequences: {len(test_sequences)}\n",
    "    \"\"\")\n",
    "\n",
    "    # Process train, validation, and test sets\n",
    "    X_train, y_train = split_X_y(train_sequences, \n",
    "                                target_column='trend',\n",
    "                                detect_trends_function = detect_trends_4,\n",
    "                                column= 'close',\n",
    "                                lower_threshold=lower_threshold, \n",
    "                                upper_threshold=upper_threshold, \n",
    "                                reverse_steps=reverse_steps,\n",
    "                                trends_to_keep=trends_to_keep)\n",
    "\n",
    "    X_val, y_val = split_X_y(val_sequences, \n",
    "                            target_column='trend',\n",
    "                            detect_trends_function = detect_trends_4,\n",
    "                            column= 'close',\n",
    "                            lower_threshold=lower_threshold, \n",
    "                            upper_threshold=upper_threshold, \n",
    "                            reverse_steps=reverse_steps,\n",
    "                            trends_to_keep=trends_to_keep)\n",
    "\n",
    "    X_test, y_test = split_X_y(test_sequences, \n",
    "                            target_column='trend',\n",
    "                            detect_trends_function = detect_trends_4,\n",
    "                            column= 'close',\n",
    "                            lower_threshold=lower_threshold, \n",
    "                            upper_threshold=upper_threshold, \n",
    "                            reverse_steps=reverse_steps,\n",
    "                            trends_to_keep=trends_to_keep)\n",
    "\n",
    "    # Checking X arrays\n",
    "    for idx, seq in enumerate(X_train):  # Loop through sequences\n",
    "        for sub_idx, feature_set in enumerate(seq):  # Loop through data points\n",
    "            for feature_idx, feature in enumerate(feature_set):  # Loop through features\n",
    "                if not isinstance(feature, (float, np.float32)):  # Check each feature\n",
    "                    print(f\"Unexpected type in X_train at sequence {idx}, data point {sub_idx}, feature {feature_idx}: {type(feature)}\")\n",
    "\n",
    "    # Checking y arrays\n",
    "    for idx, seq in enumerate(y_train):  # Loop through sequences\n",
    "        for sub_idx, label in enumerate(seq):  # Loop through data points (labels)\n",
    "            if not isinstance(label, (int, np.int64)):  # Check each label\n",
    "                print(f\"Unexpected type in y_train at sequence {idx}, data point {sub_idx}: {type(label)}\")\n",
    "\n",
    "    # Checking X arrays\n",
    "    for idx, seq in enumerate(X_val):  # Loop through sequences\n",
    "        for sub_idx, feature_set in enumerate(seq):  # Loop through data points\n",
    "            for feature_idx, feature in enumerate(feature_set):  # Loop through features\n",
    "                if not isinstance(feature, (float, np.float32)):  # Check each feature\n",
    "                    print(f\"Unexpected type in X_val at sequence {idx}, data point {sub_idx}, feature {feature_idx}: {type(feature)}\")\n",
    "    # Checking y arrays\n",
    "    for idx, seq in enumerate(y_val):  # Loop through sequences\n",
    "        for sub_idx, label in enumerate(seq):  # Loop through data points (labels)\n",
    "            if not isinstance(label, (int, np.int64)):  # Check each label\n",
    "                print(f\"Unexpected type in y_val at sequence {idx}, data point {sub_idx}: {type(label)}\")\n",
    "\n",
    "    # Checking X arrays\n",
    "    for idx, seq in enumerate(X_test):  # Loop through sequences\n",
    "        for sub_idx, feature_set in enumerate(seq):  # Loop through data points\n",
    "            for feature_idx, feature in enumerate(feature_set):  # Loop through features\n",
    "                if not isinstance(feature, (float, np.float32)):  # Check each feature\n",
    "                    print(f\"Unexpected type in X_test at sequence {idx}, data point {sub_idx}, feature {feature_idx}: {type(feature)}\")\n",
    "    # Checking y arrays\n",
    "    for idx, seq in enumerate(y_test):  # Loop through sequences\n",
    "        for sub_idx, label in enumerate(seq):  # Loop through data points (labels)\n",
    "            if not isinstance(label, (int, np.int64)):  # Check each label\n",
    "                print(f\"Unexpected type in y_test at sequence {idx}, data point {sub_idx}: {type(label)}\")\n",
    "\n",
    "    if isinstance(y_train, np.ndarray) and y_train.dtype == np.object_:\n",
    "        # Convert to numeric if needed\n",
    "        y_train = np.array(y_train, dtype=np.int64)\n",
    "\n",
    "    if isinstance(y_val, np.ndarray) and y_val.dtype == np.object_:\n",
    "        # Convert to numeric if needed\n",
    "        y_val = np.array(y_val, dtype=np.int64)\n",
    "\n",
    "    if isinstance(y_test, np.ndarray) and y_test.dtype == np.object_:\n",
    "        # Convert to numeric if needed\n",
    "        y_test = np.array(y_test, dtype=np.int64)\n",
    "\n",
    "    close_col_index = data_log_return.columns.get_loc('close') # 'date' is set as index so doesnt count as a column\n",
    "    Number_features = X_train.shape[-1]\n",
    "    print(f\"close_col_index = {close_col_index}\")\n",
    "    print(f\"Number_features = {Number_features}\")\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, Number_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All(Initial) parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'BTC-USD'\n",
    "downsampled_data_minutes = 1\n",
    "\n",
    "lower_threshold = 0.0009 \n",
    "upper_threshold = 0.015\n",
    "reverse_steps = 13\n",
    "\n",
    "exclude_columns= ['MACD', 'MACD_signal', 'ROC_10', 'OBV', 'AD_Line']\n",
    "strongly_correlated = ['close', 'open', 'SMA_5', 'high', 'low', 'EMA_10', 'SMA_10'] # Strongly correlated (correlation > 0.6)\n",
    "moderately_correlated = ['BB_middle', 'BB_lower', 'BB_upper', 'RSI_14'] # Moderately correlated (correlation between 0.3 and 0.6)\n",
    "weakly_correlated = ['SMA_50', 'volume', 'BBW', 'ATR_14'] # Weakly correlated or negligible (correlation <~ 0.3)\n",
    "exclude_columns += weakly_correlated + moderately_correlated\n",
    "\n",
    "sequence_length = 1000\n",
    "sliding_interval = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **_Build the GRU Model_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 17 13:39:57 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.14                 Driver Version: 566.14         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   47C    P8              5W /  103W |    2376MiB /   8188MiB |     22%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3468    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A      4408    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      4684    C+G   ..._x64__2p2nqsd0c76g0\\app\\ChatGPT.exe      N/A      |\n",
      "|    0   N/A  N/A      8120    C+G   ...m Files\\Mozilla Firefox\\firefox.exe      N/A      |\n",
      "|    0   N/A  N/A      9832    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     12028    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     12052    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     14520    C+G   ...on\\133.0.3065.69\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     16548    C+G   ...m Files\\Mozilla Firefox\\firefox.exe      N/A      |\n",
      "|    0   N/A  N/A     16676    C+G   ...GeForce Experience\\NVIDIA Share.exe      N/A      |\n",
      "|    0   N/A  N/A     17040    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     17904    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     18524    C+G   ....0_x64__kzh8wxbdkxb8p\\DCv2\\DCv2.exe      N/A      |\n",
      "|    0   N/A  N/A     18908    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     19296    C+G   ...354.0_x64__dt26b99r8h8gj\\RtkUWP.exe      N/A      |\n",
      "|    0   N/A  N/A     28248    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     30660    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# If not in Jupyter Notebook\n",
    "# import subprocess\n",
    "# result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
    "# print(result.stdout)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch and CUDA check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available() True\n",
      "\n",
      "GPU Device Name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Number of GPUs: 1\n",
      "Total CUDA Cores: 3072\n",
      "Current GPU Device: 0\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "\n",
    "print(\"torch.cuda.is_available()\", gpu_available)\n",
    "\n",
    "# If GPU is available, print additional information\n",
    "if gpu_available:\n",
    "    print(\"\\nGPU Device Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Total CUDA Cores:\", torch.cuda.get_device_properties(0).multi_processor_count * 128)  # NVIDIA GPUs often have 128 cores/SM\n",
    "    print(\"Current GPU Device:\", torch.cuda.current_device())\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-Directional GRU with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list_period_files_full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data\\\\Polygon_BTCUSD_4Y_1min\\\\_Polygon_BTCUSD_4Y_1min_190_days_with_indicators.csv'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'BTCUSD'\n",
    "file_name = f'Polygon_{pair}_4Y_1min'  # File name for saving data\n",
    "# BASE_FOLDER_PATH = f\"{Working_directory}/Data/{file_name}\"\n",
    "BASE_FOLDER_PATH = f\"Data/{file_name}\"\n",
    "# folder_path=f'{BASE_FOLDER_PATH}/polygon_io/12_USD_Crypto_Pairs/{file_name}'\n",
    "folder_path=f'{BASE_FOLDER_PATH}'\n",
    "if not os.path.isdir(folder_path):\n",
    "    raise FileNotFoundError(f\"Directory '{folder_path}' does not exist.\")\n",
    "file_path=f'{folder_path}/{file_name}.csv'\n",
    "number_days = 190\n",
    "with_indicators_file_path = os.path.normpath(f'{folder_path}/_{file_name}_{number_days}_days_with_indicators.csv')\n",
    "\n",
    "# LABORATORY\\_Global_Pytorch\\Continual_Learning\\Data\\Polygon_BTCUSD_4Y_1min\n",
    "\n",
    "list_period_files_full_path = [\n",
    "    # Period 1\n",
    "    with_indicators_file_path,\n",
    "    # Period 2\n",
    "    os.path.normpath(f\"{folder_path}/Polygon_BTCUSD_4Y_1min_190_days__2020-11-11__2021-05-20__with_indicators.csv\"),\n",
    "    # Period 3\n",
    "    os.path.normpath(f\"{folder_path}/Polygon_BTCUSD_4Y_1min_190_days__2021-05-20__2021-11-26__with_indicators.csv\"),\n",
    "    # Period 4\n",
    "    os.path.normpath(f\"{folder_path}/Polygon_BTCUSD_4Y_1min_190_days__2021-11-26__2022-06-04__with_indicators.csv\"),\n",
    "    # Period 5\n",
    "    os.path.normpath(f\"{folder_path}/Polygon_BTCUSD_4Y_1min_190_days__2022-06-04__2022-12-11__with_indicators.csv\")\n",
    "]\n",
    "\n",
    "with_indicators_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: Polygon_BTCUSD_4Y_1min.csv\n",
      "Found file: Polygon_BTCUSD_4Y_1min.png\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2020-11-11__2021-05-20.png\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2020-11-11__2021-05-20__with_indicators.csv\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2021-05-20__2021-11-26.png\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2021-05-20__2021-11-26__with_indicators.csv\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2021-11-26__2022-06-04.png\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2021-11-26__2022-06-04__with_indicators.csv\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2022-06-04__2022-12-11.png\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2022-06-04__2022-12-11__with_indicators.csv\n",
      "Found file: Polygon_BTCUSD_4Y_1min_525_days.png\n",
      "Found file: Polygon_BTCUSD_4Y_1min_525_days_with_indicators.csv\n",
      "Found file: _Polygon_BTCUSD_4Y_1min_190_days.png\n",
      "Found file: _Polygon_BTCUSD_4Y_1min_190_days_with_indicators.csv\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(folder_path):\n",
    "    print(f\"Found file: {file}\")\n",
    "\n",
    "# Polygon_BTCUSD_4Y_1min_190_days_with_indicators.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File path: Data\\Polygon_BTCUSD_4Y_1min\\_Polygon_BTCUSD_4Y_1min_190_days_with_indicators.csv\n",
      "\n",
      "data_retrieved - Missing timestamps time: \n",
      "DatetimeIndex([], dtype='datetime64[ns, UTC]', freq='min')\n",
      "\n",
      "data_gaussian - Missing timestamps time: \n",
      "DatetimeIndex([], dtype='datetime64[ns, UTC]', freq='min')\n",
      "\n",
      "columns_to_transform = \n",
      "Index(['open', 'high', 'low', 'close', 'SMA_5', 'SMA_10', 'EMA_10'], dtype='object'), \n",
      "len(columns_to_transform) = 7\n",
      "\n",
      "data_log_return - Missing timestamps time: \n",
      "DatetimeIndex([], dtype='datetime64[ns, UTC]', freq='min')\n",
      "\n",
      "No missing timestamps.\n",
      "\n",
      "    Number of sequences:\n",
      "        - sequences[0].shape: (1000, 7)\n",
      "        - Total sequences: 4543\n",
      "        - Train sequences: 3634\n",
      "        - Validation sequences: 454\n",
      "        - Test sequences: 455\n",
      "    \n",
      "close_col_index = 3\n",
      "Number_features = 7\n",
      "unique_classes = [0 1 2 3 4]\n",
      "num_classes = 5\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "    with_indicators_file_path = list_period_files_full_path[0],\n",
    "    downsampled_data_minutes = downsampled_data_minutes,\n",
    "    exclude_columns = exclude_columns,\n",
    "    lower_threshold = lower_threshold,\n",
    "    upper_threshold = upper_threshold,\n",
    "    reverse_steps = reverse_steps,\n",
    "    sequence_length = sequence_length,\n",
    "    sliding_interval = sliding_interval,\n",
    "    trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    "    # trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    ")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "del X_train, y_train, X_val, y_val, X_test, y_test, Number_features\n",
    "del unique_classes, num_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __*All periods data*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number_features = 7\n",
      "unique_classes = [0 1]\n",
      "num_classes = 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- 'trend': Categorized trend values based on the detected phases:\n",
    "    - 0: No trend\n",
    "    - 1: Moderate negative trend\n",
    "    - 2: Very strong negative trend\n",
    "    - 3: Moderate positive trend\n",
    "    - 4: Very strong positive trend\n",
    "\"\"\"\n",
    "\n",
    "# Initialize empty lists to combine elements\n",
    "X_train_all = []\n",
    "y_train_all = []\n",
    "X_val_all = []\n",
    "y_val_all = []\n",
    "X_test_all = []\n",
    "y_test_all = []\n",
    "# for path_ in list_period_files_full_path:\n",
    "#     with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "#         X_train_, y_train_, X_val_, y_val_, X_test_, y_test_, Number_features = process_and_return_splits(\n",
    "#             with_indicators_file_path = path_,\n",
    "#             downsampled_data_minutes = downsampled_data_minutes,\n",
    "#             exclude_columns = exclude_columns,\n",
    "#             lower_threshold = lower_threshold,\n",
    "#             upper_threshold = upper_threshold,\n",
    "#             reverse_steps = reverse_steps,\n",
    "#             sequence_length = sequence_length,\n",
    "#             sliding_interval = sliding_interval,\n",
    "#             trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    "#         )\n",
    "#     # Combine validation elements\n",
    "#     X_train_all.extend(X_train_)\n",
    "#     y_train_all.extend(y_train_)\n",
    "#     X_val_all.extend(X_val_)\n",
    "#     y_val_all.extend(y_val_)\n",
    "#     X_test_all.extend(X_test_)\n",
    "#     y_test_all.extend(y_test_)\n",
    "#     # Delete unused variables to save memory\n",
    "#     del X_train_, y_train_, X_val_, y_val_, X_test_, y_test_\n",
    "\n",
    "# X_train_all = np.array(X_train_all)\n",
    "# y_train_all = np.array(y_train_all)\n",
    "# X_val_all = np.array(X_val_all)\n",
    "# y_val_all = np.array(y_val_all)\n",
    "# X_test_all = np.array(X_test_all)\n",
    "# y_test_all = np.array(y_test_all)\n",
    "\n",
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "        with_indicators_file_path = list_period_files_full_path[0],\n",
    "        downsampled_data_minutes = downsampled_data_minutes,\n",
    "        exclude_columns = exclude_columns,\n",
    "        lower_threshold = lower_threshold,\n",
    "        upper_threshold = upper_threshold,\n",
    "        reverse_steps = reverse_steps,\n",
    "        sequence_length = sequence_length,\n",
    "        sliding_interval = sliding_interval,\n",
    "        trends_to_keep = {0, 1}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "        # trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    "    )\n",
    "    \n",
    "print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
