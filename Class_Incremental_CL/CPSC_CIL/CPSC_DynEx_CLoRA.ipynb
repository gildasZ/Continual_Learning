{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07af288c",
   "metadata": {},
   "source": [
    "## __Check first before starting__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f00b1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /mnt/mydisk/Continual_Learning_JL/Continual_Learning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "Working_directory = os.path.normpath(\"/mnt/mydisk/Continual_Learning_JL/Continual_Learning/\")\n",
    "os.chdir(Working_directory)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97bdf56",
   "metadata": {},
   "source": [
    "## __All imports__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83b85d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating system and file management\n",
    "import os\n",
    "import shutil\n",
    "import contextlib\n",
    "import traceback\n",
    "import gc\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import subprocess\n",
    "import time\n",
    "import re, pickle\n",
    "import scipy.io\n",
    "from scipy.io import loadmat\n",
    "from glob import glob\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Jupyter notebook widgets and display\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_interactions import zoom_factory, panhandler\n",
    "\n",
    "# Machine learning and preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from ta import trend, momentum, volatility, volume\n",
    "\n",
    "# Mathematical and scientific computing\n",
    "import math\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Type hinting\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "# Deep learning with PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a496fe70",
   "metadata": {},
   "source": [
    "## __üìÅ Path Settings and Constants__\n",
    "This cell defines essential paths and constants for the CPSC2018 ECG dataset processing:\n",
    "- `BASE_DIR`: Root directory of the project.\n",
    "- `save_dir`: Path to the preprocessed `.npy` files (one for each continual learning period).\n",
    "- `ECG_PATH`: Directory containing original `.mat` and `.hea` files.\n",
    "- `MAX_LEN`: Length of each ECG sample, fixed to 5000 time steps (i.e., 10 seconds at 500Hz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7748e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL\"\n",
    "save_dir = os.path.join(BASE_DIR, \"processed\")\n",
    "ECG_PATH = os.path.join(BASE_DIR, \"datas\")\n",
    "MAX_LEN = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba7249f",
   "metadata": {},
   "source": [
    "## __üè∑Ô∏è Label Mapping and Period Configuration__\n",
    "\n",
    "This section defines:\n",
    "- `snomed_map`: Mapping from SNOMED CT codes to readable class names for 9 major ECG conditions.\n",
    "- `period_label_map`: Incremental learning task structure across four periods.  \n",
    "  Class `1` is reserved for \"OTHER\" abnormalities until Period 4 when all 9 classes are explicitly categorized.\n",
    "- `print_class_distribution()`: Helper function to show class-wise data distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "103ca271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNOMED CT to readable names\n",
    "snomed_map = {\n",
    "    \"426783006\": \"NSR\",    # Ê≠£Â∏∏Á´áÊÄßÂøÉÂæã\n",
    "    \"270492004\": \"I-AVB\",  # ‰∏ÄÂ∫¶ÊàøÂÆ§ÂÇ≥Â∞éÈòªÊªØ\n",
    "    \"164889003\": \"AF\",     # ÂøÉÊàøÁ∫ñÁ∂≠È°´Âãï\n",
    "    \"164909002\": \"LBBB\",   # Â∑¶ÊùüÊîØÂÇ≥Â∞éÈòªÊªØ\n",
    "    \"59118001\":  \"RBBB\",   # Âè≥ÊùüÊîØÂÇ≥Â∞éÈòªÊªØ\n",
    "    \"284470004\": \"PAC\",    # ÂøÉÊàøÊó©ÊúüÊêèÂãï\n",
    "    \"164884008\": \"PVC\",    # ÂÆ§ÊÄßÊó©ÊúüÊêèÂãï\n",
    "    \"429622005\": \"STD\",    # ST ÊÆµÂ£ì‰Ωé\n",
    "    \"164931005\": \"STE\"     # ST ÊÆµÊä¨È´ò\n",
    "}\n",
    "\n",
    "# Period class mapping (Âõ∫ÂÆö class 1 ÊòØ„ÄåÂÖ∂‰ªñÁï∞Â∏∏„ÄçÁõ¥Âà∞ P4 ÁßªÈô§)\n",
    "period_label_map = {\n",
    "    1: {\"NSR\": 0, \"OTHER\": 1},\n",
    "    2: {\"NSR\": 0, \"I-AVB\": 2, \"AF\": 3, \"OTHER\": 1},\n",
    "    3: {\"NSR\": 0, \"I-AVB\": 2, \"AF\": 3, \"LBBB\": 4, \"RBBB\": 5, \"OTHER\": 1},\n",
    "    4: {\"NSR\": 0, \"I-AVB\": 2, \"AF\": 3, \"LBBB\": 4, \"RBBB\": 5, \"PAC\": 6, \"PVC\": 7, \"STD\": 8, \"STE\": 9}\n",
    "}\n",
    "\n",
    "def print_class_distribution(y, label_map):\n",
    "    y = np.array(y).flatten()\n",
    "    total = len(y)\n",
    "    all_labels = sorted(label_map.values())\n",
    "    print(\"\\nüìä Class Distribution\")\n",
    "    for lbl in all_labels:\n",
    "        count = np.sum(y == lbl)\n",
    "        label = [k for k, v in label_map.items() if v == lbl]\n",
    "        name = label[0] if label else str(lbl)\n",
    "        print(f\"  ‚îú‚îÄ Label {lbl:<2} ({name:<10}) ‚Üí {count:>5} samples ({(count/total)*100:5.2f}%)\")\n",
    "\n",
    "def ensure_folder(folder_path: str) -> None:\n",
    "    \"\"\"Ensure the given folder exists, create it if not.\"\"\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f95af3",
   "metadata": {},
   "source": [
    "## üì¶ EX. Load Example (Period 4) Data and View Format\n",
    "\n",
    "This example demonstrates how to load preprocessed `.npy` data for **Period 4**, and inspect the dataset shapes and label distribution.  \n",
    "Use this format as a reference when loading data in other methods (e.g., EWC, PNN, DynEx-CLoRA).\n",
    "\n",
    "Each ECG sample:\n",
    "- Has shape `(5000, 12)` ‚Üí represents 10 seconds (at 500Hz) across 12-lead channels.\n",
    "- Corresponding label is an integer ID (e.g., 0‚Äì9) defined by `period_label_map[4]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a401dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ÁØÑ‰æã:ËºâÂÖ• period 4\n",
    "# save_dir = os.path.join(BASE_DIR, \"processed\")\n",
    "# X_train = np.load(os.path.join(save_dir, \"X_train_p4.npy\"))\n",
    "# y_train = np.load(os.path.join(save_dir, \"y_train_p4.npy\"))\n",
    "# X_test = np.load(os.path.join(save_dir, \"X_test_p4.npy\"))\n",
    "# y_test = np.load(os.path.join(save_dir, \"y_test_p4.npy\"))\n",
    "\n",
    "# print(\"‚úÖ Loaded\")\n",
    "# print(\"X_train shape:\", X_train.shape)\n",
    "# print(\"y_train shape:\", y_train.shape)\n",
    "# print(\"X_test shape:\", X_test.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)\n",
    "# print_class_distribution(y_train, period_label_map[4])\n",
    "# print_class_distribution(y_test, period_label_map[4])\n",
    "\n",
    "# del X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d4b54b",
   "metadata": {},
   "source": [
    "## __Check GPU, CUDA, Pytorch__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50a987a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 10 16:05:16 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               Off |   00000000:2A:00.0 Off |                  Off |\n",
      "| 30%   44C    P2             84W /  300W |    3850MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               Off |   00000000:3D:00.0 Off |                  Off |\n",
      "| 30%   51C    P2             97W /  300W |   45329MiB /  49140MiB |    100%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000               Off |   00000000:AB:00.0 Off |                  Off |\n",
      "| 30%   45C    P2             96W /  300W |   45299MiB /  49140MiB |    100%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2845      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    0   N/A  N/A          114062      C   ...onda3/envs/CIL_env/bin/python        562MiB |\n",
      "|    0   N/A  N/A          149835      C   ...onda3/envs/CIL_env/bin/python        564MiB |\n",
      "|    0   N/A  N/A         2314326      C   python                                  766MiB |\n",
      "|    0   N/A  N/A         2514160      C   python                                  640MiB |\n",
      "|    0   N/A  N/A         4147895      C   ...onda3/envs/CIL_env/bin/python        660MiB |\n",
      "|    0   N/A  N/A         4159827      C   ...onda3/envs/CIL_env/bin/python        608MiB |\n",
      "|    1   N/A  N/A            2845      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    1   N/A  N/A           48977      C   python                                44814MiB |\n",
      "|    1   N/A  N/A         4147895      C   ...onda3/envs/CIL_env/bin/python        370MiB |\n",
      "|    2   N/A  N/A            2845      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    2   N/A  N/A           49785      C   python                                44812MiB |\n",
      "|    2   N/A  N/A         4147895      C   ...onda3/envs/CIL_env/bin/python        342MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f78325",
   "metadata": {},
   "source": [
    "### CUDA Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00875f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "             GPU Configuration Check              \n",
      "==================================================\n",
      "PyTorch Version          : 2.5.1\n",
      "GPU Available            : Yes\n",
      "--------------------------------------------------\n",
      "                   GPU Details                    \n",
      "--------------------------------------------------\n",
      "Device Name              : NVIDIA RTX A6000\n",
      "Number of GPUs           : 3\n",
      "Current Device Index     : 0\n",
      "Compute Capability       : 8.6\n",
      "Total CUDA Cores         : 10752\n",
      "Total Memory (GB)        : 47.41\n",
      "Allocated Memory (GB)    : 0.00\n",
      "Reserved Memory (GB)     : 0.00\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def check_gpu_config():\n",
    "    \"\"\"\n",
    "    Check GPU availability and display detailed configuration information.\n",
    "    \"\"\"\n",
    "    # Check if GPU is available\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    \n",
    "    # Print header\n",
    "    print(\"=\" * 50)\n",
    "    print(\"GPU Configuration Check\".center(50))\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic GPU availability\n",
    "    print(f\"{'PyTorch Version':<25}: {torch.__version__}\")\n",
    "    print(f\"{'GPU Available':<25}: {'Yes' if gpu_available else 'No'}\")\n",
    "    \n",
    "    # If GPU is available, print detailed info\n",
    "    if gpu_available:\n",
    "        print(\"-\" * 50)\n",
    "        print(\"GPU Details\".center(50))\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Device info\n",
    "        print(f\"{'Device Name':<25}: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"{'Number of GPUs':<25}: {torch.cuda.device_count()}\")\n",
    "        print(f\"{'Current Device Index':<25}: {torch.cuda.current_device()}\")\n",
    "        \n",
    "        # Compute capability and CUDA cores\n",
    "        props = torch.cuda.get_device_properties(0)\n",
    "        print(f\"{'Compute Capability':<25}: {props.major}.{props.minor}\")\n",
    "        print(f\"{'Total CUDA Cores':<25}: {props.multi_processor_count * 128}\")  # Approx. 128 cores per SM\n",
    "        \n",
    "        # Memory info\n",
    "        total_memory = props.total_memory / (1024 ** 3)  # Convert to GB\n",
    "        memory_allocated = torch.cuda.memory_allocated(0) / (1024 ** 3)\n",
    "        memory_reserved = torch.cuda.memory_reserved(0) / (1024 ** 3)\n",
    "        print(f\"{'Total Memory (GB)':<25}: {total_memory:.2f}\")\n",
    "        print(f\"{'Allocated Memory (GB)':<25}: {memory_allocated:.2f}\")\n",
    "        print(f\"{'Reserved Memory (GB)':<25}: {memory_reserved:.2f}\")\n",
    "    else:\n",
    "        print(\"-\" * 50)\n",
    "        print(\"No GPU detected. Running on CPU.\".center(50))\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_gpu_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a868ff5",
   "metadata": {},
   "source": [
    "### PyTorch Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d153a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "              PyTorch Configuration               \n",
      "==================================================\n",
      "PyTorch Version          : 2.5.1\n",
      "CUDA Compiled Version    : 12.1\n",
      "CUDA Available           : Yes\n",
      "Number of GPUs           : 3\n",
      "GPU Name                 : NVIDIA RTX A6000\n",
      "--------------------------------------------------\n",
      "Random Seed              : 42 (Seeding successful!)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def print_torch_config():\n",
    "    \"\"\"Print PyTorch and CUDA configuration in a formatted manner.\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"PyTorch Configuration\".center(50))\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic PyTorch and CUDA info\n",
    "    print(f\"{'PyTorch Version':<25}: {torch.__version__}\")\n",
    "    print(f\"{'CUDA Compiled Version':<25}: {torch.version.cuda}\")\n",
    "    print(f\"{'CUDA Available':<25}: {'Yes' if torch.cuda.is_available() else 'No'}\")\n",
    "    print(f\"{'Number of GPUs':<25}: {torch.cuda.device_count()}\")\n",
    "\n",
    "    # GPU details if available\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"{'GPU Name':<25}: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Seed setting\n",
    "    torch.manual_seed(42)\n",
    "    print(f\"{'Random Seed':<25}: 42 (Seeding successful!)\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print_torch_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbab549",
   "metadata": {},
   "source": [
    "## __‚öôÔ∏è GPU Selection ‚Äî Auto-select the least loaded GPU__\n",
    "This code automatically scans available GPUs and selects the one with the lowest current memory usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4ffdd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 0\n",
      "    - Memory Used    : 5553 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "def auto_select_cuda_device(verbose=True):\n",
    "    \"\"\"\n",
    "    Automatically selects the CUDA GPU with the least memory usage.\n",
    "    Falls back to CPU if no GPU is available.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"üö´ No CUDA GPU available. Using CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "    try:\n",
    "        # Run nvidia-smi to get memory usage of each GPU\n",
    "        smi_output = subprocess.check_output(\n",
    "            ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'],\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        memory_used = [int(x) for x in smi_output.strip().split('\\n')]\n",
    "        best_gpu = int(np.argmin(memory_used))\n",
    "\n",
    "        if verbose:\n",
    "            print(\"üéØ Automatically selected GPU:\")\n",
    "            print(f\"    - CUDA Device ID : {best_gpu}\")\n",
    "            print(f\"    - Memory Used    : {memory_used[best_gpu]} MiB\")\n",
    "            print(f\"    - Device Name    : {torch.cuda.get_device_name(best_gpu)}\")\n",
    "        return torch.device(f\"cuda:{best_gpu}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to auto-detect GPU. Falling back to cuda:0. ({e})\")\n",
    "        return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Execute and assign\n",
    "device = auto_select_cuda_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae7c5a",
   "metadata": {},
   "source": [
    "## __Model__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b4707e",
   "metadata": {},
   "source": [
    "### ResNet 18 - 1D (ResNet18_1D_big_inplane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cab8843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAConv1d(nn.Module):\n",
    "    def __init__(self, conv_layer: nn.Conv1d, rank: int):\n",
    "        super(LoRAConv1d, self).__init__()\n",
    "        self.conv = conv_layer\n",
    "        self.rank = rank\n",
    "        \n",
    "        # ÁÇ∫ÈÅ©ÊáâLoRA‰ΩéÁß©ÂàÜËß£ÂâµÂª∫AÂíåBÁü©Èô£\n",
    "        self.lora_A = nn.Parameter(torch.zeros(conv_layer.out_channels, rank))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, conv_layer.in_channels * conv_layer.kernel_size[0]))\n",
    "        \n",
    "        # ÂàùÂßãÂåñÊ¨äÈáçÔºöAÁî®Ê≠£ÊÖãÂàÜ‰ΩàÔºåBÁî®Èõ∂ÂàùÂßãÂåñ‰ª•Á¢∫‰øùË®ìÁ∑¥ÈñãÂßãÊôÇLoRAÁÑ°ÂΩ±Èüø\n",
    "        nn.init.normal_(self.lora_A, mean=0.0, std=0.01)\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ‰ΩøÁî®ÂéüÂßãÂç∑Á©çÂ±§ÁöÑÊ¨äÈáçÂíåÂèÉÊï∏ÈÄ≤Ë°åÂç∑Á©ç\n",
    "        return self.conv(x)\n",
    "        \n",
    "    def get_delta(self):\n",
    "        # Ë®àÁÆóLoRAÊ¨äÈáç‰∏¶ÈáçÂ°ëÁÇ∫Âç∑Á©çÊ†∏ÂΩ¢ÁãÄ\n",
    "        lora_weight = torch.matmul(self.lora_A, self.lora_B).view(\n",
    "            self.conv.out_channels, self.conv.in_channels, self.conv.kernel_size[0]\n",
    "        )\n",
    "        return lora_weight\n",
    "        \n",
    "    def parameters(self, recurse=True):\n",
    "        # Âè™ËøîÂõûLoRAÂèÉÊï∏Ôºå‰∏çÂåÖÊã¨ÂéüÂßãÂç∑Á©çÂ±§ÂèÉÊï∏\n",
    "        return [self.lora_A, self.lora_B]\n",
    "\n",
    "class BasicBlock1d_LoRA(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, lora_rank=None):\n",
    "        super(BasicBlock1d_LoRA, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "        self.lora_rank = lora_rank\n",
    "        self.lora_adapters = nn.ModuleList()  # ‰ΩøÁî®ModuleListÂ≠òÂÑ≤Â§öÂÄãLoRAÈÅ©ÈÖçÂô®\n",
    "\n",
    "    def add_lora_adapter(self):\n",
    "        \"\"\"Ê∑ªÂä†‰∏ÄÂÄãÊñ∞ÁöÑLoRAÈÅ©ÈÖçÂô®Âà∞conv2Â±§\"\"\"\n",
    "        new_lora = LoRAConv1d(self.conv2, self.lora_rank)\n",
    "        device = next(self.parameters()).device\n",
    "        new_lora.to(device)\n",
    "        self.lora_adapters.append(new_lora)\n",
    "        return new_lora\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        if len(self.lora_adapters) > 0:\n",
    "            # ‰ΩøÁî®ÂéüÂßãconv2ÈÄ≤Ë°åÂü∫Êú¨Âç∑Á©ç\n",
    "            base_out = self.conv2(out)\n",
    "            \n",
    "            # Â¶ÇÊûúÊúâLoRAÈÅ©ÈÖçÂô®ÔºåË®àÁÆóÊâÄÊúâLoRAÁöÑÊ¨äÈáçÂ¢ûÈáè‰∏¶ÊáâÁî®\n",
    "            if self.lora_adapters:\n",
    "                # Ë®àÁÆóÊâÄÊúâLoRAÈÅ©ÈÖçÂô®ÁöÑÊ¨äÈáçÁ∏ΩÂíå\n",
    "                lora_weight_delta = sum(adapter.get_delta() for adapter in self.lora_adapters)\n",
    "                # Ë™øÊï¥ÂæåÁöÑÊ¨äÈáç = ÂéüÂßãÊ¨äÈáç + LoRAÊ¨äÈáçÂ¢ûÈáè\n",
    "                adapted_weight = self.conv2.weight + lora_weight_delta\n",
    "                # ‰ΩøÁî®‰øÆÊîπÂæåÁöÑÊ¨äÈáçÂü∑Ë°åÂç∑Á©ç\n",
    "                out = F.conv1d(out, adapted_weight, bias=self.conv2.bias,\n",
    "                              stride=self.conv2.stride, padding=self.conv2.padding,\n",
    "                              dilation=self.conv2.dilation, groups=self.conv2.groups)\n",
    "            else:\n",
    "                out = base_out\n",
    "        else:\n",
    "            # Â¶ÇÊûúÊ≤íÊúâLoRAÈÅ©ÈÖçÂô®Ôºå‰ΩøÁî®ÂéüÂßãconv2\n",
    "            out = self.conv2(out)\n",
    "\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18_1D_LoRA(nn.Module):\n",
    "    def __init__(self, input_channels=12, output_size=9, inplanes=64, lora_rank=4):\n",
    "        super(ResNet18_1D_LoRA, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.lora_rank = lora_rank\n",
    "\n",
    "        # ÂàùÂßãÂç∑Á©çÂ±§\n",
    "        self.conv1 = nn.Conv1d(input_channels, inplanes, kernel_size=15, stride=2, padding=7, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ÊÆòÂ∑ÆÂ±§\n",
    "        self.layer1 = self._make_layer(BasicBlock1d_LoRA, 64, 2)\n",
    "        self.layer2 = self._make_layer(BasicBlock1d_LoRA, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock1d_LoRA, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlock1d_LoRA, 512, 2, stride=2)\n",
    "\n",
    "        # Ëá™ÈÅ©ÊáâÊ±†ÂåñÔºàÂπ≥ÂùáÂíåÊúÄÂ§ßÔºâ\n",
    "        self.adaptiveavgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.adaptivemaxpool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # ÂÖ®ÈÄ£Êé•Â±§Ëàádropout\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(512 * 2, output_size)  # *2Âõ†ÁÇ∫concat‰∫ÜavgÂíåmaxÊ±†Âåñ\n",
    "\n",
    "        # ÂàùÂßãÂåñÊ¨äÈáç\n",
    "        self.init_weights()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.lora_rank))\n",
    "        self.inplanes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, lora_rank=self.lora_rank))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # È†êÊúüËº∏ÂÖ•ÂΩ¢ÁãÄ: (batch_size, time_steps, channels)\n",
    "        x = x.permute(0, 2, 1)  # ‚Üí (batch_size, channels, time_steps)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        # ÊáâÁî®Âπ≥ÂùáÂíåÊúÄÂ§ßÊ±†Âåñ\n",
    "        x1 = self.adaptiveavgpool(x)\n",
    "        x2 = self.adaptivemaxpool(x)\n",
    "        \n",
    "        # ÈÄ£Êé•Ê±†ÂåñÁµêÊûú\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        \n",
    "        # Â±ïÂπ≥\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # ÊáâÁî®dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # ÊúÄÁµÇÂàÜÈ°û\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"ÂàùÂßãÂåñÁ∂≤Áµ°Ê¨äÈáç\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def add_lora_adapter(self):\n",
    "        \"\"\"ÁÇ∫ÊâÄÊúâBasicBlockÁöÑconv2Â±§Ê∑ªÂä†‰∏ÄÂÄãÊñ∞ÁöÑLoRAÈÅ©ÈÖçÂô®\"\"\"\n",
    "        lora_count = 0\n",
    "        added_loras = []\n",
    "        \n",
    "        for module in self.modules():\n",
    "            if isinstance(module, BasicBlock1d_LoRA):\n",
    "                new_lora = module.add_lora_adapter()\n",
    "                added_loras.append(new_lora)\n",
    "                lora_count += 1\n",
    "                \n",
    "        print(f\"‚úÖ Added new LoRA adapters to {lora_count} BasicBlocks\")\n",
    "        return added_loras\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        \"\"\"ËøîÂõûÂèØË®ìÁ∑¥ÂèÉÊï∏ÂàóË°®ÔºàÁî®ÊñºÂÑ™ÂåñÂô®Ôºâ‰∏¶Êèê‰æõÂèÉÊï∏Áµ±Ë®à\"\"\"\n",
    "        lora_params = []\n",
    "        lora_names = []\n",
    "        fc_params = []\n",
    "        fc_names = []\n",
    "        \n",
    "        # Ë®àÁÆóÁ∏ΩÂèÉÊï∏Êï∏Èáè\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        \n",
    "        # Êî∂ÈõÜÊâÄÊúâLoRAÂèÉÊï∏\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, LoRAConv1d):\n",
    "                lora_params.append(module.lora_A)\n",
    "                lora_names.append(f\"{name}.lora_A\")\n",
    "                lora_params.append(module.lora_B)\n",
    "                lora_names.append(f\"{name}.lora_B\")\n",
    "\n",
    "        # Ê∑ªÂä†fcÂ±§ÂèÉÊï∏\n",
    "        for name, param in self.fc.named_parameters():\n",
    "            fc_params.append(param)\n",
    "            fc_names.append(f\"fc.{name}\")\n",
    "        \n",
    "        # Ë®àÁÆóÁµ±Ë®àÊï∏Êìö\n",
    "        trainable_params = lora_params + fc_params\n",
    "        frozen_params = total_params - sum(p.numel() for p in trainable_params)\n",
    "        lora_param_count = sum(p.numel() for p in lora_params)\n",
    "        fc_param_count = sum(p.numel() for p in fc_params)\n",
    "        trainable_param_count = lora_param_count + fc_param_count\n",
    "        \n",
    "        # ÊâìÂç∞Áµ±Ë®à‰ø°ÊÅØ\n",
    "        print(f\"üìä Parameter Statistics:\")\n",
    "        print(f\"  - Total parameters: {total_params:,}\")\n",
    "        print(f\"  - Trainable parameters: {trainable_param_count:,} ({trainable_param_count/total_params*100:.2f}%)\")\n",
    "        print(f\"    - LoRA parameters: {lora_param_count:,} ({lora_param_count/total_params*100:.2f}%)\")\n",
    "        print(f\"    - FC parameters: {fc_param_count:,} ({fc_param_count/total_params*100:.2f}%)\")\n",
    "        print(f\"  - Frozen parameters: {frozen_params:,} ({frozen_params/total_params*100:.2f}%)\")\n",
    "        \n",
    "        print(f\"üß† Trainable parameter names:\")\n",
    "        for name in lora_names:\n",
    "            print(f\"  ‚úÖ {name} (LoRA)\")\n",
    "        for name in fc_names:\n",
    "            print(f\"  ‚úÖ {name} (FC)\")\n",
    "        \n",
    "        return trainable_params\n",
    "    \n",
    "    def count_lora_adapters(self):\n",
    "        \"\"\"Ë®àÁÆóÁ∂≤Áµ°‰∏≠ÊâÄÊúâLoRAÈÅ©ÈÖçÂô®ÁöÑÊï∏Èáè\"\"\"\n",
    "        total_adapters = 0\n",
    "        blocks_with_lora = 0\n",
    "        \n",
    "        for module in self.modules():\n",
    "            if isinstance(module, BasicBlock1d_LoRA):\n",
    "                if len(module.lora_adapters) > 0:\n",
    "                    blocks_with_lora += 1\n",
    "                    total_adapters += len(module.lora_adapters)\n",
    "        \n",
    "        print(f\"üìà LoRA Adapter Statistics:\")\n",
    "        print(f\"  - Total LoRA adapters: {total_adapters}\")\n",
    "        print(f\"  - BasicBlocks with adapters: {blocks_with_lora}\")\n",
    "        \n",
    "        return total_adapters\n",
    "    \n",
    "    def count_lora_groups(self):\n",
    "        blocks = [m for m in self.modules() if isinstance(m, BasicBlock1d_LoRA)]\n",
    "        if not blocks:\n",
    "            return 0\n",
    "        return len(blocks[0].lora_adapters)  # ÊâÄÊúâ block ÁöÑ group Êï∏ÊáâË©≤‰∏ÄËá¥\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ebc8c6",
   "metadata": {},
   "source": [
    "## __Training and validation function__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2616acb4",
   "metadata": {},
   "source": [
    "### Extra Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3ce00ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classwise_accuracy(student_logits_flat, y_batch, class_correct, class_total):\n",
    "    \"\"\"\n",
    "    Computes per-class accuracy by accumulating correct and total samples for each class using vectorized operations.\n",
    "    \n",
    "    Args:\n",
    "        student_logits_flat (torch.Tensor): Model predictions (logits) in shape [batch_size * seq_len, output_size]\n",
    "        y_batch (torch.Tensor): True labels in shape [batch_size * seq_len]\n",
    "        class_correct (dict): Dictionary to store correct predictions per class\n",
    "        class_total (dict): Dictionary to store total samples per class\n",
    "    \"\"\"\n",
    "    # Ensure inputs are on the same device\n",
    "    if student_logits_flat.device != y_batch.device:\n",
    "        raise ValueError(\"student_logits_flat and y_batch must be on the same device\")\n",
    "\n",
    "    # Convert logits to predicted class indices\n",
    "    predictions = torch.argmax(student_logits_flat, dim=-1)  # Shape: [batch_size * seq_len]\n",
    "\n",
    "    # Compute correct predictions mask\n",
    "    correct_mask = (predictions == y_batch)  # Shape: [batch_size * seq_len], boolean\n",
    "\n",
    "    # Get unique labels in this batch\n",
    "    unique_labels = torch.unique(y_batch)\n",
    "\n",
    "    # Update class_total and class_correct using vectorized operations\n",
    "    for label in unique_labels:\n",
    "        label = label.item()  # Convert tensor to scalar\n",
    "        if label not in class_total:\n",
    "            class_total[label] = 0\n",
    "            class_correct[label] = 0\n",
    "        \n",
    "        # Count total samples for this label\n",
    "        label_mask = (y_batch == label)\n",
    "        class_total[label] += label_mask.sum().item()\n",
    "        \n",
    "        # Count correct predictions for this label\n",
    "        class_correct[label] += (label_mask & correct_mask).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "724647c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_parameter_info(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    param_size_bytes = total_params * 4\n",
    "    param_size_MB = param_size_bytes / (1024**2)\n",
    "    return total_params, param_size_MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b7f0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(y: np.ndarray, num_classes: int, exclude_classes: list = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Ë®àÁÆó class weightsÔºàinverse frequencyÔºâÈÅøÂÖç class imbalance„ÄÇ\n",
    "    ÂèØÊéíÈô§Êüê‰∫õÈ°ûÂà•ÔºàÂ¶Ç‰∏çÂ≠òÂú®ÁöÑÈ°ûÂà•ÔºâÔºåÈÄô‰∫õÈ°ûÂà•ÁöÑÊ¨äÈáçÂ∞áË®≠ÁÇ∫ 0„ÄÇ\n",
    "    \"\"\"\n",
    "    exclude_classes = set(exclude_classes or [])\n",
    "    class_sample_counts = np.bincount(y, minlength=num_classes)\n",
    "    total_samples = len(y)\n",
    "\n",
    "    weights = np.zeros(num_classes, dtype=np.float32)\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        if cls in exclude_classes:\n",
    "            weights[cls] = 0.0\n",
    "        else:\n",
    "            count = class_sample_counts[cls]\n",
    "            weights[cls] = total_samples / (count + 1e-6)\n",
    "\n",
    "    # Normalize only non-excluded weights\n",
    "    valid_mask = np.array([cls not in exclude_classes for cls in range(num_classes)])\n",
    "    norm_sum = weights[valid_mask].sum()\n",
    "    if norm_sum > 0:\n",
    "        weights[valid_mask] /= norm_sum\n",
    "\n",
    "    print(\"\\nüìä Class Weights (normalized):\")\n",
    "    for i, w in enumerate(weights):\n",
    "        status = \" (excluded)\" if i in exclude_classes else \"\"\n",
    "        print(f\"  - Class {i}: {w:.4f}{status}\")\n",
    "    \n",
    "    return torch.tensor(weights, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abe5f03",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3028103b",
   "metadata": {},
   "source": [
    "#### v1 pure gradient = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78f75ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_dynex_clora_ecg(model, teacher_model, output_size, criterion, optimizer,\n",
    "                           X_train, y_train, X_val, y_val,\n",
    "                           num_epochs, batch_size, alpha,\n",
    "                           model_saving_folder, model_name,\n",
    "                           stop_signal_file=None, scheduler=None,\n",
    "                           period=None, stable_classes=None,\n",
    "                           similarity_threshold=0.0,\n",
    "                           class_features_dict=None, related_labels=None, device=None):\n",
    "    \n",
    "    print(f\"\\nüöÄ 'train_with_dynex_clora_ecg' started for Period {period}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_name = model_name or 'dynex_clora_model'\n",
    "    model_saving_folder = model_saving_folder or './saved_models'\n",
    "    \n",
    "    if os.path.exists(model_saving_folder):\n",
    "        shutil.rmtree(model_saving_folder)\n",
    "        print(f\"‚úÖ Removed existing folder: {model_saving_folder}\")\n",
    "    os.makedirs(model_saving_folder, exist_ok=True)\n",
    "    \n",
    "    device = device or auto_select_cuda_device()\n",
    "    model.to(device)\n",
    "    \n",
    "    if teacher_model:\n",
    "        teacher_model.to(device)\n",
    "        teacher_model.eval()\n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(\"\\n‚úÖ Data Overview:\")\n",
    "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "    \n",
    "    best_results = []\n",
    "    \n",
    "    # === Class Feature Extraction ===\n",
    "    model.eval()\n",
    "    new_class_features = {}\n",
    "    \n",
    "    # Extract features for current classes\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in train_loader:\n",
    "            # Get feature representations\n",
    "            features = extract_features(model, xb)\n",
    "            for cls in torch.unique(yb):\n",
    "                cls_mask = (yb == cls)\n",
    "                cls_feat = features[cls_mask]\n",
    "                if cls.item() not in new_class_features:\n",
    "                    new_class_features[cls.item()] = []\n",
    "                new_class_features[cls.item()].append(cls_feat)\n",
    "    \n",
    "    # Average features per class\n",
    "    for cls in new_class_features:\n",
    "        new_class_features[cls] = torch.cat(new_class_features[cls], dim=0).mean(dim=0)\n",
    "    \n",
    "    # Initialize related_labels if not provided\n",
    "    if related_labels is None:\n",
    "        related_labels = {}\n",
    "    \n",
    "    # === Similarity Computation (only for Period > 1) ===\n",
    "    if period > 1 and class_features_dict:\n",
    "        cosine_sim = torch.nn.CosineSimilarity(dim=0)\n",
    "        similarity_scores = {}\n",
    "        \n",
    "        # Calculate similarities between new and old classes\n",
    "        for new_label, new_feat in new_class_features.items():\n",
    "            similarity_scores[new_label] = {}\n",
    "            for old_label, old_feat in class_features_dict.items():\n",
    "                sim = cosine_sim(new_feat.to(device), old_feat.to(device)).item()\n",
    "                similarity_scores[new_label][old_label] = sim\n",
    "        \n",
    "        # Print similarity information\n",
    "        print(\"\\nüîé Similarity Analysis:\")\n",
    "        print(f\"  Similarity threshold: {similarity_threshold:.4f}\")\n",
    "        print(f\"  Existing classes: {sorted(list(class_features_dict.keys()))}\")\n",
    "        print(f\"  Current classes: {sorted(list(new_class_features.keys()))}\")\n",
    "        \n",
    "        # Calculate new classes (classes not in previous periods)\n",
    "        existing_classes = set(class_features_dict.keys())\n",
    "        current_classes = set(new_class_features.keys())\n",
    "        new_classes = current_classes - existing_classes\n",
    "        print(f\"  New classes: {sorted(list(new_classes))}\")\n",
    "        \n",
    "        # Print similarity scores\n",
    "        print(\"\\nüìä Similarity Scores:\")\n",
    "        for new_label, scores in similarity_scores.items():\n",
    "            if new_label in new_classes:  # Only show for new classes\n",
    "                print(f\"  New Class {new_label}:\")\n",
    "                if scores:\n",
    "                    for old_label, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
    "                        print(f\"    - Existing Class {old_label}: {score:.4f}\")\n",
    "                else:\n",
    "                    print(\"    - No existing classes to compare\")\n",
    "        \n",
    "        # Calculate average similarity statistics\n",
    "        all_similarities = [s for scores in similarity_scores.values() for s in scores.values()]\n",
    "        if all_similarities:\n",
    "            avg_similarity = np.mean(all_similarities)\n",
    "            std_similarity = np.std(all_similarities)\n",
    "            print(f\"\\n  Average similarity: {avg_similarity:.4f}, Std: {std_similarity:.4f}\")\n",
    "    \n",
    "    # === Period-specific LoRA Management Logic ===\n",
    "    to_unfreeze = set()\n",
    "    \n",
    "    # Special handling for period 1 - no LoRA adapters yet\n",
    "    if period == 1:\n",
    "        if not related_labels:\n",
    "            # Initialize related_labels for first period\n",
    "            # The base conv layers (index 'base') are associated with initial classes\n",
    "            initial_classes = list(new_class_features.keys())\n",
    "            related_labels['base'] = initial_classes\n",
    "            print(f\"\\nüîÑ Initializing related_labels for first period: {related_labels}\")\n",
    "        \n",
    "        # For first period, all base model parameters are trainable\n",
    "        print(\"\\nüîì First period: All model parameters are trainable\")\n",
    "    \n",
    "    # For periods > 1, manage LoRA adapters\n",
    "    elif period > 1 and class_features_dict:\n",
    "        new_lora_indices = []\n",
    "        cosine_sim = torch.nn.CosineSimilarity(dim=0)\n",
    "        \n",
    "        existing_classes = set(class_features_dict.keys())\n",
    "        current_classes = set(new_class_features.keys())\n",
    "        new_classes = current_classes - existing_classes\n",
    "        \n",
    "        print(f\"\\nüß© Managing LoRA adapters for {len(new_classes)} new classes...\")\n",
    "        \n",
    "        # Process each new class\n",
    "        for new_cls in new_classes:\n",
    "            new_feat = new_class_features[new_cls]\n",
    "            \n",
    "            # Calculate similarities to all existing classes\n",
    "            sims = [(old_cls, cosine_sim(new_feat.to(device), class_features_dict[old_cls].to(device)).item())\n",
    "                   for old_cls in class_features_dict]\n",
    "            \n",
    "            # Sort by similarity (highest first)\n",
    "            sims.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Check if new class is similar to any existing class\n",
    "            matched = False\n",
    "            for old_cls, sim in sims:\n",
    "                if sim >= similarity_threshold:\n",
    "                    matched = True\n",
    "                    # Find which adapter/network is associated with this old class\n",
    "                    for adapter_idx, related_cls_list in related_labels.items():\n",
    "                        if old_cls in related_cls_list:\n",
    "                            # Add this new class to the same adapter's related classes\n",
    "                            if new_cls not in related_labels[adapter_idx]:\n",
    "                                related_labels[adapter_idx].append(new_cls)\n",
    "                                print(f\"üîÑ New Class {new_cls} is similar to Class {old_cls} (sim={sim:.4f}) ‚Üí Added to adapter '{adapter_idx}'\")\n",
    "                            \n",
    "                            # Mark this adapter for unfreezing during training\n",
    "                            to_unfreeze.add(adapter_idx)\n",
    "                            break\n",
    "            \n",
    "            # If no match found, create new LoRA adapter\n",
    "            if not matched:\n",
    "                # === Âä†ÂÖ•‰∏ÄÊï¥ÁµÑ LoRA adaptersÔºàÊØèÂ±§‰∏ÄÂÄãÔºâ ===\n",
    "                model.add_lora_adapter()\n",
    "\n",
    "                # Ë®òÈåÑÈÄôÊòØÁ¨¨ÂπæÁµÑÔºàÁî® group indexÔºâ\n",
    "                group_idx = max([k for k in related_labels.keys() if isinstance(k, int)], default=-1) + 1\n",
    "\n",
    "                related_labels[group_idx] = [new_cls]\n",
    "                new_lora_indices.append(group_idx)\n",
    "                print(f\"‚ûï New Class {new_cls} is not similar to any existing class ‚Üí Created new adapter group #{group_idx}\")\n",
    "        \n",
    "        # Check stability of existing classes\n",
    "        print(\"\\nüîç Checking stability of existing classes...\")\n",
    "        for old_cls in existing_classes & current_classes:  # Intersection - classes that exist in both periods\n",
    "            if old_cls in new_class_features:\n",
    "                sim_self = cosine_sim(new_class_features[old_cls].to(device), \n",
    "                                      class_features_dict[old_cls].to(device)).item()\n",
    "                print(f\"  Class {old_cls} similarity with itself: {sim_self:.4f}\")\n",
    "                \n",
    "                # If class representation has drifted too much\n",
    "                if sim_self < similarity_threshold:\n",
    "                    for adapter_idx, related_cls_list in related_labels.items():\n",
    "                        if old_cls in related_cls_list:\n",
    "                            to_unfreeze.add(adapter_idx)\n",
    "                            print(f\"‚ö†Ô∏è Class {old_cls} has drifted (self-sim={sim_self:.4f}) ‚Üí Unfreezing adapter '{adapter_idx}'\")\n",
    "        \n",
    "        # # Freeze all LoRA adapters and conv2 weights\n",
    "        # print(\"\\nüîí Default: Freezing all LoRA adapters and base conv2 weights\")\n",
    "        # for module in model.modules():\n",
    "        #     if isinstance(module, BasicBlock1d_LoRA):\n",
    "        #         # freeze all conv2\n",
    "        #         for param in module.conv2.parameters():\n",
    "        #             param.requires_grad = False\n",
    "        #         # freeze all LoRA inside\n",
    "        #         for adapter in module.lora_adapters:\n",
    "        #             for param in adapter.parameters():\n",
    "        #                 param.requires_grad = False\n",
    "\n",
    "        # üîí Freeze ALL model parameters first\n",
    "        print(\"\\nüîí Default: Freezing ALL model parameters\")\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        ####### Check frozen parameters #######\n",
    "        print(\"\\nüîç Frozen Parameters:\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "        ####### Check frozen parameters #######\n",
    "\n",
    "        # üîì Unfreeze specific adapter groups\n",
    "        print(\"\\nüîì Unfreezing selected adapters (by group):\")\n",
    "        for adapter_group_idx in to_unfreeze:\n",
    "            if isinstance(adapter_group_idx, int):\n",
    "                for module in model.modules():\n",
    "                    if isinstance(module, BasicBlock1d_LoRA):\n",
    "                        if adapter_group_idx < len(module.lora_adapters):\n",
    "                            for param in module.lora_adapters[adapter_group_idx].parameters():\n",
    "                                param.requires_grad = True\n",
    "                print(f\"  - Adapter Group #{adapter_group_idx} (all blocks) (classes: {related_labels.get(adapter_group_idx, [])})\")\n",
    "            elif adapter_group_idx == 'base':\n",
    "                for module in model.modules():\n",
    "                    if isinstance(module, BasicBlock1d_LoRA):\n",
    "                        for p in module.conv2.parameters():\n",
    "                            p.requires_grad = True\n",
    "                print(f\"  - Base layers (classes: {related_labels.get('base', [])})\")\n",
    "\n",
    "        ####### Check frozen parameters #######\n",
    "        print(\"\\nüîç Frozen Parameters: (After Unfreeze specific adapter groups)\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "        ####### Check frozen parameters #######\n",
    "\n",
    "        # üîì Unfreeze newly added adapter groups\n",
    "        for group_idx in new_lora_indices:\n",
    "            block_counter = 0\n",
    "            for module in model.modules():\n",
    "                if isinstance(module, BasicBlock1d_LoRA):\n",
    "                    if group_idx < len(module.lora_adapters):\n",
    "                        adapter = module.lora_adapters[group_idx]\n",
    "                        for param in adapter.parameters():\n",
    "                            param.requires_grad = True\n",
    "                    block_counter += 1\n",
    "            print(f\"  - New adapter group #{group_idx} (classes: {related_labels[group_idx]})\")\n",
    "            \n",
    "        ####### Check frozen parameters #######\n",
    "        print(\"\\nüîç Frozen Parameters (After Unfreeze newly added adapter groups):\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "        ####### Check frozen parameters #######\n",
    "\n",
    "        # Always unfreeze the final layer for all periods\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        ####### Check frozen parameters #######\n",
    "        print(\"\\nüîç Frozen Parameters (After unfreeze the final layer):\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "        ####### Check frozen parameters #######\n",
    "\n",
    "    # Print summary of related_labels\n",
    "    print(f\"\\nüìã Related Labels Summary:\")\n",
    "    for adapter_idx, classes in related_labels.items():\n",
    "        print(f\"  - {'Base network' if adapter_idx == 'base' else f'Adapter #{adapter_idx}'}: Classes {classes}\")\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    print(\"\\nüîß Trainable Parameter Status:\")\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    trainable_count = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            trainable_count += 1\n",
    "            print(f\"  ‚úÖ {name:<50} | shape={list(param.shape)}\")\n",
    "    print(f\"trainable_count: {trainable_count}\")\n",
    "    \n",
    "    frozen_params = total_params - trainable_params\n",
    "    print(f\"\\nüìä Parameter Statistics:\")\n",
    "    print(f\"  - Total parameters: {total_params:,}\")\n",
    "    print(f\"  - Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
    "    print(f\"  - Frozen parameters: {frozen_params:,} ({frozen_params/total_params*100:.2f}%)\")\n",
    "    \n",
    "    # Update optimizer with only trainable parameters\n",
    "    # optimizer = torch.optim.Adam(\n",
    "    #     filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    #     lr=optimizer.param_groups[0]['lr'],\n",
    "    #     weight_decay=optimizer.param_groups[0].get('weight_decay', 0)\n",
    "    # )\n",
    "\n",
    "    # # ÈáçË®≠ÊâÄÊúâÂèÉÊï∏ÁöÑ requires_grad = True\n",
    "    # for param in model.parameters():\n",
    "    #     param.requires_grad = True\n",
    "\n",
    "    # ####### Check frozen parameters #######\n",
    "    # print(\"\\nüîç Frozen Parameters:\")\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if not param.requires_grad:\n",
    "    #         print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "    # ####### Check frozen parameters #######\n",
    "\n",
    "    # Ê™¢Êü•ÊúÄÁµÇ optimizer ÊéßÂà∂‰∫ÜÂì™‰∫õÂèÉÊï∏\n",
    "    optimizer_param_ids = set(id(p) for group in optimizer.param_groups for p in group['params'])\n",
    "\n",
    "    named_params = list(model.named_parameters())\n",
    "    print(f\"\\nüß† Parameters currently controlled by the optimizer: ({len(named_params)})\")\n",
    "    for name, param in named_params:\n",
    "        if id(param) in optimizer_param_ids:\n",
    "            print(f\"  ‚úÖ {name}\")\n",
    "        else:\n",
    "            print(f\"  ‚õî {name} (NOT included in optimizer)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Starting training...\\n\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        if stop_signal_file and os.path.exists(stop_signal_file):\n",
    "            print(\"\\nüõë Stop signal detected. Exiting training loop.\")\n",
    "            break\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        class_correct, class_total = {}, {}\n",
    "        \n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            ce_loss = criterion(logits, yb)\n",
    "            \n",
    "            # Apply distillation if teacher model is provided\n",
    "            if teacher_model and stable_classes:\n",
    "                with torch.no_grad():\n",
    "                    teacher_logits = teacher_model(xb)\n",
    "                student_stable = logits[:, stable_classes]\n",
    "                teacher_stable = teacher_logits[:, stable_classes]\n",
    "                distill_loss = F.mse_loss(student_stable, teacher_stable)\n",
    "                total_loss = alpha * distill_loss + (1 - alpha) * ce_loss\n",
    "            else:\n",
    "                total_loss = ce_loss\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += total_loss.item() * xb.size(0)\n",
    "            compute_classwise_accuracy(logits, yb, class_correct, class_total)\n",
    "        \n",
    "        train_loss = epoch_loss / len(train_loader.dataset)\n",
    "        train_acc = {int(k): f\"{(class_correct[k] / class_total[k]) * 100:.2f}%\" \n",
    "                    if class_total[k] > 0 else \"0.00%\" for k in class_total}\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        val_class_correct, val_class_total = {}, {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                outputs = model(xb)\n",
    "                val_loss += criterion(outputs, yb).item() * xb.size(0)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_correct += (preds == yb).sum().item()\n",
    "                val_total += yb.size(0)\n",
    "                compute_classwise_accuracy(outputs, yb, val_class_correct, val_class_total)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct / val_total\n",
    "        val_acc_cls = {int(k): f\"{(val_class_correct[k]/val_class_total[k])*100:.2f}%\" \n",
    "                      if val_class_total[k] > 0 else \"0.00%\" for k in val_class_total}\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Train-Class-Acc: {train_acc}\")\n",
    "        print(f\"Val Loss: {val_loss:.6f}, Val Acc: {val_acc*100:.2f}%, Val-Class-Acc: {val_acc_cls}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Save model checkpoint\n",
    "        model_path = os.path.join(model_saving_folder, f\"{model_name}_epoch_{epoch+1}.pth\")\n",
    "        current = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_acc,\n",
    "            'train_classwise_accuracy': train_acc,\n",
    "            'val_classwise_accuracy': val_acc_cls,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "            'model_path': model_path,\n",
    "            'num_lora_groups': model.count_lora_groups(),\n",
    "            'related_labels': related_labels\n",
    "        }\n",
    "        \n",
    "        # Keep top 5 best models\n",
    "        if len(best_results) < 5 or val_acc > best_results[-1]['val_accuracy']:\n",
    "            if len(best_results) == 5:\n",
    "                to_remove = best_results.pop()\n",
    "                if os.path.exists(to_remove['model_path']):\n",
    "                    os.remove(to_remove['model_path'])\n",
    "                    print(f\"üóë Removed: {to_remove['model_path']}\")\n",
    "            best_results.append(current)\n",
    "            best_results.sort(key=lambda x: (x['val_accuracy'], x['epoch']), reverse=True)\n",
    "            torch.save(current, model_path)\n",
    "            print(f\"‚úÖ Saved model: {model_path}\")\n",
    "        \n",
    "        # Update learning rate scheduler if provided\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "    \n",
    "    # End of training\n",
    "    elapsed_time = time.time() - start_time\n",
    "    total_params, param_size_MB = get_model_parameter_info(model)\n",
    "    \n",
    "    # Save best model\n",
    "    if best_results:\n",
    "        best = best_results[0]\n",
    "        best_model_path = os.path.join(model_saving_folder, f\"{model_name}_best.pth\")\n",
    "        torch.save(best, best_model_path)\n",
    "        print(f\"\\nüèÜ Best model saved as: {best_model_path} (Val Accuracy: {best['val_accuracy'] * 100:.2f}%)\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = os.path.join(model_saving_folder, f\"{model_name}_final.pth\")\n",
    "    torch.save(current, final_model_path)\n",
    "    print(f\"\\nüìå Final model saved as: {final_model_path}\")\n",
    "    \n",
    "    # Print top 5 models\n",
    "    print(\"\\nüéØ Top 5 Best Models:\")\n",
    "    for res in best_results:\n",
    "        print(f\"Epoch {res['epoch']}, Train Loss: {res['train_loss']:.6f}, Train-Acc: {res['train_classwise_accuracy']},\\n\"\n",
    "              f\"Val Loss: {res['val_loss']:.6f}, Val Acc: {res['val_accuracy']*100:.2f}%, Val-Acc: {res['val_classwise_accuracy']}, \"\n",
    "              f\"Model Path: {res['model_path']}\")\n",
    "    \n",
    "    # Print model summary\n",
    "    print(f\"\\nüß† Model Summary:\")\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    print(f\"Model Size (float32): {param_size_MB:.2f} MB\")\n",
    "    print(f\"Number of LoRA adapters: {model.count_lora_adapters()}\")\n",
    "    print(f\"Number of LoRA groups: {model.count_lora_groups()}\")\n",
    "    print(f\"Total Training Time: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    # Extract period number from folder name\n",
    "    match = re.search(r'Period_(\\d+)', model_saving_folder)\n",
    "    period_label = match.group(1) if match else str(period)\n",
    "    model_name_str = model.__class__.__name__\n",
    "    \n",
    "    # Print markdown summary\n",
    "    best_model = max(best_results, key=lambda x: x['val_accuracy'])\n",
    "    print(f\"\"\"\n",
    "---\n",
    "### Period {period_label} (alpha = {alpha}, similarity_threshold = {similarity_threshold})\n",
    "+ ##### Total training time: {elapsed_time:.2f} seconds\n",
    "+ ##### Model: {model_name_str}\n",
    "+ ##### Training and saving in *'{model_saving_folder}'*\n",
    "+ ##### Best Epoch: {best_model['epoch']}\n",
    "#### __Val Accuracy: {best_model['val_accuracy'] * 100:.2f}%__\n",
    "#### __Val-Class-Acc: {best_model['val_classwise_accuracy']}__\n",
    "#### __Total Parameters: {total_params:,}__\n",
    "#### __Model Size (float32): {param_size_MB:.2f} MB__\n",
    "#### __Number of LoRA adapters: {model.count_lora_adapters()}__\n",
    "#### __Number of LoRA groups: {model.count_lora_groups()}__\n",
    "\"\"\".strip())\n",
    "    \n",
    "    # Save class features for next period\n",
    "    if class_features_dict is None:\n",
    "        class_features_dict = {}\n",
    "    class_features_dict.update(new_class_features)\n",
    "    with open(os.path.join(model_saving_folder, \"class_features.pkl\"), 'wb') as f:\n",
    "        pickle.dump(class_features_dict, f)\n",
    "    print(f\"\\nSaved class features to: {os.path.join(model_saving_folder, 'class_features.pkl')}\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def extract_features(model, x):\n",
    "    \"\"\"Helper function to extract features from the model for similarity calculation\"\"\"\n",
    "    # This is a placeholder - you'll need to adapt this based on your actual model architecture\n",
    "    # The goal is to extract meaningful features before the classification layer\n",
    "    # For ResNet18_1D_LoRA model, this would typically be the features right before the fc layer\n",
    "    \n",
    "    # Example (pseudo-code - adapt to your actual model):\n",
    "    x = x.permute(0, 2, 1)  # Convert to (batch_size, channels, time_steps)\n",
    "    \n",
    "    # Feed through the network up to the point before classification\n",
    "    with torch.no_grad():\n",
    "        x = model.conv1(x)\n",
    "        x = model.bn1(x)\n",
    "        x = model.relu(x)\n",
    "        x = model.maxpool(x)\n",
    "        \n",
    "        x = model.layer1(x)\n",
    "        x = model.layer2(x)\n",
    "        x = model.layer3(x)\n",
    "        x = model.layer4(x)\n",
    "        \n",
    "        # Apply pooling\n",
    "        x1 = model.adaptiveavgpool(x)\n",
    "        x2 = model.adaptivemaxpool(x)\n",
    "        \n",
    "        # Concatenate pooling results\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        \n",
    "        # Flatten\n",
    "        features = x.view(x.size(0), -1)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2b7677",
   "metadata": {},
   "source": [
    "#### v2 pure optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5681c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_dynex_clora_ecg(model, teacher_model, output_size, criterion, optimizer,\n",
    "                           X_train, y_train, X_val, y_val,\n",
    "                           num_epochs, batch_size, alpha,\n",
    "                           model_saving_folder, model_name,\n",
    "                           stop_signal_file=None, scheduler=None,\n",
    "                           period=None, stable_classes=None,\n",
    "                           similarity_threshold=0.0,\n",
    "                           class_features_dict=None, related_labels=None, device=None):\n",
    "    \n",
    "    print(f\"\\nüöÄ 'train_with_dynex_clora_ecg' started for Period {period}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_name = model_name or 'dynex_clora_model'\n",
    "    model_saving_folder = model_saving_folder or './saved_models'\n",
    "    \n",
    "    if os.path.exists(model_saving_folder):\n",
    "        shutil.rmtree(model_saving_folder)\n",
    "        print(f\"‚úÖ Removed existing folder: {model_saving_folder}\")\n",
    "    os.makedirs(model_saving_folder, exist_ok=True)\n",
    "    \n",
    "    device = device or auto_select_cuda_device()\n",
    "    model.to(device)\n",
    "    \n",
    "    if teacher_model:\n",
    "        teacher_model.to(device)\n",
    "        teacher_model.eval()\n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(\"\\n‚úÖ Data Overview:\")\n",
    "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "    \n",
    "    best_results = []\n",
    "    \n",
    "    # === Class Feature Extraction ===\n",
    "    model.eval()\n",
    "    new_class_features = {}\n",
    "    \n",
    "    # Extract features for current classes\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in train_loader:\n",
    "            # Get feature representations\n",
    "            features = extract_features(model, xb)\n",
    "            for cls in torch.unique(yb):\n",
    "                cls_mask = (yb == cls)\n",
    "                cls_feat = features[cls_mask]\n",
    "                if cls.item() not in new_class_features:\n",
    "                    new_class_features[cls.item()] = []\n",
    "                new_class_features[cls.item()].append(cls_feat)\n",
    "    \n",
    "    # Average features per class\n",
    "    for cls in new_class_features:\n",
    "        new_class_features[cls] = torch.cat(new_class_features[cls], dim=0).mean(dim=0)\n",
    "    \n",
    "    # Initialize related_labels if not provided\n",
    "    if related_labels is None:\n",
    "        related_labels = {}\n",
    "    \n",
    "    # === Similarity Computation (only for Period > 1) ===\n",
    "    if period > 1 and class_features_dict:\n",
    "        cosine_sim = torch.nn.CosineSimilarity(dim=0)\n",
    "        similarity_scores = {}\n",
    "        \n",
    "        # Calculate similarities between new and old classes\n",
    "        for new_label, new_feat in new_class_features.items():\n",
    "            similarity_scores[new_label] = {}\n",
    "            for old_label, old_feat in class_features_dict.items():\n",
    "                sim = cosine_sim(new_feat.to(device), old_feat.to(device)).item()\n",
    "                similarity_scores[new_label][old_label] = sim\n",
    "        \n",
    "        # Print similarity information\n",
    "        print(\"\\nüîé Similarity Analysis:\")\n",
    "        print(f\"  Similarity threshold: {similarity_threshold:.4f}\")\n",
    "        print(f\"  Existing classes: {sorted(list(class_features_dict.keys()))}\")\n",
    "        print(f\"  Current classes: {sorted(list(new_class_features.keys()))}\")\n",
    "        \n",
    "        # Calculate new classes (classes not in previous periods)\n",
    "        existing_classes = set(class_features_dict.keys())\n",
    "        current_classes = set(new_class_features.keys())\n",
    "        new_classes = current_classes - existing_classes\n",
    "        print(f\"  New classes: {sorted(list(new_classes))}\")\n",
    "        \n",
    "        # Print similarity scores\n",
    "        print(\"\\nüìä Similarity Scores:\")\n",
    "        for new_label, scores in similarity_scores.items():\n",
    "            if new_label in new_classes:  # Only show for new classes\n",
    "                print(f\"  New Class {new_label}:\")\n",
    "                if scores:\n",
    "                    for old_label, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
    "                        print(f\"    - Existing Class {old_label}: {score:.4f}\")\n",
    "                else:\n",
    "                    print(\"    - No existing classes to compare\")\n",
    "        \n",
    "        # Calculate average similarity statistics\n",
    "        all_similarities = [s for scores in similarity_scores.values() for s in scores.values()]\n",
    "        if all_similarities:\n",
    "            avg_similarity = np.mean(all_similarities)\n",
    "            std_similarity = np.std(all_similarities)\n",
    "            print(f\"\\n  Average similarity: {avg_similarity:.4f}, Std: {std_similarity:.4f}\")\n",
    "    \n",
    "    # === Period-specific LoRA Management Logic ===\n",
    "    to_unfreeze = set()\n",
    "    \n",
    "    # Special handling for period 1 - no LoRA adapters yet\n",
    "    if period == 1:\n",
    "        if not related_labels:\n",
    "            # Initialize related_labels for first period\n",
    "            # The base conv layers (index 'base') are associated with initial classes\n",
    "            initial_classes = list(new_class_features.keys())\n",
    "            related_labels['base'] = initial_classes\n",
    "            print(f\"\\nüîÑ Initializing related_labels for first period: {related_labels}\")\n",
    "        \n",
    "        # For first period, all base model parameters are trainable\n",
    "        print(\"\\nüîì First period: All model parameters are trainable\")\n",
    "    \n",
    "    # For periods > 1, manage LoRA adapters\n",
    "    elif period > 1 and class_features_dict:\n",
    "        new_lora_indices = []\n",
    "        cosine_sim = torch.nn.CosineSimilarity(dim=0)\n",
    "        \n",
    "        existing_classes = set(class_features_dict.keys())\n",
    "        current_classes = set(new_class_features.keys())\n",
    "        new_classes = current_classes - existing_classes\n",
    "        \n",
    "        print(f\"\\nüß© Managing LoRA adapters for {len(new_classes)} new classes...\")\n",
    "        \n",
    "        # Process each new class\n",
    "        for new_cls in new_classes:\n",
    "            new_feat = new_class_features[new_cls]\n",
    "            \n",
    "            # Calculate similarities to all existing classes\n",
    "            sims = [(old_cls, cosine_sim(new_feat.to(device), class_features_dict[old_cls].to(device)).item())\n",
    "                   for old_cls in class_features_dict]\n",
    "            \n",
    "            # Sort by similarity (highest first)\n",
    "            sims.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Check if new class is similar to any existing class\n",
    "            matched = False\n",
    "            for old_cls, sim in sims:\n",
    "                if sim >= similarity_threshold:\n",
    "                    matched = True\n",
    "                    # Find which adapter/network is associated with this old class\n",
    "                    for adapter_idx, related_cls_list in related_labels.items():\n",
    "                        if old_cls in related_cls_list:\n",
    "                            # Add this new class to the same adapter's related classes\n",
    "                            if new_cls not in related_labels[adapter_idx]:\n",
    "                                related_labels[adapter_idx].append(new_cls)\n",
    "                                print(f\"üîÑ New Class {new_cls} is similar to Class {old_cls} (sim={sim:.4f}) ‚Üí Added to adapter '{adapter_idx}'\")\n",
    "                            \n",
    "                            # Mark this adapter for unfreezing during training\n",
    "                            to_unfreeze.add(adapter_idx)\n",
    "                            break\n",
    "            \n",
    "            # If no match found, create new LoRA adapter\n",
    "            if not matched:\n",
    "                # === Âä†ÂÖ•‰∏ÄÊï¥ÁµÑ LoRA adaptersÔºàÊØèÂ±§‰∏ÄÂÄãÔºâ ===\n",
    "                model.add_lora_adapter()\n",
    "\n",
    "                # Ë®òÈåÑÈÄôÊòØÁ¨¨ÂπæÁµÑÔºàÁî® group indexÔºâ\n",
    "                group_idx = max([k for k in related_labels.keys() if isinstance(k, int)], default=-1) + 1\n",
    "\n",
    "                related_labels[group_idx] = [new_cls]\n",
    "                new_lora_indices.append(group_idx)\n",
    "                print(f\"‚ûï New Class {new_cls} is not similar to any existing class ‚Üí Created new adapter group #{group_idx}\")\n",
    "        \n",
    "        # Check stability of existing classes\n",
    "        print(\"\\nüîç Checking stability of existing classes...\")\n",
    "        for old_cls in existing_classes & current_classes:  # Intersection - classes that exist in both periods\n",
    "            if old_cls in new_class_features:\n",
    "                sim_self = cosine_sim(new_class_features[old_cls].to(device), \n",
    "                                      class_features_dict[old_cls].to(device)).item()\n",
    "                print(f\"  Class {old_cls} similarity with itself: {sim_self:.4f}\")\n",
    "                \n",
    "                # If class representation has drifted too much\n",
    "                if sim_self < similarity_threshold:\n",
    "                    for adapter_idx, related_cls_list in related_labels.items():\n",
    "                        if old_cls in related_cls_list:\n",
    "                            to_unfreeze.add(adapter_idx)\n",
    "                            print(f\"‚ö†Ô∏è Class {old_cls} has drifted (self-sim={sim_self:.4f}) ‚Üí Unfreezing adapter '{adapter_idx}'\")\n",
    "        \n",
    "        # Freeze all LoRA adapters and conv2 weights\n",
    "        print(\"\\nüîí Default: Freezing all LoRA adapters and base conv2 weights\")\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, BasicBlock1d_LoRA):\n",
    "                # freeze all conv2\n",
    "                for param in module.conv2.parameters():\n",
    "                    param.requires_grad = False\n",
    "                # freeze all LoRA inside\n",
    "                for adapter in module.lora_adapters:\n",
    "                    for param in adapter.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "        # # üîí Freeze ALL model parameters first\n",
    "        # print(\"\\nüîí Default: Freezing ALL model parameters\")\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     param.requires_grad = False\n",
    "        \n",
    "        ####### Check frozen parameters #######\n",
    "        print(\"\\nüîç Frozen Parameters:\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "        ####### Check frozen parameters #######\n",
    "\n",
    "        # üîì Unfreeze specific adapter groups\n",
    "        print(\"\\nüîì Unfreezing selected adapters (by group):\")\n",
    "        for adapter_group_idx in to_unfreeze:\n",
    "            if isinstance(adapter_group_idx, int):\n",
    "                for module in model.modules():\n",
    "                    if isinstance(module, BasicBlock1d_LoRA):\n",
    "                        if adapter_group_idx < len(module.lora_adapters):\n",
    "                            for param in module.lora_adapters[adapter_group_idx].parameters():\n",
    "                                param.requires_grad = True\n",
    "                print(f\"  - Adapter Group #{adapter_group_idx} (all blocks) (classes: {related_labels.get(adapter_group_idx, [])})\")\n",
    "            elif adapter_group_idx == 'base':\n",
    "                for module in model.modules():\n",
    "                    if isinstance(module, BasicBlock1d_LoRA):\n",
    "                        for p in module.conv2.parameters():\n",
    "                            p.requires_grad = True\n",
    "                print(f\"  - Base layers (classes: {related_labels.get('base', [])})\")\n",
    "\n",
    "        ####### Check frozen parameters #######\n",
    "        print(\"\\nüîç Frozen Parameters: (After Unfreeze specific adapter groups)\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "        ####### Check frozen parameters #######\n",
    "\n",
    "        # üîì Unfreeze newly added adapter groups\n",
    "        for group_idx in new_lora_indices:\n",
    "            block_counter = 0\n",
    "            for module in model.modules():\n",
    "                if isinstance(module, BasicBlock1d_LoRA):\n",
    "                    if group_idx < len(module.lora_adapters):\n",
    "                        adapter = module.lora_adapters[group_idx]\n",
    "                        for param in adapter.parameters():\n",
    "                            param.requires_grad = True\n",
    "                    block_counter += 1\n",
    "            print(f\"  - New adapter group #{group_idx} (classes: {related_labels[group_idx]})\")\n",
    "            \n",
    "        ####### Check frozen parameters #######\n",
    "        print(\"\\nüîç Frozen Parameters (After Unfreeze newly added adapter groups):\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "        ####### Check frozen parameters #######\n",
    "\n",
    "        # Always unfreeze the final layer for all periods\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        ####### Check frozen parameters #######\n",
    "        print(\"\\nüîç Frozen Parameters (After unfreeze the final layer):\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "        ####### Check frozen parameters #######\n",
    "\n",
    "    # Print summary of related_labels\n",
    "    print(f\"\\nüìã Related Labels Summary:\")\n",
    "    for adapter_idx, classes in related_labels.items():\n",
    "        print(f\"  - {'Base network' if adapter_idx == 'base' else f'Adapter #{adapter_idx}'}: Classes {classes}\")\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    print(\"\\nüîß Trainable Parameter Status:\")\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    trainable_count = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            trainable_count += 1\n",
    "            print(f\"  ‚úÖ {name:<50} | shape={list(param.shape)}\")\n",
    "    print(f\"trainable_count: {trainable_count}\")\n",
    "    \n",
    "    frozen_params = total_params - trainable_params\n",
    "    print(f\"\\nüìä Parameter Statistics:\")\n",
    "    print(f\"  - Total parameters: {total_params:,}\")\n",
    "    print(f\"  - Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
    "    print(f\"  - Frozen parameters: {frozen_params:,} ({frozen_params/total_params*100:.2f}%)\")\n",
    "    \n",
    "    # Update optimizer with only trainable parameters\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=optimizer.param_groups[0]['lr'],\n",
    "        weight_decay=optimizer.param_groups[0].get('weight_decay', 0)\n",
    "    )\n",
    "\n",
    "    # ÈáçË®≠ÊâÄÊúâÂèÉÊï∏ÁöÑ requires_grad = True\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    ####### Check frozen parameters #######\n",
    "    print(\"\\nüîç Frozen Parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "    ####### Check frozen parameters #######\n",
    "\n",
    "    # Ê™¢Êü•ÊúÄÁµÇ optimizer ÊéßÂà∂‰∫ÜÂì™‰∫õÂèÉÊï∏\n",
    "    optimizer_param_ids = set(id(p) for group in optimizer.param_groups for p in group['params'])\n",
    "\n",
    "    named_params = list(model.named_parameters())\n",
    "    print(f\"\\nüß† Parameters currently controlled by the optimizer: ({len(named_params)})\")\n",
    "    for name, param in named_params:\n",
    "        if id(param) in optimizer_param_ids:\n",
    "            print(f\"  ‚úÖ {name}\")\n",
    "        else:\n",
    "            print(f\"  ‚õî {name} (NOT included in optimizer)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Starting training...\\n\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        if stop_signal_file and os.path.exists(stop_signal_file):\n",
    "            print(\"\\nüõë Stop signal detected. Exiting training loop.\")\n",
    "            break\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        class_correct, class_total = {}, {}\n",
    "        \n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            ce_loss = criterion(logits, yb)\n",
    "            \n",
    "            # Apply distillation if teacher model is provided\n",
    "            if teacher_model and stable_classes:\n",
    "                with torch.no_grad():\n",
    "                    teacher_logits = teacher_model(xb)\n",
    "                student_stable = logits[:, stable_classes]\n",
    "                teacher_stable = teacher_logits[:, stable_classes]\n",
    "                distill_loss = F.mse_loss(student_stable, teacher_stable)\n",
    "                total_loss = alpha * distill_loss + (1 - alpha) * ce_loss\n",
    "            else:\n",
    "                total_loss = ce_loss\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += total_loss.item() * xb.size(0)\n",
    "            compute_classwise_accuracy(logits, yb, class_correct, class_total)\n",
    "        \n",
    "        train_loss = epoch_loss / len(train_loader.dataset)\n",
    "        train_acc = {int(k): f\"{(class_correct[k] / class_total[k]) * 100:.2f}%\" \n",
    "                    if class_total[k] > 0 else \"0.00%\" for k in class_total}\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        val_class_correct, val_class_total = {}, {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                outputs = model(xb)\n",
    "                val_loss += criterion(outputs, yb).item() * xb.size(0)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_correct += (preds == yb).sum().item()\n",
    "                val_total += yb.size(0)\n",
    "                compute_classwise_accuracy(outputs, yb, val_class_correct, val_class_total)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct / val_total\n",
    "        val_acc_cls = {int(k): f\"{(val_class_correct[k]/val_class_total[k])*100:.2f}%\" \n",
    "                      if val_class_total[k] > 0 else \"0.00%\" for k in val_class_total}\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Train-Class-Acc: {train_acc}\")\n",
    "        print(f\"Val Loss: {val_loss:.6f}, Val Acc: {val_acc*100:.2f}%, Val-Class-Acc: {val_acc_cls}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Save model checkpoint\n",
    "        model_path = os.path.join(model_saving_folder, f\"{model_name}_epoch_{epoch+1}.pth\")\n",
    "        current = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_acc,\n",
    "            'train_classwise_accuracy': train_acc,\n",
    "            'val_classwise_accuracy': val_acc_cls,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "            'model_path': model_path,\n",
    "            'num_lora_groups': model.count_lora_groups(),\n",
    "            'related_labels': related_labels\n",
    "        }\n",
    "        \n",
    "        # Keep top 5 best models\n",
    "        if len(best_results) < 5 or val_acc > best_results[-1]['val_accuracy']:\n",
    "            if len(best_results) == 5:\n",
    "                to_remove = best_results.pop()\n",
    "                if os.path.exists(to_remove['model_path']):\n",
    "                    os.remove(to_remove['model_path'])\n",
    "                    print(f\"üóë Removed: {to_remove['model_path']}\")\n",
    "            best_results.append(current)\n",
    "            best_results.sort(key=lambda x: (x['val_accuracy'], x['epoch']), reverse=True)\n",
    "            torch.save(current, model_path)\n",
    "            print(f\"‚úÖ Saved model: {model_path}\")\n",
    "        \n",
    "        # Update learning rate scheduler if provided\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "    \n",
    "    # End of training\n",
    "    elapsed_time = time.time() - start_time\n",
    "    total_params, param_size_MB = get_model_parameter_info(model)\n",
    "    \n",
    "    # Save best model\n",
    "    if best_results:\n",
    "        best = best_results[0]\n",
    "        best_model_path = os.path.join(model_saving_folder, f\"{model_name}_best.pth\")\n",
    "        torch.save(best, best_model_path)\n",
    "        print(f\"\\nüèÜ Best model saved as: {best_model_path} (Val Accuracy: {best['val_accuracy'] * 100:.2f}%)\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = os.path.join(model_saving_folder, f\"{model_name}_final.pth\")\n",
    "    torch.save(current, final_model_path)\n",
    "    print(f\"\\nüìå Final model saved as: {final_model_path}\")\n",
    "    \n",
    "    # Print top 5 models\n",
    "    print(\"\\nüéØ Top 5 Best Models:\")\n",
    "    for res in best_results:\n",
    "        print(f\"Epoch {res['epoch']}, Train Loss: {res['train_loss']:.6f}, Train-Acc: {res['train_classwise_accuracy']},\\n\"\n",
    "              f\"Val Loss: {res['val_loss']:.6f}, Val Acc: {res['val_accuracy']*100:.2f}%, Val-Acc: {res['val_classwise_accuracy']}, \"\n",
    "              f\"Model Path: {res['model_path']}\")\n",
    "    \n",
    "    # Print model summary\n",
    "    print(f\"\\nüß† Model Summary:\")\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    print(f\"Model Size (float32): {param_size_MB:.2f} MB\")\n",
    "    print(f\"Number of LoRA adapters: {model.count_lora_adapters()}\")\n",
    "    print(f\"Number of LoRA groups: {model.count_lora_groups()}\")\n",
    "    print(f\"Total Training Time: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    # Extract period number from folder name\n",
    "    match = re.search(r'Period_(\\d+)', model_saving_folder)\n",
    "    period_label = match.group(1) if match else str(period)\n",
    "    model_name_str = model.__class__.__name__\n",
    "    \n",
    "    # Print markdown summary\n",
    "    best_model = max(best_results, key=lambda x: x['val_accuracy'])\n",
    "    print(f\"\"\"\n",
    "---\n",
    "### Period {period_label} (alpha = {alpha}, similarity_threshold = {similarity_threshold})\n",
    "+ ##### Total training time: {elapsed_time:.2f} seconds\n",
    "+ ##### Model: {model_name_str}\n",
    "+ ##### Training and saving in *'{model_saving_folder}'*\n",
    "+ ##### Best Epoch: {best_model['epoch']}\n",
    "#### __Val Accuracy: {best_model['val_accuracy'] * 100:.2f}%__\n",
    "#### __Val-Class-Acc: {best_model['val_classwise_accuracy']}__\n",
    "#### __Total Parameters: {total_params:,}__\n",
    "#### __Model Size (float32): {param_size_MB:.2f} MB__\n",
    "#### __Number of LoRA adapters: {model.count_lora_adapters()}__\n",
    "#### __Number of LoRA groups: {model.count_lora_groups()}__\n",
    "\"\"\".strip())\n",
    "    \n",
    "    # Save class features for next period\n",
    "    if class_features_dict is None:\n",
    "        class_features_dict = {}\n",
    "    class_features_dict.update(new_class_features)\n",
    "    with open(os.path.join(model_saving_folder, \"class_features.pkl\"), 'wb') as f:\n",
    "        pickle.dump(class_features_dict, f)\n",
    "    print(f\"\\nSaved class features to: {os.path.join(model_saving_folder, 'class_features.pkl')}\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def extract_features(model, x):\n",
    "    \"\"\"Helper function to extract features from the model for similarity calculation\"\"\"\n",
    "    # This is a placeholder - you'll need to adapt this based on your actual model architecture\n",
    "    # The goal is to extract meaningful features before the classification layer\n",
    "    # For ResNet18_1D_LoRA model, this would typically be the features right before the fc layer\n",
    "    \n",
    "    # Example (pseudo-code - adapt to your actual model):\n",
    "    x = x.permute(0, 2, 1)  # Convert to (batch_size, channels, time_steps)\n",
    "    \n",
    "    # Feed through the network up to the point before classification\n",
    "    with torch.no_grad():\n",
    "        x = model.conv1(x)\n",
    "        x = model.bn1(x)\n",
    "        x = model.relu(x)\n",
    "        x = model.maxpool(x)\n",
    "        \n",
    "        x = model.layer1(x)\n",
    "        x = model.layer2(x)\n",
    "        x = model.layer3(x)\n",
    "        x = model.layer4(x)\n",
    "        \n",
    "        # Apply pooling\n",
    "        x1 = model.adaptiveavgpool(x)\n",
    "        x2 = model.adaptivemaxpool(x)\n",
    "        \n",
    "        # Concatenate pooling results\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        \n",
    "        # Flatten\n",
    "        features = x.view(x.size(0), -1)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3282b8f5",
   "metadata": {},
   "source": [
    "#### v3 pure optimizer, all freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e422876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_dynex_clora_ecg(model, teacher_model, output_size, criterion, optimizer,\n",
    "                           X_train, y_train, X_val, y_val,\n",
    "                           num_epochs, batch_size, alpha,\n",
    "                           model_saving_folder, model_name,\n",
    "                           stop_signal_file=None, scheduler=None,\n",
    "                           period=None, stable_classes=None,\n",
    "                           similarity_threshold=0.0,\n",
    "                           class_features_dict=None, related_labels=None, device=None):\n",
    "    \n",
    "    print(f\"\\nüöÄ 'train_with_dynex_clora_ecg' started for Period {period}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_name = model_name or 'dynex_clora_model'\n",
    "    model_saving_folder = model_saving_folder or './saved_models'\n",
    "    \n",
    "    if os.path.exists(model_saving_folder):\n",
    "        shutil.rmtree(model_saving_folder)\n",
    "        print(f\"‚úÖ Removed existing folder: {model_saving_folder}\")\n",
    "    os.makedirs(model_saving_folder, exist_ok=True)\n",
    "    \n",
    "    device = device or auto_select_cuda_device()\n",
    "    model.to(device)\n",
    "    \n",
    "    if teacher_model:\n",
    "        teacher_model.to(device)\n",
    "        teacher_model.eval()\n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(\"\\n‚úÖ Data Overview:\")\n",
    "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "    \n",
    "    best_results = []\n",
    "    \n",
    "    # === Class Feature Extraction ===\n",
    "    model.eval()\n",
    "    new_class_features = {}\n",
    "    \n",
    "    # Extract features for current classes\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in train_loader:\n",
    "            # Get feature representations\n",
    "            features = extract_features(model, xb)\n",
    "            for cls in torch.unique(yb):\n",
    "                cls_mask = (yb == cls)\n",
    "                cls_feat = features[cls_mask]\n",
    "                if cls.item() not in new_class_features:\n",
    "                    new_class_features[cls.item()] = []\n",
    "                new_class_features[cls.item()].append(cls_feat)\n",
    "    \n",
    "    # Average features per class\n",
    "    for cls in new_class_features:\n",
    "        new_class_features[cls] = torch.cat(new_class_features[cls], dim=0).mean(dim=0)\n",
    "    \n",
    "    # Initialize related_labels if not provided\n",
    "    if related_labels is None:\n",
    "        related_labels = {}\n",
    "    \n",
    "    # === Similarity Computation (only for Period > 1) ===\n",
    "    if period > 1 and class_features_dict:\n",
    "        cosine_sim = torch.nn.CosineSimilarity(dim=0)\n",
    "        similarity_scores = {}\n",
    "        \n",
    "        # Calculate similarities between new and old classes\n",
    "        for new_label, new_feat in new_class_features.items():\n",
    "            similarity_scores[new_label] = {}\n",
    "            for old_label, old_feat in class_features_dict.items():\n",
    "                sim = cosine_sim(new_feat.to(device), old_feat.to(device)).item()\n",
    "                similarity_scores[new_label][old_label] = sim\n",
    "        \n",
    "        # Print similarity information\n",
    "        print(\"\\nüîé Similarity Analysis:\")\n",
    "        print(f\"  Similarity threshold: {similarity_threshold:.4f}\")\n",
    "        print(f\"  Existing classes: {sorted(list(class_features_dict.keys()))}\")\n",
    "        print(f\"  Current classes: {sorted(list(new_class_features.keys()))}\")\n",
    "        \n",
    "        # Calculate new classes (classes not in previous periods)\n",
    "        existing_classes = set(class_features_dict.keys())\n",
    "        current_classes = set(new_class_features.keys())\n",
    "        new_classes = current_classes - existing_classes\n",
    "        print(f\"  New classes: {sorted(list(new_classes))}\")\n",
    "        \n",
    "        # Print similarity scores\n",
    "        print(\"\\nüìä Similarity Scores:\")\n",
    "        for new_label, scores in similarity_scores.items():\n",
    "            if new_label in new_classes:  # Only show for new classes\n",
    "                print(f\"  New Class {new_label}:\")\n",
    "                if scores:\n",
    "                    for old_label, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
    "                        print(f\"    - Existing Class {old_label}: {score:.4f}\")\n",
    "                else:\n",
    "                    print(\"    - No existing classes to compare\")\n",
    "        \n",
    "        # Calculate average similarity statistics\n",
    "        all_similarities = [s for scores in similarity_scores.values() for s in scores.values()]\n",
    "        if all_similarities:\n",
    "            avg_similarity = np.mean(all_similarities)\n",
    "            std_similarity = np.std(all_similarities)\n",
    "            print(f\"\\n  Average similarity: {avg_similarity:.4f}, Std: {std_similarity:.4f}\")\n",
    "    \n",
    "    # === Period-specific LoRA Management Logic ===\n",
    "    to_unfreeze = set()\n",
    "    \n",
    "    # Special handling for period 1 - no LoRA adapters yet\n",
    "    if period == 1:\n",
    "        if not related_labels:\n",
    "            # Initialize related_labels for first period\n",
    "            # The base conv layers (index 'base') are associated with initial classes\n",
    "            initial_classes = list(new_class_features.keys())\n",
    "            related_labels['base'] = initial_classes\n",
    "            print(f\"\\nüîÑ Initializing related_labels for first period: {related_labels}\")\n",
    "        \n",
    "        # For first period, all base model parameters are trainable\n",
    "        print(\"\\nüîì First period: All model parameters are trainable\")\n",
    "    \n",
    "    # For periods > 1, manage LoRA adapters\n",
    "    elif period > 1 and class_features_dict:\n",
    "        new_lora_indices = []\n",
    "        cosine_sim = torch.nn.CosineSimilarity(dim=0)\n",
    "        \n",
    "        existing_classes = set(class_features_dict.keys())\n",
    "        current_classes = set(new_class_features.keys())\n",
    "        new_classes = current_classes - existing_classes\n",
    "        \n",
    "        print(f\"\\nüß© Managing LoRA adapters for {len(new_classes)} new classes...\")\n",
    "        \n",
    "        # Process each new class\n",
    "        for new_cls in new_classes:\n",
    "            new_feat = new_class_features[new_cls]\n",
    "            \n",
    "            # Calculate similarities to all existing classes\n",
    "            sims = [(old_cls, cosine_sim(new_feat.to(device), class_features_dict[old_cls].to(device)).item())\n",
    "                   for old_cls in class_features_dict]\n",
    "            \n",
    "            # Sort by similarity (highest first)\n",
    "            sims.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Check if new class is similar to any existing class\n",
    "            matched = False\n",
    "            for old_cls, sim in sims:\n",
    "                if sim >= similarity_threshold:\n",
    "                    matched = True\n",
    "                    # Find which adapter/network is associated with this old class\n",
    "                    for adapter_idx, related_cls_list in related_labels.items():\n",
    "                        if old_cls in related_cls_list:\n",
    "                            # Add this new class to the same adapter's related classes\n",
    "                            if new_cls not in related_labels[adapter_idx]:\n",
    "                                related_labels[adapter_idx].append(new_cls)\n",
    "                                print(f\"üîÑ New Class {new_cls} is similar to Class {old_cls} (sim={sim:.4f}) ‚Üí Added to adapter '{adapter_idx}'\")\n",
    "                            \n",
    "                            # Mark this adapter for unfreezing during training\n",
    "                            to_unfreeze.add(adapter_idx)\n",
    "                            break\n",
    "            \n",
    "            # If no match found, create new LoRA adapter\n",
    "            if not matched:\n",
    "                # === Âä†ÂÖ•‰∏ÄÊï¥ÁµÑ LoRA adaptersÔºàÊØèÂ±§‰∏ÄÂÄãÔºâ ===\n",
    "                model.add_lora_adapter()\n",
    "\n",
    "                # Ë®òÈåÑÈÄôÊòØÁ¨¨ÂπæÁµÑÔºàÁî® group indexÔºâ\n",
    "                group_idx = max([k for k in related_labels.keys() if isinstance(k, int)], default=-1) + 1\n",
    "\n",
    "                related_labels[group_idx] = [new_cls]\n",
    "                new_lora_indices.append(group_idx)\n",
    "                print(f\"‚ûï New Class {new_cls} is not similar to any existing class ‚Üí Created new adapter group #{group_idx}\")\n",
    "        \n",
    "        # Check stability of existing classes\n",
    "        print(\"\\nüîç Checking stability of existing classes...\")\n",
    "        for old_cls in existing_classes & current_classes:  # Intersection - classes that exist in both periods\n",
    "            if old_cls in new_class_features:\n",
    "                sim_self = cosine_sim(new_class_features[old_cls].to(device), \n",
    "                                      class_features_dict[old_cls].to(device)).item()\n",
    "                print(f\"  Class {old_cls} similarity with itself: {sim_self:.4f}\")\n",
    "                \n",
    "                # If class representation has drifted too much\n",
    "                if sim_self < similarity_threshold:\n",
    "                    for adapter_idx, related_cls_list in related_labels.items():\n",
    "                        if old_cls in related_cls_list:\n",
    "                            to_unfreeze.add(adapter_idx)\n",
    "                            print(f\"‚ö†Ô∏è Class {old_cls} has drifted (self-sim={sim_self:.4f}) ‚Üí Unfreezing adapter '{adapter_idx}'\")\n",
    "        \n",
    "        # # Freeze all LoRA adapters and conv2 weights\n",
    "        # print(\"\\nüîí Default: Freezing all LoRA adapters and base conv2 weights\")\n",
    "        # for module in model.modules():\n",
    "        #     if isinstance(module, BasicBlock1d_LoRA):\n",
    "        #         # freeze all conv2\n",
    "        #         for param in module.conv2.parameters():\n",
    "        #             param.requires_grad = False\n",
    "        #         # freeze all LoRA inside\n",
    "        #         for adapter in module.lora_adapters:\n",
    "        #             for param in adapter.parameters():\n",
    "        #                 param.requires_grad = False\n",
    "\n",
    "        # üîí Freeze ALL model parameters first\n",
    "        print(\"\\nüîí Default: Freezing ALL model parameters\")\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        ####### Check frozen parameters #######\n",
    "        print(\"\\nüîç Frozen Parameters:\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "        ####### Check frozen parameters #######\n",
    "\n",
    "        # üîì Unfreeze specific adapter groups\n",
    "        print(\"\\nüîì Unfreezing selected adapters (by group):\")\n",
    "        for adapter_group_idx in to_unfreeze:\n",
    "            if isinstance(adapter_group_idx, int):\n",
    "                for module in model.modules():\n",
    "                    if isinstance(module, BasicBlock1d_LoRA):\n",
    "                        if adapter_group_idx < len(module.lora_adapters):\n",
    "                            for param in module.lora_adapters[adapter_group_idx].parameters():\n",
    "                                param.requires_grad = True\n",
    "                print(f\"  - Adapter Group #{adapter_group_idx} (all blocks) (classes: {related_labels.get(adapter_group_idx, [])})\")\n",
    "            elif adapter_group_idx == 'base':\n",
    "                for module in model.modules():\n",
    "                    if isinstance(module, BasicBlock1d_LoRA):\n",
    "                        for p in module.conv2.parameters():\n",
    "                            p.requires_grad = True\n",
    "                print(f\"  - Base layers (classes: {related_labels.get('base', [])})\")\n",
    "\n",
    "        ####### Check frozen parameters #######\n",
    "        print(\"\\nüîç Frozen Parameters: (After Unfreeze specific adapter groups)\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "        ####### Check frozen parameters #######\n",
    "\n",
    "        # üîì Unfreeze newly added adapter groups\n",
    "        for group_idx in new_lora_indices:\n",
    "            block_counter = 0\n",
    "            for module in model.modules():\n",
    "                if isinstance(module, BasicBlock1d_LoRA):\n",
    "                    if group_idx < len(module.lora_adapters):\n",
    "                        adapter = module.lora_adapters[group_idx]\n",
    "                        for param in adapter.parameters():\n",
    "                            param.requires_grad = True\n",
    "                    block_counter += 1\n",
    "            print(f\"  - New adapter group #{group_idx} (classes: {related_labels[group_idx]})\")\n",
    "            \n",
    "        ####### Check frozen parameters #######\n",
    "        print(\"\\nüîç Frozen Parameters (After Unfreeze newly added adapter groups):\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "        ####### Check frozen parameters #######\n",
    "\n",
    "        # Always unfreeze the final layer for all periods\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        ####### Check frozen parameters #######\n",
    "        print(\"\\nüîç Frozen Parameters (After unfreeze the final layer):\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "        ####### Check frozen parameters #######\n",
    "\n",
    "    # Print summary of related_labels\n",
    "    print(f\"\\nüìã Related Labels Summary:\")\n",
    "    for adapter_idx, classes in related_labels.items():\n",
    "        print(f\"  - {'Base network' if adapter_idx == 'base' else f'Adapter #{adapter_idx}'}: Classes {classes}\")\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    print(\"\\nüîß Trainable Parameter Status:\")\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    trainable_count = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            trainable_count += 1\n",
    "            print(f\"  ‚úÖ {name:<50} | shape={list(param.shape)}\")\n",
    "    print(f\"trainable_count: {trainable_count}\")\n",
    "    \n",
    "    frozen_params = total_params - trainable_params\n",
    "    print(f\"\\nüìä Parameter Statistics:\")\n",
    "    print(f\"  - Total parameters: {total_params:,}\")\n",
    "    print(f\"  - Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
    "    print(f\"  - Frozen parameters: {frozen_params:,} ({frozen_params/total_params*100:.2f}%)\")\n",
    "    \n",
    "    # Update optimizer with only trainable parameters\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=optimizer.param_groups[0]['lr'],\n",
    "        weight_decay=optimizer.param_groups[0].get('weight_decay', 0)\n",
    "    )\n",
    "\n",
    "    # ÈáçË®≠ÊâÄÊúâÂèÉÊï∏ÁöÑ requires_grad = True\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    ####### Check frozen parameters #######\n",
    "    print(\"\\nüîç Frozen Parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "    ####### Check frozen parameters #######\n",
    "\n",
    "    # Ê™¢Êü•ÊúÄÁµÇ optimizer ÊéßÂà∂‰∫ÜÂì™‰∫õÂèÉÊï∏\n",
    "    optimizer_param_ids = set(id(p) for group in optimizer.param_groups for p in group['params'])\n",
    "\n",
    "    named_params = list(model.named_parameters())\n",
    "    print(f\"\\nüß† Parameters currently controlled by the optimizer: ({len(named_params)})\")\n",
    "    for name, param in named_params:\n",
    "        if id(param) in optimizer_param_ids:\n",
    "            print(f\"  ‚úÖ {name}\")\n",
    "        else:\n",
    "            print(f\"  ‚õî {name} (NOT included in optimizer)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Starting training...\\n\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        if stop_signal_file and os.path.exists(stop_signal_file):\n",
    "            print(\"\\nüõë Stop signal detected. Exiting training loop.\")\n",
    "            break\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        class_correct, class_total = {}, {}\n",
    "        \n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            ce_loss = criterion(logits, yb)\n",
    "            \n",
    "            # Apply distillation if teacher model is provided\n",
    "            if teacher_model and stable_classes:\n",
    "                with torch.no_grad():\n",
    "                    teacher_logits = teacher_model(xb)\n",
    "                student_stable = logits[:, stable_classes]\n",
    "                teacher_stable = teacher_logits[:, stable_classes]\n",
    "                distill_loss = F.mse_loss(student_stable, teacher_stable)\n",
    "                total_loss = alpha * distill_loss + (1 - alpha) * ce_loss\n",
    "            else:\n",
    "                total_loss = ce_loss\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += total_loss.item() * xb.size(0)\n",
    "            compute_classwise_accuracy(logits, yb, class_correct, class_total)\n",
    "        \n",
    "        train_loss = epoch_loss / len(train_loader.dataset)\n",
    "        train_acc = {int(k): f\"{(class_correct[k] / class_total[k]) * 100:.2f}%\" \n",
    "                    if class_total[k] > 0 else \"0.00%\" for k in class_total}\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        val_class_correct, val_class_total = {}, {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                outputs = model(xb)\n",
    "                val_loss += criterion(outputs, yb).item() * xb.size(0)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_correct += (preds == yb).sum().item()\n",
    "                val_total += yb.size(0)\n",
    "                compute_classwise_accuracy(outputs, yb, val_class_correct, val_class_total)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct / val_total\n",
    "        val_acc_cls = {int(k): f\"{(val_class_correct[k]/val_class_total[k])*100:.2f}%\" \n",
    "                      if val_class_total[k] > 0 else \"0.00%\" for k in val_class_total}\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Train-Class-Acc: {train_acc}\")\n",
    "        print(f\"Val Loss: {val_loss:.6f}, Val Acc: {val_acc*100:.2f}%, Val-Class-Acc: {val_acc_cls}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Save model checkpoint\n",
    "        model_path = os.path.join(model_saving_folder, f\"{model_name}_epoch_{epoch+1}.pth\")\n",
    "        current = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_acc,\n",
    "            'train_classwise_accuracy': train_acc,\n",
    "            'val_classwise_accuracy': val_acc_cls,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "            'model_path': model_path,\n",
    "            'num_lora_groups': model.count_lora_groups(),\n",
    "            'related_labels': related_labels\n",
    "        }\n",
    "        \n",
    "        # Keep top 5 best models\n",
    "        if len(best_results) < 5 or val_acc > best_results[-1]['val_accuracy']:\n",
    "            if len(best_results) == 5:\n",
    "                to_remove = best_results.pop()\n",
    "                if os.path.exists(to_remove['model_path']):\n",
    "                    os.remove(to_remove['model_path'])\n",
    "                    print(f\"üóë Removed: {to_remove['model_path']}\")\n",
    "            best_results.append(current)\n",
    "            best_results.sort(key=lambda x: (x['val_accuracy'], x['epoch']), reverse=True)\n",
    "            torch.save(current, model_path)\n",
    "            print(f\"‚úÖ Saved model: {model_path}\")\n",
    "        \n",
    "        # Update learning rate scheduler if provided\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "    \n",
    "    # End of training\n",
    "    elapsed_time = time.time() - start_time\n",
    "    total_params, param_size_MB = get_model_parameter_info(model)\n",
    "    \n",
    "    # Save best model\n",
    "    if best_results:\n",
    "        best = best_results[0]\n",
    "        best_model_path = os.path.join(model_saving_folder, f\"{model_name}_best.pth\")\n",
    "        torch.save(best, best_model_path)\n",
    "        print(f\"\\nüèÜ Best model saved as: {best_model_path} (Val Accuracy: {best['val_accuracy'] * 100:.2f}%)\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = os.path.join(model_saving_folder, f\"{model_name}_final.pth\")\n",
    "    torch.save(current, final_model_path)\n",
    "    print(f\"\\nüìå Final model saved as: {final_model_path}\")\n",
    "    \n",
    "    # Print top 5 models\n",
    "    print(\"\\nüéØ Top 5 Best Models:\")\n",
    "    for res in best_results:\n",
    "        print(f\"Epoch {res['epoch']}, Train Loss: {res['train_loss']:.6f}, Train-Acc: {res['train_classwise_accuracy']},\\n\"\n",
    "              f\"Val Loss: {res['val_loss']:.6f}, Val Acc: {res['val_accuracy']*100:.2f}%, Val-Acc: {res['val_classwise_accuracy']}, \"\n",
    "              f\"Model Path: {res['model_path']}\")\n",
    "    \n",
    "    # Print model summary\n",
    "    print(f\"\\nüß† Model Summary:\")\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    print(f\"Model Size (float32): {param_size_MB:.2f} MB\")\n",
    "    print(f\"Number of LoRA adapters: {model.count_lora_adapters()}\")\n",
    "    print(f\"Number of LoRA groups: {model.count_lora_groups()}\")\n",
    "    print(f\"Total Training Time: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    # Extract period number from folder name\n",
    "    match = re.search(r'Period_(\\d+)', model_saving_folder)\n",
    "    period_label = match.group(1) if match else str(period)\n",
    "    model_name_str = model.__class__.__name__\n",
    "    \n",
    "    # Print markdown summary\n",
    "    best_model = max(best_results, key=lambda x: x['val_accuracy'])\n",
    "    print(f\"\"\"\n",
    "---\n",
    "### Period {period_label} (alpha = {alpha}, similarity_threshold = {similarity_threshold})\n",
    "+ ##### Total training time: {elapsed_time:.2f} seconds\n",
    "+ ##### Model: {model_name_str}\n",
    "+ ##### Training and saving in *'{model_saving_folder}'*\n",
    "+ ##### Best Epoch: {best_model['epoch']}\n",
    "#### __Val Accuracy: {best_model['val_accuracy'] * 100:.2f}%__\n",
    "#### __Val-Class-Acc: {best_model['val_classwise_accuracy']}__\n",
    "#### __Total Parameters: {total_params:,}__\n",
    "#### __Model Size (float32): {param_size_MB:.2f} MB__\n",
    "#### __Number of LoRA adapters: {model.count_lora_adapters()}__\n",
    "#### __Number of LoRA groups: {model.count_lora_groups()}__\n",
    "\"\"\".strip())\n",
    "    \n",
    "    # Save class features for next period\n",
    "    if class_features_dict is None:\n",
    "        class_features_dict = {}\n",
    "    class_features_dict.update(new_class_features)\n",
    "    with open(os.path.join(model_saving_folder, \"class_features.pkl\"), 'wb') as f:\n",
    "        pickle.dump(class_features_dict, f)\n",
    "    print(f\"\\nSaved class features to: {os.path.join(model_saving_folder, 'class_features.pkl')}\")\n",
    "    \n",
    "    # === Manual memory cleanup ===\n",
    "    del X_train, y_train, X_val, y_val\n",
    "    del train_loader, val_loader\n",
    "    del new_class_features, class_features_dict\n",
    "    del related_labels, similarity_scores\n",
    "    del model_path, best_model_path, final_model_path\n",
    "    del epoch_loss, val_loss, val_correct, val_total\n",
    "    del val_class_correct, val_class_total, preds\n",
    "    del best_model, best_results\n",
    "    del optimizer, scheduler, criterion\n",
    "    del model, model_saving_folder, period_label, device\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"üßπ Cleaned up all training memory.\")\n",
    "\n",
    "\n",
    "def extract_features(model, x):\n",
    "    \"\"\"Helper function to extract features from the model for similarity calculation\"\"\"\n",
    "    # This is a placeholder - you'll need to adapt this based on your actual model architecture\n",
    "    # The goal is to extract meaningful features before the classification layer\n",
    "    # For ResNet18_1D_LoRA model, this would typically be the features right before the fc layer\n",
    "    \n",
    "    # Example (pseudo-code - adapt to your actual model):\n",
    "    x = x.permute(0, 2, 1)  # Convert to (batch_size, channels, time_steps)\n",
    "    \n",
    "    # Feed through the network up to the point before classification\n",
    "    with torch.no_grad():\n",
    "        x = model.conv1(x)\n",
    "        x = model.bn1(x)\n",
    "        x = model.relu(x)\n",
    "        x = model.maxpool(x)\n",
    "        \n",
    "        x = model.layer1(x)\n",
    "        x = model.layer2(x)\n",
    "        x = model.layer3(x)\n",
    "        x = model.layer4(x)\n",
    "        \n",
    "        # Apply pooling\n",
    "        x1 = model.adaptiveavgpool(x)\n",
    "        x2 = model.adaptivemaxpool(x)\n",
    "        \n",
    "        # Concatenate pooling results\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        \n",
    "        # Flatten\n",
    "        features = x.view(x.size(0), -1)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1085b2",
   "metadata": {},
   "source": [
    "## __Training__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8ee5f8",
   "metadata": {},
   "source": [
    "### Period 1 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c948b89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n",
      "ResNet18_1D_LoRA(\n",
      "  (conv1): Conv1d(12, 64, kernel_size=(15,), stride=(2,), padding=(7,), bias=False)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (lora_adapters): ModuleList()\n",
      "    )\n",
      "    (1): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (lora_adapters): ModuleList()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (lora_adapters): ModuleList()\n",
      "    )\n",
      "    (1): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (lora_adapters): ModuleList()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (lora_adapters): ModuleList()\n",
      "    )\n",
      "    (1): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (lora_adapters): ModuleList()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (lora_adapters): ModuleList()\n",
      "    )\n",
      "    (1): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (lora_adapters): ModuleList()\n",
      "    )\n",
      "  )\n",
      "  (adaptiveavgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (adaptivemaxpool): AdaptiveMaxPool1d(output_size=1)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "üì¶ Model Summary from: Class_Incremental_CL/CPSC_CIL/ResNet18_Selection/ResNet18_big_inplane_v1/ResNet18_big_inplane_1D_best.pth\n",
      "üìå Epoch: 63\n",
      "üßÆ Train Loss: 0.007664\n",
      "üéØ Val Loss: 0.800983\n",
      "‚úÖ Val Accuracy: 88.86%\n",
      "üìé Learning Rate: 0.0006561000000000001\n",
      "üìÅ Stored Model Path: Class_Incremental_CL/CPSC_CIL/ResNet18_Selection/ResNet18_big_inplane_v1/ResNet18_big_inplane_1D_epoch_63.pth\n",
      "üß† Total Parameters: 3,857,026\n",
      "üìè Model Size (float32): 14.71 MB\n",
      "\n",
      "üìä Train Class-wise Accuracy:\n",
      "  ‚îî‚îÄ Class 0 : 99.86%\n",
      "  ‚îî‚îÄ Class 1 : 99.59%\n",
      "\n",
      "üìä Val Class-wise Accuracy:\n",
      "  ‚îî‚îÄ Class 0 : 91.85%\n",
      "  ‚îî‚îÄ Class 1 : 85.87%\n",
      "\n",
      "---\n",
      "### Period 1 Summary (Markdown Format)\n",
      "+ **Epoch:** 63\n",
      "+ **Train Loss:** 0.0076635455248283595\n",
      "+ **Val Loss:** 0.800983331773592\n",
      "+ **Val Accuracy:** 88.86%\n",
      "+ **Learning Rate:** 0.0006561000000000001\n",
      "+ **Stored Model Path:** `Class_Incremental_CL/CPSC_CIL/ResNet18_Selection/ResNet18_big_inplane_v1/ResNet18_big_inplane_1D_epoch_63.pth`\n",
      "+ **Total Parameters:** 3,857,026\n",
      "+ **Model Size (float32):** 14.71 MB\n",
      "+ **Train-Class-Acc:** {0: '99.86%', 1: '99.59%'}\n",
      "+ **Val-Class-Acc:** {0: '91.85%', 1: '85.87%'}\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1514991/2299765635.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "def display_model_summary_with_params(model_folder, model_filename=\"ResNet18_big_inplane_1D_best.pth\", input_channels=12, output_size=10):\n",
    "    model_path = os.path.join(model_folder, model_filename)\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"‚ùå File not found: {model_path}\")\n",
    "        return\n",
    "\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "    # === ÈÇÑÂéüÊ®°Âûã‰∏¶ËºâÂÖ•ÂèÉÊï∏ ===\n",
    "    model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    total_params, param_size_MB = get_model_parameter_info(model)\n",
    "\n",
    "    # === È°ØÁ§∫ÊëòË¶Å ===\n",
    "    epoch = checkpoint.get(\"epoch\", \"?\")\n",
    "    train_loss = checkpoint.get(\"train_loss\", \"?\")\n",
    "    val_loss = checkpoint.get(\"val_loss\", \"?\")\n",
    "    val_acc = checkpoint.get(\"val_accuracy\", \"?\")\n",
    "    train_acc_dict = checkpoint.get(\"train_classwise_accuracy\", {})\n",
    "    val_acc_dict = checkpoint.get(\"val_classwise_accuracy\", {})\n",
    "    lr = checkpoint.get(\"learning_rate\", \"?\")\n",
    "    stored_path = checkpoint.get(\"model_path\", \"N/A\")\n",
    "    print(f\"Model Architecture:\")\n",
    "    print(model)\n",
    "    print(f\"\\nüì¶ Model Summary from: {model_path}\")\n",
    "    print(f\"üìå Epoch: {epoch}\")\n",
    "    print(f\"üßÆ Train Loss: {train_loss:.6f}\" if isinstance(train_loss, float) else f\"üßÆ Train Loss: {train_loss}\")\n",
    "    print(f\"üéØ Val Loss: {val_loss:.6f}\" if isinstance(val_loss, float) else f\"üéØ Val Loss: {val_loss}\")\n",
    "    print(f\"‚úÖ Val Accuracy: {val_acc*100:.2f}%\" if isinstance(val_acc, float) else f\"‚úÖ Val Accuracy: {val_acc}\")\n",
    "    print(f\"üìé Learning Rate: {lr}\")\n",
    "    print(f\"üìÅ Stored Model Path: {stored_path}\")\n",
    "    print(f\"üß† Total Parameters: {total_params:,}\")\n",
    "    print(f\"üìè Model Size (float32): {param_size_MB:.2f} MB\")\n",
    "\n",
    "    print(\"\\nüìä Train Class-wise Accuracy:\")\n",
    "    for c, acc in train_acc_dict.items():\n",
    "        print(f\"  ‚îî‚îÄ Class {c:<2}: {acc}\")\n",
    "\n",
    "    print(\"\\nüìä Val Class-wise Accuracy:\")\n",
    "    for c, acc in val_acc_dict.items():\n",
    "        print(f\"  ‚îî‚îÄ Class {c:<2}: {acc}\")\n",
    "\n",
    "    print(\"\\n---\\n### Period 1 Summary (Markdown Format)\")\n",
    "    print(f\"+ **Epoch:** {epoch}\")\n",
    "    print(f\"+ **Train Loss:** {train_loss}\")\n",
    "    print(f\"+ **Val Loss:** {val_loss}\")\n",
    "    print(f\"+ **Val Accuracy:** {val_acc*100:.2f}%\" if isinstance(val_acc, float) else f\"+ **Val Accuracy:** {val_acc}\")\n",
    "    print(f\"+ **Learning Rate:** {lr}\")\n",
    "    print(f\"+ **Stored Model Path:** `{stored_path}`\")\n",
    "    print(f\"+ **Total Parameters:** {total_params:,}\")\n",
    "    print(f\"+ **Model Size (float32):** {param_size_MB:.2f} MB\")\n",
    "    print(f\"+ **Train-Class-Acc:** {train_acc_dict}\")\n",
    "    print(f\"+ **Val-Class-Acc:** {val_acc_dict}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# Example call:\n",
    "display_model_summary_with_params(\n",
    "    model_folder=os.path.join(\"Class_Incremental_CL\", \"CPSC_CIL\", \"ResNet18_Selection\", \"ResNet18_big_inplane_v1\"),\n",
    "    input_channels=12,\n",
    "    output_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a690ca",
   "metadata": {},
   "source": [
    "### Generate class features (period1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bf72fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 1\n",
      "    - Memory Used    : 18 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1514991/3142115414.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location='cpu')\n",
      "Extracting Features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [00:00<00:00, 59.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved class_features_dict to: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/ResNet18_Selection/ResNet18_big_inplane_v1/class_features.pkl\n"
     ]
    }
   ],
   "source": [
    "def generate_class_features_period1(\n",
    "    model_path: str,\n",
    "    save_path: str,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    input_channels: int = 12,\n",
    "    output_size: int = 2,\n",
    "    batch_size: int = 64\n",
    "):\n",
    "    # === ËºâÂÖ•Ê®°Âûã ===\n",
    "    device = auto_select_cuda_device()\n",
    "    model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size)\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # === Ë£Ω‰Ωú Dataloader ===\n",
    "    X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    dataloader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # === ÈñãÂßãËêÉÂèñÁâπÂæµ ===\n",
    "    class_features_dict = {}\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            features = extract_features(model, xb)  # shape: [B, F]\n",
    "            for cls in torch.unique(yb):\n",
    "                cls_mask = (yb == cls)\n",
    "                cls_feat = features[cls_mask]\n",
    "                cls_id = cls.item()\n",
    "                if cls_id not in class_features_dict:\n",
    "                    class_features_dict[cls_id] = []\n",
    "                class_features_dict[cls_id].append(cls_feat.cpu())\n",
    "\n",
    "    # === Âπ≥ÂùáÊØèÂÄã class ÁöÑ feature ÂêëÈáè ===\n",
    "    for cls in class_features_dict:\n",
    "        class_features_dict[cls] = torch.cat(class_features_dict[cls], dim=0).mean(dim=0)\n",
    "\n",
    "    # === ÂÑ≤Â≠òÁÇ∫ .pkl Êñπ‰æø Period 2 ËºâÂÖ• ===\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(class_features_dict, f)\n",
    "    \n",
    "    print(f\"‚úÖ Saved class_features_dict to: {save_path}\")\n",
    "    return class_features_dict\n",
    "\n",
    "\n",
    "model_path = os.path.join(BASE_DIR, \"ResNet18_Selection\", \"ResNet18_big_inplane_v1\", \"ResNet18_big_inplane_1D_best.pth\")\n",
    "save_path = os.path.join(BASE_DIR, \"ResNet18_Selection\", \"ResNet18_big_inplane_v1\", \"class_features.pkl\")\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{1}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{1}.npy\"))\n",
    "\n",
    "class_features_dict = generate_class_features_period1(\n",
    "    model_path=model_path,\n",
    "    save_path=save_path,\n",
    "    X_train=X_train,  # ‰Ω†Âæû npy ËºâÂÖ•ÁöÑ Period 1 Ë≥áÊñô\n",
    "    y_train=y_train,\n",
    "    input_channels=12,\n",
    "    output_size=2,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e2f418",
   "metadata": {},
   "source": [
    "### Period 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6e0b93",
   "metadata": {},
   "source": [
    "#### v1 pure grasient = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760dc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 2\n",
      "    - Memory Used    : 375 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ Loaded class features from: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/ResNet18_Selection/ResNet18_big_inplane_v1/class_features.pkl\n",
      "üîÑ Number of LoRA groups: 0\n",
      "üîç Not loaded: fc.weight, shape=torch.Size([4, 1024])\n",
      "üîç Not loaded: fc.bias, shape=torch.Size([4])\n",
      "‚úÖ Loaded shared weights from Period 1 (excluding FC & LoRA)\n",
      "\n",
      "üöÄ 'train_with_dynex_clora_ecg' started for Period 2\n",
      "\n",
      "‚úÖ Removed existing folder: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1514991/1892791193.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Data Overview:\n",
      "X_train: torch.Size([3263, 5000, 12]), y_train: torch.Size([3263])\n",
      "X_val: torch.Size([816, 5000, 12]), y_val: torch.Size([816])\n",
      "\n",
      "üîé Similarity Analysis:\n",
      "  Similarity threshold: 0.9900\n",
      "  Existing classes: [0, 1]\n",
      "  Current classes: [0, 1, 2, 3]\n",
      "  New classes: [2, 3]\n",
      "\n",
      "üìä Similarity Scores:\n",
      "  New Class 2:\n",
      "    - Existing Class 1: 0.9665\n",
      "    - Existing Class 0: 0.8415\n",
      "  New Class 3:\n",
      "    - Existing Class 1: 0.9912\n",
      "    - Existing Class 0: 0.8097\n",
      "\n",
      "  Average similarity: 0.9055, Std: 0.0837\n",
      "\n",
      "üß© Managing LoRA adapters for 2 new classes...\n",
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n",
      "‚ûï New Class 2 is not similar to any existing class ‚Üí Created new adapter group #0\n",
      "üîÑ New Class 3 is similar to Class 1 (sim=0.9912) ‚Üí Added to adapter 'base'\n",
      "\n",
      "üîç Checking stability of existing classes...\n",
      "  Class 0 similarity with itself: 0.9998\n",
      "  Class 1 similarity with itself: 0.9948\n",
      "\n",
      "üîí Default: Freezing ALL model parameters\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå fc.weight                                          | shape=[4, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[4]\n",
      "\n",
      "üîì Unfreezing selected adapters (by group):\n",
      "  - Base layers (classes: [0, 1, 3])\n",
      "\n",
      "üîç Frozen Parameters: (After Unfreeze specific adapter groups)\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå fc.weight                                          | shape=[4, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[4]\n",
      "  - New adapter group #0 (classes: [2])\n",
      "\n",
      "üîç Frozen Parameters (After Unfreeze newly added adapter groups):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå fc.weight                                          | shape=[4, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[4]\n",
      "\n",
      "üîç Frozen Parameters (After unfreeze the final layer):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "\n",
      "üìã Related Labels Summary:\n",
      "  - Base network: Classes [0, 1, 3]\n",
      "  - Adapter #0: Classes [2]\n",
      "\n",
      "üîß Trainable Parameter Status:\n",
      "  ‚úÖ layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ fc.weight                                          | shape=[4, 1024]\n",
      "  ‚úÖ fc.bias                                            | shape=[4]\n",
      "trainable_count: 26\n",
      "\n",
      "üìä Parameter Statistics:\n",
      "  - Total parameters: 3,889,796\n",
      "  - Trainable parameters: 2,123,780 (54.60%)\n",
      "  - Frozen parameters: 1,766,016 (45.40%)\n",
      "\n",
      "üß† Parameters currently controlled by the optimizer: (78)\n",
      "  ‚úÖ conv1.weight\n",
      "  ‚úÖ bn1.weight\n",
      "  ‚úÖ bn1.bias\n",
      "  ‚úÖ layer1.0.conv1.weight\n",
      "  ‚úÖ layer1.0.bn1.weight\n",
      "  ‚úÖ layer1.0.bn1.bias\n",
      "  ‚úÖ layer1.0.conv2.weight\n",
      "  ‚úÖ layer1.0.bn2.weight\n",
      "  ‚úÖ layer1.0.bn2.bias\n",
      "  ‚õî layer1.0.lora_adapters.0.lora_A (NOT included in optimizer)\n",
      "  ‚õî layer1.0.lora_adapters.0.lora_B (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.conv1.weight\n",
      "  ‚úÖ layer1.1.bn1.weight\n",
      "  ‚úÖ layer1.1.bn1.bias\n",
      "  ‚úÖ layer1.1.conv2.weight\n",
      "  ‚úÖ layer1.1.bn2.weight\n",
      "  ‚úÖ layer1.1.bn2.bias\n",
      "  ‚õî layer1.1.lora_adapters.0.lora_A (NOT included in optimizer)\n",
      "  ‚õî layer1.1.lora_adapters.0.lora_B (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.conv1.weight\n",
      "  ‚úÖ layer2.0.bn1.weight\n",
      "  ‚úÖ layer2.0.bn1.bias\n",
      "  ‚úÖ layer2.0.conv2.weight\n",
      "  ‚úÖ layer2.0.bn2.weight\n",
      "  ‚úÖ layer2.0.bn2.bias\n",
      "  ‚úÖ layer2.0.downsample.0.weight\n",
      "  ‚úÖ layer2.0.downsample.1.weight\n",
      "  ‚úÖ layer2.0.downsample.1.bias\n",
      "  ‚õî layer2.0.lora_adapters.0.lora_A (NOT included in optimizer)\n",
      "  ‚õî layer2.0.lora_adapters.0.lora_B (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.conv1.weight\n",
      "  ‚úÖ layer2.1.bn1.weight\n",
      "  ‚úÖ layer2.1.bn1.bias\n",
      "  ‚úÖ layer2.1.conv2.weight\n",
      "  ‚úÖ layer2.1.bn2.weight\n",
      "  ‚úÖ layer2.1.bn2.bias\n",
      "  ‚õî layer2.1.lora_adapters.0.lora_A (NOT included in optimizer)\n",
      "  ‚õî layer2.1.lora_adapters.0.lora_B (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.conv1.weight\n",
      "  ‚úÖ layer3.0.bn1.weight\n",
      "  ‚úÖ layer3.0.bn1.bias\n",
      "  ‚úÖ layer3.0.conv2.weight\n",
      "  ‚úÖ layer3.0.bn2.weight\n",
      "  ‚úÖ layer3.0.bn2.bias\n",
      "  ‚úÖ layer3.0.downsample.0.weight\n",
      "  ‚úÖ layer3.0.downsample.1.weight\n",
      "  ‚úÖ layer3.0.downsample.1.bias\n",
      "  ‚õî layer3.0.lora_adapters.0.lora_A (NOT included in optimizer)\n",
      "  ‚õî layer3.0.lora_adapters.0.lora_B (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.conv1.weight\n",
      "  ‚úÖ layer3.1.bn1.weight\n",
      "  ‚úÖ layer3.1.bn1.bias\n",
      "  ‚úÖ layer3.1.conv2.weight\n",
      "  ‚úÖ layer3.1.bn2.weight\n",
      "  ‚úÖ layer3.1.bn2.bias\n",
      "  ‚õî layer3.1.lora_adapters.0.lora_A (NOT included in optimizer)\n",
      "  ‚õî layer3.1.lora_adapters.0.lora_B (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.conv1.weight\n",
      "  ‚úÖ layer4.0.bn1.weight\n",
      "  ‚úÖ layer4.0.bn1.bias\n",
      "  ‚úÖ layer4.0.conv2.weight\n",
      "  ‚úÖ layer4.0.bn2.weight\n",
      "  ‚úÖ layer4.0.bn2.bias\n",
      "  ‚úÖ layer4.0.downsample.0.weight\n",
      "  ‚úÖ layer4.0.downsample.1.weight\n",
      "  ‚úÖ layer4.0.downsample.1.bias\n",
      "  ‚õî layer4.0.lora_adapters.0.lora_A (NOT included in optimizer)\n",
      "  ‚õî layer4.0.lora_adapters.0.lora_B (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.conv1.weight\n",
      "  ‚úÖ layer4.1.bn1.weight\n",
      "  ‚úÖ layer4.1.bn1.bias\n",
      "  ‚úÖ layer4.1.conv2.weight\n",
      "  ‚úÖ layer4.1.bn2.weight\n",
      "  ‚úÖ layer4.1.bn2.bias\n",
      "  ‚õî layer4.1.lora_adapters.0.lora_A (NOT included in optimizer)\n",
      "  ‚õî layer4.1.lora_adapters.0.lora_B (NOT included in optimizer)\n",
      "  ‚úÖ fc.weight\n",
      "  ‚úÖ fc.bias\n",
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/200, Train Loss: 23.417211, Train-Class-Acc: {0: '71.39%', 1: '57.68%', 2: '59.97%', 3: '67.52%'}\n",
      "Val Loss: 6.709059, Val Acc: 79.66%, Val-Class-Acc: {0: '66.30%', 1: '79.51%', 2: '76.39%', 3: '91.80%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_1.pth\n",
      "Epoch 2/200, Train Loss: 10.346132, Train-Class-Acc: {0: '75.34%', 1: '67.01%', 2: '80.24%', 3: '82.27%'}\n",
      "Val Loss: 7.234287, Val Acc: 82.97%, Val-Class-Acc: {0: '74.46%', 1: '86.07%', 2: '82.64%', 3: '86.48%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_2.pth\n",
      "Epoch 3/200, Train Loss: 9.347928, Train-Class-Acc: {0: '77.79%', 1: '70.29%', 2: '82.32%', 3: '83.61%'}\n",
      "Val Loss: 5.888928, Val Acc: 80.88%, Val-Class-Acc: {0: '88.04%', 1: '61.07%', 2: '77.78%', 3: '97.13%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_3.pth\n",
      "Epoch 4/200, Train Loss: 6.621762, Train-Class-Acc: {0: '81.20%', 1: '74.28%', 2: '82.67%', 3: '86.78%'}\n",
      "Val Loss: 4.423857, Val Acc: 85.54%, Val-Class-Acc: {0: '88.04%', 1: '75.00%', 2: '88.89%', 3: '92.21%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_4.pth\n",
      "Epoch 5/200, Train Loss: 4.917291, Train-Class-Acc: {0: '82.15%', 1: '77.66%', 2: '87.00%', 3: '88.52%'}\n",
      "Val Loss: 2.896735, Val Acc: 87.50%, Val-Class-Acc: {0: '82.07%', 1: '85.25%', 2: '83.33%', 3: '96.31%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_5.pth\n",
      "Epoch 6/200, Train Loss: 4.510915, Train-Class-Acc: {0: '83.79%', 1: '80.02%', 2: '87.35%', 3: '88.52%'}\n",
      "Val Loss: 6.038038, Val Acc: 84.68%, Val-Class-Acc: {0: '94.02%', 1: '77.46%', 2: '77.78%', 3: '88.93%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_1.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_6.pth\n",
      "Epoch 7/200, Train Loss: 3.586254, Train-Class-Acc: {0: '82.97%', 1: '81.25%', 2: '87.18%', 3: '91.39%'}\n",
      "Val Loss: 3.246995, Val Acc: 86.64%, Val-Class-Acc: {0: '84.78%', 1: '82.79%', 2: '82.64%', 3: '94.26%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_3.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_7.pth\n",
      "Epoch 8/200, Train Loss: 3.312709, Train-Class-Acc: {0: '86.51%', 1: '82.48%', 2: '89.60%', 3: '92.73%'}\n",
      "Val Loss: 3.417720, Val Acc: 85.17%, Val-Class-Acc: {0: '80.98%', 1: '81.15%', 2: '90.97%', 3: '88.93%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_2.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_8.pth\n",
      "Epoch 9/200, Train Loss: 2.622192, Train-Class-Acc: {0: '88.42%', 1: '83.81%', 2: '90.29%', 3: '91.91%'}\n",
      "Val Loss: 3.316367, Val Acc: 87.87%, Val-Class-Acc: {0: '88.04%', 1: '82.38%', 2: '86.81%', 3: '93.85%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_6.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_9.pth\n",
      "Epoch 10/200, Train Loss: 2.769249, Train-Class-Acc: {0: '87.19%', 1: '83.81%', 2: '89.08%', 3: '92.11%'}\n",
      "Val Loss: 3.540238, Val Acc: 85.91%, Val-Class-Acc: {0: '84.78%', 1: '78.69%', 2: '79.86%', 3: '97.54%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_8.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_10.pth\n",
      "Epoch 11/200, Train Loss: 2.169625, Train-Class-Acc: {0: '87.33%', 1: '86.68%', 2: '90.12%', 3: '94.47%'}\n",
      "Val Loss: 4.011090, Val Acc: 86.76%, Val-Class-Acc: {0: '89.67%', 1: '84.02%', 2: '80.56%', 3: '90.98%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_4.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_11.pth\n",
      "Epoch 12/200, Train Loss: 2.304151, Train-Class-Acc: {0: '88.56%', 1: '86.58%', 2: '90.81%', 3: '92.93%'}\n",
      "Val Loss: 4.280770, Val Acc: 86.27%, Val-Class-Acc: {0: '89.13%', 1: '73.77%', 2: '86.11%', 3: '96.72%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_10.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_12.pth\n",
      "Epoch 13/200, Train Loss: 2.423415, Train-Class-Acc: {0: '87.47%', 1: '85.96%', 2: '90.99%', 3: '93.44%'}\n",
      "Val Loss: 3.541319, Val Acc: 86.76%, Val-Class-Acc: {0: '88.59%', 1: '78.69%', 2: '80.56%', 3: '97.13%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_12.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_13.pth\n",
      "Epoch 14/200, Train Loss: 2.024594, Train-Class-Acc: {0: '92.23%', 1: '89.45%', 2: '92.37%', 3: '94.47%'}\n",
      "Val Loss: 6.854451, Val Acc: 85.42%, Val-Class-Acc: {0: '76.63%', 1: '84.02%', 2: '88.19%', 3: '91.80%'}, LR: 0.001000\n",
      "Epoch 15/200, Train Loss: 1.499443, Train-Class-Acc: {0: '90.74%', 1: '90.37%', 2: '94.28%', 3: '95.49%'}\n",
      "Val Loss: 3.174679, Val Acc: 87.13%, Val-Class-Acc: {0: '89.13%', 1: '78.69%', 2: '88.19%', 3: '93.44%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_7.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_15.pth\n",
      "Epoch 16/200, Train Loss: 1.631403, Train-Class-Acc: {0: '91.42%', 1: '89.04%', 2: '93.76%', 3: '95.70%'}\n",
      "Val Loss: 4.246742, Val Acc: 87.01%, Val-Class-Acc: {0: '88.59%', 1: '81.56%', 2: '84.72%', 3: '92.62%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_11.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_16.pth\n",
      "Epoch 17/200, Train Loss: 1.396328, Train-Class-Acc: {0: '91.55%', 1: '90.27%', 2: '94.80%', 3: '94.67%'}\n",
      "Val Loss: 3.121085, Val Acc: 85.78%, Val-Class-Acc: {0: '83.70%', 1: '77.05%', 2: '86.81%', 3: '95.49%'}, LR: 0.000900\n",
      "Epoch 18/200, Train Loss: 1.205925, Train-Class-Acc: {0: '93.05%', 1: '92.32%', 2: '95.32%', 3: '95.59%'}\n",
      "Val Loss: 5.000877, Val Acc: 87.75%, Val-Class-Acc: {0: '83.70%', 1: '86.48%', 2: '89.58%', 3: '90.98%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_13.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_18.pth\n",
      "Epoch 19/200, Train Loss: 1.572320, Train-Class-Acc: {0: '93.19%', 1: '90.98%', 2: '95.49%', 3: '96.52%'}\n",
      "Val Loss: 3.758833, Val Acc: 87.87%, Val-Class-Acc: {0: '91.85%', 1: '78.28%', 2: '85.42%', 3: '95.90%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_16.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_19.pth\n",
      "Epoch 20/200, Train Loss: 1.474527, Train-Class-Acc: {0: '93.73%', 1: '91.70%', 2: '94.97%', 3: '95.59%'}\n",
      "Val Loss: 3.472934, Val Acc: 87.01%, Val-Class-Acc: {0: '83.70%', 1: '82.38%', 2: '87.50%', 3: '93.85%'}, LR: 0.000900\n",
      "Epoch 21/200, Train Loss: 1.357687, Train-Class-Acc: {0: '92.37%', 1: '92.52%', 2: '93.93%', 3: '96.11%'}\n",
      "Val Loss: 3.367718, Val Acc: 86.89%, Val-Class-Acc: {0: '90.76%', 1: '75.82%', 2: '85.42%', 3: '95.90%'}, LR: 0.000900\n",
      "Epoch 22/200, Train Loss: 1.144209, Train-Class-Acc: {0: '93.60%', 1: '91.50%', 2: '95.84%', 3: '96.21%'}\n",
      "Val Loss: 5.381049, Val Acc: 82.84%, Val-Class-Acc: {0: '67.93%', 1: '79.51%', 2: '84.72%', 3: '96.31%'}, LR: 0.000900\n",
      "Epoch 23/200, Train Loss: 1.003343, Train-Class-Acc: {0: '94.82%', 1: '92.93%', 2: '95.15%', 3: '95.70%'}\n",
      "Val Loss: 4.141609, Val Acc: 86.76%, Val-Class-Acc: {0: '92.39%', 1: '73.77%', 2: '86.11%', 3: '95.90%'}, LR: 0.000900\n",
      "Epoch 24/200, Train Loss: 0.813718, Train-Class-Acc: {0: '95.10%', 1: '94.16%', 2: '96.36%', 3: '97.23%'}\n",
      "Val Loss: 4.230147, Val Acc: 87.13%, Val-Class-Acc: {0: '91.30%', 1: '79.92%', 2: '80.56%', 3: '95.08%'}, LR: 0.000900\n",
      "Epoch 25/200, Train Loss: 0.838490, Train-Class-Acc: {0: '94.41%', 1: '93.24%', 2: '95.84%', 3: '97.34%'}\n",
      "Val Loss: 6.107569, Val Acc: 86.64%, Val-Class-Acc: {0: '90.22%', 1: '83.20%', 2: '84.03%', 3: '88.93%'}, LR: 0.000900\n",
      "Epoch 26/200, Train Loss: 1.106776, Train-Class-Acc: {0: '94.55%', 1: '93.55%', 2: '94.45%', 3: '96.41%'}\n",
      "Val Loss: 6.031222, Val Acc: 85.17%, Val-Class-Acc: {0: '84.24%', 1: '79.51%', 2: '82.64%', 3: '93.03%'}, LR: 0.000900\n",
      "Epoch 27/200, Train Loss: 0.867079, Train-Class-Acc: {0: '94.96%', 1: '93.75%', 2: '96.36%', 3: '97.44%'}\n",
      "Val Loss: 6.296101, Val Acc: 83.70%, Val-Class-Acc: {0: '88.04%', 1: '64.34%', 2: '88.19%', 3: '97.13%'}, LR: 0.000900\n",
      "Epoch 28/200, Train Loss: 0.843037, Train-Class-Acc: {0: '95.23%', 1: '93.95%', 2: '97.05%', 3: '96.82%'}\n",
      "Val Loss: 4.511196, Val Acc: 86.15%, Val-Class-Acc: {0: '83.70%', 1: '75.82%', 2: '90.28%', 3: '95.90%'}, LR: 0.000810\n",
      "Epoch 29/200, Train Loss: 0.655441, Train-Class-Acc: {0: '94.96%', 1: '93.55%', 2: '97.57%', 3: '97.64%'}\n",
      "Val Loss: 3.825146, Val Acc: 85.78%, Val-Class-Acc: {0: '87.50%', 1: '72.95%', 2: '86.81%', 3: '96.72%'}, LR: 0.000810\n",
      "Epoch 30/200, Train Loss: 0.755340, Train-Class-Acc: {0: '95.10%', 1: '95.39%', 2: '96.01%', 3: '97.34%'}\n",
      "Val Loss: 3.904623, Val Acc: 86.27%, Val-Class-Acc: {0: '78.80%', 1: '81.56%', 2: '86.11%', 3: '96.72%'}, LR: 0.000810\n",
      "Epoch 31/200, Train Loss: 0.424487, Train-Class-Acc: {0: '96.05%', 1: '94.47%', 2: '97.92%', 3: '97.85%'}\n",
      "Val Loss: 3.914832, Val Acc: 87.13%, Val-Class-Acc: {0: '85.87%', 1: '79.51%', 2: '88.19%', 3: '95.08%'}, LR: 0.000810\n",
      "Epoch 32/200, Train Loss: 0.324939, Train-Class-Acc: {0: '94.96%', 1: '96.62%', 2: '96.71%', 3: '98.87%'}\n",
      "Val Loss: 3.842302, Val Acc: 86.40%, Val-Class-Acc: {0: '80.43%', 1: '79.92%', 2: '88.19%', 3: '96.31%'}, LR: 0.000810\n",
      "Epoch 33/200, Train Loss: 0.807370, Train-Class-Acc: {0: '97.41%', 1: '95.90%', 2: '97.92%', 3: '98.05%'}\n",
      "Val Loss: 4.347148, Val Acc: 85.42%, Val-Class-Acc: {0: '84.24%', 1: '75.41%', 2: '84.72%', 3: '96.72%'}, LR: 0.000810\n",
      "Epoch 34/200, Train Loss: 0.718449, Train-Class-Acc: {0: '96.05%', 1: '95.18%', 2: '96.36%', 3: '98.05%'}\n",
      "Val Loss: 4.089486, Val Acc: 86.15%, Val-Class-Acc: {0: '88.04%', 1: '75.00%', 2: '87.50%', 3: '95.08%'}, LR: 0.000810\n",
      "Epoch 35/200, Train Loss: 0.663749, Train-Class-Acc: {0: '95.23%', 1: '95.18%', 2: '97.05%', 3: '97.54%'}\n",
      "Val Loss: 3.945200, Val Acc: 85.78%, Val-Class-Acc: {0: '87.50%', 1: '77.05%', 2: '81.94%', 3: '95.49%'}, LR: 0.000810\n",
      "Epoch 36/200, Train Loss: 0.231535, Train-Class-Acc: {0: '97.14%', 1: '97.23%', 2: '97.92%', 3: '99.18%'}\n",
      "Val Loss: 3.843347, Val Acc: 86.76%, Val-Class-Acc: {0: '88.59%', 1: '76.64%', 2: '87.50%', 3: '95.08%'}, LR: 0.000810\n",
      "Epoch 37/200, Train Loss: 0.390594, Train-Class-Acc: {0: '97.14%', 1: '97.03%', 2: '98.61%', 3: '98.77%'}\n",
      "Val Loss: 3.802850, Val Acc: 85.91%, Val-Class-Acc: {0: '88.59%', 1: '75.41%', 2: '83.33%', 3: '95.90%'}, LR: 0.000810\n",
      "Epoch 38/200, Train Loss: 0.353232, Train-Class-Acc: {0: '97.14%', 1: '96.52%', 2: '97.92%', 3: '98.67%'}\n",
      "Val Loss: 4.831292, Val Acc: 87.01%, Val-Class-Acc: {0: '89.13%', 1: '75.41%', 2: '88.89%', 3: '95.90%'}, LR: 0.000810\n",
      "Epoch 39/200, Train Loss: 0.525709, Train-Class-Acc: {0: '97.41%', 1: '96.93%', 2: '97.75%', 3: '97.75%'}\n",
      "Val Loss: 4.136831, Val Acc: 87.01%, Val-Class-Acc: {0: '84.24%', 1: '79.92%', 2: '88.19%', 3: '95.49%'}, LR: 0.000729\n",
      "Epoch 40/200, Train Loss: 0.509913, Train-Class-Acc: {0: '97.96%', 1: '97.23%', 2: '97.92%', 3: '98.46%'}\n",
      "Val Loss: 5.034140, Val Acc: 85.42%, Val-Class-Acc: {0: '90.22%', 1: '70.08%', 2: '86.11%', 3: '96.72%'}, LR: 0.000729\n",
      "Epoch 41/200, Train Loss: 0.228622, Train-Class-Acc: {0: '98.09%', 1: '97.44%', 2: '98.44%', 3: '98.77%'}\n",
      "Val Loss: 4.013663, Val Acc: 86.64%, Val-Class-Acc: {0: '85.87%', 1: '78.69%', 2: '86.11%', 3: '95.49%'}, LR: 0.000729\n",
      "Epoch 42/200, Train Loss: 0.322135, Train-Class-Acc: {0: '96.87%', 1: '97.54%', 2: '98.96%', 3: '98.36%'}\n",
      "Val Loss: 4.533842, Val Acc: 87.50%, Val-Class-Acc: {0: '87.50%', 1: '81.56%', 2: '82.64%', 3: '96.31%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_15.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_42.pth\n",
      "Epoch 43/200, Train Loss: 0.490806, Train-Class-Acc: {0: '97.82%', 1: '96.82%', 2: '98.09%', 3: '99.08%'}\n",
      "Val Loss: 4.097282, Val Acc: 87.62%, Val-Class-Acc: {0: '85.87%', 1: '81.56%', 2: '84.72%', 3: '96.72%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_5.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_43.pth\n",
      "Epoch 44/200, Train Loss: 0.414041, Train-Class-Acc: {0: '97.55%', 1: '97.03%', 2: '98.61%', 3: '98.57%'}\n",
      "Val Loss: 4.696299, Val Acc: 86.52%, Val-Class-Acc: {0: '87.50%', 1: '81.15%', 2: '84.72%', 3: '92.21%'}, LR: 0.000729\n",
      "Epoch 45/200, Train Loss: 0.255078, Train-Class-Acc: {0: '97.28%', 1: '96.82%', 2: '98.44%', 3: '99.08%'}\n",
      "Val Loss: 3.980981, Val Acc: 87.99%, Val-Class-Acc: {0: '91.30%', 1: '79.10%', 2: '87.50%', 3: '94.67%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_42.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_45.pth\n",
      "Epoch 46/200, Train Loss: 0.306638, Train-Class-Acc: {0: '97.96%', 1: '97.54%', 2: '97.23%', 3: '99.08%'}\n",
      "Val Loss: 4.299976, Val Acc: 86.76%, Val-Class-Acc: {0: '82.61%', 1: '81.15%', 2: '86.11%', 3: '95.90%'}, LR: 0.000729\n",
      "Epoch 47/200, Train Loss: 0.332136, Train-Class-Acc: {0: '97.55%', 1: '97.23%', 2: '99.13%', 3: '98.67%'}\n",
      "Val Loss: 5.237145, Val Acc: 86.76%, Val-Class-Acc: {0: '90.22%', 1: '75.00%', 2: '84.72%', 3: '97.13%'}, LR: 0.000729\n",
      "Epoch 48/200, Train Loss: 0.340000, Train-Class-Acc: {0: '97.82%', 1: '96.41%', 2: '98.79%', 3: '98.77%'}\n",
      "Val Loss: 5.849060, Val Acc: 85.66%, Val-Class-Acc: {0: '77.17%', 1: '78.28%', 2: '89.58%', 3: '97.13%'}, LR: 0.000729\n",
      "Epoch 49/200, Train Loss: 0.362657, Train-Class-Acc: {0: '97.41%', 1: '97.03%', 2: '99.13%', 3: '98.87%'}\n",
      "Val Loss: 4.432414, Val Acc: 87.13%, Val-Class-Acc: {0: '87.50%', 1: '77.05%', 2: '87.50%', 3: '96.72%'}, LR: 0.000729\n",
      "Epoch 50/200, Train Loss: 0.316249, Train-Class-Acc: {0: '97.96%', 1: '97.13%', 2: '97.92%', 3: '99.39%'}\n",
      "Val Loss: 4.569543, Val Acc: 87.13%, Val-Class-Acc: {0: '85.33%', 1: '79.10%', 2: '90.28%', 3: '94.67%'}, LR: 0.000656\n",
      "Epoch 51/200, Train Loss: 0.246213, Train-Class-Acc: {0: '97.82%', 1: '97.54%', 2: '98.96%', 3: '99.18%'}\n",
      "Val Loss: 4.221124, Val Acc: 86.40%, Val-Class-Acc: {0: '81.52%', 1: '77.87%', 2: '88.89%', 3: '97.13%'}, LR: 0.000656\n",
      "Epoch 52/200, Train Loss: 0.259312, Train-Class-Acc: {0: '98.37%', 1: '97.95%', 2: '98.27%', 3: '99.39%'}\n",
      "Val Loss: 5.289852, Val Acc: 87.25%, Val-Class-Acc: {0: '84.24%', 1: '86.48%', 2: '83.33%', 3: '92.62%'}, LR: 0.000656\n",
      "Epoch 53/200, Train Loss: 0.320789, Train-Class-Acc: {0: '97.28%', 1: '97.44%', 2: '98.96%', 3: '98.36%'}\n",
      "Val Loss: 4.597102, Val Acc: 86.76%, Val-Class-Acc: {0: '86.96%', 1: '80.74%', 2: '83.33%', 3: '94.67%'}, LR: 0.000656\n",
      "Epoch 54/200, Train Loss: 0.301210, Train-Class-Acc: {0: '99.05%', 1: '97.85%', 2: '98.44%', 3: '98.57%'}\n",
      "Val Loss: 4.812081, Val Acc: 87.25%, Val-Class-Acc: {0: '84.78%', 1: '79.10%', 2: '86.11%', 3: '97.95%'}, LR: 0.000656\n",
      "Epoch 55/200, Train Loss: 0.186578, Train-Class-Acc: {0: '98.09%', 1: '98.26%', 2: '98.27%', 3: '99.59%'}\n",
      "Val Loss: 4.638245, Val Acc: 87.38%, Val-Class-Acc: {0: '91.30%', 1: '75.82%', 2: '86.11%', 3: '96.72%'}, LR: 0.000656\n",
      "Epoch 56/200, Train Loss: 0.259314, Train-Class-Acc: {0: '98.64%', 1: '98.16%', 2: '99.48%', 3: '98.77%'}\n",
      "Val Loss: 4.827869, Val Acc: 86.76%, Val-Class-Acc: {0: '85.87%', 1: '78.28%', 2: '87.50%', 3: '95.49%'}, LR: 0.000656\n",
      "Epoch 57/200, Train Loss: 0.290413, Train-Class-Acc: {0: '97.96%', 1: '97.03%', 2: '97.92%', 3: '99.08%'}\n",
      "Val Loss: 5.622697, Val Acc: 87.25%, Val-Class-Acc: {0: '89.13%', 1: '76.23%', 2: '89.58%', 3: '95.49%'}, LR: 0.000656\n",
      "Epoch 58/200, Train Loss: 0.226692, Train-Class-Acc: {0: '97.68%', 1: '97.95%', 2: '98.79%', 3: '98.87%'}\n",
      "Val Loss: 4.443873, Val Acc: 87.75%, Val-Class-Acc: {0: '90.22%', 1: '77.46%', 2: '86.81%', 3: '96.72%'}, LR: 0.000656\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_43.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_58.pth\n",
      "Epoch 59/200, Train Loss: 0.310255, Train-Class-Acc: {0: '98.09%', 1: '97.54%', 2: '98.44%', 3: '98.87%'}\n",
      "Val Loss: 4.486587, Val Acc: 86.27%, Val-Class-Acc: {0: '89.13%', 1: '74.59%', 2: '84.72%', 3: '96.72%'}, LR: 0.000656\n",
      "Epoch 60/200, Train Loss: 0.148369, Train-Class-Acc: {0: '98.23%', 1: '97.85%', 2: '98.61%', 3: '99.39%'}\n",
      "Val Loss: 4.601184, Val Acc: 86.15%, Val-Class-Acc: {0: '84.24%', 1: '76.64%', 2: '88.89%', 3: '95.49%'}, LR: 0.000656\n",
      "Epoch 61/200, Train Loss: 0.112933, Train-Class-Acc: {0: '98.64%', 1: '98.46%', 2: '99.13%', 3: '98.98%'}\n",
      "Val Loss: 4.171439, Val Acc: 85.54%, Val-Class-Acc: {0: '84.24%', 1: '78.28%', 2: '85.42%', 3: '93.85%'}, LR: 0.000590\n",
      "Epoch 62/200, Train Loss: 0.158668, Train-Class-Acc: {0: '97.82%', 1: '98.26%', 2: '98.61%', 3: '99.39%'}\n",
      "Val Loss: 4.577508, Val Acc: 87.13%, Val-Class-Acc: {0: '84.78%', 1: '81.56%', 2: '86.11%', 3: '95.08%'}, LR: 0.000590\n",
      "Epoch 63/200, Train Loss: 0.226618, Train-Class-Acc: {0: '98.91%', 1: '98.46%', 2: '98.79%', 3: '99.28%'}\n",
      "Val Loss: 4.454747, Val Acc: 86.64%, Val-Class-Acc: {0: '85.33%', 1: '80.74%', 2: '84.72%', 3: '94.67%'}, LR: 0.000590\n",
      "Epoch 64/200, Train Loss: 0.248318, Train-Class-Acc: {0: '98.91%', 1: '98.46%', 2: '98.79%', 3: '99.59%'}\n",
      "Val Loss: 4.686567, Val Acc: 87.50%, Val-Class-Acc: {0: '85.87%', 1: '81.15%', 2: '89.58%', 3: '93.85%'}, LR: 0.000590\n",
      "Epoch 65/200, Train Loss: 0.235960, Train-Class-Acc: {0: '98.23%', 1: '98.05%', 2: '98.61%', 3: '99.08%'}\n",
      "Val Loss: 6.933327, Val Acc: 85.54%, Val-Class-Acc: {0: '84.78%', 1: '73.36%', 2: '87.50%', 3: '97.13%'}, LR: 0.000590\n",
      "Epoch 66/200, Train Loss: 0.188167, Train-Class-Acc: {0: '98.23%', 1: '98.36%', 2: '99.48%', 3: '99.59%'}\n",
      "Val Loss: 4.969900, Val Acc: 87.62%, Val-Class-Acc: {0: '89.67%', 1: '74.18%', 2: '90.97%', 3: '97.54%'}, LR: 0.000590\n",
      "Epoch 67/200, Train Loss: 0.195667, Train-Class-Acc: {0: '99.05%', 1: '98.36%', 2: '98.79%', 3: '99.39%'}\n",
      "Val Loss: 4.625749, Val Acc: 87.13%, Val-Class-Acc: {0: '86.41%', 1: '79.51%', 2: '90.28%', 3: '93.44%'}, LR: 0.000590\n",
      "Epoch 68/200, Train Loss: 0.150650, Train-Class-Acc: {0: '98.50%', 1: '98.36%', 2: '98.27%', 3: '99.18%'}\n",
      "Val Loss: 4.744587, Val Acc: 87.13%, Val-Class-Acc: {0: '91.30%', 1: '75.00%', 2: '86.81%', 3: '96.31%'}, LR: 0.000590\n",
      "Epoch 69/200, Train Loss: 0.186806, Train-Class-Acc: {0: '98.77%', 1: '97.44%', 2: '99.48%', 3: '99.18%'}\n",
      "Val Loss: 4.401258, Val Acc: 86.76%, Val-Class-Acc: {0: '87.50%', 1: '77.46%', 2: '83.33%', 3: '97.54%'}, LR: 0.000590\n",
      "Epoch 70/200, Train Loss: 0.142510, Train-Class-Acc: {0: '98.50%', 1: '98.77%', 2: '98.96%', 3: '98.87%'}\n",
      "Val Loss: 4.151589, Val Acc: 87.38%, Val-Class-Acc: {0: '90.22%', 1: '76.64%', 2: '84.72%', 3: '97.54%'}, LR: 0.000590\n",
      "Epoch 71/200, Train Loss: 0.122719, Train-Class-Acc: {0: '99.59%', 1: '99.08%', 2: '99.31%', 3: '99.59%'}\n",
      "Val Loss: 4.030642, Val Acc: 86.76%, Val-Class-Acc: {0: '86.96%', 1: '77.05%', 2: '86.81%', 3: '96.31%'}, LR: 0.000590\n",
      "Epoch 72/200, Train Loss: 0.089438, Train-Class-Acc: {0: '99.18%', 1: '99.08%', 2: '99.48%', 3: '99.49%'}\n",
      "Val Loss: 4.849601, Val Acc: 86.52%, Val-Class-Acc: {0: '87.50%', 1: '73.77%', 2: '86.81%', 3: '98.36%'}, LR: 0.000531\n",
      "Epoch 73/200, Train Loss: 0.136491, Train-Class-Acc: {0: '99.18%', 1: '98.46%', 2: '99.48%', 3: '99.49%'}\n",
      "Val Loss: 4.713412, Val Acc: 86.64%, Val-Class-Acc: {0: '85.33%', 1: '77.46%', 2: '85.42%', 3: '97.54%'}, LR: 0.000531\n",
      "Epoch 74/200, Train Loss: 0.100366, Train-Class-Acc: {0: '98.64%', 1: '98.67%', 2: '99.65%', 3: '99.39%'}\n",
      "Val Loss: 4.477244, Val Acc: 86.76%, Val-Class-Acc: {0: '85.87%', 1: '78.28%', 2: '84.72%', 3: '97.13%'}, LR: 0.000531\n",
      "Epoch 75/200, Train Loss: 0.086637, Train-Class-Acc: {0: '98.50%', 1: '99.28%', 2: '99.48%', 3: '99.69%'}\n",
      "Val Loss: 4.544057, Val Acc: 87.99%, Val-Class-Acc: {0: '86.96%', 1: '81.15%', 2: '86.11%', 3: '96.72%'}, LR: 0.000531\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_18.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_75.pth\n",
      "Epoch 76/200, Train Loss: 0.077016, Train-Class-Acc: {0: '98.91%', 1: '98.87%', 2: '99.13%', 3: '99.59%'}\n",
      "Val Loss: 4.552934, Val Acc: 87.25%, Val-Class-Acc: {0: '89.67%', 1: '75.41%', 2: '88.19%', 3: '96.72%'}, LR: 0.000531\n",
      "Epoch 77/200, Train Loss: 0.073667, Train-Class-Acc: {0: '99.73%', 1: '99.28%', 2: '99.48%', 3: '99.59%'}\n",
      "Val Loss: 4.651043, Val Acc: 87.38%, Val-Class-Acc: {0: '88.59%', 1: '75.82%', 2: '92.36%', 3: '95.08%'}, LR: 0.000531\n",
      "Epoch 78/200, Train Loss: 0.188587, Train-Class-Acc: {0: '98.64%', 1: '98.57%', 2: '98.79%', 3: '98.98%'}\n",
      "Val Loss: 5.867117, Val Acc: 87.01%, Val-Class-Acc: {0: '88.04%', 1: '82.38%', 2: '88.19%', 3: '90.16%'}, LR: 0.000531\n",
      "Epoch 79/200, Train Loss: 0.256661, Train-Class-Acc: {0: '98.50%', 1: '98.46%', 2: '98.61%', 3: '99.18%'}\n",
      "Val Loss: 5.184738, Val Acc: 87.75%, Val-Class-Acc: {0: '88.59%', 1: '79.92%', 2: '87.50%', 3: '95.08%'}, LR: 0.000531\n",
      "Epoch 80/200, Train Loss: 0.246907, Train-Class-Acc: {0: '98.64%', 1: '97.85%', 2: '98.61%', 3: '99.39%'}\n",
      "Val Loss: 4.767887, Val Acc: 87.13%, Val-Class-Acc: {0: '85.87%', 1: '84.43%', 2: '81.25%', 3: '94.26%'}, LR: 0.000531\n",
      "Epoch 81/200, Train Loss: 0.172328, Train-Class-Acc: {0: '98.91%', 1: '98.98%', 2: '98.79%', 3: '99.28%'}\n",
      "Val Loss: 5.084621, Val Acc: 88.11%, Val-Class-Acc: {0: '88.59%', 1: '79.10%', 2: '88.19%', 3: '96.72%'}, LR: 0.000531\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_58.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_81.pth\n",
      "Epoch 82/200, Train Loss: 0.107708, Train-Class-Acc: {0: '99.05%', 1: '98.87%', 2: '99.83%', 3: '99.28%'}\n",
      "Val Loss: 4.579900, Val Acc: 87.25%, Val-Class-Acc: {0: '86.96%', 1: '78.28%', 2: '86.81%', 3: '96.72%'}, LR: 0.000531\n",
      "Epoch 83/200, Train Loss: 0.102279, Train-Class-Acc: {0: '98.91%', 1: '98.98%', 2: '99.48%', 3: '99.69%'}\n",
      "Val Loss: 5.295449, Val Acc: 87.87%, Val-Class-Acc: {0: '94.02%', 1: '74.59%', 2: '87.50%', 3: '96.72%'}, LR: 0.000478\n",
      "Epoch 84/200, Train Loss: 0.097969, Train-Class-Acc: {0: '99.18%', 1: '98.77%', 2: '99.65%', 3: '99.59%'}\n",
      "Val Loss: 5.218019, Val Acc: 86.40%, Val-Class-Acc: {0: '84.78%', 1: '79.10%', 2: '85.42%', 3: '95.49%'}, LR: 0.000478\n",
      "Epoch 85/200, Train Loss: 0.200211, Train-Class-Acc: {0: '99.59%', 1: '99.18%', 2: '99.65%', 3: '99.80%'}\n",
      "Val Loss: 5.990748, Val Acc: 85.42%, Val-Class-Acc: {0: '80.98%', 1: '81.97%', 2: '88.19%', 3: '90.57%'}, LR: 0.000478\n",
      "Epoch 86/200, Train Loss: 0.237046, Train-Class-Acc: {0: '98.91%', 1: '98.87%', 2: '99.48%', 3: '98.87%'}\n",
      "Val Loss: 5.440688, Val Acc: 87.87%, Val-Class-Acc: {0: '92.39%', 1: '76.64%', 2: '87.50%', 3: '95.90%'}, LR: 0.000478\n",
      "Epoch 87/200, Train Loss: 0.076341, Train-Class-Acc: {0: '99.05%', 1: '99.08%', 2: '99.48%', 3: '99.69%'}\n",
      "Val Loss: 5.211191, Val Acc: 86.89%, Val-Class-Acc: {0: '94.02%', 1: '72.54%', 2: '86.81%', 3: '95.90%'}, LR: 0.000478\n",
      "Epoch 88/200, Train Loss: 0.123621, Train-Class-Acc: {0: '98.64%', 1: '98.46%', 2: '98.79%', 3: '99.69%'}\n",
      "Val Loss: 5.858735, Val Acc: 87.50%, Val-Class-Acc: {0: '88.59%', 1: '75.41%', 2: '92.36%', 3: '95.90%'}, LR: 0.000478\n",
      "Epoch 89/200, Train Loss: 0.100262, Train-Class-Acc: {0: '99.32%', 1: '98.57%', 2: '99.13%', 3: '99.59%'}\n",
      "Val Loss: 4.889126, Val Acc: 88.60%, Val-Class-Acc: {0: '91.85%', 1: '79.10%', 2: '88.19%', 3: '95.90%'}, LR: 0.000478\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_9.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_89.pth\n",
      "Epoch 90/200, Train Loss: 0.079229, Train-Class-Acc: {0: '99.46%', 1: '99.18%', 2: '99.31%', 3: '99.49%'}\n",
      "Val Loss: 5.508683, Val Acc: 86.89%, Val-Class-Acc: {0: '87.50%', 1: '77.05%', 2: '89.58%', 3: '94.67%'}, LR: 0.000478\n",
      "Epoch 91/200, Train Loss: 0.070570, Train-Class-Acc: {0: '98.91%', 1: '99.18%', 2: '99.31%', 3: '99.59%'}\n",
      "Val Loss: 5.504031, Val Acc: 86.40%, Val-Class-Acc: {0: '77.72%', 1: '78.28%', 2: '90.28%', 3: '98.77%'}, LR: 0.000478\n",
      "Epoch 92/200, Train Loss: 0.107683, Train-Class-Acc: {0: '98.91%', 1: '98.67%', 2: '99.31%', 3: '99.80%'}\n",
      "Val Loss: 5.101945, Val Acc: 86.52%, Val-Class-Acc: {0: '90.76%', 1: '71.72%', 2: '88.19%', 3: '97.13%'}, LR: 0.000478\n",
      "Epoch 93/200, Train Loss: 0.406265, Train-Class-Acc: {0: '99.05%', 1: '98.36%', 2: '99.65%', 3: '98.98%'}\n",
      "Val Loss: 5.740042, Val Acc: 86.03%, Val-Class-Acc: {0: '91.85%', 1: '70.08%', 2: '88.19%', 3: '96.31%'}, LR: 0.000478\n",
      "Epoch 94/200, Train Loss: 0.077869, Train-Class-Acc: {0: '99.05%', 1: '98.98%', 2: '99.48%', 3: '99.59%'}\n",
      "Val Loss: 5.845739, Val Acc: 86.15%, Val-Class-Acc: {0: '82.61%', 1: '79.51%', 2: '88.89%', 3: '93.85%'}, LR: 0.000430\n",
      "Epoch 95/200, Train Loss: 0.109958, Train-Class-Acc: {0: '99.05%', 1: '98.67%', 2: '99.65%', 3: '99.59%'}\n",
      "Val Loss: 5.564923, Val Acc: 87.25%, Val-Class-Acc: {0: '87.50%', 1: '76.64%', 2: '88.89%', 3: '96.72%'}, LR: 0.000430\n",
      "Epoch 96/200, Train Loss: 0.123198, Train-Class-Acc: {0: '99.18%', 1: '98.87%', 2: '99.31%', 3: '99.59%'}\n",
      "Val Loss: 4.947298, Val Acc: 87.75%, Val-Class-Acc: {0: '86.96%', 1: '80.74%', 2: '88.19%', 3: '95.08%'}, LR: 0.000430\n",
      "Epoch 97/200, Train Loss: 0.098491, Train-Class-Acc: {0: '98.77%', 1: '98.77%', 2: '99.13%', 3: '99.49%'}\n",
      "Val Loss: 5.759886, Val Acc: 86.64%, Val-Class-Acc: {0: '91.85%', 1: '74.59%', 2: '86.81%', 3: '94.67%'}, LR: 0.000430\n",
      "Epoch 98/200, Train Loss: 0.176693, Train-Class-Acc: {0: '99.46%', 1: '98.87%', 2: '98.96%', 3: '99.39%'}\n",
      "Val Loss: 7.162568, Val Acc: 85.91%, Val-Class-Acc: {0: '79.35%', 1: '78.69%', 2: '86.81%', 3: '97.54%'}, LR: 0.000430\n",
      "Epoch 99/200, Train Loss: 0.154454, Train-Class-Acc: {0: '98.91%', 1: '98.87%', 2: '99.31%', 3: '99.39%'}\n",
      "Val Loss: 6.343537, Val Acc: 86.03%, Val-Class-Acc: {0: '86.41%', 1: '74.59%', 2: '85.42%', 3: '97.54%'}, LR: 0.000430\n",
      "Epoch 100/200, Train Loss: 0.174592, Train-Class-Acc: {0: '99.18%', 1: '99.18%', 2: '99.13%', 3: '99.18%'}\n",
      "Val Loss: 5.458591, Val Acc: 86.89%, Val-Class-Acc: {0: '90.76%', 1: '74.59%', 2: '86.81%', 3: '96.31%'}, LR: 0.000430\n",
      "Epoch 101/200, Train Loss: 0.067019, Train-Class-Acc: {0: '99.59%', 1: '99.39%', 2: '99.31%', 3: '99.80%'}\n",
      "Val Loss: 5.694702, Val Acc: 87.25%, Val-Class-Acc: {0: '90.76%', 1: '76.23%', 2: '84.72%', 3: '97.13%'}, LR: 0.000430\n",
      "Epoch 102/200, Train Loss: 0.074778, Train-Class-Acc: {0: '99.18%', 1: '98.87%', 2: '99.65%', 3: '99.59%'}\n",
      "Val Loss: 5.546902, Val Acc: 87.87%, Val-Class-Acc: {0: '83.70%', 1: '79.10%', 2: '94.44%', 3: '95.90%'}, LR: 0.000430\n",
      "Epoch 103/200, Train Loss: 0.065725, Train-Class-Acc: {0: '99.32%', 1: '99.39%', 2: '99.83%', 3: '99.90%'}\n",
      "Val Loss: 5.544728, Val Acc: 87.38%, Val-Class-Acc: {0: '84.78%', 1: '81.56%', 2: '88.89%', 3: '94.26%'}, LR: 0.000430\n",
      "Epoch 104/200, Train Loss: 0.075578, Train-Class-Acc: {0: '99.05%', 1: '98.98%', 2: '98.96%', 3: '99.59%'}\n",
      "Val Loss: 5.327632, Val Acc: 86.76%, Val-Class-Acc: {0: '84.24%', 1: '79.51%', 2: '88.19%', 3: '95.08%'}, LR: 0.000430\n",
      "Epoch 105/200, Train Loss: 0.129045, Train-Class-Acc: {0: '99.32%', 1: '99.18%', 2: '99.65%', 3: '99.69%'}\n",
      "Val Loss: 6.047130, Val Acc: 86.64%, Val-Class-Acc: {0: '88.59%', 1: '76.64%', 2: '85.42%', 3: '95.90%'}, LR: 0.000387\n",
      "Epoch 106/200, Train Loss: 0.094467, Train-Class-Acc: {0: '99.05%', 1: '98.77%', 2: '99.31%', 3: '99.59%'}\n",
      "Val Loss: 5.977200, Val Acc: 87.01%, Val-Class-Acc: {0: '89.67%', 1: '78.69%', 2: '83.33%', 3: '95.49%'}, LR: 0.000387\n",
      "Epoch 107/200, Train Loss: 0.039379, Train-Class-Acc: {0: '99.73%', 1: '99.69%', 2: '99.31%', 3: '99.80%'}\n",
      "Val Loss: 5.666868, Val Acc: 86.15%, Val-Class-Acc: {0: '86.41%', 1: '78.28%', 2: '84.03%', 3: '95.08%'}, LR: 0.000387\n",
      "Epoch 108/200, Train Loss: 0.036651, Train-Class-Acc: {0: '99.73%', 1: '99.59%', 2: '99.65%', 3: '99.80%'}\n",
      "Val Loss: 5.746250, Val Acc: 85.66%, Val-Class-Acc: {0: '88.59%', 1: '73.36%', 2: '85.42%', 3: '95.90%'}, LR: 0.000387\n",
      "Epoch 109/200, Train Loss: 0.047647, Train-Class-Acc: {0: '99.46%', 1: '99.39%', 2: '99.31%', 3: '99.90%'}\n",
      "Val Loss: 5.961023, Val Acc: 85.91%, Val-Class-Acc: {0: '88.59%', 1: '77.05%', 2: '80.56%', 3: '95.90%'}, LR: 0.000387\n",
      "Epoch 110/200, Train Loss: 0.037099, Train-Class-Acc: {0: '99.32%', 1: '99.18%', 2: '99.83%', 3: '99.69%'}\n",
      "Val Loss: 5.919207, Val Acc: 87.25%, Val-Class-Acc: {0: '89.67%', 1: '78.69%', 2: '84.03%', 3: '95.90%'}, LR: 0.000387\n",
      "Epoch 111/200, Train Loss: 0.052422, Train-Class-Acc: {0: '99.86%', 1: '99.69%', 2: '99.48%', 3: '99.69%'}\n",
      "Val Loss: 6.028224, Val Acc: 86.15%, Val-Class-Acc: {0: '83.70%', 1: '76.23%', 2: '90.28%', 3: '95.49%'}, LR: 0.000387\n",
      "Epoch 112/200, Train Loss: 0.033801, Train-Class-Acc: {0: '99.73%', 1: '99.69%', 2: '99.65%', 3: '99.90%'}\n",
      "Val Loss: 6.070863, Val Acc: 86.03%, Val-Class-Acc: {0: '84.24%', 1: '78.69%', 2: '85.42%', 3: '95.08%'}, LR: 0.000387\n",
      "Epoch 113/200, Train Loss: 0.065352, Train-Class-Acc: {0: '99.05%', 1: '98.57%', 2: '99.48%', 3: '99.69%'}\n",
      "Val Loss: 5.933816, Val Acc: 86.52%, Val-Class-Acc: {0: '87.50%', 1: '76.23%', 2: '88.19%', 3: '95.08%'}, LR: 0.000387\n",
      "Epoch 114/200, Train Loss: 0.105921, Train-Class-Acc: {0: '98.77%', 1: '99.28%', 2: '100.00%', 3: '99.69%'}\n",
      "Val Loss: 6.146171, Val Acc: 87.01%, Val-Class-Acc: {0: '88.59%', 1: '81.56%', 2: '81.94%', 3: '94.26%'}, LR: 0.000387\n",
      "Epoch 115/200, Train Loss: 0.204312, Train-Class-Acc: {0: '99.59%', 1: '99.28%', 2: '99.13%', 3: '99.49%'}\n",
      "Val Loss: 6.058543, Val Acc: 85.91%, Val-Class-Acc: {0: '86.96%', 1: '75.82%', 2: '88.19%', 3: '93.85%'}, LR: 0.000387\n",
      "Epoch 116/200, Train Loss: 0.062379, Train-Class-Acc: {0: '99.73%', 1: '99.80%', 2: '99.31%', 3: '99.49%'}\n",
      "Val Loss: 6.305577, Val Acc: 85.42%, Val-Class-Acc: {0: '87.50%', 1: '75.41%', 2: '83.33%', 3: '95.08%'}, LR: 0.000349\n",
      "Epoch 117/200, Train Loss: 0.047487, Train-Class-Acc: {0: '99.73%', 1: '99.69%', 2: '99.48%', 3: '99.69%'}\n",
      "Val Loss: 5.457770, Val Acc: 87.38%, Val-Class-Acc: {0: '86.96%', 1: '77.87%', 2: '86.81%', 3: '97.54%'}, LR: 0.000349\n",
      "Epoch 118/200, Train Loss: 0.056585, Train-Class-Acc: {0: '99.59%', 1: '99.49%', 2: '99.83%', 3: '99.69%'}\n",
      "Val Loss: 6.058308, Val Acc: 87.87%, Val-Class-Acc: {0: '90.76%', 1: '77.46%', 2: '86.11%', 3: '97.13%'}, LR: 0.000349\n",
      "Epoch 119/200, Train Loss: 0.081382, Train-Class-Acc: {0: '99.73%', 1: '99.28%', 2: '99.48%', 3: '99.69%'}\n",
      "Val Loss: 5.411173, Val Acc: 88.36%, Val-Class-Acc: {0: '89.67%', 1: '79.92%', 2: '85.42%', 3: '97.54%'}, LR: 0.000349\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_19.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_119.pth\n",
      "Epoch 120/200, Train Loss: 0.044778, Train-Class-Acc: {0: '99.32%', 1: '99.59%', 2: '99.31%', 3: '99.80%'}\n",
      "Val Loss: 5.087485, Val Acc: 87.38%, Val-Class-Acc: {0: '84.24%', 1: '81.15%', 2: '86.11%', 3: '96.72%'}, LR: 0.000349\n",
      "Epoch 121/200, Train Loss: 0.141854, Train-Class-Acc: {0: '99.46%', 1: '98.98%', 2: '99.83%', 3: '99.69%'}\n",
      "Val Loss: 6.495772, Val Acc: 87.25%, Val-Class-Acc: {0: '87.50%', 1: '81.56%', 2: '82.64%', 3: '95.49%'}, LR: 0.000349\n",
      "Epoch 122/200, Train Loss: 0.103060, Train-Class-Acc: {0: '99.32%', 1: '99.08%', 2: '99.31%', 3: '99.28%'}\n",
      "Val Loss: 5.143342, Val Acc: 87.87%, Val-Class-Acc: {0: '86.41%', 1: '81.97%', 2: '84.72%', 3: '96.72%'}, LR: 0.000349\n",
      "Epoch 123/200, Train Loss: 0.035061, Train-Class-Acc: {0: '99.86%', 1: '99.59%', 2: '99.83%', 3: '99.59%'}\n",
      "Val Loss: 5.211752, Val Acc: 88.11%, Val-Class-Acc: {0: '84.78%', 1: '84.02%', 2: '84.03%', 3: '97.13%'}, LR: 0.000349\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_45.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_123.pth\n",
      "Epoch 124/200, Train Loss: 0.045719, Train-Class-Acc: {0: '98.77%', 1: '99.39%', 2: '99.48%', 3: '100.00%'}\n",
      "Val Loss: 5.191885, Val Acc: 87.25%, Val-Class-Acc: {0: '88.59%', 1: '82.38%', 2: '84.72%', 3: '92.62%'}, LR: 0.000349\n",
      "Epoch 125/200, Train Loss: 0.057736, Train-Class-Acc: {0: '99.59%', 1: '99.39%', 2: '99.83%', 3: '99.49%'}\n",
      "Val Loss: 5.265820, Val Acc: 87.62%, Val-Class-Acc: {0: '91.30%', 1: '79.92%', 2: '85.42%', 3: '93.85%'}, LR: 0.000349\n",
      "Epoch 126/200, Train Loss: 0.079195, Train-Class-Acc: {0: '99.73%', 1: '99.18%', 2: '99.31%', 3: '99.80%'}\n",
      "Val Loss: 6.036287, Val Acc: 86.27%, Val-Class-Acc: {0: '87.50%', 1: '81.56%', 2: '79.17%', 3: '94.26%'}, LR: 0.000349\n",
      "Epoch 127/200, Train Loss: 0.073118, Train-Class-Acc: {0: '99.32%', 1: '99.39%', 2: '99.48%', 3: '99.69%'}\n",
      "Val Loss: 5.644475, Val Acc: 87.50%, Val-Class-Acc: {0: '86.41%', 1: '79.10%', 2: '89.58%', 3: '95.49%'}, LR: 0.000314\n",
      "Epoch 128/200, Train Loss: 0.047942, Train-Class-Acc: {0: '99.86%', 1: '99.49%', 2: '100.00%', 3: '99.69%'}\n",
      "Val Loss: 5.423399, Val Acc: 87.87%, Val-Class-Acc: {0: '91.30%', 1: '78.69%', 2: '85.42%', 3: '95.90%'}, LR: 0.000314\n",
      "Epoch 129/200, Train Loss: 0.055461, Train-Class-Acc: {0: '99.59%', 1: '99.49%', 2: '99.48%', 3: '99.80%'}\n",
      "Val Loss: 6.430201, Val Acc: 87.99%, Val-Class-Acc: {0: '88.04%', 1: '78.69%', 2: '88.19%', 3: '97.13%'}, LR: 0.000314\n",
      "Epoch 130/200, Train Loss: 0.071391, Train-Class-Acc: {0: '99.46%', 1: '99.39%', 2: '100.00%', 3: '99.80%'}\n",
      "Val Loss: 6.611562, Val Acc: 87.75%, Val-Class-Acc: {0: '89.13%', 1: '81.97%', 2: '88.89%', 3: '91.80%'}, LR: 0.000314\n",
      "Epoch 131/200, Train Loss: 0.151250, Train-Class-Acc: {0: '99.59%', 1: '98.87%', 2: '99.48%', 3: '99.28%'}\n",
      "Val Loss: 6.724837, Val Acc: 86.27%, Val-Class-Acc: {0: '90.22%', 1: '75.00%', 2: '84.72%', 3: '95.49%'}, LR: 0.000314\n",
      "Epoch 132/200, Train Loss: 0.060868, Train-Class-Acc: {0: '99.46%', 1: '99.59%', 2: '99.65%', 3: '99.69%'}\n",
      "Val Loss: 6.314772, Val Acc: 85.78%, Val-Class-Acc: {0: '84.24%', 1: '78.28%', 2: '82.64%', 3: '96.31%'}, LR: 0.000314\n",
      "Epoch 133/200, Train Loss: 0.065281, Train-Class-Acc: {0: '99.05%', 1: '99.28%', 2: '99.48%', 3: '99.90%'}\n",
      "Val Loss: 6.207775, Val Acc: 87.87%, Val-Class-Acc: {0: '90.76%', 1: '78.28%', 2: '87.50%', 3: '95.49%'}, LR: 0.000314\n",
      "Epoch 134/200, Train Loss: 0.033884, Train-Class-Acc: {0: '99.73%', 1: '99.39%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 5.906883, Val Acc: 87.62%, Val-Class-Acc: {0: '86.41%', 1: '79.92%', 2: '88.19%', 3: '95.90%'}, LR: 0.000314\n",
      "Epoch 135/200, Train Loss: 0.046845, Train-Class-Acc: {0: '99.59%', 1: '99.90%', 2: '99.65%', 3: '99.80%'}\n",
      "Val Loss: 6.492691, Val Acc: 87.25%, Val-Class-Acc: {0: '86.41%', 1: '75.41%', 2: '92.36%', 3: '96.72%'}, LR: 0.000314\n",
      "Epoch 136/200, Train Loss: 0.053909, Train-Class-Acc: {0: '99.05%', 1: '99.49%', 2: '99.65%', 3: '100.00%'}\n",
      "Val Loss: 6.478051, Val Acc: 87.25%, Val-Class-Acc: {0: '86.41%', 1: '76.23%', 2: '90.97%', 3: '96.72%'}, LR: 0.000314\n",
      "Epoch 137/200, Train Loss: 0.031567, Train-Class-Acc: {0: '99.86%', 1: '99.69%', 2: '99.83%', 3: '99.80%'}\n",
      "Val Loss: 6.327012, Val Acc: 86.64%, Val-Class-Acc: {0: '89.13%', 1: '75.41%', 2: '84.72%', 3: '97.13%'}, LR: 0.000314\n",
      "Epoch 138/200, Train Loss: 0.052384, Train-Class-Acc: {0: '99.59%', 1: '99.39%', 2: '99.48%', 3: '99.90%'}\n",
      "Val Loss: 6.189072, Val Acc: 87.25%, Val-Class-Acc: {0: '82.61%', 1: '78.69%', 2: '90.97%', 3: '97.13%'}, LR: 0.000282\n",
      "Epoch 139/200, Train Loss: 0.060277, Train-Class-Acc: {0: '99.46%', 1: '99.69%', 2: '99.83%', 3: '99.90%'}\n",
      "Val Loss: 7.149200, Val Acc: 86.64%, Val-Class-Acc: {0: '92.39%', 1: '72.13%', 2: '90.97%', 3: '94.26%'}, LR: 0.000282\n",
      "Epoch 140/200, Train Loss: 0.090368, Train-Class-Acc: {0: '99.46%', 1: '99.39%', 2: '99.65%', 3: '99.90%'}\n",
      "Val Loss: 7.155804, Val Acc: 87.13%, Val-Class-Acc: {0: '81.52%', 1: '81.56%', 2: '88.89%', 3: '95.90%'}, LR: 0.000282\n",
      "Epoch 141/200, Train Loss: 0.069868, Train-Class-Acc: {0: '99.46%', 1: '99.39%', 2: '99.83%', 3: '99.80%'}\n",
      "Val Loss: 7.775274, Val Acc: 86.27%, Val-Class-Acc: {0: '84.24%', 1: '75.82%', 2: '88.19%', 3: '97.13%'}, LR: 0.000282\n",
      "Epoch 142/200, Train Loss: 0.028524, Train-Class-Acc: {0: '99.73%', 1: '99.59%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 7.201569, Val Acc: 87.01%, Val-Class-Acc: {0: '86.41%', 1: '77.87%', 2: '89.58%', 3: '95.08%'}, LR: 0.000282\n",
      "Epoch 143/200, Train Loss: 0.019053, Train-Class-Acc: {0: '99.73%', 1: '99.80%', 2: '99.83%', 3: '99.80%'}\n",
      "Val Loss: 7.131536, Val Acc: 86.40%, Val-Class-Acc: {0: '86.41%', 1: '76.23%', 2: '89.58%', 3: '94.67%'}, LR: 0.000282\n",
      "Epoch 144/200, Train Loss: 0.047246, Train-Class-Acc: {0: '99.59%', 1: '99.59%', 2: '99.65%', 3: '99.80%'}\n",
      "Val Loss: 6.576575, Val Acc: 87.50%, Val-Class-Acc: {0: '88.59%', 1: '78.69%', 2: '89.58%', 3: '94.26%'}, LR: 0.000282\n",
      "Epoch 145/200, Train Loss: 0.025159, Train-Class-Acc: {0: '99.73%', 1: '99.69%', 2: '99.83%', 3: '99.69%'}\n",
      "Val Loss: 7.210591, Val Acc: 86.64%, Val-Class-Acc: {0: '86.96%', 1: '75.00%', 2: '88.89%', 3: '96.72%'}, LR: 0.000282\n",
      "Epoch 146/200, Train Loss: 0.044742, Train-Class-Acc: {0: '99.46%', 1: '99.49%', 2: '99.83%', 3: '99.80%'}\n",
      "Val Loss: 6.627078, Val Acc: 87.01%, Val-Class-Acc: {0: '90.22%', 1: '77.46%', 2: '86.81%', 3: '94.26%'}, LR: 0.000282\n",
      "Epoch 147/200, Train Loss: 0.024657, Train-Class-Acc: {0: '99.73%', 1: '99.80%', 2: '99.83%', 3: '99.59%'}\n",
      "Val Loss: 6.359045, Val Acc: 86.64%, Val-Class-Acc: {0: '90.22%', 1: '76.23%', 2: '87.50%', 3: '93.85%'}, LR: 0.000282\n",
      "Epoch 148/200, Train Loss: 0.009394, Train-Class-Acc: {0: '99.86%', 1: '99.90%', 2: '99.83%', 3: '99.80%'}\n",
      "Val Loss: 6.639954, Val Acc: 86.89%, Val-Class-Acc: {0: '90.22%', 1: '73.36%', 2: '89.58%', 3: '96.31%'}, LR: 0.000282\n",
      "Epoch 149/200, Train Loss: 0.017660, Train-Class-Acc: {0: '99.73%', 1: '99.59%', 2: '99.65%', 3: '100.00%'}\n",
      "Val Loss: 6.206581, Val Acc: 88.11%, Val-Class-Acc: {0: '89.67%', 1: '80.33%', 2: '89.58%', 3: '93.85%'}, LR: 0.000254\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_75.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_149.pth\n",
      "Epoch 150/200, Train Loss: 0.031339, Train-Class-Acc: {0: '99.86%', 1: '99.59%', 2: '99.83%', 3: '99.69%'}\n",
      "Val Loss: 6.183799, Val Acc: 87.38%, Val-Class-Acc: {0: '88.04%', 1: '79.51%', 2: '86.11%', 3: '95.49%'}, LR: 0.000254\n",
      "Epoch 151/200, Train Loss: 0.008418, Train-Class-Acc: {0: '99.73%', 1: '99.90%', 2: '99.65%', 3: '100.00%'}\n",
      "Val Loss: 6.186228, Val Acc: 87.62%, Val-Class-Acc: {0: '89.67%', 1: '79.10%', 2: '87.50%', 3: '94.67%'}, LR: 0.000254\n",
      "Epoch 152/200, Train Loss: 0.025052, Train-Class-Acc: {0: '100.00%', 1: '99.69%', 2: '99.83%', 3: '99.90%'}\n",
      "Val Loss: 7.670684, Val Acc: 87.50%, Val-Class-Acc: {0: '85.33%', 1: '78.28%', 2: '88.89%', 3: '97.54%'}, LR: 0.000254\n",
      "Epoch 153/200, Train Loss: 0.018173, Train-Class-Acc: {0: '99.73%', 1: '99.69%', 2: '99.83%', 3: '99.90%'}\n",
      "Val Loss: 7.388327, Val Acc: 86.27%, Val-Class-Acc: {0: '89.13%', 1: '71.72%', 2: '90.97%', 3: '95.90%'}, LR: 0.000254\n",
      "Epoch 154/200, Train Loss: 0.013153, Train-Class-Acc: {0: '99.73%', 1: '99.80%', 2: '99.65%', 3: '99.90%'}\n",
      "Val Loss: 7.265613, Val Acc: 87.01%, Val-Class-Acc: {0: '86.41%', 1: '76.64%', 2: '87.50%', 3: '97.54%'}, LR: 0.000254\n",
      "Epoch 155/200, Train Loss: 0.024859, Train-Class-Acc: {0: '99.73%', 1: '99.80%', 2: '99.65%', 3: '99.69%'}\n",
      "Val Loss: 7.388958, Val Acc: 87.87%, Val-Class-Acc: {0: '86.96%', 1: '77.46%', 2: '88.89%', 3: '98.36%'}, LR: 0.000254\n",
      "Epoch 156/200, Train Loss: 0.066056, Train-Class-Acc: {0: '99.73%', 1: '99.59%', 2: '99.65%', 3: '100.00%'}\n",
      "Val Loss: 6.304983, Val Acc: 88.48%, Val-Class-Acc: {0: '89.67%', 1: '81.15%', 2: '89.58%', 3: '94.26%'}, LR: 0.000254\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_81.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_156.pth\n",
      "Epoch 157/200, Train Loss: 0.037766, Train-Class-Acc: {0: '99.59%', 1: '99.59%', 2: '99.65%', 3: '99.80%'}\n",
      "Val Loss: 6.441024, Val Acc: 87.13%, Val-Class-Acc: {0: '89.67%', 1: '76.23%', 2: '88.89%', 3: '95.08%'}, LR: 0.000254\n",
      "Epoch 158/200, Train Loss: 0.025715, Train-Class-Acc: {0: '99.86%', 1: '99.69%', 2: '99.83%', 3: '99.80%'}\n",
      "Val Loss: 6.109724, Val Acc: 87.01%, Val-Class-Acc: {0: '83.70%', 1: '79.10%', 2: '88.89%', 3: '96.31%'}, LR: 0.000254\n",
      "Epoch 159/200, Train Loss: 0.021672, Train-Class-Acc: {0: '99.73%', 1: '99.80%', 2: '99.83%', 3: '99.69%'}\n",
      "Val Loss: 7.001855, Val Acc: 86.52%, Val-Class-Acc: {0: '90.22%', 1: '72.13%', 2: '88.19%', 3: '97.13%'}, LR: 0.000254\n",
      "Epoch 160/200, Train Loss: 0.022108, Train-Class-Acc: {0: '100.00%', 1: '99.80%', 2: '99.65%', 3: '99.80%'}\n",
      "Val Loss: 7.028149, Val Acc: 86.03%, Val-Class-Acc: {0: '90.22%', 1: '74.59%', 2: '90.97%', 3: '91.39%'}, LR: 0.000229\n",
      "Epoch 161/200, Train Loss: 0.020016, Train-Class-Acc: {0: '100.00%', 1: '99.69%', 2: '99.65%', 3: '99.90%'}\n",
      "Val Loss: 6.882567, Val Acc: 86.03%, Val-Class-Acc: {0: '80.98%', 1: '78.28%', 2: '92.36%', 3: '93.85%'}, LR: 0.000229\n",
      "Epoch 162/200, Train Loss: 0.027682, Train-Class-Acc: {0: '99.46%', 1: '99.80%', 2: '99.83%', 3: '99.90%'}\n",
      "Val Loss: 6.942801, Val Acc: 86.89%, Val-Class-Acc: {0: '86.96%', 1: '76.23%', 2: '91.67%', 3: '94.67%'}, LR: 0.000229\n",
      "Epoch 163/200, Train Loss: 0.019330, Train-Class-Acc: {0: '99.73%', 1: '99.80%', 2: '100.00%', 3: '99.80%'}\n",
      "Val Loss: 6.937306, Val Acc: 86.52%, Val-Class-Acc: {0: '88.04%', 1: '74.59%', 2: '89.58%', 3: '95.49%'}, LR: 0.000229\n",
      "Epoch 164/200, Train Loss: 0.024040, Train-Class-Acc: {0: '99.73%', 1: '99.69%', 2: '99.83%', 3: '99.80%'}\n",
      "Val Loss: 7.339559, Val Acc: 86.64%, Val-Class-Acc: {0: '89.67%', 1: '76.64%', 2: '85.42%', 3: '95.08%'}, LR: 0.000229\n",
      "Epoch 165/200, Train Loss: 0.021700, Train-Class-Acc: {0: '99.73%', 1: '99.80%', 2: '99.83%', 3: '100.00%'}\n",
      "Val Loss: 6.720987, Val Acc: 87.50%, Val-Class-Acc: {0: '86.41%', 1: '79.51%', 2: '89.58%', 3: '95.08%'}, LR: 0.000229\n",
      "Epoch 166/200, Train Loss: 0.016854, Train-Class-Acc: {0: '99.86%', 1: '99.49%', 2: '99.83%', 3: '99.90%'}\n",
      "Val Loss: 6.564167, Val Acc: 87.13%, Val-Class-Acc: {0: '83.15%', 1: '82.79%', 2: '85.42%', 3: '95.49%'}, LR: 0.000229\n",
      "Epoch 167/200, Train Loss: 0.026965, Train-Class-Acc: {0: '99.46%', 1: '99.80%', 2: '99.83%', 3: '99.80%'}\n",
      "Val Loss: 6.509857, Val Acc: 87.25%, Val-Class-Acc: {0: '91.30%', 1: '79.10%', 2: '82.64%', 3: '95.08%'}, LR: 0.000229\n",
      "Epoch 168/200, Train Loss: 0.023139, Train-Class-Acc: {0: '99.73%', 1: '99.59%', 2: '99.65%', 3: '99.69%'}\n",
      "Val Loss: 6.412903, Val Acc: 86.15%, Val-Class-Acc: {0: '83.15%', 1: '79.92%', 2: '84.72%', 3: '95.49%'}, LR: 0.000229\n",
      "Epoch 169/200, Train Loss: 0.009519, Train-Class-Acc: {0: '99.86%', 1: '99.90%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 6.388662, Val Acc: 87.25%, Val-Class-Acc: {0: '86.96%', 1: '78.28%', 2: '88.19%', 3: '95.90%'}, LR: 0.000229\n",
      "Epoch 170/200, Train Loss: 0.057516, Train-Class-Acc: {0: '99.46%', 1: '99.49%', 2: '99.31%', 3: '100.00%'}\n",
      "Val Loss: 6.558498, Val Acc: 86.52%, Val-Class-Acc: {0: '82.07%', 1: '83.20%', 2: '87.50%', 3: '92.62%'}, LR: 0.000229\n",
      "Epoch 171/200, Train Loss: 0.006193, Train-Class-Acc: {0: '99.86%', 1: '99.80%', 2: '100.00%', 3: '100.00%'}\n",
      "Val Loss: 6.846830, Val Acc: 86.40%, Val-Class-Acc: {0: '83.15%', 1: '83.20%', 2: '83.33%', 3: '93.85%'}, LR: 0.000206\n",
      "Epoch 172/200, Train Loss: 0.020459, Train-Class-Acc: {0: '100.00%', 1: '99.59%', 2: '99.48%', 3: '99.90%'}\n",
      "Val Loss: 6.960106, Val Acc: 87.38%, Val-Class-Acc: {0: '85.33%', 1: '79.10%', 2: '90.28%', 3: '95.49%'}, LR: 0.000206\n",
      "Epoch 173/200, Train Loss: 0.003959, Train-Class-Acc: {0: '99.86%', 1: '99.80%', 2: '99.83%', 3: '100.00%'}\n",
      "Val Loss: 6.822603, Val Acc: 87.50%, Val-Class-Acc: {0: '86.41%', 1: '79.92%', 2: '91.67%', 3: '93.44%'}, LR: 0.000206\n",
      "Epoch 174/200, Train Loss: 0.019480, Train-Class-Acc: {0: '99.59%', 1: '99.59%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 6.524060, Val Acc: 87.38%, Val-Class-Acc: {0: '85.33%', 1: '79.92%', 2: '90.28%', 3: '94.67%'}, LR: 0.000206\n",
      "Epoch 175/200, Train Loss: 0.041078, Train-Class-Acc: {0: '99.73%', 1: '99.59%', 2: '99.65%', 3: '99.80%'}\n",
      "Val Loss: 6.566186, Val Acc: 87.38%, Val-Class-Acc: {0: '90.22%', 1: '74.59%', 2: '90.97%', 3: '95.90%'}, LR: 0.000206\n",
      "Epoch 176/200, Train Loss: 0.012484, Train-Class-Acc: {0: '99.86%', 1: '99.69%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 6.409048, Val Acc: 87.87%, Val-Class-Acc: {0: '88.04%', 1: '77.05%', 2: '88.89%', 3: '97.95%'}, LR: 0.000206\n",
      "Epoch 177/200, Train Loss: 0.017629, Train-Class-Acc: {0: '99.86%', 1: '99.90%', 2: '99.83%', 3: '100.00%'}\n",
      "Val Loss: 6.450905, Val Acc: 87.87%, Val-Class-Acc: {0: '89.13%', 1: '76.23%', 2: '90.97%', 3: '96.72%'}, LR: 0.000206\n",
      "Epoch 178/200, Train Loss: 0.039414, Train-Class-Acc: {0: '99.86%', 1: '99.69%', 2: '99.83%', 3: '99.59%'}\n",
      "Val Loss: 6.257469, Val Acc: 87.13%, Val-Class-Acc: {0: '89.13%', 1: '77.46%', 2: '89.58%', 3: '93.85%'}, LR: 0.000206\n",
      "Epoch 179/200, Train Loss: 0.024283, Train-Class-Acc: {0: '99.73%', 1: '99.49%', 2: '99.65%', 3: '100.00%'}\n",
      "Val Loss: 6.821034, Val Acc: 86.89%, Val-Class-Acc: {0: '86.41%', 1: '75.00%', 2: '92.36%', 3: '95.90%'}, LR: 0.000206\n",
      "Epoch 180/200, Train Loss: 0.010577, Train-Class-Acc: {0: '99.59%', 1: '99.80%', 2: '99.65%', 3: '99.90%'}\n",
      "Val Loss: 6.265682, Val Acc: 87.99%, Val-Class-Acc: {0: '84.78%', 1: '80.74%', 2: '89.58%', 3: '96.72%'}, LR: 0.000206\n",
      "Epoch 181/200, Train Loss: 0.022834, Train-Class-Acc: {0: '100.00%', 1: '100.00%', 2: '99.83%', 3: '99.80%'}\n",
      "Val Loss: 6.614025, Val Acc: 87.99%, Val-Class-Acc: {0: '87.50%', 1: '76.64%', 2: '91.67%', 3: '97.54%'}, LR: 0.000206\n",
      "Epoch 182/200, Train Loss: 0.021438, Train-Class-Acc: {0: '100.00%', 1: '99.80%', 2: '100.00%', 3: '100.00%'}\n",
      "Val Loss: 6.974504, Val Acc: 86.76%, Val-Class-Acc: {0: '80.98%', 1: '77.46%', 2: '92.36%', 3: '97.13%'}, LR: 0.000185\n",
      "Epoch 183/200, Train Loss: 0.013587, Train-Class-Acc: {0: '99.73%', 1: '99.80%', 2: '99.83%', 3: '100.00%'}\n",
      "Val Loss: 6.673974, Val Acc: 88.24%, Val-Class-Acc: {0: '89.67%', 1: '77.05%', 2: '89.58%', 3: '97.54%'}, LR: 0.000185\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_123.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_183.pth\n",
      "Epoch 184/200, Train Loss: 0.001376, Train-Class-Acc: {0: '100.00%', 1: '99.90%', 2: '100.00%', 3: '100.00%'}\n",
      "Val Loss: 6.567639, Val Acc: 87.38%, Val-Class-Acc: {0: '82.61%', 1: '79.10%', 2: '90.97%', 3: '97.13%'}, LR: 0.000185\n",
      "Epoch 185/200, Train Loss: 0.010664, Train-Class-Acc: {0: '99.59%', 1: '99.90%', 2: '99.65%', 3: '100.00%'}\n",
      "Val Loss: 6.462432, Val Acc: 87.50%, Val-Class-Acc: {0: '83.70%', 1: '79.92%', 2: '91.67%', 3: '95.49%'}, LR: 0.000185\n",
      "Epoch 186/200, Train Loss: 0.011014, Train-Class-Acc: {0: '99.73%', 1: '99.69%', 2: '100.00%', 3: '100.00%'}\n",
      "Val Loss: 6.254055, Val Acc: 87.25%, Val-Class-Acc: {0: '88.04%', 1: '77.46%', 2: '89.58%', 3: '95.08%'}, LR: 0.000185\n",
      "Epoch 187/200, Train Loss: 0.016033, Train-Class-Acc: {0: '99.73%', 1: '99.80%', 2: '100.00%', 3: '99.80%'}\n",
      "Val Loss: 7.024847, Val Acc: 87.13%, Val-Class-Acc: {0: '91.30%', 1: '76.23%', 2: '83.33%', 3: '97.13%'}, LR: 0.000185\n",
      "Epoch 188/200, Train Loss: 0.014294, Train-Class-Acc: {0: '100.00%', 1: '99.69%', 2: '99.65%', 3: '100.00%'}\n",
      "Val Loss: 6.637127, Val Acc: 87.13%, Val-Class-Acc: {0: '86.96%', 1: '77.87%', 2: '87.50%', 3: '96.31%'}, LR: 0.000185\n",
      "Epoch 189/200, Train Loss: 0.002914, Train-Class-Acc: {0: '99.86%', 1: '99.90%', 2: '100.00%', 3: '100.00%'}\n",
      "Val Loss: 6.549710, Val Acc: 86.76%, Val-Class-Acc: {0: '83.15%', 1: '77.46%', 2: '89.58%', 3: '97.13%'}, LR: 0.000185\n",
      "Epoch 190/200, Train Loss: 0.002357, Train-Class-Acc: {0: '99.86%', 1: '100.00%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 6.715079, Val Acc: 86.64%, Val-Class-Acc: {0: '87.50%', 1: '75.00%', 2: '86.11%', 3: '97.95%'}, LR: 0.000185\n",
      "Epoch 191/200, Train Loss: 0.000001, Train-Class-Acc: {0: '100.00%', 1: '100.00%', 2: '100.00%', 3: '100.00%'}\n",
      "Val Loss: 6.780091, Val Acc: 87.01%, Val-Class-Acc: {0: '88.59%', 1: '75.82%', 2: '86.11%', 3: '97.54%'}, LR: 0.000185\n",
      "Epoch 192/200, Train Loss: 0.005807, Train-Class-Acc: {0: '100.00%', 1: '99.90%', 2: '99.65%', 3: '99.90%'}\n",
      "Val Loss: 7.507347, Val Acc: 86.03%, Val-Class-Acc: {0: '86.96%', 1: '71.72%', 2: '88.89%', 3: '97.95%'}, LR: 0.000185\n",
      "Epoch 193/200, Train Loss: 0.006239, Train-Class-Acc: {0: '99.86%', 1: '99.90%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 7.321779, Val Acc: 86.64%, Val-Class-Acc: {0: '86.96%', 1: '76.64%', 2: '84.72%', 3: '97.54%'}, LR: 0.000167\n",
      "Epoch 194/200, Train Loss: 0.021120, Train-Class-Acc: {0: '99.86%', 1: '99.59%', 2: '99.65%', 3: '99.90%'}\n",
      "Val Loss: 6.382917, Val Acc: 86.89%, Val-Class-Acc: {0: '85.33%', 1: '79.51%', 2: '86.11%', 3: '95.90%'}, LR: 0.000167\n",
      "Epoch 195/200, Train Loss: 0.018850, Train-Class-Acc: {0: '99.73%', 1: '99.80%', 2: '99.83%', 3: '99.80%'}\n",
      "Val Loss: 6.597758, Val Acc: 87.13%, Val-Class-Acc: {0: '85.33%', 1: '79.51%', 2: '84.72%', 3: '97.54%'}, LR: 0.000167\n",
      "Epoch 196/200, Train Loss: 0.002948, Train-Class-Acc: {0: '100.00%', 1: '99.90%', 2: '100.00%', 3: '100.00%'}\n",
      "Val Loss: 6.739803, Val Acc: 87.13%, Val-Class-Acc: {0: '87.50%', 1: '78.28%', 2: '86.11%', 3: '96.31%'}, LR: 0.000167\n",
      "Epoch 197/200, Train Loss: 0.011703, Train-Class-Acc: {0: '99.86%', 1: '99.80%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 6.757707, Val Acc: 86.52%, Val-Class-Acc: {0: '86.41%', 1: '79.10%', 2: '84.72%', 3: '95.08%'}, LR: 0.000167\n",
      "Epoch 198/200, Train Loss: 0.012260, Train-Class-Acc: {0: '99.86%', 1: '99.90%', 2: '100.00%', 3: '100.00%'}\n",
      "Val Loss: 6.570277, Val Acc: 87.01%, Val-Class-Acc: {0: '83.70%', 1: '83.20%', 2: '81.94%', 3: '96.31%'}, LR: 0.000167\n",
      "Epoch 199/200, Train Loss: 0.000125, Train-Class-Acc: {0: '100.00%', 1: '100.00%', 2: '100.00%', 3: '100.00%'}\n",
      "Val Loss: 6.555670, Val Acc: 86.76%, Val-Class-Acc: {0: '84.24%', 1: '81.56%', 2: '82.64%', 3: '96.31%'}, LR: 0.000167\n",
      "Epoch 200/200, Train Loss: 0.003048, Train-Class-Acc: {0: '100.00%', 1: '100.00%', 2: '99.83%', 3: '99.90%'}\n",
      "Val Loss: 6.583869, Val Acc: 86.52%, Val-Class-Acc: {0: '82.07%', 1: '79.92%', 2: '86.11%', 3: '96.72%'}, LR: 0.000167\n",
      "\n",
      "üèÜ Best model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_best.pth (Val Accuracy: 88.60%)\n",
      "\n",
      "üìå Final model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_final.pth\n",
      "\n",
      "üéØ Top 5 Best Models:\n",
      "Epoch 89, Train Loss: 0.100262, Train-Acc: {0: '99.32%', 1: '98.57%', 2: '99.13%', 3: '99.59%'},\n",
      "Val Loss: 4.889126, Val Acc: 88.60%, Val-Acc: {0: '91.85%', 1: '79.10%', 2: '88.19%', 3: '95.90%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_89.pth\n",
      "Epoch 156, Train Loss: 0.066056, Train-Acc: {0: '99.73%', 1: '99.59%', 2: '99.65%', 3: '100.00%'},\n",
      "Val Loss: 6.304983, Val Acc: 88.48%, Val-Acc: {0: '89.67%', 1: '81.15%', 2: '89.58%', 3: '94.26%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_156.pth\n",
      "Epoch 119, Train Loss: 0.081382, Train-Acc: {0: '99.73%', 1: '99.28%', 2: '99.48%', 3: '99.69%'},\n",
      "Val Loss: 5.411173, Val Acc: 88.36%, Val-Acc: {0: '89.67%', 1: '79.92%', 2: '85.42%', 3: '97.54%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_119.pth\n",
      "Epoch 183, Train Loss: 0.013587, Train-Acc: {0: '99.73%', 1: '99.80%', 2: '99.83%', 3: '100.00%'},\n",
      "Val Loss: 6.673974, Val Acc: 88.24%, Val-Acc: {0: '89.67%', 1: '77.05%', 2: '89.58%', 3: '97.54%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_183.pth\n",
      "Epoch 149, Train Loss: 0.017660, Train-Acc: {0: '99.73%', 1: '99.59%', 2: '99.65%', 3: '100.00%'},\n",
      "Val Loss: 6.206581, Val Acc: 88.11%, Val-Acc: {0: '89.67%', 1: '80.33%', 2: '89.58%', 3: '93.85%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_149.pth\n",
      "\n",
      "üß† Model Summary:\n",
      "Total Parameters: 3,889,796\n",
      "Model Size (float32): 14.84 MB\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "Number of LoRA adapters: 8\n",
      "Number of LoRA groups: 1\n",
      "Total Training Time: 270.49 seconds\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "---\n",
      "### Period 2 (alpha = 0.0, similarity_threshold = 0.99)\n",
      "+ ##### Total training time: 270.49 seconds\n",
      "+ ##### Model: ResNet18_1D_LoRA\n",
      "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2'*\n",
      "+ ##### Best Epoch: 89\n",
      "#### __Val Accuracy: 88.60%__\n",
      "#### __Val-Class-Acc: {0: '91.85%', 1: '79.10%', 2: '88.19%', 3: '95.90%'}__\n",
      "#### __Total Parameters: 3,889,796__\n",
      "#### __Model Size (float32): 14.84 MB__\n",
      "#### __Number of LoRA adapters: 8__\n",
      "#### __Number of LoRA groups: 1__\n",
      "\n",
      "Saved class features to: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v1/Period_2/class_features.pkl\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå Period 2: DynEx-CLoRA Training (ECG)\n",
    "# ================================\n",
    "period = 2\n",
    "\n",
    "# ==== Paths ====\n",
    "stop_signal_file = os.path.join(BASE_DIR, \"stop_training.txt\")\n",
    "model_saving_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v1\", f\"Period_{period}\")\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# ==== Load Period 2 Data ====\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{period}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{period}.npy\"))\n",
    "X_val   = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val   = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "# ==== Device ====\n",
    "device = auto_select_cuda_device()\n",
    "\n",
    "# ==== ËºâÂÖ• Period 1 ÁöÑ class features ====\n",
    "prev_folder = os.path.join(BASE_DIR, \"ResNet18_Selection\", \"ResNet18_big_inplane_v1\")\n",
    "class_features_path = os.path.join(prev_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, \"rb\") as f:\n",
    "    class_features_dict = pickle.load(f)\n",
    "print(f\"‚úÖ Loaded class features from: {class_features_path}\")\n",
    "\n",
    "# ==== ËºâÂÖ• Period 1 È†êË®ìÁ∑¥Ê®°Âûã ====\n",
    "prev_model_path = os.path.join(prev_folder, \"ResNet18_big_inplane_1D_best.pth\")\n",
    "checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "\n",
    "input_channels = X_train.shape[2]\n",
    "output_size = len(np.unique(y_train))\n",
    "\n",
    "# ==== Âª∫Á´ã teacher modelÔºàoutput_size Ë¶ÅÊâ£ÊéâÊñ∞È°ûÂà•Êï∏Ôºâ\n",
    "teacher_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size - 2).to(device)\n",
    "teacher_model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "teacher_model.eval()\n",
    "\n",
    "# ==== Âª∫Á´ã student model ====\n",
    "model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size).to(device)\n",
    "\n",
    "# ==== Ê†πÊìö LoRA Êï∏ÈáèÂêåÊ≠• adapter ÁµêÊßã ====\n",
    "num_lora_groups = checkpoint.get(\"num_lora_groups\", 0)\n",
    "print(f\"üîÑ Number of LoRA groups: {num_lora_groups}\")\n",
    "related_labels = checkpoint.get(\"related_labels\", {\"base\": [0, 1]})\n",
    "for _ in range(num_lora_groups):\n",
    "    model.add_lora_adapter()\n",
    "\n",
    "# ==== Ë§áË£Ω shared Ê¨äÈáçÔºàÊéíÈô§ fc / lora_adapterÔºâ====\n",
    "model_dict = model.state_dict()\n",
    "prev_state_dict = checkpoint[\"model_state_dict\"]\n",
    "filtered_dict = {\n",
    "    k: v for k, v in prev_state_dict.items()\n",
    "    if k in model_dict and model_dict[k].shape == v.shape and k not in [\"fc.weight\", \"fc.bias\"]\n",
    "}\n",
    "model.load_state_dict(filtered_dict, strict=False)\n",
    "for k in model_dict:\n",
    "    if k not in filtered_dict:\n",
    "        print(f\"üîç Not loaded: {k}, shape={model_dict[k].shape}\")\n",
    "print(\"‚úÖ Loaded shared weights from Period 1 (excluding FC only)\")\n",
    "\n",
    "# ==== Ë®ìÁ∑¥ÂèÉÊï∏ ====\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "alpha = 0.0 # 0.0 for no distillation\n",
    "similarity_threshold = 0.99\n",
    "stable_classes = [0]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# ==== ÈñãÂßãË®ìÁ∑¥ ====\n",
    "train_with_dynex_clora_ecg(\n",
    "    model=model,\n",
    "    teacher_model=None,\n",
    "    output_size=output_size,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    alpha=alpha,\n",
    "    model_saving_folder=model_saving_folder,\n",
    "    model_name=\"ResNet18_1D_LoRA\",\n",
    "    stop_signal_file=stop_signal_file,\n",
    "    scheduler=scheduler,\n",
    "    period=period,\n",
    "    stable_classes=stable_classes,\n",
    "    similarity_threshold=similarity_threshold,\n",
    "    class_features_dict=class_features_dict,\n",
    "    related_labels=related_labels,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ==== Cleanup ====\n",
    "del X_train, y_train, X_val, y_val\n",
    "del model, teacher_model, checkpoint, model_dict, prev_state_dict, filtered_dict\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d5d95",
   "metadata": {},
   "source": [
    "#### v2 no distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ccf16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 1\n",
      "    - Memory Used    : 575 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ Loaded class features from: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/ResNet18_Selection/ResNet18_big_inplane_v1/class_features.pkl\n",
      "üîÑ Number of LoRA groups: 0\n",
      "üîç Not loaded: fc.weight, shape=torch.Size([4, 1024])\n",
      "üîç Not loaded: fc.bias, shape=torch.Size([4])\n",
      "‚úÖ Loaded shared weights from Period 1 (excluding FC & LoRA)\n",
      "\n",
      "üöÄ 'train_with_dynex_clora_ecg' started for Period 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1514991/1220977959.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Removed existing folder: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2\n",
      "\n",
      "‚úÖ Data Overview:\n",
      "X_train: torch.Size([3263, 5000, 12]), y_train: torch.Size([3263])\n",
      "X_val: torch.Size([816, 5000, 12]), y_val: torch.Size([816])\n",
      "\n",
      "üîé Similarity Analysis:\n",
      "  Similarity threshold: 0.9900\n",
      "  Existing classes: [0, 1]\n",
      "  Current classes: [0, 1, 2, 3]\n",
      "  New classes: [2, 3]\n",
      "\n",
      "üìä Similarity Scores:\n",
      "  New Class 2:\n",
      "    - Existing Class 1: 0.9665\n",
      "    - Existing Class 0: 0.8415\n",
      "  New Class 3:\n",
      "    - Existing Class 1: 0.9912\n",
      "    - Existing Class 0: 0.8097\n",
      "\n",
      "  Average similarity: 0.9055, Std: 0.0837\n",
      "\n",
      "üß© Managing LoRA adapters for 2 new classes...\n",
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n",
      "‚ûï New Class 2 is not similar to any existing class ‚Üí Created new adapter group #0\n",
      "üîÑ New Class 3 is similar to Class 1 (sim=0.9912) ‚Üí Added to adapter 'base'\n",
      "\n",
      "üîç Checking stability of existing classes...\n",
      "  Class 0 similarity with itself: 0.9998\n",
      "  Class 1 similarity with itself: 0.9948\n",
      "\n",
      "üîí Default: Freezing all LoRA adapters and base conv2 weights\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "  ‚ùå layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "\n",
      "üîì Unfreezing selected adapters (by group):\n",
      "  - Base layers (classes: [0, 1, 3])\n",
      "\n",
      "üîç Frozen Parameters: (After Unfreeze specific adapter groups)\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  - New adapter group #0 (classes: [2])\n",
      "\n",
      "üîç Frozen Parameters (After Unfreeze newly added adapter groups):\n",
      "\n",
      "üîç Frozen Parameters (After unfreeze the final layer):\n",
      "\n",
      "üìã Related Labels Summary:\n",
      "  - Base network: Classes [0, 1, 3]\n",
      "  - Adapter #0: Classes [2]\n",
      "\n",
      "üîß Trainable Parameter Status:\n",
      "  ‚úÖ conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚úÖ bn1.weight                                         | shape=[64]\n",
      "  ‚úÖ bn1.bias                                           | shape=[64]\n",
      "  ‚úÖ layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚úÖ layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚úÖ layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚úÖ layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚úÖ layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚úÖ layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚úÖ layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚úÖ layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚úÖ layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚úÖ layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚úÖ layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚úÖ layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚úÖ layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚úÖ layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚úÖ layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚úÖ layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚úÖ layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚úÖ layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚úÖ layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚úÖ layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚úÖ layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚úÖ layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚úÖ layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚úÖ layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚úÖ layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚úÖ layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚úÖ layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚úÖ layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚úÖ layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚úÖ layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚úÖ layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚úÖ layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚úÖ layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚úÖ layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚úÖ layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚úÖ layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚úÖ layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ fc.weight                                          | shape=[4, 1024]\n",
      "  ‚úÖ fc.bias                                            | shape=[4]\n",
      "trainable_count: 78\n",
      "\n",
      "üìä Parameter Statistics:\n",
      "  - Total parameters: 3,889,796\n",
      "  - Trainable parameters: 3,889,796 (100.00%)\n",
      "  - Frozen parameters: 0 (0.00%)\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "\n",
      "üß† Parameters currently controlled by the optimizer: (78)\n",
      "  ‚úÖ conv1.weight\n",
      "  ‚úÖ bn1.weight\n",
      "  ‚úÖ bn1.bias\n",
      "  ‚úÖ layer1.0.conv1.weight\n",
      "  ‚úÖ layer1.0.bn1.weight\n",
      "  ‚úÖ layer1.0.bn1.bias\n",
      "  ‚úÖ layer1.0.conv2.weight\n",
      "  ‚úÖ layer1.0.bn2.weight\n",
      "  ‚úÖ layer1.0.bn2.bias\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer1.1.conv1.weight\n",
      "  ‚úÖ layer1.1.bn1.weight\n",
      "  ‚úÖ layer1.1.bn1.bias\n",
      "  ‚úÖ layer1.1.conv2.weight\n",
      "  ‚úÖ layer1.1.bn2.weight\n",
      "  ‚úÖ layer1.1.bn2.bias\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer2.0.conv1.weight\n",
      "  ‚úÖ layer2.0.bn1.weight\n",
      "  ‚úÖ layer2.0.bn1.bias\n",
      "  ‚úÖ layer2.0.conv2.weight\n",
      "  ‚úÖ layer2.0.bn2.weight\n",
      "  ‚úÖ layer2.0.bn2.bias\n",
      "  ‚úÖ layer2.0.downsample.0.weight\n",
      "  ‚úÖ layer2.0.downsample.1.weight\n",
      "  ‚úÖ layer2.0.downsample.1.bias\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer2.1.conv1.weight\n",
      "  ‚úÖ layer2.1.bn1.weight\n",
      "  ‚úÖ layer2.1.bn1.bias\n",
      "  ‚úÖ layer2.1.conv2.weight\n",
      "  ‚úÖ layer2.1.bn2.weight\n",
      "  ‚úÖ layer2.1.bn2.bias\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer3.0.conv1.weight\n",
      "  ‚úÖ layer3.0.bn1.weight\n",
      "  ‚úÖ layer3.0.bn1.bias\n",
      "  ‚úÖ layer3.0.conv2.weight\n",
      "  ‚úÖ layer3.0.bn2.weight\n",
      "  ‚úÖ layer3.0.bn2.bias\n",
      "  ‚úÖ layer3.0.downsample.0.weight\n",
      "  ‚úÖ layer3.0.downsample.1.weight\n",
      "  ‚úÖ layer3.0.downsample.1.bias\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer3.1.conv1.weight\n",
      "  ‚úÖ layer3.1.bn1.weight\n",
      "  ‚úÖ layer3.1.bn1.bias\n",
      "  ‚úÖ layer3.1.conv2.weight\n",
      "  ‚úÖ layer3.1.bn2.weight\n",
      "  ‚úÖ layer3.1.bn2.bias\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer4.0.conv1.weight\n",
      "  ‚úÖ layer4.0.bn1.weight\n",
      "  ‚úÖ layer4.0.bn1.bias\n",
      "  ‚úÖ layer4.0.conv2.weight\n",
      "  ‚úÖ layer4.0.bn2.weight\n",
      "  ‚úÖ layer4.0.bn2.bias\n",
      "  ‚úÖ layer4.0.downsample.0.weight\n",
      "  ‚úÖ layer4.0.downsample.1.weight\n",
      "  ‚úÖ layer4.0.downsample.1.bias\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer4.1.conv1.weight\n",
      "  ‚úÖ layer4.1.bn1.weight\n",
      "  ‚úÖ layer4.1.bn1.bias\n",
      "  ‚úÖ layer4.1.conv2.weight\n",
      "  ‚úÖ layer4.1.bn2.weight\n",
      "  ‚úÖ layer4.1.bn2.bias\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ fc.weight\n",
      "  ‚úÖ fc.bias\n",
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/200, Train Loss: 20.106839, Train-Class-Acc: {0: '68.66%', 1: '59.02%', 2: '66.72%', 3: '67.83%'}\n",
      "Val Loss: 5.837386, Val Acc: 80.27%, Val-Class-Acc: {0: '83.70%', 1: '72.13%', 2: '75.00%', 3: '88.93%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_1.pth\n",
      "Epoch 2/200, Train Loss: 8.774411, Train-Class-Acc: {0: '71.39%', 1: '68.24%', 2: '79.20%', 3: '84.73%'}\n",
      "Val Loss: 8.359423, Val Acc: 81.37%, Val-Class-Acc: {0: '76.63%', 1: '81.15%', 2: '86.81%', 3: '81.97%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_2.pth\n",
      "Epoch 3/200, Train Loss: 6.540909, Train-Class-Acc: {0: '73.02%', 1: '69.57%', 2: '81.98%', 3: '86.07%'}\n",
      "Val Loss: 2.969136, Val Acc: 82.84%, Val-Class-Acc: {0: '72.83%', 1: '79.92%', 2: '77.78%', 3: '96.31%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_3.pth\n",
      "Epoch 4/200, Train Loss: 4.941518, Train-Class-Acc: {0: '76.70%', 1: '72.95%', 2: '82.84%', 3: '86.68%'}\n",
      "Val Loss: 3.666663, Val Acc: 83.82%, Val-Class-Acc: {0: '70.11%', 1: '77.05%', 2: '90.97%', 3: '96.72%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_4.pth\n",
      "Epoch 5/200, Train Loss: 5.021065, Train-Class-Acc: {0: '75.48%', 1: '73.67%', 2: '84.40%', 3: '88.11%'}\n",
      "Val Loss: 8.057350, Val Acc: 80.64%, Val-Class-Acc: {0: '90.22%', 1: '61.07%', 2: '87.50%', 3: '88.93%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_5.pth\n",
      "Epoch 6/200, Train Loss: 4.567223, Train-Class-Acc: {0: '76.02%', 1: '74.39%', 2: '83.36%', 3: '88.63%'}\n",
      "Val Loss: 4.692350, Val Acc: 83.46%, Val-Class-Acc: {0: '88.59%', 1: '64.34%', 2: '86.11%', 3: '97.13%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_1.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_6.pth\n",
      "Epoch 7/200, Train Loss: 4.281070, Train-Class-Acc: {0: '76.16%', 1: '73.67%', 2: '82.84%', 3: '86.99%'}\n",
      "Val Loss: 2.619952, Val Acc: 83.82%, Val-Class-Acc: {0: '66.30%', 1: '88.52%', 2: '85.42%', 3: '91.39%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_5.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_7.pth\n",
      "Epoch 8/200, Train Loss: 2.612233, Train-Class-Acc: {0: '77.93%', 1: '75.31%', 2: '85.10%', 3: '88.22%'}\n",
      "Val Loss: 2.496225, Val Acc: 84.80%, Val-Class-Acc: {0: '90.22%', 1: '78.69%', 2: '76.39%', 3: '91.80%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_2.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_8.pth\n",
      "Epoch 9/200, Train Loss: 3.234830, Train-Class-Acc: {0: '79.70%', 1: '76.64%', 2: '87.69%', 3: '89.04%'}\n",
      "Val Loss: 7.311569, Val Acc: 82.35%, Val-Class-Acc: {0: '85.87%', 1: '74.18%', 2: '86.81%', 3: '85.25%'}, LR: 0.001000\n",
      "Epoch 10/200, Train Loss: 2.603903, Train-Class-Acc: {0: '79.16%', 1: '75.00%', 2: '85.79%', 3: '90.88%'}\n",
      "Val Loss: 1.606960, Val Acc: 84.44%, Val-Class-Acc: {0: '80.43%', 1: '78.28%', 2: '80.56%', 3: '95.90%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_3.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_10.pth\n",
      "Epoch 11/200, Train Loss: 1.275340, Train-Class-Acc: {0: '80.65%', 1: '82.17%', 2: '89.43%', 3: '93.03%'}\n",
      "Val Loss: 3.812552, Val Acc: 83.82%, Val-Class-Acc: {0: '88.59%', 1: '75.41%', 2: '90.28%', 3: '84.84%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_6.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_11.pth\n",
      "Epoch 12/200, Train Loss: 2.530563, Train-Class-Acc: {0: '78.88%', 1: '76.13%', 2: '86.31%', 3: '90.57%'}\n",
      "Val Loss: 1.630060, Val Acc: 84.31%, Val-Class-Acc: {0: '82.07%', 1: '74.18%', 2: '84.72%', 3: '95.90%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_4.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_12.pth\n",
      "Epoch 13/200, Train Loss: 1.833216, Train-Class-Acc: {0: '82.43%', 1: '79.71%', 2: '87.18%', 3: '92.01%'}\n",
      "Val Loss: 2.708430, Val Acc: 85.54%, Val-Class-Acc: {0: '84.24%', 1: '74.59%', 2: '89.58%', 3: '95.08%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_7.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_13.pth\n",
      "Epoch 14/200, Train Loss: 1.666455, Train-Class-Acc: {0: '83.51%', 1: '83.20%', 2: '92.72%', 3: '93.55%'}\n",
      "Val Loss: 1.917074, Val Acc: 84.80%, Val-Class-Acc: {0: '73.91%', 1: '80.74%', 2: '88.19%', 3: '95.08%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_11.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_14.pth\n",
      "Epoch 15/200, Train Loss: 1.540393, Train-Class-Acc: {0: '84.60%', 1: '82.58%', 2: '90.64%', 3: '93.03%'}\n",
      "Val Loss: 2.082661, Val Acc: 85.42%, Val-Class-Acc: {0: '83.70%', 1: '81.15%', 2: '84.03%', 3: '91.80%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_12.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_15.pth\n",
      "Epoch 16/200, Train Loss: 1.347563, Train-Class-Acc: {0: '81.06%', 1: '81.66%', 2: '89.08%', 3: '92.62%'}\n",
      "Val Loss: 1.374975, Val Acc: 86.64%, Val-Class-Acc: {0: '85.87%', 1: '81.15%', 2: '84.03%', 3: '94.26%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_10.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_16.pth\n",
      "Epoch 17/200, Train Loss: 1.378993, Train-Class-Acc: {0: '83.79%', 1: '84.02%', 2: '91.16%', 3: '93.34%'}\n",
      "Val Loss: 1.415142, Val Acc: 84.44%, Val-Class-Acc: {0: '84.24%', 1: '72.54%', 2: '87.50%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 18/200, Train Loss: 1.157675, Train-Class-Acc: {0: '84.60%', 1: '85.76%', 2: '92.20%', 3: '93.24%'}\n",
      "Val Loss: 10.670702, Val Acc: 83.21%, Val-Class-Acc: {0: '69.02%', 1: '86.07%', 2: '88.19%', 3: '88.11%'}, LR: 0.001000\n",
      "Epoch 19/200, Train Loss: 1.661982, Train-Class-Acc: {0: '84.60%', 1: '81.15%', 2: '89.08%', 3: '90.68%'}\n",
      "Val Loss: 3.504838, Val Acc: 83.46%, Val-Class-Acc: {0: '79.89%', 1: '73.77%', 2: '89.58%', 3: '92.21%'}, LR: 0.001000\n",
      "Epoch 20/200, Train Loss: 1.364956, Train-Class-Acc: {0: '81.34%', 1: '81.45%', 2: '89.95%', 3: '92.11%'}\n",
      "Val Loss: 1.404529, Val Acc: 87.75%, Val-Class-Acc: {0: '90.22%', 1: '82.38%', 2: '81.94%', 3: '94.67%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_8.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_20.pth\n",
      "Epoch 21/200, Train Loss: 0.909584, Train-Class-Acc: {0: '86.65%', 1: '86.99%', 2: '93.07%', 3: '95.39%'}\n",
      "Val Loss: 1.562581, Val Acc: 87.01%, Val-Class-Acc: {0: '86.41%', 1: '81.56%', 2: '83.33%', 3: '95.08%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_14.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_21.pth\n",
      "Epoch 22/200, Train Loss: 0.800665, Train-Class-Acc: {0: '87.47%', 1: '86.89%', 2: '92.37%', 3: '95.18%'}\n",
      "Val Loss: 1.853799, Val Acc: 84.80%, Val-Class-Acc: {0: '75.00%', 1: '86.07%', 2: '84.72%', 3: '90.98%'}, LR: 0.001000\n",
      "Epoch 23/200, Train Loss: 1.035076, Train-Class-Acc: {0: '86.92%', 1: '84.73%', 2: '91.33%', 3: '93.75%'}\n",
      "Val Loss: 2.982796, Val Acc: 82.72%, Val-Class-Acc: {0: '86.41%', 1: '75.82%', 2: '87.50%', 3: '84.02%'}, LR: 0.001000\n",
      "Epoch 24/200, Train Loss: 1.322360, Train-Class-Acc: {0: '86.51%', 1: '84.53%', 2: '91.51%', 3: '92.83%'}\n",
      "Val Loss: 3.107150, Val Acc: 85.42%, Val-Class-Acc: {0: '83.70%', 1: '76.23%', 2: '90.97%', 3: '92.62%'}, LR: 0.001000\n",
      "Epoch 25/200, Train Loss: 0.578177, Train-Class-Acc: {0: '88.56%', 1: '88.11%', 2: '94.11%', 3: '95.80%'}\n",
      "Val Loss: 1.668872, Val Acc: 85.54%, Val-Class-Acc: {0: '89.67%', 1: '77.46%', 2: '78.47%', 3: '94.67%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_15.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_25.pth\n",
      "Epoch 26/200, Train Loss: 0.992215, Train-Class-Acc: {0: '88.83%', 1: '88.11%', 2: '93.59%', 3: '94.88%'}\n",
      "Val Loss: 2.372101, Val Acc: 85.05%, Val-Class-Acc: {0: '80.43%', 1: '80.74%', 2: '82.64%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 27/200, Train Loss: 1.863681, Train-Class-Acc: {0: '85.42%', 1: '82.99%', 2: '89.77%', 3: '92.73%'}\n",
      "Val Loss: 1.626032, Val Acc: 82.60%, Val-Class-Acc: {0: '65.22%', 1: '78.69%', 2: '90.28%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 28/200, Train Loss: 2.000882, Train-Class-Acc: {0: '83.65%', 1: '82.58%', 2: '90.12%', 3: '93.14%'}\n",
      "Val Loss: 1.149415, Val Acc: 82.60%, Val-Class-Acc: {0: '65.76%', 1: '76.64%', 2: '90.28%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 29/200, Train Loss: 1.178479, Train-Class-Acc: {0: '84.88%', 1: '84.22%', 2: '88.91%', 3: '93.75%'}\n",
      "Val Loss: 1.224274, Val Acc: 83.46%, Val-Class-Acc: {0: '75.00%', 1: '79.92%', 2: '81.94%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 30/200, Train Loss: 1.446747, Train-Class-Acc: {0: '86.92%', 1: '84.53%', 2: '92.37%', 3: '94.16%'}\n",
      "Val Loss: 2.716738, Val Acc: 86.27%, Val-Class-Acc: {0: '81.52%', 1: '83.61%', 2: '85.42%', 3: '93.03%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_13.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_30.pth\n",
      "Epoch 31/200, Train Loss: 1.091409, Train-Class-Acc: {0: '87.60%', 1: '85.55%', 2: '92.37%', 3: '94.88%'}\n",
      "Val Loss: 1.844537, Val Acc: 83.82%, Val-Class-Acc: {0: '74.46%', 1: '87.70%', 2: '76.39%', 3: '91.39%'}, LR: 0.001000\n",
      "Epoch 32/200, Train Loss: 0.571498, Train-Class-Acc: {0: '88.69%', 1: '87.60%', 2: '93.59%', 3: '95.90%'}\n",
      "Val Loss: 1.049108, Val Acc: 84.44%, Val-Class-Acc: {0: '76.09%', 1: '80.33%', 2: '84.03%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 33/200, Train Loss: 1.161249, Train-Class-Acc: {0: '88.01%', 1: '87.81%', 2: '92.20%', 3: '94.77%'}\n",
      "Val Loss: 2.830419, Val Acc: 85.42%, Val-Class-Acc: {0: '82.07%', 1: '79.92%', 2: '86.81%', 3: '92.62%'}, LR: 0.001000\n",
      "Epoch 34/200, Train Loss: 1.515222, Train-Class-Acc: {0: '83.79%', 1: '81.76%', 2: '88.73%', 3: '92.11%'}\n",
      "Val Loss: 1.530541, Val Acc: 80.88%, Val-Class-Acc: {0: '88.59%', 1: '54.51%', 2: '90.28%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 35/200, Train Loss: 1.682660, Train-Class-Acc: {0: '78.07%', 1: '79.51%', 2: '89.60%', 3: '92.83%'}\n",
      "Val Loss: 1.492929, Val Acc: 84.80%, Val-Class-Acc: {0: '80.98%', 1: '76.23%', 2: '87.50%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 36/200, Train Loss: 0.474494, Train-Class-Acc: {0: '90.05%', 1: '86.89%', 2: '93.76%', 3: '95.39%'}\n",
      "Val Loss: 1.050535, Val Acc: 84.68%, Val-Class-Acc: {0: '78.80%', 1: '77.46%', 2: '85.42%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 37/200, Train Loss: 0.346052, Train-Class-Acc: {0: '91.83%', 1: '91.60%', 2: '94.80%', 3: '96.21%'}\n",
      "Val Loss: 1.383302, Val Acc: 85.66%, Val-Class-Acc: {0: '83.15%', 1: '77.46%', 2: '86.11%', 3: '95.49%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_25.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_37.pth\n",
      "Epoch 38/200, Train Loss: 0.434984, Train-Class-Acc: {0: '93.87%', 1: '91.50%', 2: '96.71%', 3: '96.62%'}\n",
      "Val Loss: 1.302750, Val Acc: 87.38%, Val-Class-Acc: {0: '91.85%', 1: '79.10%', 2: '81.94%', 3: '95.49%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_37.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_38.pth\n",
      "Epoch 39/200, Train Loss: 0.499533, Train-Class-Acc: {0: '92.37%', 1: '91.80%', 2: '95.67%', 3: '97.75%'}\n",
      "Val Loss: 1.028993, Val Acc: 87.13%, Val-Class-Acc: {0: '85.87%', 1: '81.15%', 2: '84.03%', 3: '95.90%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_30.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_39.pth\n",
      "Epoch 40/200, Train Loss: 1.165409, Train-Class-Acc: {0: '92.64%', 1: '90.88%', 2: '95.15%', 3: '95.49%'}\n",
      "Val Loss: 1.825105, Val Acc: 84.56%, Val-Class-Acc: {0: '91.30%', 1: '71.31%', 2: '87.50%', 3: '90.98%'}, LR: 0.001000\n",
      "Epoch 41/200, Train Loss: 0.322060, Train-Class-Acc: {0: '91.69%', 1: '92.93%', 2: '96.53%', 3: '96.93%'}\n",
      "Val Loss: 0.911307, Val Acc: 85.66%, Val-Class-Acc: {0: '83.15%', 1: '74.59%', 2: '88.89%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 42/200, Train Loss: 1.102249, Train-Class-Acc: {0: '90.60%', 1: '89.24%', 2: '94.11%', 3: '95.90%'}\n",
      "Val Loss: 7.299482, Val Acc: 82.11%, Val-Class-Acc: {0: '68.48%', 1: '85.66%', 2: '86.81%', 3: '86.07%'}, LR: 0.001000\n",
      "Epoch 43/200, Train Loss: 0.620519, Train-Class-Acc: {0: '90.87%', 1: '88.63%', 2: '94.97%', 3: '95.70%'}\n",
      "Val Loss: 1.148788, Val Acc: 84.44%, Val-Class-Acc: {0: '73.91%', 1: '79.92%', 2: '86.81%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 44/200, Train Loss: 0.775727, Train-Class-Acc: {0: '91.28%', 1: '90.37%', 2: '95.49%', 3: '96.41%'}\n",
      "Val Loss: 1.758306, Val Acc: 85.29%, Val-Class-Acc: {0: '80.43%', 1: '77.05%', 2: '86.81%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 45/200, Train Loss: 0.918791, Train-Class-Acc: {0: '87.47%', 1: '86.89%', 2: '92.20%', 3: '94.77%'}\n",
      "Val Loss: 3.153337, Val Acc: 85.54%, Val-Class-Acc: {0: '86.41%', 1: '79.92%', 2: '80.56%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 46/200, Train Loss: 0.715384, Train-Class-Acc: {0: '91.14%', 1: '87.91%', 2: '92.55%', 3: '95.59%'}\n",
      "Val Loss: 0.832840, Val Acc: 83.58%, Val-Class-Acc: {0: '69.57%', 1: '76.23%', 2: '91.67%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 47/200, Train Loss: 1.081609, Train-Class-Acc: {0: '88.42%', 1: '85.96%', 2: '92.89%', 3: '94.36%'}\n",
      "Val Loss: 1.784735, Val Acc: 86.52%, Val-Class-Acc: {0: '88.04%', 1: '81.15%', 2: '81.25%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 48/200, Train Loss: 0.790584, Train-Class-Acc: {0: '93.73%', 1: '90.98%', 2: '94.45%', 3: '95.80%'}\n",
      "Val Loss: 2.228938, Val Acc: 83.58%, Val-Class-Acc: {0: '60.87%', 1: '86.89%', 2: '86.81%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 49/200, Train Loss: 1.131692, Train-Class-Acc: {0: '88.69%', 1: '87.50%', 2: '93.76%', 3: '95.39%'}\n",
      "Val Loss: 2.840203, Val Acc: 85.29%, Val-Class-Acc: {0: '92.39%', 1: '78.28%', 2: '81.25%', 3: '89.34%'}, LR: 0.001000\n",
      "Epoch 50/200, Train Loss: 0.624561, Train-Class-Acc: {0: '89.10%', 1: '88.01%', 2: '95.32%', 3: '95.29%'}\n",
      "Val Loss: 1.389884, Val Acc: 84.80%, Val-Class-Acc: {0: '88.59%', 1: '70.49%', 2: '86.11%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 51/200, Train Loss: 1.035606, Train-Class-Acc: {0: '91.83%', 1: '90.06%', 2: '95.15%', 3: '96.11%'}\n",
      "Val Loss: 1.708898, Val Acc: 84.44%, Val-Class-Acc: {0: '85.33%', 1: '69.67%', 2: '87.50%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 52/200, Train Loss: 0.976065, Train-Class-Acc: {0: '89.78%', 1: '87.91%', 2: '93.24%', 3: '94.77%'}\n",
      "Val Loss: 1.761762, Val Acc: 84.44%, Val-Class-Acc: {0: '85.33%', 1: '79.51%', 2: '77.08%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 53/200, Train Loss: 0.724563, Train-Class-Acc: {0: '93.60%', 1: '90.78%', 2: '94.80%', 3: '96.11%'}\n",
      "Val Loss: 1.021542, Val Acc: 84.44%, Val-Class-Acc: {0: '86.41%', 1: '68.03%', 2: '88.89%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 54/200, Train Loss: 0.566708, Train-Class-Acc: {0: '91.42%', 1: '90.57%', 2: '95.84%', 3: '96.72%'}\n",
      "Val Loss: 2.034434, Val Acc: 85.42%, Val-Class-Acc: {0: '86.96%', 1: '72.54%', 2: '84.72%', 3: '97.54%'}, LR: 0.001000\n",
      "Epoch 55/200, Train Loss: 0.480266, Train-Class-Acc: {0: '92.37%', 1: '92.42%', 2: '96.36%', 3: '96.31%'}\n",
      "Val Loss: 0.926922, Val Acc: 86.52%, Val-Class-Acc: {0: '90.22%', 1: '75.82%', 2: '80.56%', 3: '97.95%'}, LR: 0.001000\n",
      "Epoch 56/200, Train Loss: 0.536026, Train-Class-Acc: {0: '92.78%', 1: '93.14%', 2: '95.49%', 3: '96.52%'}\n",
      "Val Loss: 1.366520, Val Acc: 85.54%, Val-Class-Acc: {0: '86.41%', 1: '81.15%', 2: '79.17%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 57/200, Train Loss: 0.377383, Train-Class-Acc: {0: '91.83%', 1: '92.01%', 2: '95.49%', 3: '96.82%'}\n",
      "Val Loss: 1.551648, Val Acc: 84.80%, Val-Class-Acc: {0: '91.30%', 1: '73.36%', 2: '82.64%', 3: '92.62%'}, LR: 0.001000\n",
      "Epoch 58/200, Train Loss: 0.637231, Train-Class-Acc: {0: '91.96%', 1: '89.45%', 2: '93.07%', 3: '95.90%'}\n",
      "Val Loss: 1.257800, Val Acc: 85.05%, Val-Class-Acc: {0: '79.89%', 1: '78.69%', 2: '82.64%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 59/200, Train Loss: 0.734735, Train-Class-Acc: {0: '93.19%', 1: '91.80%', 2: '97.05%', 3: '97.13%'}\n",
      "Val Loss: 2.597227, Val Acc: 85.17%, Val-Class-Acc: {0: '88.04%', 1: '72.95%', 2: '89.58%', 3: '92.62%'}, LR: 0.001000\n",
      "Epoch 60/200, Train Loss: 0.331127, Train-Class-Acc: {0: '95.10%', 1: '96.00%', 2: '98.44%', 3: '98.05%'}\n",
      "Val Loss: 1.709589, Val Acc: 84.56%, Val-Class-Acc: {0: '77.72%', 1: '76.23%', 2: '86.11%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 61/200, Train Loss: 0.528521, Train-Class-Acc: {0: '93.60%', 1: '91.70%', 2: '96.88%', 3: '96.52%'}\n",
      "Val Loss: 2.788077, Val Acc: 86.40%, Val-Class-Acc: {0: '77.72%', 1: '82.38%', 2: '88.19%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 62/200, Train Loss: 0.467121, Train-Class-Acc: {0: '93.60%', 1: '93.03%', 2: '97.92%', 3: '97.44%'}\n",
      "Val Loss: 1.300314, Val Acc: 84.68%, Val-Class-Acc: {0: '83.15%', 1: '72.95%', 2: '86.81%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 63/200, Train Loss: 0.406621, Train-Class-Acc: {0: '94.28%', 1: '94.47%', 2: '96.36%', 3: '97.54%'}\n",
      "Val Loss: 1.119435, Val Acc: 85.78%, Val-Class-Acc: {0: '85.33%', 1: '77.87%', 2: '87.50%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 64/200, Train Loss: 0.663686, Train-Class-Acc: {0: '95.50%', 1: '93.14%', 2: '96.36%', 3: '96.82%'}\n",
      "Val Loss: 2.343803, Val Acc: 82.60%, Val-Class-Acc: {0: '75.54%', 1: '68.03%', 2: '92.36%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 65/200, Train Loss: 0.515997, Train-Class-Acc: {0: '93.87%', 1: '91.91%', 2: '94.80%', 3: '95.80%'}\n",
      "Val Loss: 1.202730, Val Acc: 84.56%, Val-Class-Acc: {0: '87.50%', 1: '70.49%', 2: '86.11%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 66/200, Train Loss: 0.212140, Train-Class-Acc: {0: '93.73%', 1: '93.95%', 2: '96.53%', 3: '97.75%'}\n",
      "Val Loss: 1.609357, Val Acc: 83.95%, Val-Class-Acc: {0: '86.96%', 1: '68.44%', 2: '87.50%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 67/200, Train Loss: 0.073103, Train-Class-Acc: {0: '97.14%', 1: '96.00%', 2: '97.75%', 3: '98.77%'}\n",
      "Val Loss: 1.169553, Val Acc: 86.89%, Val-Class-Acc: {0: '85.33%', 1: '79.51%', 2: '88.89%', 3: '94.26%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_16.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_67.pth\n",
      "Epoch 68/200, Train Loss: 0.155134, Train-Class-Acc: {0: '97.82%', 1: '97.44%', 2: '98.61%', 3: '98.67%'}\n",
      "Val Loss: 1.639305, Val Acc: 86.64%, Val-Class-Acc: {0: '88.04%', 1: '77.87%', 2: '83.33%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 69/200, Train Loss: 0.055245, Train-Class-Acc: {0: '97.55%', 1: '97.34%', 2: '98.61%', 3: '99.39%'}\n",
      "Val Loss: 1.998907, Val Acc: 86.03%, Val-Class-Acc: {0: '85.87%', 1: '75.82%', 2: '91.67%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 70/200, Train Loss: 0.393210, Train-Class-Acc: {0: '96.73%', 1: '96.21%', 2: '95.15%', 3: '97.13%'}\n",
      "Val Loss: 1.890645, Val Acc: 84.80%, Val-Class-Acc: {0: '77.17%', 1: '81.56%', 2: '83.33%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 71/200, Train Loss: 0.368778, Train-Class-Acc: {0: '95.23%', 1: '93.55%', 2: '97.40%', 3: '97.23%'}\n",
      "Val Loss: 1.417572, Val Acc: 86.15%, Val-Class-Acc: {0: '80.98%', 1: '81.56%', 2: '87.50%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 72/200, Train Loss: 0.358641, Train-Class-Acc: {0: '95.23%', 1: '94.88%', 2: '96.88%', 3: '98.16%'}\n",
      "Val Loss: 2.183208, Val Acc: 83.82%, Val-Class-Acc: {0: '88.04%', 1: '68.44%', 2: '88.19%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 73/200, Train Loss: 0.686442, Train-Class-Acc: {0: '93.60%', 1: '91.50%', 2: '95.67%', 3: '95.80%'}\n",
      "Val Loss: 2.770651, Val Acc: 85.66%, Val-Class-Acc: {0: '85.87%', 1: '76.64%', 2: '86.81%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 74/200, Train Loss: 0.456703, Train-Class-Acc: {0: '95.50%', 1: '94.77%', 2: '96.88%', 3: '98.05%'}\n",
      "Val Loss: 1.366264, Val Acc: 82.97%, Val-Class-Acc: {0: '58.70%', 1: '87.30%', 2: '89.58%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 75/200, Train Loss: 0.393232, Train-Class-Acc: {0: '94.28%', 1: '92.83%', 2: '95.49%', 3: '96.21%'}\n",
      "Val Loss: 1.323195, Val Acc: 85.29%, Val-Class-Acc: {0: '76.63%', 1: '77.87%', 2: '90.97%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 76/200, Train Loss: 0.813475, Train-Class-Acc: {0: '93.73%', 1: '91.50%', 2: '93.76%', 3: '96.21%'}\n",
      "Val Loss: 1.373562, Val Acc: 86.03%, Val-Class-Acc: {0: '81.52%', 1: '84.43%', 2: '88.19%', 3: '89.75%'}, LR: 0.001000\n",
      "Epoch 77/200, Train Loss: 0.275877, Train-Class-Acc: {0: '93.60%', 1: '93.34%', 2: '96.71%', 3: '96.93%'}\n",
      "Val Loss: 1.180766, Val Acc: 85.54%, Val-Class-Acc: {0: '75.00%', 1: '80.74%', 2: '89.58%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 78/200, Train Loss: 0.174158, Train-Class-Acc: {0: '95.91%', 1: '96.62%', 2: '97.92%', 3: '97.44%'}\n",
      "Val Loss: 1.228973, Val Acc: 85.54%, Val-Class-Acc: {0: '88.59%', 1: '73.36%', 2: '80.56%', 3: '98.36%'}, LR: 0.001000\n",
      "Epoch 79/200, Train Loss: 0.244854, Train-Class-Acc: {0: '96.87%', 1: '95.70%', 2: '98.44%', 3: '98.26%'}\n",
      "Val Loss: 1.113750, Val Acc: 83.58%, Val-Class-Acc: {0: '73.91%', 1: '77.46%', 2: '84.72%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 80/200, Train Loss: 0.362944, Train-Class-Acc: {0: '95.23%', 1: '94.06%', 2: '97.40%', 3: '97.34%'}\n",
      "Val Loss: 2.899609, Val Acc: 85.42%, Val-Class-Acc: {0: '89.67%', 1: '76.23%', 2: '88.19%', 3: '89.75%'}, LR: 0.001000\n",
      "Epoch 81/200, Train Loss: 0.315459, Train-Class-Acc: {0: '96.59%', 1: '96.41%', 2: '97.40%', 3: '98.05%'}\n",
      "Val Loss: 1.327120, Val Acc: 85.54%, Val-Class-Acc: {0: '85.33%', 1: '74.18%', 2: '87.50%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 82/200, Train Loss: 0.279011, Train-Class-Acc: {0: '96.73%', 1: '95.49%', 2: '98.27%', 3: '98.26%'}\n",
      "Val Loss: 4.632245, Val Acc: 83.46%, Val-Class-Acc: {0: '77.17%', 1: '78.28%', 2: '87.50%', 3: '90.98%'}, LR: 0.001000\n",
      "Epoch 83/200, Train Loss: 0.134838, Train-Class-Acc: {0: '96.73%', 1: '96.62%', 2: '98.79%', 3: '98.77%'}\n",
      "Val Loss: 1.418481, Val Acc: 86.40%, Val-Class-Acc: {0: '84.24%', 1: '82.79%', 2: '84.03%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 84/200, Train Loss: 0.374074, Train-Class-Acc: {0: '96.46%', 1: '95.39%', 2: '96.36%', 3: '97.64%'}\n",
      "Val Loss: 1.343630, Val Acc: 83.58%, Val-Class-Acc: {0: '83.70%', 1: '70.90%', 2: '89.58%', 3: '92.62%'}, LR: 0.001000\n",
      "Epoch 85/200, Train Loss: 0.327011, Train-Class-Acc: {0: '94.55%', 1: '93.34%', 2: '96.88%', 3: '97.23%'}\n",
      "Val Loss: 1.668926, Val Acc: 85.29%, Val-Class-Acc: {0: '72.28%', 1: '84.84%', 2: '88.19%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 86/200, Train Loss: 0.203549, Train-Class-Acc: {0: '97.41%', 1: '96.93%', 2: '97.92%', 3: '98.36%'}\n",
      "Val Loss: 1.776613, Val Acc: 85.54%, Val-Class-Acc: {0: '88.59%', 1: '75.00%', 2: '84.72%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 87/200, Train Loss: 0.155305, Train-Class-Acc: {0: '96.73%', 1: '95.49%', 2: '98.27%', 3: '98.46%'}\n",
      "Val Loss: 1.121709, Val Acc: 85.54%, Val-Class-Acc: {0: '84.78%', 1: '78.28%', 2: '84.72%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 88/200, Train Loss: 0.245463, Train-Class-Acc: {0: '97.14%', 1: '95.49%', 2: '97.75%', 3: '97.85%'}\n",
      "Val Loss: 1.385361, Val Acc: 85.66%, Val-Class-Acc: {0: '81.52%', 1: '78.69%', 2: '84.03%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 89/200, Train Loss: 0.568183, Train-Class-Acc: {0: '93.60%', 1: '92.21%', 2: '96.36%', 3: '96.52%'}\n",
      "Val Loss: 1.831949, Val Acc: 84.80%, Val-Class-Acc: {0: '79.89%', 1: '75.41%', 2: '90.97%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 90/200, Train Loss: 0.146751, Train-Class-Acc: {0: '96.19%', 1: '95.70%', 2: '98.27%', 3: '98.16%'}\n",
      "Val Loss: 1.080056, Val Acc: 85.66%, Val-Class-Acc: {0: '89.13%', 1: '73.36%', 2: '82.64%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 91/200, Train Loss: 0.095458, Train-Class-Acc: {0: '99.05%', 1: '97.95%', 2: '98.61%', 3: '98.98%'}\n",
      "Val Loss: 1.165339, Val Acc: 87.01%, Val-Class-Acc: {0: '87.50%', 1: '79.10%', 2: '87.50%', 3: '94.26%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_67.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_91.pth\n",
      "Epoch 92/200, Train Loss: 0.029703, Train-Class-Acc: {0: '99.18%', 1: '98.87%', 2: '99.65%', 3: '99.69%'}\n",
      "Val Loss: 1.378662, Val Acc: 86.89%, Val-Class-Acc: {0: '82.61%', 1: '84.84%', 2: '81.25%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 93/200, Train Loss: 0.220019, Train-Class-Acc: {0: '97.96%', 1: '96.52%', 2: '97.75%', 3: '98.67%'}\n",
      "Val Loss: 2.074661, Val Acc: 86.52%, Val-Class-Acc: {0: '80.43%', 1: '82.79%', 2: '87.50%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 94/200, Train Loss: 0.466306, Train-Class-Acc: {0: '96.46%', 1: '96.11%', 2: '98.96%', 3: '98.05%'}\n",
      "Val Loss: 1.753112, Val Acc: 83.95%, Val-Class-Acc: {0: '76.63%', 1: '87.30%', 2: '80.56%', 3: '88.11%'}, LR: 0.001000\n",
      "Epoch 95/200, Train Loss: 0.345191, Train-Class-Acc: {0: '95.10%', 1: '95.08%', 2: '97.75%', 3: '97.75%'}\n",
      "Val Loss: 1.598741, Val Acc: 84.93%, Val-Class-Acc: {0: '84.78%', 1: '75.82%', 2: '84.03%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 96/200, Train Loss: 0.335967, Train-Class-Acc: {0: '97.55%', 1: '96.52%', 2: '97.40%', 3: '98.05%'}\n",
      "Val Loss: 2.263590, Val Acc: 85.54%, Val-Class-Acc: {0: '80.43%', 1: '79.10%', 2: '88.19%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 97/200, Train Loss: 0.223678, Train-Class-Acc: {0: '98.09%', 1: '97.34%', 2: '98.09%', 3: '98.87%'}\n",
      "Val Loss: 1.547971, Val Acc: 86.27%, Val-Class-Acc: {0: '86.41%', 1: '79.92%', 2: '84.72%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 98/200, Train Loss: 0.791697, Train-Class-Acc: {0: '92.37%', 1: '90.98%', 2: '92.89%', 3: '96.52%'}\n",
      "Val Loss: 1.953028, Val Acc: 82.72%, Val-Class-Acc: {0: '82.61%', 1: '69.26%', 2: '81.25%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 99/200, Train Loss: 0.145155, Train-Class-Acc: {0: '94.55%', 1: '95.90%', 2: '97.75%', 3: '98.16%'}\n",
      "Val Loss: 2.085304, Val Acc: 83.95%, Val-Class-Acc: {0: '90.22%', 1: '72.13%', 2: '78.47%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 100/200, Train Loss: 0.283679, Train-Class-Acc: {0: '95.37%', 1: '94.88%', 2: '97.23%', 3: '98.57%'}\n",
      "Val Loss: 2.739993, Val Acc: 80.51%, Val-Class-Acc: {0: '47.83%', 1: '87.30%', 2: '87.50%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 101/200, Train Loss: 1.014439, Train-Class-Acc: {0: '90.05%', 1: '90.37%', 2: '94.97%', 3: '97.03%'}\n",
      "Val Loss: 2.539170, Val Acc: 83.70%, Val-Class-Acc: {0: '85.87%', 1: '70.90%', 2: '85.42%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 102/200, Train Loss: 0.959793, Train-Class-Acc: {0: '92.51%', 1: '88.83%', 2: '95.15%', 3: '95.39%'}\n",
      "Val Loss: 11.439662, Val Acc: 81.37%, Val-Class-Acc: {0: '80.43%', 1: '80.33%', 2: '87.50%', 3: '79.51%'}, LR: 0.001000\n",
      "Epoch 103/200, Train Loss: 0.375038, Train-Class-Acc: {0: '93.32%', 1: '91.80%', 2: '96.01%', 3: '96.21%'}\n",
      "Val Loss: 1.974209, Val Acc: 84.68%, Val-Class-Acc: {0: '85.87%', 1: '77.87%', 2: '89.58%', 3: '87.70%'}, LR: 0.001000\n",
      "Epoch 104/200, Train Loss: 0.225056, Train-Class-Acc: {0: '97.14%', 1: '96.11%', 2: '97.75%', 3: '98.46%'}\n",
      "Val Loss: 1.037716, Val Acc: 86.27%, Val-Class-Acc: {0: '78.26%', 1: '80.33%', 2: '88.89%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 105/200, Train Loss: 0.292054, Train-Class-Acc: {0: '95.91%', 1: '96.62%', 2: '98.09%', 3: '98.36%'}\n",
      "Val Loss: 1.192323, Val Acc: 85.54%, Val-Class-Acc: {0: '89.13%', 1: '70.08%', 2: '87.50%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 106/200, Train Loss: 0.303376, Train-Class-Acc: {0: '97.14%', 1: '96.21%', 2: '98.44%', 3: '98.77%'}\n",
      "Val Loss: 1.744400, Val Acc: 83.33%, Val-Class-Acc: {0: '76.09%', 1: '76.64%', 2: '81.94%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 107/200, Train Loss: 0.250119, Train-Class-Acc: {0: '95.91%', 1: '94.57%', 2: '97.05%', 3: '97.85%'}\n",
      "Val Loss: 1.010012, Val Acc: 86.40%, Val-Class-Acc: {0: '79.35%', 1: '79.51%', 2: '90.28%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 108/200, Train Loss: 0.385526, Train-Class-Acc: {0: '97.41%', 1: '95.90%', 2: '96.53%', 3: '97.64%'}\n",
      "Val Loss: 1.262755, Val Acc: 86.64%, Val-Class-Acc: {0: '90.76%', 1: '76.64%', 2: '82.64%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 109/200, Train Loss: 0.233535, Train-Class-Acc: {0: '97.82%', 1: '96.62%', 2: '98.44%', 3: '98.36%'}\n",
      "Val Loss: 1.581643, Val Acc: 85.29%, Val-Class-Acc: {0: '78.26%', 1: '81.15%', 2: '81.94%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 110/200, Train Loss: 0.433495, Train-Class-Acc: {0: '94.82%', 1: '93.85%', 2: '95.84%', 3: '96.93%'}\n",
      "Val Loss: 3.060954, Val Acc: 84.31%, Val-Class-Acc: {0: '86.41%', 1: '72.95%', 2: '86.81%', 3: '92.62%'}, LR: 0.001000\n",
      "Epoch 111/200, Train Loss: 0.580141, Train-Class-Acc: {0: '96.05%', 1: '94.67%', 2: '97.23%', 3: '98.05%'}\n",
      "Val Loss: 1.136228, Val Acc: 83.46%, Val-Class-Acc: {0: '68.48%', 1: '81.56%', 2: '88.19%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 112/200, Train Loss: 0.422528, Train-Class-Acc: {0: '92.51%', 1: '93.24%', 2: '95.49%', 3: '97.34%'}\n",
      "Val Loss: 1.535447, Val Acc: 84.19%, Val-Class-Acc: {0: '90.76%', 1: '63.11%', 2: '88.89%', 3: '97.54%'}, LR: 0.001000\n",
      "Epoch 113/200, Train Loss: 0.144795, Train-Class-Acc: {0: '96.19%', 1: '95.70%', 2: '97.23%', 3: '97.64%'}\n",
      "Val Loss: 1.198990, Val Acc: 85.29%, Val-Class-Acc: {0: '72.83%', 1: '84.84%', 2: '88.19%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 114/200, Train Loss: 0.092927, Train-Class-Acc: {0: '97.41%', 1: '97.64%', 2: '99.13%', 3: '98.98%'}\n",
      "Val Loss: 2.050466, Val Acc: 84.56%, Val-Class-Acc: {0: '88.04%', 1: '74.18%', 2: '82.64%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 115/200, Train Loss: 0.597769, Train-Class-Acc: {0: '97.00%', 1: '94.98%', 2: '97.57%', 3: '97.54%'}\n",
      "Val Loss: 1.207332, Val Acc: 84.68%, Val-Class-Acc: {0: '85.33%', 1: '77.87%', 2: '77.08%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 116/200, Train Loss: 0.270252, Train-Class-Acc: {0: '96.05%', 1: '96.21%', 2: '97.57%', 3: '98.16%'}\n",
      "Val Loss: 1.366664, Val Acc: 85.66%, Val-Class-Acc: {0: '80.43%', 1: '77.05%', 2: '87.50%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 117/200, Train Loss: 0.122062, Train-Class-Acc: {0: '97.41%', 1: '96.62%', 2: '98.96%', 3: '98.26%'}\n",
      "Val Loss: 1.491788, Val Acc: 86.40%, Val-Class-Acc: {0: '80.43%', 1: '80.74%', 2: '88.19%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 118/200, Train Loss: 0.136546, Train-Class-Acc: {0: '97.14%', 1: '97.13%', 2: '98.79%', 3: '98.67%'}\n",
      "Val Loss: 2.127539, Val Acc: 83.70%, Val-Class-Acc: {0: '65.22%', 1: '84.43%', 2: '85.42%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 119/200, Train Loss: 0.027295, Train-Class-Acc: {0: '98.37%', 1: '98.67%', 2: '99.48%', 3: '99.69%'}\n",
      "Val Loss: 1.738929, Val Acc: 86.15%, Val-Class-Acc: {0: '82.07%', 1: '79.10%', 2: '87.50%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 120/200, Train Loss: 0.039781, Train-Class-Acc: {0: '99.05%', 1: '99.08%', 2: '99.65%', 3: '99.39%'}\n",
      "Val Loss: 1.797792, Val Acc: 85.05%, Val-Class-Acc: {0: '77.17%', 1: '82.79%', 2: '81.94%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 121/200, Train Loss: 0.152443, Train-Class-Acc: {0: '97.82%', 1: '97.64%', 2: '98.61%', 3: '97.85%'}\n",
      "Val Loss: 1.957447, Val Acc: 84.68%, Val-Class-Acc: {0: '78.26%', 1: '76.23%', 2: '87.50%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 122/200, Train Loss: 0.586538, Train-Class-Acc: {0: '97.41%', 1: '96.11%', 2: '98.27%', 3: '98.05%'}\n",
      "Val Loss: 16.040085, Val Acc: 75.25%, Val-Class-Acc: {0: '47.83%', 1: '88.93%', 2: '79.17%', 3: '79.92%'}, LR: 0.001000\n",
      "Epoch 123/200, Train Loss: 0.827046, Train-Class-Acc: {0: '92.92%', 1: '92.62%', 2: '95.15%', 3: '96.11%'}\n",
      "Val Loss: 1.904540, Val Acc: 85.42%, Val-Class-Acc: {0: '93.48%', 1: '73.36%', 2: '78.47%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 124/200, Train Loss: 0.273701, Train-Class-Acc: {0: '95.23%', 1: '95.39%', 2: '95.49%', 3: '98.26%'}\n",
      "Val Loss: 1.363192, Val Acc: 84.07%, Val-Class-Acc: {0: '88.04%', 1: '66.39%', 2: '90.28%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 125/200, Train Loss: 0.202268, Train-Class-Acc: {0: '96.19%', 1: '96.52%', 2: '95.84%', 3: '98.67%'}\n",
      "Val Loss: 1.670660, Val Acc: 86.89%, Val-Class-Acc: {0: '88.04%', 1: '77.87%', 2: '86.11%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 126/200, Train Loss: 0.221820, Train-Class-Acc: {0: '96.73%', 1: '96.21%', 2: '97.40%', 3: '98.98%'}\n",
      "Val Loss: 1.741609, Val Acc: 84.56%, Val-Class-Acc: {0: '73.91%', 1: '79.92%', 2: '90.97%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 127/200, Train Loss: 0.297914, Train-Class-Acc: {0: '96.73%', 1: '97.03%', 2: '99.13%', 3: '99.08%'}\n",
      "Val Loss: 1.415321, Val Acc: 86.52%, Val-Class-Acc: {0: '89.13%', 1: '75.82%', 2: '84.03%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 128/200, Train Loss: 0.095919, Train-Class-Acc: {0: '97.68%', 1: '97.54%', 2: '98.44%', 3: '99.49%'}\n",
      "Val Loss: 1.536542, Val Acc: 86.15%, Val-Class-Acc: {0: '84.24%', 1: '78.69%', 2: '86.11%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 129/200, Train Loss: 0.198955, Train-Class-Acc: {0: '97.55%', 1: '98.26%', 2: '99.48%', 3: '98.77%'}\n",
      "Val Loss: 1.517293, Val Acc: 85.54%, Val-Class-Acc: {0: '87.50%', 1: '71.31%', 2: '87.50%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 130/200, Train Loss: 0.067875, Train-Class-Acc: {0: '98.09%', 1: '97.13%', 2: '98.61%', 3: '99.49%'}\n",
      "Val Loss: 1.269461, Val Acc: 85.91%, Val-Class-Acc: {0: '86.96%', 1: '78.28%', 2: '88.89%', 3: '90.98%'}, LR: 0.001000\n",
      "Epoch 131/200, Train Loss: 0.256904, Train-Class-Acc: {0: '98.37%', 1: '98.26%', 2: '98.27%', 3: '98.46%'}\n",
      "Val Loss: 1.467730, Val Acc: 85.17%, Val-Class-Acc: {0: '80.43%', 1: '75.00%', 2: '89.58%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 132/200, Train Loss: 0.340680, Train-Class-Acc: {0: '94.96%', 1: '94.88%', 2: '96.88%', 3: '98.05%'}\n",
      "Val Loss: 2.183910, Val Acc: 82.97%, Val-Class-Acc: {0: '82.07%', 1: '67.21%', 2: '85.42%', 3: '97.95%'}, LR: 0.001000\n",
      "Epoch 133/200, Train Loss: 0.155445, Train-Class-Acc: {0: '97.96%', 1: '97.75%', 2: '98.09%', 3: '98.87%'}\n",
      "Val Loss: 1.327324, Val Acc: 84.31%, Val-Class-Acc: {0: '81.52%', 1: '74.59%', 2: '84.03%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 134/200, Train Loss: 0.037637, Train-Class-Acc: {0: '98.77%', 1: '98.46%', 2: '98.61%', 3: '99.18%'}\n",
      "Val Loss: 1.518282, Val Acc: 86.15%, Val-Class-Acc: {0: '83.15%', 1: '77.05%', 2: '88.19%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 135/200, Train Loss: 0.022383, Train-Class-Acc: {0: '98.50%', 1: '98.67%', 2: '99.65%', 3: '99.80%'}\n",
      "Val Loss: 1.314513, Val Acc: 87.01%, Val-Class-Acc: {0: '87.50%', 1: '78.28%', 2: '87.50%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 136/200, Train Loss: 0.026383, Train-Class-Acc: {0: '99.46%', 1: '99.39%', 2: '99.31%', 3: '99.59%'}\n",
      "Val Loss: 1.397366, Val Acc: 86.89%, Val-Class-Acc: {0: '89.13%', 1: '75.82%', 2: '87.50%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 137/200, Train Loss: 0.101038, Train-Class-Acc: {0: '98.91%', 1: '98.16%', 2: '99.13%', 3: '99.18%'}\n",
      "Val Loss: 1.655957, Val Acc: 87.13%, Val-Class-Acc: {0: '86.41%', 1: '81.15%', 2: '82.64%', 3: '96.31%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_21.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_137.pth\n",
      "Epoch 138/200, Train Loss: 0.065103, Train-Class-Acc: {0: '98.50%', 1: '98.46%', 2: '99.13%', 3: '99.28%'}\n",
      "Val Loss: 2.734675, Val Acc: 84.93%, Val-Class-Acc: {0: '86.96%', 1: '74.59%', 2: '83.33%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 139/200, Train Loss: 0.085190, Train-Class-Acc: {0: '98.37%', 1: '97.64%', 2: '97.40%', 3: '98.98%'}\n",
      "Val Loss: 1.618884, Val Acc: 86.89%, Val-Class-Acc: {0: '87.50%', 1: '80.33%', 2: '83.33%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 140/200, Train Loss: 0.023476, Train-Class-Acc: {0: '98.77%', 1: '98.87%', 2: '99.31%', 3: '99.69%'}\n",
      "Val Loss: 1.718465, Val Acc: 86.03%, Val-Class-Acc: {0: '89.13%', 1: '69.26%', 2: '90.28%', 3: '97.95%'}, LR: 0.001000\n",
      "Epoch 141/200, Train Loss: 0.216314, Train-Class-Acc: {0: '99.18%', 1: '98.36%', 2: '99.83%', 3: '99.08%'}\n",
      "Val Loss: 1.505487, Val Acc: 84.19%, Val-Class-Acc: {0: '84.24%', 1: '68.03%', 2: '89.58%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 142/200, Train Loss: 0.330090, Train-Class-Acc: {0: '95.78%', 1: '96.31%', 2: '97.92%', 3: '98.77%'}\n",
      "Val Loss: 2.404962, Val Acc: 85.42%, Val-Class-Acc: {0: '89.13%', 1: '73.36%', 2: '86.81%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 143/200, Train Loss: 0.302088, Train-Class-Acc: {0: '97.96%', 1: '98.26%', 2: '99.48%', 3: '99.18%'}\n",
      "Val Loss: 2.460475, Val Acc: 84.93%, Val-Class-Acc: {0: '86.96%', 1: '72.95%', 2: '86.11%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 144/200, Train Loss: 0.401540, Train-Class-Acc: {0: '96.87%', 1: '96.31%', 2: '98.27%', 3: '98.16%'}\n",
      "Val Loss: 3.033867, Val Acc: 82.23%, Val-Class-Acc: {0: '75.54%', 1: '70.08%', 2: '88.89%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 145/200, Train Loss: 0.440085, Train-Class-Acc: {0: '93.19%', 1: '92.83%', 2: '95.49%', 3: '97.03%'}\n",
      "Val Loss: 1.514387, Val Acc: 86.15%, Val-Class-Acc: {0: '80.43%', 1: '81.15%', 2: '86.81%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 146/200, Train Loss: 0.240529, Train-Class-Acc: {0: '97.82%', 1: '97.44%', 2: '97.75%', 3: '99.08%'}\n",
      "Val Loss: 1.475209, Val Acc: 86.15%, Val-Class-Acc: {0: '79.89%', 1: '79.51%', 2: '88.19%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 147/200, Train Loss: 0.261241, Train-Class-Acc: {0: '96.46%', 1: '96.21%', 2: '98.27%', 3: '98.87%'}\n",
      "Val Loss: 1.467970, Val Acc: 86.03%, Val-Class-Acc: {0: '90.22%', 1: '71.72%', 2: '87.50%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 148/200, Train Loss: 0.155465, Train-Class-Acc: {0: '96.73%', 1: '96.82%', 2: '98.79%', 3: '99.08%'}\n",
      "Val Loss: 1.647296, Val Acc: 86.40%, Val-Class-Acc: {0: '84.24%', 1: '78.69%', 2: '89.58%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 149/200, Train Loss: 0.113895, Train-Class-Acc: {0: '98.09%', 1: '97.85%', 2: '97.75%', 3: '97.85%'}\n",
      "Val Loss: 1.521938, Val Acc: 85.29%, Val-Class-Acc: {0: '89.13%', 1: '71.72%', 2: '86.81%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 150/200, Train Loss: 0.144595, Train-Class-Acc: {0: '97.96%', 1: '97.75%', 2: '98.61%', 3: '99.39%'}\n",
      "Val Loss: 1.564538, Val Acc: 85.54%, Val-Class-Acc: {0: '79.89%', 1: '76.23%', 2: '90.28%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 151/200, Train Loss: 0.174811, Train-Class-Acc: {0: '97.14%', 1: '97.54%', 2: '99.13%', 3: '99.49%'}\n",
      "Val Loss: 1.535425, Val Acc: 85.78%, Val-Class-Acc: {0: '84.78%', 1: '75.00%', 2: '88.89%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 152/200, Train Loss: 0.053819, Train-Class-Acc: {0: '98.37%', 1: '98.36%', 2: '98.61%', 3: '99.39%'}\n",
      "Val Loss: 1.741684, Val Acc: 86.64%, Val-Class-Acc: {0: '88.59%', 1: '77.05%', 2: '84.03%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 153/200, Train Loss: 0.104722, Train-Class-Acc: {0: '99.46%', 1: '98.57%', 2: '99.13%', 3: '99.69%'}\n",
      "Val Loss: 1.149462, Val Acc: 87.50%, Val-Class-Acc: {0: '80.43%', 1: '83.20%', 2: '90.28%', 3: '95.49%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_91.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_153.pth\n",
      "Epoch 154/200, Train Loss: 0.404377, Train-Class-Acc: {0: '97.96%', 1: '96.21%', 2: '98.27%', 3: '97.95%'}\n",
      "Val Loss: 1.207335, Val Acc: 85.05%, Val-Class-Acc: {0: '79.89%', 1: '76.23%', 2: '88.89%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 155/200, Train Loss: 0.176735, Train-Class-Acc: {0: '97.68%', 1: '96.82%', 2: '98.79%', 3: '98.16%'}\n",
      "Val Loss: 1.886833, Val Acc: 85.91%, Val-Class-Acc: {0: '80.98%', 1: '81.97%', 2: '86.81%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 156/200, Train Loss: 0.251485, Train-Class-Acc: {0: '98.23%', 1: '97.64%', 2: '98.61%', 3: '98.98%'}\n",
      "Val Loss: 2.092996, Val Acc: 85.91%, Val-Class-Acc: {0: '83.70%', 1: '72.95%', 2: '91.67%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 157/200, Train Loss: 0.056344, Train-Class-Acc: {0: '97.68%', 1: '98.05%', 2: '97.92%', 3: '99.49%'}\n",
      "Val Loss: 1.822616, Val Acc: 86.03%, Val-Class-Acc: {0: '84.24%', 1: '75.41%', 2: '88.89%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 158/200, Train Loss: 0.055383, Train-Class-Acc: {0: '99.32%', 1: '99.69%', 2: '99.48%', 3: '99.69%'}\n",
      "Val Loss: 1.749625, Val Acc: 86.27%, Val-Class-Acc: {0: '79.35%', 1: '80.74%', 2: '88.19%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 159/200, Train Loss: 0.865613, Train-Class-Acc: {0: '95.23%', 1: '94.36%', 2: '96.36%', 3: '98.16%'}\n",
      "Val Loss: 2.551006, Val Acc: 84.07%, Val-Class-Acc: {0: '82.61%', 1: '75.82%', 2: '81.94%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 160/200, Train Loss: 0.402228, Train-Class-Acc: {0: '95.10%', 1: '94.06%', 2: '96.19%', 3: '96.93%'}\n",
      "Val Loss: 1.491507, Val Acc: 86.89%, Val-Class-Acc: {0: '83.15%', 1: '81.56%', 2: '84.03%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 161/200, Train Loss: 0.160200, Train-Class-Acc: {0: '96.87%', 1: '96.62%', 2: '98.96%', 3: '99.18%'}\n",
      "Val Loss: 1.838779, Val Acc: 85.91%, Val-Class-Acc: {0: '86.41%', 1: '79.10%', 2: '85.42%', 3: '92.62%'}, LR: 0.001000\n",
      "Epoch 162/200, Train Loss: 0.244863, Train-Class-Acc: {0: '95.91%', 1: '96.11%', 2: '97.92%', 3: '98.46%'}\n",
      "Val Loss: 1.455962, Val Acc: 86.03%, Val-Class-Acc: {0: '93.48%', 1: '72.13%', 2: '84.03%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 163/200, Train Loss: 0.364551, Train-Class-Acc: {0: '96.46%', 1: '96.11%', 2: '97.92%', 3: '98.57%'}\n",
      "Val Loss: 1.351460, Val Acc: 86.03%, Val-Class-Acc: {0: '75.00%', 1: '87.70%', 2: '84.72%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 164/200, Train Loss: 0.230738, Train-Class-Acc: {0: '97.96%', 1: '96.72%', 2: '98.09%', 3: '98.98%'}\n",
      "Val Loss: 1.258722, Val Acc: 87.50%, Val-Class-Acc: {0: '84.24%', 1: '81.97%', 2: '85.42%', 3: '96.72%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_39.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_164.pth\n",
      "Epoch 165/200, Train Loss: 0.122317, Train-Class-Acc: {0: '99.46%', 1: '98.46%', 2: '99.31%', 3: '99.39%'}\n",
      "Val Loss: 2.067147, Val Acc: 85.91%, Val-Class-Acc: {0: '75.54%', 1: '81.56%', 2: '87.50%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 166/200, Train Loss: 0.168191, Train-Class-Acc: {0: '97.14%', 1: '97.03%', 2: '97.23%', 3: '98.46%'}\n",
      "Val Loss: 1.536473, Val Acc: 85.54%, Val-Class-Acc: {0: '85.33%', 1: '74.59%', 2: '85.42%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 167/200, Train Loss: 0.223446, Train-Class-Acc: {0: '97.55%', 1: '98.26%', 2: '98.44%', 3: '98.67%'}\n",
      "Val Loss: 1.815312, Val Acc: 85.54%, Val-Class-Acc: {0: '86.41%', 1: '73.36%', 2: '86.11%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 168/200, Train Loss: 0.333820, Train-Class-Acc: {0: '96.73%', 1: '96.41%', 2: '97.92%', 3: '98.26%'}\n",
      "Val Loss: 1.867730, Val Acc: 87.25%, Val-Class-Acc: {0: '82.61%', 1: '81.97%', 2: '89.58%', 3: '94.67%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_137.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_168.pth\n",
      "Epoch 169/200, Train Loss: 0.171982, Train-Class-Acc: {0: '97.14%', 1: '96.00%', 2: '98.09%', 3: '98.36%'}\n",
      "Val Loss: 1.989896, Val Acc: 85.29%, Val-Class-Acc: {0: '88.04%', 1: '72.95%', 2: '84.03%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 170/200, Train Loss: 0.065748, Train-Class-Acc: {0: '97.82%', 1: '97.85%', 2: '98.44%', 3: '99.39%'}\n",
      "Val Loss: 1.689568, Val Acc: 87.01%, Val-Class-Acc: {0: '82.07%', 1: '81.56%', 2: '86.81%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 171/200, Train Loss: 0.015833, Train-Class-Acc: {0: '99.32%', 1: '99.49%', 2: '99.83%', 3: '99.69%'}\n",
      "Val Loss: 1.625917, Val Acc: 86.64%, Val-Class-Acc: {0: '86.96%', 1: '80.33%', 2: '85.42%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 172/200, Train Loss: 0.012037, Train-Class-Acc: {0: '99.86%', 1: '99.49%', 2: '99.65%', 3: '99.80%'}\n",
      "Val Loss: 1.648381, Val Acc: 85.91%, Val-Class-Acc: {0: '77.17%', 1: '82.38%', 2: '86.11%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 173/200, Train Loss: 0.091893, Train-Class-Acc: {0: '99.32%', 1: '98.77%', 2: '99.13%', 3: '99.59%'}\n",
      "Val Loss: 1.791923, Val Acc: 86.76%, Val-Class-Acc: {0: '89.13%', 1: '75.82%', 2: '84.72%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 174/200, Train Loss: 0.165597, Train-Class-Acc: {0: '98.09%', 1: '98.16%', 2: '98.96%', 3: '99.49%'}\n",
      "Val Loss: 1.617908, Val Acc: 87.50%, Val-Class-Acc: {0: '83.15%', 1: '80.74%', 2: '89.58%', 3: '96.31%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_168.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_174.pth\n",
      "Epoch 175/200, Train Loss: 0.070040, Train-Class-Acc: {0: '98.09%', 1: '97.85%', 2: '99.13%', 3: '99.39%'}\n",
      "Val Loss: 1.648553, Val Acc: 85.91%, Val-Class-Acc: {0: '86.41%', 1: '77.05%', 2: '81.25%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 176/200, Train Loss: 0.032104, Train-Class-Acc: {0: '99.46%', 1: '99.18%', 2: '99.48%', 3: '99.18%'}\n",
      "Val Loss: 1.718377, Val Acc: 84.19%, Val-Class-Acc: {0: '86.41%', 1: '67.62%', 2: '87.50%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 177/200, Train Loss: 0.133527, Train-Class-Acc: {0: '99.18%', 1: '98.57%', 2: '99.48%', 3: '99.18%'}\n",
      "Val Loss: 1.482388, Val Acc: 85.54%, Val-Class-Acc: {0: '89.13%', 1: '78.69%', 2: '85.42%', 3: '89.75%'}, LR: 0.001000\n",
      "Epoch 178/200, Train Loss: 0.064248, Train-Class-Acc: {0: '99.32%', 1: '98.67%', 2: '99.83%', 3: '99.69%'}\n",
      "Val Loss: 1.508940, Val Acc: 85.05%, Val-Class-Acc: {0: '85.33%', 1: '71.72%', 2: '88.19%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 179/200, Train Loss: 0.055735, Train-Class-Acc: {0: '98.91%', 1: '99.28%', 2: '99.65%', 3: '99.28%'}\n",
      "Val Loss: 1.732573, Val Acc: 85.91%, Val-Class-Acc: {0: '88.04%', 1: '75.82%', 2: '89.58%', 3: '92.21%'}, LR: 0.001000\n",
      "Epoch 180/200, Train Loss: 0.059049, Train-Class-Acc: {0: '99.32%', 1: '98.87%', 2: '99.13%', 3: '99.18%'}\n",
      "Val Loss: 1.684796, Val Acc: 86.40%, Val-Class-Acc: {0: '84.78%', 1: '78.28%', 2: '88.89%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 181/200, Train Loss: 0.043311, Train-Class-Acc: {0: '99.32%', 1: '99.28%', 2: '98.79%', 3: '99.39%'}\n",
      "Val Loss: 1.684020, Val Acc: 85.66%, Val-Class-Acc: {0: '84.24%', 1: '77.46%', 2: '81.94%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 182/200, Train Loss: 0.012338, Train-Class-Acc: {0: '99.46%', 1: '99.49%', 2: '99.65%', 3: '99.90%'}\n",
      "Val Loss: 1.641458, Val Acc: 85.17%, Val-Class-Acc: {0: '85.87%', 1: '72.95%', 2: '87.50%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 183/200, Train Loss: 0.029922, Train-Class-Acc: {0: '99.18%', 1: '99.18%', 2: '99.65%', 3: '99.80%'}\n",
      "Val Loss: 1.781834, Val Acc: 85.66%, Val-Class-Acc: {0: '78.80%', 1: '79.51%', 2: '86.11%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 184/200, Train Loss: 0.073538, Train-Class-Acc: {0: '99.05%', 1: '98.98%', 2: '99.65%', 3: '99.69%'}\n",
      "Val Loss: 2.075055, Val Acc: 84.56%, Val-Class-Acc: {0: '84.24%', 1: '79.51%', 2: '86.81%', 3: '88.52%'}, LR: 0.001000\n",
      "Epoch 185/200, Train Loss: 0.065267, Train-Class-Acc: {0: '98.91%', 1: '98.87%', 2: '99.31%', 3: '99.18%'}\n",
      "Val Loss: 1.485613, Val Acc: 85.66%, Val-Class-Acc: {0: '78.26%', 1: '79.92%', 2: '85.42%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 186/200, Train Loss: 0.273749, Train-Class-Acc: {0: '95.64%', 1: '96.72%', 2: '98.61%', 3: '98.67%'}\n",
      "Val Loss: 2.411360, Val Acc: 83.21%, Val-Class-Acc: {0: '91.85%', 1: '64.34%', 2: '86.81%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 187/200, Train Loss: 0.515990, Train-Class-Acc: {0: '96.19%', 1: '94.36%', 2: '97.92%', 3: '97.54%'}\n",
      "Val Loss: 1.498710, Val Acc: 85.42%, Val-Class-Acc: {0: '80.43%', 1: '78.28%', 2: '84.72%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 188/200, Train Loss: 0.143128, Train-Class-Acc: {0: '97.55%', 1: '97.64%', 2: '98.79%', 3: '98.77%'}\n",
      "Val Loss: 2.095484, Val Acc: 85.91%, Val-Class-Acc: {0: '86.96%', 1: '75.82%', 2: '82.64%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 189/200, Train Loss: 0.028711, Train-Class-Acc: {0: '98.77%', 1: '98.57%', 2: '99.48%', 3: '99.69%'}\n",
      "Val Loss: 2.083882, Val Acc: 85.17%, Val-Class-Acc: {0: '81.52%', 1: '77.05%', 2: '84.03%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 190/200, Train Loss: 0.039063, Train-Class-Acc: {0: '99.59%', 1: '99.18%', 2: '99.13%', 3: '99.49%'}\n",
      "Val Loss: 2.403011, Val Acc: 84.80%, Val-Class-Acc: {0: '83.70%', 1: '74.59%', 2: '82.64%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 191/200, Train Loss: 0.066997, Train-Class-Acc: {0: '99.46%', 1: '98.46%', 2: '98.44%', 3: '98.67%'}\n",
      "Val Loss: 1.846903, Val Acc: 85.91%, Val-Class-Acc: {0: '83.70%', 1: '79.51%', 2: '84.03%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 192/200, Train Loss: 0.220951, Train-Class-Acc: {0: '96.46%', 1: '96.52%', 2: '98.44%', 3: '98.26%'}\n",
      "Val Loss: 1.388245, Val Acc: 87.13%, Val-Class-Acc: {0: '81.52%', 1: '84.02%', 2: '84.72%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 193/200, Train Loss: 0.068474, Train-Class-Acc: {0: '98.50%', 1: '98.05%', 2: '98.27%', 3: '99.18%'}\n",
      "Val Loss: 1.395656, Val Acc: 84.80%, Val-Class-Acc: {0: '86.96%', 1: '72.13%', 2: '86.11%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 194/200, Train Loss: 0.211828, Train-Class-Acc: {0: '98.50%', 1: '97.64%', 2: '98.44%', 3: '99.18%'}\n",
      "Val Loss: 1.624462, Val Acc: 85.54%, Val-Class-Acc: {0: '86.41%', 1: '72.95%', 2: '88.19%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 195/200, Train Loss: 0.101911, Train-Class-Acc: {0: '98.77%', 1: '98.67%', 2: '98.79%', 3: '99.49%'}\n",
      "Val Loss: 1.300687, Val Acc: 85.91%, Val-Class-Acc: {0: '76.09%', 1: '81.56%', 2: '88.19%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 196/200, Train Loss: 0.206463, Train-Class-Acc: {0: '99.32%', 1: '98.67%', 2: '98.96%', 3: '99.28%'}\n",
      "Val Loss: 1.694613, Val Acc: 85.91%, Val-Class-Acc: {0: '85.87%', 1: '75.00%', 2: '86.11%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 197/200, Train Loss: 0.169230, Train-Class-Acc: {0: '98.64%', 1: '98.16%', 2: '98.44%', 3: '98.98%'}\n",
      "Val Loss: 2.222305, Val Acc: 85.29%, Val-Class-Acc: {0: '84.78%', 1: '75.41%', 2: '84.03%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 198/200, Train Loss: 0.123508, Train-Class-Acc: {0: '98.37%', 1: '97.75%', 2: '98.96%', 3: '98.57%'}\n",
      "Val Loss: 1.929039, Val Acc: 84.93%, Val-Class-Acc: {0: '83.70%', 1: '73.77%', 2: '84.72%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 199/200, Train Loss: 0.113029, Train-Class-Acc: {0: '98.77%', 1: '98.98%', 2: '97.92%', 3: '99.59%'}\n",
      "Val Loss: 2.398377, Val Acc: 86.52%, Val-Class-Acc: {0: '77.17%', 1: '81.97%', 2: '89.58%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 200/200, Train Loss: 0.082811, Train-Class-Acc: {0: '98.37%', 1: '98.67%', 2: '98.44%', 3: '99.59%'}\n",
      "Val Loss: 1.538321, Val Acc: 86.64%, Val-Class-Acc: {0: '85.87%', 1: '77.46%', 2: '85.42%', 3: '97.13%'}, LR: 0.001000\n",
      "\n",
      "üèÜ Best model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_best.pth (Val Accuracy: 87.75%)\n",
      "\n",
      "üìå Final model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_final.pth\n",
      "\n",
      "üéØ Top 5 Best Models:\n",
      "Epoch 20, Train Loss: 1.364956, Train-Acc: {0: '81.34%', 1: '81.45%', 2: '89.95%', 3: '92.11%'},\n",
      "Val Loss: 1.404529, Val Acc: 87.75%, Val-Acc: {0: '90.22%', 1: '82.38%', 2: '81.94%', 3: '94.67%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_20.pth\n",
      "Epoch 174, Train Loss: 0.165597, Train-Acc: {0: '98.09%', 1: '98.16%', 2: '98.96%', 3: '99.49%'},\n",
      "Val Loss: 1.617908, Val Acc: 87.50%, Val-Acc: {0: '83.15%', 1: '80.74%', 2: '89.58%', 3: '96.31%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_174.pth\n",
      "Epoch 164, Train Loss: 0.230738, Train-Acc: {0: '97.96%', 1: '96.72%', 2: '98.09%', 3: '98.98%'},\n",
      "Val Loss: 1.258722, Val Acc: 87.50%, Val-Acc: {0: '84.24%', 1: '81.97%', 2: '85.42%', 3: '96.72%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_164.pth\n",
      "Epoch 153, Train Loss: 0.104722, Train-Acc: {0: '99.46%', 1: '98.57%', 2: '99.13%', 3: '99.69%'},\n",
      "Val Loss: 1.149462, Val Acc: 87.50%, Val-Acc: {0: '80.43%', 1: '83.20%', 2: '90.28%', 3: '95.49%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_153.pth\n",
      "Epoch 38, Train Loss: 0.434984, Train-Acc: {0: '93.87%', 1: '91.50%', 2: '96.71%', 3: '96.62%'},\n",
      "Val Loss: 1.302750, Val Acc: 87.38%, Val-Acc: {0: '91.85%', 1: '79.10%', 2: '81.94%', 3: '95.49%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_38.pth\n",
      "\n",
      "üß† Model Summary:\n",
      "Total Parameters: 3,889,796\n",
      "Model Size (float32): 14.84 MB\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "Number of LoRA adapters: 8\n",
      "Number of LoRA groups: 1\n",
      "Total Training Time: 326.96 seconds\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "---\n",
      "### Period 2 (alpha = 0.0, similarity_threshold = 0.99)\n",
      "+ ##### Total training time: 326.96 seconds\n",
      "+ ##### Model: ResNet18_1D_LoRA\n",
      "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2'*\n",
      "+ ##### Best Epoch: 20\n",
      "#### __Val Accuracy: 87.75%__\n",
      "#### __Val-Class-Acc: {0: '90.22%', 1: '82.38%', 2: '81.94%', 3: '94.67%'}__\n",
      "#### __Total Parameters: 3,889,796__\n",
      "#### __Model Size (float32): 14.84 MB__\n",
      "#### __Number of LoRA adapters: 8__\n",
      "#### __Number of LoRA groups: 1__\n",
      "\n",
      "Saved class features to: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v2/Period_2/class_features.pkl\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå Period 2: DynEx-CLoRA Training (ECG)\n",
    "# ================================\n",
    "period = 2\n",
    "\n",
    "# ==== Paths ====\n",
    "stop_signal_file = os.path.join(BASE_DIR, \"stop_training.txt\")\n",
    "model_saving_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v2\", f\"Period_{period}\")\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# ==== Load Period 2 Data ====\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{period}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{period}.npy\"))\n",
    "X_val   = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val   = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "# ==== Device ====\n",
    "device = auto_select_cuda_device()\n",
    "\n",
    "# ==== ËºâÂÖ• Period 1 ÁöÑ class features ====\n",
    "prev_folder = os.path.join(BASE_DIR, \"ResNet18_Selection\", \"ResNet18_big_inplane_v1\")\n",
    "class_features_path = os.path.join(prev_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, \"rb\") as f:\n",
    "    class_features_dict = pickle.load(f)\n",
    "print(f\"‚úÖ Loaded class features from: {class_features_path}\")\n",
    "\n",
    "# ==== ËºâÂÖ• Period 1 È†êË®ìÁ∑¥Ê®°Âûã ====\n",
    "prev_model_path = os.path.join(prev_folder, \"ResNet18_big_inplane_1D_best.pth\")\n",
    "checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "\n",
    "input_channels = X_train.shape[2]\n",
    "output_size = len(np.unique(y_train))\n",
    "\n",
    "# ==== Âª∫Á´ã teacher modelÔºàoutput_size Ë¶ÅÊâ£ÊéâÊñ∞È°ûÂà•Êï∏Ôºâ\n",
    "teacher_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size - 2).to(device)\n",
    "teacher_model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "teacher_model.eval()\n",
    "\n",
    "# ==== Âª∫Á´ã student model ====\n",
    "model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size).to(device)\n",
    "\n",
    "# ==== Ê†πÊìö LoRA Êï∏ÈáèÂêåÊ≠• adapter ÁµêÊßã ====\n",
    "num_lora_groups = checkpoint.get(\"num_lora_groups\", 0)\n",
    "print(f\"üîÑ Number of LoRA groups: {num_lora_groups}\")\n",
    "related_labels = checkpoint.get(\"related_labels\", {\"base\": [0, 1]})\n",
    "for _ in range(num_lora_groups):\n",
    "    model.add_lora_adapter()\n",
    "\n",
    "# ==== Ë§áË£Ω shared Ê¨äÈáçÔºàÊéíÈô§ fc / lora_adapterÔºâ====\n",
    "model_dict = model.state_dict()\n",
    "prev_state_dict = checkpoint[\"model_state_dict\"]\n",
    "filtered_dict = {\n",
    "    k: v for k, v in prev_state_dict.items()\n",
    "    if k in model_dict and model_dict[k].shape == v.shape and k not in [\"fc.weight\", \"fc.bias\"]\n",
    "}\n",
    "model.load_state_dict(filtered_dict, strict=False)\n",
    "for k in model_dict:\n",
    "    if k not in filtered_dict:\n",
    "        print(f\"üîç Not loaded: {k}, shape={model_dict[k].shape}\")\n",
    "print(\"‚úÖ Loaded shared weights from Period 1 (excluding FC only)\")\n",
    "\n",
    "# ==== Ë®ìÁ∑¥ÂèÉÊï∏ ====\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "alpha = 0.0 # 0.0 for no distillation\n",
    "similarity_threshold = 0.99\n",
    "stable_classes = [0]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# ==== ÈñãÂßãË®ìÁ∑¥ ====\n",
    "train_with_dynex_clora_ecg(\n",
    "    model=model,\n",
    "    teacher_model=None,\n",
    "    output_size=output_size,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    alpha=alpha,\n",
    "    model_saving_folder=model_saving_folder,\n",
    "    model_name=\"ResNet18_1D_LoRA\",\n",
    "    stop_signal_file=stop_signal_file,\n",
    "    scheduler=scheduler,\n",
    "    period=period,\n",
    "    stable_classes=stable_classes,\n",
    "    similarity_threshold=similarity_threshold,\n",
    "    class_features_dict=class_features_dict,\n",
    "    related_labels=related_labels,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ==== Cleanup ====\n",
    "del X_train, y_train, X_val, y_val\n",
    "del model, teacher_model, checkpoint, model_dict, prev_state_dict, filtered_dict\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023c077",
   "metadata": {},
   "source": [
    "#### ‚úîÔ∏è v3 no distillation, all freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab559ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 1\n",
      "    - Memory Used    : 433 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ Loaded class features from: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/ResNet18_Selection/ResNet18_big_inplane_v1/class_features.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1514991/550759108.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Number of LoRA groups: 0\n",
      "üîç Not loaded: fc.weight, shape=torch.Size([4, 1024])\n",
      "üîç Not loaded: fc.bias, shape=torch.Size([4])\n",
      "‚úÖ Loaded shared weights from Period 1 (excluding FC & LoRA)\n",
      "\n",
      "üöÄ 'train_with_dynex_clora_ecg' started for Period 2\n",
      "\n",
      "‚úÖ Removed existing folder: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2\n",
      "\n",
      "‚úÖ Data Overview:\n",
      "X_train: torch.Size([3263, 5000, 12]), y_train: torch.Size([3263])\n",
      "X_val: torch.Size([816, 5000, 12]), y_val: torch.Size([816])\n",
      "\n",
      "üîé Similarity Analysis:\n",
      "  Similarity threshold: 0.9900\n",
      "  Existing classes: [0, 1]\n",
      "  Current classes: [0, 1, 2, 3]\n",
      "  New classes: [2, 3]\n",
      "\n",
      "üìä Similarity Scores:\n",
      "  New Class 2:\n",
      "    - Existing Class 1: 0.9665\n",
      "    - Existing Class 0: 0.8415\n",
      "  New Class 3:\n",
      "    - Existing Class 1: 0.9912\n",
      "    - Existing Class 0: 0.8097\n",
      "\n",
      "  Average similarity: 0.9055, Std: 0.0837\n",
      "\n",
      "üß© Managing LoRA adapters for 2 new classes...\n",
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n",
      "‚ûï New Class 2 is not similar to any existing class ‚Üí Created new adapter group #0\n",
      "üîÑ New Class 3 is similar to Class 1 (sim=0.9912) ‚Üí Added to adapter 'base'\n",
      "\n",
      "üîç Checking stability of existing classes...\n",
      "  Class 0 similarity with itself: 0.9998\n",
      "  Class 1 similarity with itself: 0.9948\n",
      "\n",
      "üîí Default: Freezing ALL model parameters\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå fc.weight                                          | shape=[4, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[4]\n",
      "\n",
      "üîì Unfreezing selected adapters (by group):\n",
      "  - Base layers (classes: [0, 1, 3])\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå fc.weight                                          | shape=[4, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[4]\n",
      "  - New adapter group #0 (classes: [2])\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå fc.weight                                          | shape=[4, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[4]\n",
      "\n",
      "üìã Related Labels Summary:\n",
      "  - Base network: Classes [0, 1, 3]\n",
      "  - Adapter #0: Classes [2]\n",
      "\n",
      "üîß Trainable Parameter Status:\n",
      "  ‚úÖ layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ fc.weight                                          | shape=[4, 1024]\n",
      "  ‚úÖ fc.bias                                            | shape=[4]\n",
      "trainable_count: 26\n",
      "\n",
      "üìä Parameter Statistics:\n",
      "  - Total parameters: 3,889,796\n",
      "  - Trainable parameters: 2,123,780 (54.60%)\n",
      "  - Frozen parameters: 1,766,016 (45.40%)\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "\n",
      "üß† Parameters currently controlled by the optimizer: (78)\n",
      "  ‚õî conv1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer1.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.0.conv2.weight\n",
      "  ‚õî layer1.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer1.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.conv2.weight\n",
      "  ‚õî layer1.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer2.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.conv2.weight\n",
      "  ‚õî layer2.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer2.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.conv2.weight\n",
      "  ‚õî layer2.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer3.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.conv2.weight\n",
      "  ‚õî layer3.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer3.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.conv2.weight\n",
      "  ‚õî layer3.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer4.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.conv2.weight\n",
      "  ‚õî layer4.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer4.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.conv2.weight\n",
      "  ‚õî layer4.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ fc.weight\n",
      "  ‚úÖ fc.bias\n",
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/200, Train Loss: 23.726127, Train-Class-Acc: {0: '73.84%', 1: '54.82%', 2: '59.27%', 3: '60.66%'}\n",
      "Val Loss: 9.407657, Val Acc: 79.66%, Val-Class-Acc: {0: '82.07%', 1: '79.92%', 2: '82.64%', 3: '75.82%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_1.pth\n",
      "Epoch 2/200, Train Loss: 11.690031, Train-Class-Acc: {0: '77.79%', 1: '68.75%', 2: '77.30%', 3: '78.69%'}\n",
      "Val Loss: 5.446003, Val Acc: 84.80%, Val-Class-Acc: {0: '94.57%', 1: '77.87%', 2: '79.17%', 3: '87.70%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_2.pth\n",
      "Epoch 3/200, Train Loss: 8.125509, Train-Class-Acc: {0: '79.97%', 1: '73.16%', 2: '79.03%', 3: '84.43%'}\n",
      "Val Loss: 8.026685, Val Acc: 82.72%, Val-Class-Acc: {0: '80.98%', 1: '86.89%', 2: '72.92%', 3: '85.66%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_3.pth\n",
      "Epoch 4/200, Train Loss: 6.739996, Train-Class-Acc: {0: '81.74%', 1: '75.51%', 2: '85.44%', 3: '85.14%'}\n",
      "Val Loss: 6.091496, Val Acc: 83.21%, Val-Class-Acc: {0: '92.93%', 1: '66.39%', 2: '86.81%', 3: '90.57%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_4.pth\n",
      "Epoch 5/200, Train Loss: 5.562592, Train-Class-Acc: {0: '82.29%', 1: '76.23%', 2: '85.44%', 3: '88.63%'}\n",
      "Val Loss: 4.473873, Val Acc: 85.42%, Val-Class-Acc: {0: '89.67%', 1: '70.08%', 2: '90.28%', 3: '94.67%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_5.pth\n",
      "Epoch 6/200, Train Loss: 4.076502, Train-Class-Acc: {0: '83.79%', 1: '80.12%', 2: '84.58%', 3: '89.45%'}\n",
      "Val Loss: 4.601261, Val Acc: 86.27%, Val-Class-Acc: {0: '84.78%', 1: '87.70%', 2: '85.42%', 3: '86.48%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_1.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_6.pth\n",
      "Epoch 7/200, Train Loss: 3.356677, Train-Class-Acc: {0: '86.65%', 1: '82.38%', 2: '89.43%', 3: '89.14%'}\n",
      "Val Loss: 6.179455, Val Acc: 83.33%, Val-Class-Acc: {0: '80.98%', 1: '72.54%', 2: '81.94%', 3: '96.72%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_3.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_7.pth\n",
      "Epoch 8/200, Train Loss: 3.763821, Train-Class-Acc: {0: '85.83%', 1: '82.58%', 2: '87.35%', 3: '90.68%'}\n",
      "Val Loss: 7.281055, Val Acc: 84.56%, Val-Class-Acc: {0: '85.87%', 1: '80.33%', 2: '89.58%', 3: '84.84%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_4.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_8.pth\n",
      "Epoch 9/200, Train Loss: 2.899702, Train-Class-Acc: {0: '86.24%', 1: '83.71%', 2: '89.95%', 3: '92.62%'}\n",
      "Val Loss: 4.969434, Val Acc: 85.78%, Val-Class-Acc: {0: '92.93%', 1: '72.54%', 2: '83.33%', 3: '95.08%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_7.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_9.pth\n",
      "Epoch 10/200, Train Loss: 2.632962, Train-Class-Acc: {0: '87.33%', 1: '84.43%', 2: '89.77%', 3: '92.42%'}\n",
      "Val Loss: 4.178075, Val Acc: 86.64%, Val-Class-Acc: {0: '89.67%', 1: '78.69%', 2: '81.94%', 3: '95.08%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_8.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_10.pth\n",
      "Epoch 11/200, Train Loss: 2.326704, Train-Class-Acc: {0: '88.42%', 1: '87.30%', 2: '91.68%', 3: '92.73%'}\n",
      "Val Loss: 4.922159, Val Acc: 83.33%, Val-Class-Acc: {0: '70.11%', 1: '78.69%', 2: '86.11%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 12/200, Train Loss: 2.118011, Train-Class-Acc: {0: '90.19%', 1: '87.50%', 2: '91.68%', 3: '93.85%'}\n",
      "Val Loss: 8.513686, Val Acc: 84.19%, Val-Class-Acc: {0: '91.30%', 1: '80.74%', 2: '84.72%', 3: '81.97%'}, LR: 0.001000\n",
      "Epoch 13/200, Train Loss: 1.875079, Train-Class-Acc: {0: '90.33%', 1: '88.11%', 2: '90.47%', 3: '93.85%'}\n",
      "Val Loss: 3.619920, Val Acc: 88.11%, Val-Class-Acc: {0: '89.13%', 1: '80.74%', 2: '88.19%', 3: '94.67%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_2.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_13.pth\n",
      "Epoch 14/200, Train Loss: 1.795159, Train-Class-Acc: {0: '90.33%', 1: '88.73%', 2: '92.72%', 3: '94.47%'}\n",
      "Val Loss: 3.761562, Val Acc: 87.38%, Val-Class-Acc: {0: '85.33%', 1: '81.97%', 2: '87.50%', 3: '94.26%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_5.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_14.pth\n",
      "Epoch 15/200, Train Loss: 1.571173, Train-Class-Acc: {0: '91.42%', 1: '89.96%', 2: '93.59%', 3: '94.36%'}\n",
      "Val Loss: 4.521288, Val Acc: 85.54%, Val-Class-Acc: {0: '92.39%', 1: '75.82%', 2: '84.72%', 3: '90.57%'}, LR: 0.001000\n",
      "Epoch 16/200, Train Loss: 1.292174, Train-Class-Acc: {0: '92.37%', 1: '90.37%', 2: '93.41%', 3: '95.08%'}\n",
      "Val Loss: 5.204896, Val Acc: 85.66%, Val-Class-Acc: {0: '82.61%', 1: '83.61%', 2: '84.03%', 3: '90.98%'}, LR: 0.001000\n",
      "Epoch 17/200, Train Loss: 1.629957, Train-Class-Acc: {0: '93.87%', 1: '90.57%', 2: '92.72%', 3: '94.67%'}\n",
      "Val Loss: 4.177458, Val Acc: 86.40%, Val-Class-Acc: {0: '91.30%', 1: '76.23%', 2: '83.33%', 3: '94.67%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_9.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_17.pth\n",
      "Epoch 18/200, Train Loss: 1.305219, Train-Class-Acc: {0: '92.51%', 1: '90.88%', 2: '93.93%', 3: '96.52%'}\n",
      "Val Loss: 4.558483, Val Acc: 86.27%, Val-Class-Acc: {0: '79.89%', 1: '87.70%', 2: '81.94%', 3: '92.21%'}, LR: 0.001000\n",
      "Epoch 19/200, Train Loss: 1.641335, Train-Class-Acc: {0: '92.10%', 1: '89.24%', 2: '92.20%', 3: '95.29%'}\n",
      "Val Loss: 4.523687, Val Acc: 85.05%, Val-Class-Acc: {0: '78.80%', 1: '86.48%', 2: '73.61%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 20/200, Train Loss: 1.227183, Train-Class-Acc: {0: '93.46%', 1: '92.42%', 2: '94.28%', 3: '95.59%'}\n",
      "Val Loss: 3.840194, Val Acc: 87.38%, Val-Class-Acc: {0: '90.22%', 1: '77.87%', 2: '84.03%', 3: '96.72%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_6.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_20.pth\n",
      "Epoch 21/200, Train Loss: 1.038065, Train-Class-Acc: {0: '93.05%', 1: '93.14%', 2: '94.97%', 3: '97.03%'}\n",
      "Val Loss: 4.301615, Val Acc: 88.11%, Val-Class-Acc: {0: '92.93%', 1: '80.33%', 2: '84.03%', 3: '94.67%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_17.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_21.pth\n",
      "Epoch 22/200, Train Loss: 1.402296, Train-Class-Acc: {0: '91.55%', 1: '90.78%', 2: '94.28%', 3: '95.80%'}\n",
      "Val Loss: 3.662723, Val Acc: 86.27%, Val-Class-Acc: {0: '80.98%', 1: '84.84%', 2: '84.72%', 3: '92.62%'}, LR: 0.001000\n",
      "Epoch 23/200, Train Loss: 1.208525, Train-Class-Acc: {0: '92.37%', 1: '92.62%', 2: '94.63%', 3: '96.72%'}\n",
      "Val Loss: 4.678982, Val Acc: 87.50%, Val-Class-Acc: {0: '92.93%', 1: '75.41%', 2: '90.28%', 3: '93.85%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_10.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_23.pth\n",
      "Epoch 24/200, Train Loss: 0.898836, Train-Class-Acc: {0: '94.82%', 1: '94.06%', 2: '95.67%', 3: '97.03%'}\n",
      "Val Loss: 3.936969, Val Acc: 86.76%, Val-Class-Acc: {0: '86.96%', 1: '77.05%', 2: '91.67%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 25/200, Train Loss: 0.924863, Train-Class-Acc: {0: '95.37%', 1: '94.16%', 2: '97.23%', 3: '96.52%'}\n",
      "Val Loss: 4.306276, Val Acc: 86.40%, Val-Class-Acc: {0: '80.43%', 1: '80.33%', 2: '86.81%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 26/200, Train Loss: 0.983808, Train-Class-Acc: {0: '95.10%', 1: '94.57%', 2: '96.01%', 3: '96.62%'}\n",
      "Val Loss: 5.239329, Val Acc: 87.75%, Val-Class-Acc: {0: '90.22%', 1: '81.15%', 2: '85.42%', 3: '93.85%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_14.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_26.pth\n",
      "Epoch 27/200, Train Loss: 0.929206, Train-Class-Acc: {0: '94.14%', 1: '93.85%', 2: '96.19%', 3: '96.82%'}\n",
      "Val Loss: 4.370120, Val Acc: 86.52%, Val-Class-Acc: {0: '80.98%', 1: '81.97%', 2: '83.33%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 28/200, Train Loss: 1.021441, Train-Class-Acc: {0: '93.87%', 1: '93.03%', 2: '95.32%', 3: '96.11%'}\n",
      "Val Loss: 3.921064, Val Acc: 87.25%, Val-Class-Acc: {0: '89.13%', 1: '74.59%', 2: '88.89%', 3: '97.54%'}, LR: 0.001000\n",
      "Epoch 29/200, Train Loss: 0.857934, Train-Class-Acc: {0: '95.23%', 1: '93.55%', 2: '97.23%', 3: '97.44%'}\n",
      "Val Loss: 5.002027, Val Acc: 86.52%, Val-Class-Acc: {0: '86.96%', 1: '82.38%', 2: '86.11%', 3: '90.57%'}, LR: 0.001000\n",
      "Epoch 30/200, Train Loss: 0.634094, Train-Class-Acc: {0: '96.19%', 1: '94.88%', 2: '96.53%', 3: '97.34%'}\n",
      "Val Loss: 4.099267, Val Acc: 86.40%, Val-Class-Acc: {0: '83.70%', 1: '77.46%', 2: '90.28%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 31/200, Train Loss: 0.930951, Train-Class-Acc: {0: '95.78%', 1: '94.06%', 2: '96.88%', 3: '97.44%'}\n",
      "Val Loss: 4.004187, Val Acc: 87.75%, Val-Class-Acc: {0: '90.22%', 1: '80.74%', 2: '84.72%', 3: '94.67%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_20.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_31.pth\n",
      "Epoch 32/200, Train Loss: 0.722794, Train-Class-Acc: {0: '96.46%', 1: '95.18%', 2: '95.49%', 3: '97.64%'}\n",
      "Val Loss: 4.703459, Val Acc: 85.17%, Val-Class-Acc: {0: '80.43%', 1: '78.69%', 2: '85.42%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 33/200, Train Loss: 0.534535, Train-Class-Acc: {0: '94.96%', 1: '95.18%', 2: '96.71%', 3: '97.13%'}\n",
      "Val Loss: 4.290665, Val Acc: 85.05%, Val-Class-Acc: {0: '82.07%', 1: '81.56%', 2: '81.94%', 3: '92.62%'}, LR: 0.001000\n",
      "Epoch 34/200, Train Loss: 0.545546, Train-Class-Acc: {0: '96.32%', 1: '96.00%', 2: '97.05%', 3: '98.16%'}\n",
      "Val Loss: 4.380289, Val Acc: 87.38%, Val-Class-Acc: {0: '91.85%', 1: '79.10%', 2: '84.72%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 35/200, Train Loss: 0.505036, Train-Class-Acc: {0: '96.87%', 1: '95.59%', 2: '97.05%', 3: '97.85%'}\n",
      "Val Loss: 4.235007, Val Acc: 86.15%, Val-Class-Acc: {0: '77.72%', 1: '86.48%', 2: '88.89%', 3: '90.57%'}, LR: 0.001000\n",
      "Epoch 36/200, Train Loss: 0.500215, Train-Class-Acc: {0: '95.78%', 1: '95.80%', 2: '97.75%', 3: '98.46%'}\n",
      "Val Loss: 5.294917, Val Acc: 85.29%, Val-Class-Acc: {0: '84.24%', 1: '75.41%', 2: '83.33%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 37/200, Train Loss: 0.801437, Train-Class-Acc: {0: '96.46%', 1: '95.18%', 2: '96.88%', 3: '97.95%'}\n",
      "Val Loss: 4.497319, Val Acc: 87.13%, Val-Class-Acc: {0: '85.33%', 1: '86.07%', 2: '80.56%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 38/200, Train Loss: 0.667389, Train-Class-Acc: {0: '95.37%', 1: '95.08%', 2: '96.53%', 3: '97.95%'}\n",
      "Val Loss: 5.474742, Val Acc: 85.17%, Val-Class-Acc: {0: '84.24%', 1: '75.82%', 2: '80.56%', 3: '97.95%'}, LR: 0.001000\n",
      "Epoch 39/200, Train Loss: 0.743899, Train-Class-Acc: {0: '96.59%', 1: '95.80%', 2: '97.40%', 3: '97.95%'}\n",
      "Val Loss: 5.373737, Val Acc: 86.03%, Val-Class-Acc: {0: '86.41%', 1: '75.00%', 2: '87.50%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 40/200, Train Loss: 0.547303, Train-Class-Acc: {0: '96.32%', 1: '95.49%', 2: '97.23%', 3: '97.23%'}\n",
      "Val Loss: 5.008770, Val Acc: 87.01%, Val-Class-Acc: {0: '88.59%', 1: '81.15%', 2: '86.11%', 3: '92.21%'}, LR: 0.001000\n",
      "Epoch 41/200, Train Loss: 0.511492, Train-Class-Acc: {0: '96.59%', 1: '95.39%', 2: '96.71%', 3: '98.46%'}\n",
      "Val Loss: 4.686790, Val Acc: 87.01%, Val-Class-Acc: {0: '90.22%', 1: '75.82%', 2: '88.19%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 42/200, Train Loss: 0.337226, Train-Class-Acc: {0: '97.41%', 1: '96.93%', 2: '97.40%', 3: '98.26%'}\n",
      "Val Loss: 3.939054, Val Acc: 86.52%, Val-Class-Acc: {0: '90.22%', 1: '75.41%', 2: '86.11%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 43/200, Train Loss: 0.454509, Train-Class-Acc: {0: '97.55%', 1: '97.54%', 2: '98.44%', 3: '98.05%'}\n",
      "Val Loss: 4.830019, Val Acc: 87.99%, Val-Class-Acc: {0: '89.13%', 1: '81.56%', 2: '84.72%', 3: '95.49%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_23.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_43.pth\n",
      "Epoch 44/200, Train Loss: 0.391154, Train-Class-Acc: {0: '97.68%', 1: '96.31%', 2: '97.05%', 3: '98.67%'}\n",
      "Val Loss: 5.396765, Val Acc: 87.38%, Val-Class-Acc: {0: '86.96%', 1: '75.82%', 2: '90.28%', 3: '97.54%'}, LR: 0.001000\n",
      "Epoch 45/200, Train Loss: 0.479537, Train-Class-Acc: {0: '96.05%', 1: '95.80%', 2: '97.75%', 3: '98.36%'}\n",
      "Val Loss: 4.480865, Val Acc: 86.89%, Val-Class-Acc: {0: '92.93%', 1: '79.51%', 2: '79.17%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 46/200, Train Loss: 0.578059, Train-Class-Acc: {0: '97.55%', 1: '96.00%', 2: '97.57%', 3: '98.16%'}\n",
      "Val Loss: 4.512233, Val Acc: 86.40%, Val-Class-Acc: {0: '80.98%', 1: '85.66%', 2: '84.72%', 3: '92.21%'}, LR: 0.001000\n",
      "Epoch 47/200, Train Loss: 0.665264, Train-Class-Acc: {0: '97.14%', 1: '95.80%', 2: '96.88%', 3: '97.64%'}\n",
      "Val Loss: 4.256055, Val Acc: 85.91%, Val-Class-Acc: {0: '84.24%', 1: '79.92%', 2: '84.03%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 48/200, Train Loss: 0.645216, Train-Class-Acc: {0: '95.10%', 1: '95.59%', 2: '96.36%', 3: '98.26%'}\n",
      "Val Loss: 5.640886, Val Acc: 86.76%, Val-Class-Acc: {0: '92.39%', 1: '77.87%', 2: '81.94%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 49/200, Train Loss: 0.389044, Train-Class-Acc: {0: '97.28%', 1: '96.31%', 2: '98.09%', 3: '98.36%'}\n",
      "Val Loss: 6.032426, Val Acc: 86.40%, Val-Class-Acc: {0: '86.41%', 1: '77.46%', 2: '86.81%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 50/200, Train Loss: 0.412724, Train-Class-Acc: {0: '96.87%', 1: '96.41%', 2: '97.57%', 3: '97.95%'}\n",
      "Val Loss: 5.039075, Val Acc: 87.38%, Val-Class-Acc: {0: '91.30%', 1: '81.15%', 2: '89.58%', 3: '89.34%'}, LR: 0.001000\n",
      "Epoch 51/200, Train Loss: 0.455162, Train-Class-Acc: {0: '95.78%', 1: '96.82%', 2: '96.53%', 3: '98.46%'}\n",
      "Val Loss: 5.101534, Val Acc: 87.62%, Val-Class-Acc: {0: '91.30%', 1: '76.64%', 2: '85.42%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 52/200, Train Loss: 0.421059, Train-Class-Acc: {0: '98.09%', 1: '96.52%', 2: '97.40%', 3: '98.57%'}\n",
      "Val Loss: 4.298472, Val Acc: 87.62%, Val-Class-Acc: {0: '82.07%', 1: '82.79%', 2: '88.19%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 53/200, Train Loss: 0.340154, Train-Class-Acc: {0: '97.55%', 1: '97.13%', 2: '98.44%', 3: '98.57%'}\n",
      "Val Loss: 4.913665, Val Acc: 86.52%, Val-Class-Acc: {0: '86.96%', 1: '78.28%', 2: '87.50%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 54/200, Train Loss: 0.209330, Train-Class-Acc: {0: '97.96%', 1: '97.54%', 2: '98.79%', 3: '98.87%'}\n",
      "Val Loss: 4.597390, Val Acc: 87.75%, Val-Class-Acc: {0: '90.76%', 1: '78.69%', 2: '87.50%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 55/200, Train Loss: 0.151846, Train-Class-Acc: {0: '97.82%', 1: '97.75%', 2: '98.79%', 3: '99.49%'}\n",
      "Val Loss: 5.338815, Val Acc: 87.25%, Val-Class-Acc: {0: '91.30%', 1: '74.59%', 2: '88.89%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 56/200, Train Loss: 0.212943, Train-Class-Acc: {0: '98.50%', 1: '97.75%', 2: '98.61%', 3: '99.18%'}\n",
      "Val Loss: 4.424332, Val Acc: 87.75%, Val-Class-Acc: {0: '89.13%', 1: '78.69%', 2: '86.11%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 57/200, Train Loss: 0.290164, Train-Class-Acc: {0: '96.73%', 1: '97.23%', 2: '99.31%', 3: '98.98%'}\n",
      "Val Loss: 4.478000, Val Acc: 87.50%, Val-Class-Acc: {0: '85.87%', 1: '81.97%', 2: '86.11%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 58/200, Train Loss: 0.323131, Train-Class-Acc: {0: '97.82%', 1: '97.64%', 2: '98.79%', 3: '98.46%'}\n",
      "Val Loss: 5.519855, Val Acc: 86.76%, Val-Class-Acc: {0: '93.48%', 1: '74.18%', 2: '81.94%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 59/200, Train Loss: 0.276903, Train-Class-Acc: {0: '97.41%', 1: '97.23%', 2: '98.27%', 3: '98.57%'}\n",
      "Val Loss: 6.322272, Val Acc: 86.27%, Val-Class-Acc: {0: '89.13%', 1: '78.28%', 2: '84.03%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 60/200, Train Loss: 0.297828, Train-Class-Acc: {0: '97.68%', 1: '97.85%', 2: '97.05%', 3: '99.08%'}\n",
      "Val Loss: 6.096183, Val Acc: 86.64%, Val-Class-Acc: {0: '88.04%', 1: '82.79%', 2: '82.64%', 3: '91.80%'}, LR: 0.001000\n",
      "Epoch 61/200, Train Loss: 0.406975, Train-Class-Acc: {0: '97.28%', 1: '96.72%', 2: '98.79%', 3: '98.57%'}\n",
      "Val Loss: 4.762590, Val Acc: 87.62%, Val-Class-Acc: {0: '88.59%', 1: '81.56%', 2: '86.81%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 62/200, Train Loss: 0.280416, Train-Class-Acc: {0: '98.37%', 1: '97.34%', 2: '98.61%', 3: '99.08%'}\n",
      "Val Loss: 5.068097, Val Acc: 87.25%, Val-Class-Acc: {0: '82.07%', 1: '86.07%', 2: '86.81%', 3: '92.62%'}, LR: 0.001000\n",
      "Epoch 63/200, Train Loss: 0.848456, Train-Class-Acc: {0: '95.23%', 1: '96.11%', 2: '97.05%', 3: '98.16%'}\n",
      "Val Loss: 7.827087, Val Acc: 85.29%, Val-Class-Acc: {0: '86.96%', 1: '82.79%', 2: '81.94%', 3: '88.52%'}, LR: 0.001000\n",
      "Epoch 64/200, Train Loss: 0.606740, Train-Class-Acc: {0: '96.32%', 1: '95.80%', 2: '97.92%', 3: '98.26%'}\n",
      "Val Loss: 4.947811, Val Acc: 86.76%, Val-Class-Acc: {0: '86.96%', 1: '83.61%', 2: '75.69%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 65/200, Train Loss: 0.332922, Train-Class-Acc: {0: '97.82%', 1: '97.34%', 2: '97.92%', 3: '98.98%'}\n",
      "Val Loss: 5.250357, Val Acc: 88.36%, Val-Class-Acc: {0: '91.85%', 1: '80.74%', 2: '83.33%', 3: '96.31%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_26.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_65.pth\n",
      "Epoch 66/200, Train Loss: 0.282649, Train-Class-Acc: {0: '98.09%', 1: '97.44%', 2: '98.09%', 3: '98.87%'}\n",
      "Val Loss: 4.837066, Val Acc: 87.50%, Val-Class-Acc: {0: '85.87%', 1: '82.79%', 2: '83.33%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 67/200, Train Loss: 0.250354, Train-Class-Acc: {0: '98.09%', 1: '97.75%', 2: '98.27%', 3: '98.67%'}\n",
      "Val Loss: 5.791595, Val Acc: 86.64%, Val-Class-Acc: {0: '89.13%', 1: '75.00%', 2: '89.58%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 68/200, Train Loss: 0.189088, Train-Class-Acc: {0: '98.23%', 1: '98.05%', 2: '98.79%', 3: '99.28%'}\n",
      "Val Loss: 4.706031, Val Acc: 88.11%, Val-Class-Acc: {0: '87.50%', 1: '82.79%', 2: '83.33%', 3: '96.72%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_31.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_68.pth\n",
      "Epoch 69/200, Train Loss: 0.334419, Train-Class-Acc: {0: '98.77%', 1: '97.75%', 2: '98.79%', 3: '98.98%'}\n",
      "Val Loss: 4.949397, Val Acc: 87.25%, Val-Class-Acc: {0: '85.33%', 1: '83.20%', 2: '86.81%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 70/200, Train Loss: 0.229321, Train-Class-Acc: {0: '97.41%', 1: '97.95%', 2: '98.79%', 3: '98.87%'}\n",
      "Val Loss: 8.570161, Val Acc: 83.82%, Val-Class-Acc: {0: '83.15%', 1: '70.90%', 2: '83.33%', 3: '97.54%'}, LR: 0.001000\n",
      "Epoch 71/200, Train Loss: 0.317682, Train-Class-Acc: {0: '97.28%', 1: '97.03%', 2: '98.44%', 3: '98.57%'}\n",
      "Val Loss: 5.377419, Val Acc: 86.89%, Val-Class-Acc: {0: '86.96%', 1: '86.07%', 2: '81.25%', 3: '90.98%'}, LR: 0.001000\n",
      "Epoch 72/200, Train Loss: 0.219632, Train-Class-Acc: {0: '97.96%', 1: '98.57%', 2: '97.92%', 3: '99.39%'}\n",
      "Val Loss: 5.049119, Val Acc: 86.64%, Val-Class-Acc: {0: '86.96%', 1: '80.74%', 2: '80.56%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 73/200, Train Loss: 0.200261, Train-Class-Acc: {0: '98.50%', 1: '98.05%', 2: '98.09%', 3: '99.18%'}\n",
      "Val Loss: 6.088396, Val Acc: 85.42%, Val-Class-Acc: {0: '79.89%', 1: '77.46%', 2: '86.11%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 74/200, Train Loss: 0.270017, Train-Class-Acc: {0: '98.23%', 1: '98.67%', 2: '98.79%', 3: '98.87%'}\n",
      "Val Loss: 4.705102, Val Acc: 86.76%, Val-Class-Acc: {0: '84.24%', 1: '81.15%', 2: '84.72%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 75/200, Train Loss: 0.279592, Train-Class-Acc: {0: '98.77%', 1: '98.36%', 2: '99.83%', 3: '99.28%'}\n",
      "Val Loss: 5.094159, Val Acc: 87.13%, Val-Class-Acc: {0: '85.33%', 1: '81.56%', 2: '82.64%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 76/200, Train Loss: 0.217010, Train-Class-Acc: {0: '98.23%', 1: '98.57%', 2: '98.96%', 3: '99.08%'}\n",
      "Val Loss: 5.618857, Val Acc: 85.17%, Val-Class-Acc: {0: '80.98%', 1: '81.97%', 2: '85.42%', 3: '91.39%'}, LR: 0.001000\n",
      "Epoch 77/200, Train Loss: 0.289812, Train-Class-Acc: {0: '98.23%', 1: '97.54%', 2: '97.57%', 3: '98.87%'}\n",
      "Val Loss: 4.594654, Val Acc: 86.52%, Val-Class-Acc: {0: '83.15%', 1: '80.33%', 2: '85.42%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 78/200, Train Loss: 0.357114, Train-Class-Acc: {0: '98.64%', 1: '98.05%', 2: '98.61%', 3: '99.28%'}\n",
      "Val Loss: 4.569647, Val Acc: 87.50%, Val-Class-Acc: {0: '91.85%', 1: '80.74%', 2: '84.03%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 79/200, Train Loss: 0.149128, Train-Class-Acc: {0: '99.32%', 1: '98.26%', 2: '98.96%', 3: '99.28%'}\n",
      "Val Loss: 5.462902, Val Acc: 86.76%, Val-Class-Acc: {0: '86.96%', 1: '78.28%', 2: '86.81%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 80/200, Train Loss: 0.214546, Train-Class-Acc: {0: '97.00%', 1: '97.85%', 2: '98.79%', 3: '99.18%'}\n",
      "Val Loss: 6.236010, Val Acc: 86.15%, Val-Class-Acc: {0: '85.33%', 1: '78.28%', 2: '89.58%', 3: '92.62%'}, LR: 0.001000\n",
      "Epoch 81/200, Train Loss: 0.169068, Train-Class-Acc: {0: '98.50%', 1: '98.46%', 2: '98.96%', 3: '99.90%'}\n",
      "Val Loss: 5.299570, Val Acc: 87.75%, Val-Class-Acc: {0: '89.67%', 1: '81.15%', 2: '84.72%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 82/200, Train Loss: 0.128354, Train-Class-Acc: {0: '98.37%', 1: '98.16%', 2: '98.61%', 3: '99.80%'}\n",
      "Val Loss: 5.568712, Val Acc: 86.52%, Val-Class-Acc: {0: '83.15%', 1: '86.48%', 2: '81.94%', 3: '91.80%'}, LR: 0.001000\n",
      "Epoch 83/200, Train Loss: 0.235303, Train-Class-Acc: {0: '98.50%', 1: '98.26%', 2: '98.44%', 3: '98.67%'}\n",
      "Val Loss: 5.473567, Val Acc: 87.75%, Val-Class-Acc: {0: '89.13%', 1: '78.69%', 2: '87.50%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 84/200, Train Loss: 0.333571, Train-Class-Acc: {0: '97.55%', 1: '97.23%', 2: '97.75%', 3: '98.87%'}\n",
      "Val Loss: 5.177514, Val Acc: 86.64%, Val-Class-Acc: {0: '84.24%', 1: '80.33%', 2: '85.42%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 85/200, Train Loss: 0.380728, Train-Class-Acc: {0: '97.96%', 1: '97.44%', 2: '98.79%', 3: '98.87%'}\n",
      "Val Loss: 6.869996, Val Acc: 85.54%, Val-Class-Acc: {0: '91.85%', 1: '71.31%', 2: '88.19%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 86/200, Train Loss: 0.475430, Train-Class-Acc: {0: '98.09%', 1: '96.41%', 2: '97.92%', 3: '98.05%'}\n",
      "Val Loss: 5.621025, Val Acc: 87.01%, Val-Class-Acc: {0: '88.04%', 1: '80.74%', 2: '84.72%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 87/200, Train Loss: 0.342253, Train-Class-Acc: {0: '98.23%', 1: '97.85%', 2: '98.61%', 3: '98.98%'}\n",
      "Val Loss: 7.234125, Val Acc: 86.40%, Val-Class-Acc: {0: '80.43%', 1: '85.66%', 2: '90.97%', 3: '88.93%'}, LR: 0.001000\n",
      "Epoch 88/200, Train Loss: 0.242200, Train-Class-Acc: {0: '98.23%', 1: '98.26%', 2: '98.27%', 3: '98.87%'}\n",
      "Val Loss: 5.493444, Val Acc: 86.40%, Val-Class-Acc: {0: '86.96%', 1: '79.51%', 2: '82.64%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 89/200, Train Loss: 0.251305, Train-Class-Acc: {0: '98.37%', 1: '98.05%', 2: '98.96%', 3: '98.87%'}\n",
      "Val Loss: 5.461237, Val Acc: 86.27%, Val-Class-Acc: {0: '90.22%', 1: '79.92%', 2: '77.08%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 90/200, Train Loss: 0.429183, Train-Class-Acc: {0: '98.23%', 1: '97.75%', 2: '98.09%', 3: '98.87%'}\n",
      "Val Loss: 7.292191, Val Acc: 85.78%, Val-Class-Acc: {0: '85.87%', 1: '76.64%', 2: '82.64%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 91/200, Train Loss: 0.306716, Train-Class-Acc: {0: '98.23%', 1: '98.16%', 2: '98.79%', 3: '98.67%'}\n",
      "Val Loss: 5.862340, Val Acc: 86.03%, Val-Class-Acc: {0: '90.76%', 1: '75.41%', 2: '83.33%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 92/200, Train Loss: 0.306806, Train-Class-Acc: {0: '98.23%', 1: '98.05%', 2: '98.79%', 3: '98.67%'}\n",
      "Val Loss: 4.900229, Val Acc: 88.24%, Val-Class-Acc: {0: '88.59%', 1: '84.43%', 2: '84.03%', 3: '94.26%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_43.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_92.pth\n",
      "Epoch 93/200, Train Loss: 0.160486, Train-Class-Acc: {0: '98.64%', 1: '98.46%', 2: '98.27%', 3: '99.69%'}\n",
      "Val Loss: 5.758892, Val Acc: 87.62%, Val-Class-Acc: {0: '92.39%', 1: '76.23%', 2: '84.72%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 94/200, Train Loss: 0.250096, Train-Class-Acc: {0: '97.68%', 1: '98.16%', 2: '98.79%', 3: '99.28%'}\n",
      "Val Loss: 5.783746, Val Acc: 87.01%, Val-Class-Acc: {0: '92.93%', 1: '77.05%', 2: '79.86%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 95/200, Train Loss: 0.152067, Train-Class-Acc: {0: '99.18%', 1: '98.16%', 2: '98.61%', 3: '98.98%'}\n",
      "Val Loss: 5.671264, Val Acc: 86.52%, Val-Class-Acc: {0: '88.59%', 1: '81.15%', 2: '79.17%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 96/200, Train Loss: 0.223382, Train-Class-Acc: {0: '98.37%', 1: '98.05%', 2: '99.31%', 3: '99.08%'}\n",
      "Val Loss: 5.222311, Val Acc: 87.01%, Val-Class-Acc: {0: '89.13%', 1: '78.69%', 2: '84.03%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 97/200, Train Loss: 0.187646, Train-Class-Acc: {0: '98.77%', 1: '98.36%', 2: '99.13%', 3: '98.87%'}\n",
      "Val Loss: 7.608387, Val Acc: 85.17%, Val-Class-Acc: {0: '93.48%', 1: '68.44%', 2: '80.56%', 3: '98.36%'}, LR: 0.001000\n",
      "Epoch 98/200, Train Loss: 0.780802, Train-Class-Acc: {0: '97.82%', 1: '96.52%', 2: '97.23%', 3: '97.34%'}\n",
      "Val Loss: 6.080070, Val Acc: 87.25%, Val-Class-Acc: {0: '87.50%', 1: '78.28%', 2: '88.89%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 99/200, Train Loss: 0.260205, Train-Class-Acc: {0: '98.50%', 1: '98.05%', 2: '98.09%', 3: '98.87%'}\n",
      "Val Loss: 5.801136, Val Acc: 86.27%, Val-Class-Acc: {0: '84.24%', 1: '79.10%', 2: '82.64%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 100/200, Train Loss: 0.402998, Train-Class-Acc: {0: '98.23%', 1: '96.72%', 2: '97.57%', 3: '98.46%'}\n",
      "Val Loss: 7.184768, Val Acc: 86.27%, Val-Class-Acc: {0: '85.33%', 1: '81.15%', 2: '81.94%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 101/200, Train Loss: 0.296930, Train-Class-Acc: {0: '98.09%', 1: '97.75%', 2: '98.09%', 3: '99.08%'}\n",
      "Val Loss: 6.946747, Val Acc: 85.29%, Val-Class-Acc: {0: '76.63%', 1: '78.28%', 2: '92.36%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 102/200, Train Loss: 0.281196, Train-Class-Acc: {0: '97.55%', 1: '97.64%', 2: '98.44%', 3: '99.08%'}\n",
      "Val Loss: 5.722888, Val Acc: 87.87%, Val-Class-Acc: {0: '93.48%', 1: '77.46%', 2: '90.97%', 3: '92.21%'}, LR: 0.001000\n",
      "Epoch 103/200, Train Loss: 0.133337, Train-Class-Acc: {0: '98.64%', 1: '98.77%', 2: '99.13%', 3: '99.18%'}\n",
      "Val Loss: 5.599724, Val Acc: 86.89%, Val-Class-Acc: {0: '82.07%', 1: '80.74%', 2: '87.50%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 104/200, Train Loss: 0.286009, Train-Class-Acc: {0: '98.23%', 1: '98.05%', 2: '99.13%', 3: '99.18%'}\n",
      "Val Loss: 7.032483, Val Acc: 85.42%, Val-Class-Acc: {0: '86.41%', 1: '71.72%', 2: '84.72%', 3: '98.77%'}, LR: 0.001000\n",
      "Epoch 105/200, Train Loss: 0.214669, Train-Class-Acc: {0: '97.68%', 1: '97.64%', 2: '98.61%', 3: '99.59%'}\n",
      "Val Loss: 7.156389, Val Acc: 86.76%, Val-Class-Acc: {0: '88.59%', 1: '75.82%', 2: '85.42%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 106/200, Train Loss: 0.176008, Train-Class-Acc: {0: '99.18%', 1: '98.46%', 2: '98.79%', 3: '99.49%'}\n",
      "Val Loss: 6.240062, Val Acc: 86.76%, Val-Class-Acc: {0: '84.78%', 1: '78.28%', 2: '91.67%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 107/200, Train Loss: 0.150992, Train-Class-Acc: {0: '98.09%', 1: '98.67%', 2: '98.61%', 3: '99.49%'}\n",
      "Val Loss: 5.305538, Val Acc: 88.60%, Val-Class-Acc: {0: '89.13%', 1: '83.20%', 2: '85.42%', 3: '95.49%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_13.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_107.pth\n",
      "Epoch 108/200, Train Loss: 0.067752, Train-Class-Acc: {0: '99.05%', 1: '99.08%', 2: '99.48%', 3: '99.59%'}\n",
      "Val Loss: 6.007310, Val Acc: 88.60%, Val-Class-Acc: {0: '93.48%', 1: '78.28%', 2: '85.42%', 3: '97.13%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_21.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_108.pth\n",
      "Epoch 109/200, Train Loss: 0.078286, Train-Class-Acc: {0: '99.32%', 1: '99.18%', 2: '99.48%', 3: '99.59%'}\n",
      "Val Loss: 5.990589, Val Acc: 87.75%, Val-Class-Acc: {0: '93.48%', 1: '74.59%', 2: '86.11%', 3: '97.54%'}, LR: 0.001000\n",
      "Epoch 110/200, Train Loss: 0.104172, Train-Class-Acc: {0: '99.18%', 1: '98.77%', 2: '98.96%', 3: '99.59%'}\n",
      "Val Loss: 6.216670, Val Acc: 87.38%, Val-Class-Acc: {0: '89.13%', 1: '78.69%', 2: '82.64%', 3: '97.54%'}, LR: 0.001000\n",
      "Epoch 111/200, Train Loss: 0.067086, Train-Class-Acc: {0: '99.18%', 1: '99.39%', 2: '99.65%', 3: '99.49%'}\n",
      "Val Loss: 6.374230, Val Acc: 87.75%, Val-Class-Acc: {0: '93.48%', 1: '81.97%', 2: '79.17%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 112/200, Train Loss: 0.179123, Train-Class-Acc: {0: '98.91%', 1: '98.57%', 2: '98.79%', 3: '99.28%'}\n",
      "Val Loss: 5.815607, Val Acc: 87.50%, Val-Class-Acc: {0: '86.96%', 1: '79.92%', 2: '83.33%', 3: '97.95%'}, LR: 0.001000\n",
      "Epoch 113/200, Train Loss: 0.223013, Train-Class-Acc: {0: '97.82%', 1: '97.34%', 2: '99.31%', 3: '99.28%'}\n",
      "Val Loss: 7.013481, Val Acc: 86.52%, Val-Class-Acc: {0: '84.78%', 1: '83.20%', 2: '77.78%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 114/200, Train Loss: 0.236897, Train-Class-Acc: {0: '97.68%', 1: '98.36%', 2: '98.79%', 3: '99.49%'}\n",
      "Val Loss: 5.823470, Val Acc: 87.38%, Val-Class-Acc: {0: '91.30%', 1: '75.41%', 2: '92.36%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 115/200, Train Loss: 0.421608, Train-Class-Acc: {0: '98.37%', 1: '98.05%', 2: '98.79%', 3: '99.18%'}\n",
      "Val Loss: 6.546623, Val Acc: 87.38%, Val-Class-Acc: {0: '86.96%', 1: '79.92%', 2: '87.50%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 116/200, Train Loss: 0.236196, Train-Class-Acc: {0: '98.23%', 1: '97.44%', 2: '98.79%', 3: '99.28%'}\n",
      "Val Loss: 5.420122, Val Acc: 87.75%, Val-Class-Acc: {0: '81.52%', 1: '84.43%', 2: '88.19%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 117/200, Train Loss: 0.261829, Train-Class-Acc: {0: '98.37%', 1: '97.85%', 2: '98.61%', 3: '99.28%'}\n",
      "Val Loss: 5.705172, Val Acc: 87.50%, Val-Class-Acc: {0: '80.98%', 1: '86.89%', 2: '85.42%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 118/200, Train Loss: 0.191521, Train-Class-Acc: {0: '98.50%', 1: '97.95%', 2: '99.13%', 3: '99.49%'}\n",
      "Val Loss: 6.708986, Val Acc: 85.54%, Val-Class-Acc: {0: '78.80%', 1: '80.74%', 2: '84.03%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 119/200, Train Loss: 0.288674, Train-Class-Acc: {0: '98.91%', 1: '98.16%', 2: '98.79%', 3: '98.98%'}\n",
      "Val Loss: 6.028588, Val Acc: 87.87%, Val-Class-Acc: {0: '84.78%', 1: '83.20%', 2: '86.81%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 120/200, Train Loss: 0.107266, Train-Class-Acc: {0: '98.64%', 1: '99.08%', 2: '98.79%', 3: '99.59%'}\n",
      "Val Loss: 7.285882, Val Acc: 87.13%, Val-Class-Acc: {0: '92.93%', 1: '73.77%', 2: '86.11%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 121/200, Train Loss: 0.155292, Train-Class-Acc: {0: '98.23%', 1: '98.87%', 2: '99.31%', 3: '99.28%'}\n",
      "Val Loss: 7.075344, Val Acc: 85.78%, Val-Class-Acc: {0: '90.22%', 1: '76.64%', 2: '83.33%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 122/200, Train Loss: 0.155653, Train-Class-Acc: {0: '98.64%', 1: '98.05%', 2: '98.96%', 3: '99.39%'}\n",
      "Val Loss: 6.612171, Val Acc: 87.62%, Val-Class-Acc: {0: '89.67%', 1: '82.79%', 2: '80.56%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 123/200, Train Loss: 0.094744, Train-Class-Acc: {0: '99.32%', 1: '99.18%', 2: '99.13%', 3: '99.49%'}\n",
      "Val Loss: 6.819553, Val Acc: 88.11%, Val-Class-Acc: {0: '89.13%', 1: '81.15%', 2: '84.72%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 124/200, Train Loss: 0.120648, Train-Class-Acc: {0: '99.18%', 1: '99.08%', 2: '99.13%', 3: '99.69%'}\n",
      "Val Loss: 6.135642, Val Acc: 87.38%, Val-Class-Acc: {0: '91.30%', 1: '78.28%', 2: '88.89%', 3: '92.62%'}, LR: 0.001000\n",
      "Epoch 125/200, Train Loss: 0.072659, Train-Class-Acc: {0: '99.32%', 1: '99.28%', 2: '99.13%', 3: '99.59%'}\n",
      "Val Loss: 6.323648, Val Acc: 86.89%, Val-Class-Acc: {0: '89.13%', 1: '79.92%', 2: '84.03%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 126/200, Train Loss: 0.061589, Train-Class-Acc: {0: '99.18%', 1: '99.28%', 2: '99.65%', 3: '99.49%'}\n",
      "Val Loss: 6.327080, Val Acc: 86.89%, Val-Class-Acc: {0: '88.59%', 1: '76.64%', 2: '86.11%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 127/200, Train Loss: 0.060386, Train-Class-Acc: {0: '99.32%', 1: '99.18%', 2: '99.48%', 3: '99.80%'}\n",
      "Val Loss: 6.240766, Val Acc: 86.03%, Val-Class-Acc: {0: '80.43%', 1: '84.02%', 2: '88.19%', 3: '90.98%'}, LR: 0.001000\n",
      "Epoch 128/200, Train Loss: 0.086235, Train-Class-Acc: {0: '98.77%', 1: '99.18%', 2: '99.65%', 3: '99.80%'}\n",
      "Val Loss: 6.048343, Val Acc: 86.64%, Val-Class-Acc: {0: '90.76%', 1: '76.23%', 2: '88.89%', 3: '92.62%'}, LR: 0.001000\n",
      "Epoch 129/200, Train Loss: 0.248574, Train-Class-Acc: {0: '99.32%', 1: '98.67%', 2: '99.48%', 3: '99.39%'}\n",
      "Val Loss: 9.240943, Val Acc: 85.17%, Val-Class-Acc: {0: '87.50%', 1: '69.67%', 2: '85.42%', 3: '98.77%'}, LR: 0.001000\n",
      "Epoch 130/200, Train Loss: 0.353096, Train-Class-Acc: {0: '98.64%', 1: '97.64%', 2: '98.79%', 3: '98.77%'}\n",
      "Val Loss: 6.644287, Val Acc: 87.62%, Val-Class-Acc: {0: '89.67%', 1: '81.15%', 2: '82.64%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 131/200, Train Loss: 0.215316, Train-Class-Acc: {0: '98.77%', 1: '98.57%', 2: '98.61%', 3: '99.39%'}\n",
      "Val Loss: 6.067679, Val Acc: 87.38%, Val-Class-Acc: {0: '90.22%', 1: '76.23%', 2: '89.58%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 132/200, Train Loss: 0.091102, Train-Class-Acc: {0: '98.77%', 1: '98.67%', 2: '99.31%', 3: '99.59%'}\n",
      "Val Loss: 6.693338, Val Acc: 87.13%, Val-Class-Acc: {0: '86.41%', 1: '82.79%', 2: '84.03%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 133/200, Train Loss: 0.120788, Train-Class-Acc: {0: '99.18%', 1: '98.98%', 2: '99.48%', 3: '99.69%'}\n",
      "Val Loss: 6.341134, Val Acc: 87.01%, Val-Class-Acc: {0: '82.61%', 1: '87.30%', 2: '81.25%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 134/200, Train Loss: 0.190052, Train-Class-Acc: {0: '98.91%', 1: '98.57%', 2: '98.61%', 3: '99.59%'}\n",
      "Val Loss: 7.104959, Val Acc: 86.89%, Val-Class-Acc: {0: '88.59%', 1: '83.61%', 2: '82.64%', 3: '91.39%'}, LR: 0.001000\n",
      "Epoch 135/200, Train Loss: 0.187242, Train-Class-Acc: {0: '98.09%', 1: '98.46%', 2: '99.31%', 3: '98.98%'}\n",
      "Val Loss: 6.024447, Val Acc: 85.29%, Val-Class-Acc: {0: '84.24%', 1: '77.46%', 2: '84.03%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 136/200, Train Loss: 0.168744, Train-Class-Acc: {0: '98.91%', 1: '98.26%', 2: '99.13%', 3: '99.39%'}\n",
      "Val Loss: 6.192962, Val Acc: 86.40%, Val-Class-Acc: {0: '90.22%', 1: '75.82%', 2: '84.72%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 137/200, Train Loss: 0.199465, Train-Class-Acc: {0: '98.50%', 1: '98.67%', 2: '98.79%', 3: '99.28%'}\n",
      "Val Loss: 6.216332, Val Acc: 86.52%, Val-Class-Acc: {0: '91.30%', 1: '79.10%', 2: '78.47%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 138/200, Train Loss: 0.068308, Train-Class-Acc: {0: '99.05%', 1: '98.87%', 2: '98.96%', 3: '99.59%'}\n",
      "Val Loss: 6.445183, Val Acc: 86.64%, Val-Class-Acc: {0: '87.50%', 1: '85.25%', 2: '79.86%', 3: '91.39%'}, LR: 0.001000\n",
      "Epoch 139/200, Train Loss: 0.156925, Train-Class-Acc: {0: '99.18%', 1: '99.39%', 2: '98.96%', 3: '99.59%'}\n",
      "Val Loss: 5.738415, Val Acc: 86.89%, Val-Class-Acc: {0: '81.52%', 1: '84.84%', 2: '84.72%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 140/200, Train Loss: 0.189409, Train-Class-Acc: {0: '99.18%', 1: '99.28%', 2: '99.48%', 3: '99.69%'}\n",
      "Val Loss: 6.158074, Val Acc: 87.01%, Val-Class-Acc: {0: '85.87%', 1: '81.97%', 2: '86.11%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 141/200, Train Loss: 0.564712, Train-Class-Acc: {0: '98.91%', 1: '97.44%', 2: '98.44%', 3: '99.08%'}\n",
      "Val Loss: 6.468234, Val Acc: 86.40%, Val-Class-Acc: {0: '88.04%', 1: '85.66%', 2: '84.72%', 3: '86.89%'}, LR: 0.001000\n",
      "Epoch 142/200, Train Loss: 0.593322, Train-Class-Acc: {0: '97.82%', 1: '96.72%', 2: '98.27%', 3: '97.54%'}\n",
      "Val Loss: 7.221106, Val Acc: 86.76%, Val-Class-Acc: {0: '85.33%', 1: '85.66%', 2: '78.47%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 143/200, Train Loss: 0.248773, Train-Class-Acc: {0: '98.09%', 1: '98.16%', 2: '97.92%', 3: '99.18%'}\n",
      "Val Loss: 5.851908, Val Acc: 86.64%, Val-Class-Acc: {0: '93.48%', 1: '79.92%', 2: '82.64%', 3: '90.57%'}, LR: 0.001000\n",
      "Epoch 144/200, Train Loss: 0.161738, Train-Class-Acc: {0: '99.32%', 1: '98.87%', 2: '99.13%', 3: '99.18%'}\n",
      "Val Loss: 5.448581, Val Acc: 87.75%, Val-Class-Acc: {0: '89.67%', 1: '85.25%', 2: '83.33%', 3: '91.39%'}, LR: 0.001000\n",
      "Epoch 145/200, Train Loss: 0.113628, Train-Class-Acc: {0: '99.18%', 1: '99.18%', 2: '98.96%', 3: '99.39%'}\n",
      "Val Loss: 6.263291, Val Acc: 87.87%, Val-Class-Acc: {0: '92.93%', 1: '76.64%', 2: '84.72%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 146/200, Train Loss: 0.069697, Train-Class-Acc: {0: '99.59%', 1: '98.98%', 2: '99.65%', 3: '99.80%'}\n",
      "Val Loss: 5.661075, Val Acc: 87.75%, Val-Class-Acc: {0: '88.04%', 1: '81.56%', 2: '85.42%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 147/200, Train Loss: 0.036136, Train-Class-Acc: {0: '99.32%', 1: '99.39%', 2: '99.65%', 3: '99.69%'}\n",
      "Val Loss: 5.980588, Val Acc: 88.97%, Val-Class-Acc: {0: '90.22%', 1: '81.97%', 2: '88.19%', 3: '95.49%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_68.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_147.pth\n",
      "Epoch 148/200, Train Loss: 0.063901, Train-Class-Acc: {0: '99.46%', 1: '99.08%', 2: '99.48%', 3: '99.69%'}\n",
      "Val Loss: 5.918105, Val Acc: 87.87%, Val-Class-Acc: {0: '90.22%', 1: '78.28%', 2: '87.50%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 149/200, Train Loss: 0.155460, Train-Class-Acc: {0: '98.77%', 1: '99.28%', 2: '99.65%', 3: '99.49%'}\n",
      "Val Loss: 6.578139, Val Acc: 87.25%, Val-Class-Acc: {0: '83.70%', 1: '80.33%', 2: '90.28%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 150/200, Train Loss: 0.197243, Train-Class-Acc: {0: '99.32%', 1: '98.77%', 2: '98.96%', 3: '99.39%'}\n",
      "Val Loss: 6.872206, Val Acc: 86.76%, Val-Class-Acc: {0: '83.15%', 1: '84.84%', 2: '81.94%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 151/200, Train Loss: 0.088316, Train-Class-Acc: {0: '99.32%', 1: '99.28%', 2: '99.13%', 3: '99.69%'}\n",
      "Val Loss: 6.251981, Val Acc: 86.64%, Val-Class-Acc: {0: '80.43%', 1: '85.66%', 2: '84.03%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 152/200, Train Loss: 0.078133, Train-Class-Acc: {0: '99.32%', 1: '99.59%', 2: '99.83%', 3: '99.59%'}\n",
      "Val Loss: 6.522184, Val Acc: 87.25%, Val-Class-Acc: {0: '82.07%', 1: '86.07%', 2: '84.03%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 153/200, Train Loss: 0.048148, Train-Class-Acc: {0: '99.86%', 1: '99.69%', 2: '99.83%', 3: '99.90%'}\n",
      "Val Loss: 6.603817, Val Acc: 86.89%, Val-Class-Acc: {0: '85.87%', 1: '79.51%', 2: '82.64%', 3: '97.54%'}, LR: 0.001000\n",
      "Epoch 154/200, Train Loss: 0.126939, Train-Class-Acc: {0: '98.50%', 1: '98.87%', 2: '99.31%', 3: '99.69%'}\n",
      "Val Loss: 7.247232, Val Acc: 85.66%, Val-Class-Acc: {0: '82.61%', 1: '79.92%', 2: '86.11%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 155/200, Train Loss: 0.085605, Train-Class-Acc: {0: '99.32%', 1: '98.98%', 2: '99.31%', 3: '99.69%'}\n",
      "Val Loss: 6.748831, Val Acc: 87.25%, Val-Class-Acc: {0: '89.67%', 1: '80.33%', 2: '82.64%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 156/200, Train Loss: 0.073787, Train-Class-Acc: {0: '99.05%', 1: '99.39%', 2: '99.48%', 3: '99.59%'}\n",
      "Val Loss: 6.528615, Val Acc: 87.38%, Val-Class-Acc: {0: '88.59%', 1: '80.33%', 2: '85.42%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 157/200, Train Loss: 0.064704, Train-Class-Acc: {0: '99.18%', 1: '99.28%', 2: '99.48%', 3: '99.49%'}\n",
      "Val Loss: 7.316015, Val Acc: 86.76%, Val-Class-Acc: {0: '94.02%', 1: '76.64%', 2: '80.56%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 158/200, Train Loss: 0.140010, Train-Class-Acc: {0: '99.05%', 1: '98.67%', 2: '99.31%', 3: '99.59%'}\n",
      "Val Loss: 8.135611, Val Acc: 86.40%, Val-Class-Acc: {0: '89.67%', 1: '75.41%', 2: '85.42%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 159/200, Train Loss: 0.069198, Train-Class-Acc: {0: '98.77%', 1: '99.18%', 2: '99.48%', 3: '99.90%'}\n",
      "Val Loss: 7.547018, Val Acc: 86.76%, Val-Class-Acc: {0: '88.04%', 1: '78.28%', 2: '86.81%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 160/200, Train Loss: 0.080334, Train-Class-Acc: {0: '99.46%', 1: '98.57%', 2: '99.65%', 3: '99.49%'}\n",
      "Val Loss: 7.928013, Val Acc: 85.78%, Val-Class-Acc: {0: '80.43%', 1: '84.43%', 2: '81.94%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 161/200, Train Loss: 0.105866, Train-Class-Acc: {0: '98.91%', 1: '99.28%', 2: '98.79%', 3: '99.59%'}\n",
      "Val Loss: 6.920279, Val Acc: 86.89%, Val-Class-Acc: {0: '90.76%', 1: '79.51%', 2: '80.56%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 162/200, Train Loss: 0.075420, Train-Class-Acc: {0: '99.18%', 1: '99.08%', 2: '99.31%', 3: '99.69%'}\n",
      "Val Loss: 5.955096, Val Acc: 87.87%, Val-Class-Acc: {0: '91.30%', 1: '77.87%', 2: '86.81%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 163/200, Train Loss: 0.091390, Train-Class-Acc: {0: '99.59%', 1: '99.28%', 2: '99.48%', 3: '99.49%'}\n",
      "Val Loss: 6.329710, Val Acc: 88.24%, Val-Class-Acc: {0: '88.04%', 1: '82.38%', 2: '85.42%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 164/200, Train Loss: 0.062843, Train-Class-Acc: {0: '99.59%', 1: '99.69%', 2: '99.31%', 3: '99.69%'}\n",
      "Val Loss: 5.987062, Val Acc: 87.87%, Val-Class-Acc: {0: '88.59%', 1: '80.33%', 2: '86.11%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 165/200, Train Loss: 0.137077, Train-Class-Acc: {0: '99.46%', 1: '98.87%', 2: '98.96%', 3: '99.59%'}\n",
      "Val Loss: 6.552004, Val Acc: 87.50%, Val-Class-Acc: {0: '90.22%', 1: '79.92%', 2: '86.11%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 166/200, Train Loss: 0.169047, Train-Class-Acc: {0: '99.46%', 1: '98.87%', 2: '99.13%', 3: '99.28%'}\n",
      "Val Loss: 6.390336, Val Acc: 87.25%, Val-Class-Acc: {0: '86.96%', 1: '80.74%', 2: '85.42%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 167/200, Train Loss: 0.253732, Train-Class-Acc: {0: '98.23%', 1: '98.05%', 2: '99.65%', 3: '98.98%'}\n",
      "Val Loss: 6.601159, Val Acc: 87.01%, Val-Class-Acc: {0: '91.30%', 1: '77.87%', 2: '81.25%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 168/200, Train Loss: 0.244363, Train-Class-Acc: {0: '98.23%', 1: '98.36%', 2: '99.13%', 3: '99.39%'}\n",
      "Val Loss: 5.524067, Val Acc: 88.24%, Val-Class-Acc: {0: '92.39%', 1: '79.51%', 2: '85.42%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 169/200, Train Loss: 0.150015, Train-Class-Acc: {0: '99.05%', 1: '99.18%', 2: '98.79%', 3: '99.59%'}\n",
      "Val Loss: 6.140064, Val Acc: 87.99%, Val-Class-Acc: {0: '85.87%', 1: '88.11%', 2: '85.42%', 3: '90.98%'}, LR: 0.001000\n",
      "Epoch 170/200, Train Loss: 0.101932, Train-Class-Acc: {0: '99.32%', 1: '98.98%', 2: '99.65%', 3: '99.18%'}\n",
      "Val Loss: 6.991129, Val Acc: 87.38%, Val-Class-Acc: {0: '90.76%', 1: '83.20%', 2: '80.56%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 171/200, Train Loss: 0.157377, Train-Class-Acc: {0: '99.05%', 1: '98.36%', 2: '99.48%', 3: '99.18%'}\n",
      "Val Loss: 6.614409, Val Acc: 87.13%, Val-Class-Acc: {0: '90.22%', 1: '78.69%', 2: '86.11%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 172/200, Train Loss: 0.347333, Train-Class-Acc: {0: '98.37%', 1: '98.46%', 2: '98.61%', 3: '99.08%'}\n",
      "Val Loss: 7.557681, Val Acc: 87.25%, Val-Class-Acc: {0: '89.13%', 1: '79.10%', 2: '81.25%', 3: '97.54%'}, LR: 0.001000\n",
      "Epoch 173/200, Train Loss: 0.144283, Train-Class-Acc: {0: '99.32%', 1: '98.57%', 2: '99.13%', 3: '99.18%'}\n",
      "Val Loss: 5.706416, Val Acc: 87.75%, Val-Class-Acc: {0: '89.13%', 1: '79.51%', 2: '86.81%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 174/200, Train Loss: 0.288654, Train-Class-Acc: {0: '98.37%', 1: '98.36%', 2: '98.96%', 3: '99.59%'}\n",
      "Val Loss: 7.096078, Val Acc: 88.24%, Val-Class-Acc: {0: '87.50%', 1: '82.38%', 2: '90.97%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 175/200, Train Loss: 0.196416, Train-Class-Acc: {0: '98.77%', 1: '98.98%', 2: '99.13%', 3: '99.08%'}\n",
      "Val Loss: 7.176498, Val Acc: 85.78%, Val-Class-Acc: {0: '84.24%', 1: '73.77%', 2: '90.28%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 176/200, Train Loss: 0.137912, Train-Class-Acc: {0: '99.32%', 1: '98.98%', 2: '98.79%', 3: '99.49%'}\n",
      "Val Loss: 7.441650, Val Acc: 87.13%, Val-Class-Acc: {0: '86.96%', 1: '81.15%', 2: '83.33%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 177/200, Train Loss: 0.103620, Train-Class-Acc: {0: '99.05%', 1: '99.59%', 2: '99.48%', 3: '99.80%'}\n",
      "Val Loss: 6.790907, Val Acc: 88.97%, Val-Class-Acc: {0: '90.22%', 1: '81.15%', 2: '90.28%', 3: '95.08%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_92.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_177.pth\n",
      "Epoch 178/200, Train Loss: 0.139062, Train-Class-Acc: {0: '99.46%', 1: '98.87%', 2: '99.13%', 3: '99.59%'}\n",
      "Val Loss: 7.339399, Val Acc: 86.76%, Val-Class-Acc: {0: '87.50%', 1: '82.38%', 2: '81.94%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 179/200, Train Loss: 0.117905, Train-Class-Acc: {0: '99.18%', 1: '98.87%', 2: '99.65%', 3: '99.69%'}\n",
      "Val Loss: 5.856943, Val Acc: 87.99%, Val-Class-Acc: {0: '88.59%', 1: '81.15%', 2: '85.42%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 180/200, Train Loss: 0.079434, Train-Class-Acc: {0: '99.59%', 1: '99.59%', 2: '99.31%', 3: '99.59%'}\n",
      "Val Loss: 6.092575, Val Acc: 88.36%, Val-Class-Acc: {0: '87.50%', 1: '84.02%', 2: '86.11%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 181/200, Train Loss: 0.253208, Train-Class-Acc: {0: '98.91%', 1: '99.18%', 2: '99.48%', 3: '99.49%'}\n",
      "Val Loss: 5.608325, Val Acc: 87.87%, Val-Class-Acc: {0: '82.07%', 1: '86.07%', 2: '89.58%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 182/200, Train Loss: 0.173862, Train-Class-Acc: {0: '99.18%', 1: '98.87%', 2: '99.48%', 3: '99.49%'}\n",
      "Val Loss: 7.063344, Val Acc: 87.75%, Val-Class-Acc: {0: '90.76%', 1: '79.51%', 2: '83.33%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 183/200, Train Loss: 0.123433, Train-Class-Acc: {0: '99.59%', 1: '99.28%', 2: '98.79%', 3: '99.49%'}\n",
      "Val Loss: 6.248309, Val Acc: 87.01%, Val-Class-Acc: {0: '86.96%', 1: '80.33%', 2: '82.64%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 184/200, Train Loss: 0.081388, Train-Class-Acc: {0: '99.32%', 1: '99.49%', 2: '99.48%', 3: '99.39%'}\n",
      "Val Loss: 6.768281, Val Acc: 87.01%, Val-Class-Acc: {0: '90.76%', 1: '77.05%', 2: '81.94%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 185/200, Train Loss: 0.127052, Train-Class-Acc: {0: '98.91%', 1: '98.67%', 2: '99.31%', 3: '99.80%'}\n",
      "Val Loss: 6.359721, Val Acc: 87.99%, Val-Class-Acc: {0: '86.41%', 1: '85.25%', 2: '88.89%', 3: '91.39%'}, LR: 0.001000\n",
      "Epoch 186/200, Train Loss: 0.049895, Train-Class-Acc: {0: '99.05%', 1: '99.28%', 2: '99.48%', 3: '99.90%'}\n",
      "Val Loss: 6.232819, Val Acc: 88.11%, Val-Class-Acc: {0: '89.13%', 1: '83.20%', 2: '84.72%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 187/200, Train Loss: 0.093580, Train-Class-Acc: {0: '99.32%', 1: '99.28%', 2: '99.65%', 3: '99.59%'}\n",
      "Val Loss: 7.175386, Val Acc: 86.52%, Val-Class-Acc: {0: '76.09%', 1: '85.66%', 2: '86.11%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 188/200, Train Loss: 0.270159, Train-Class-Acc: {0: '98.64%', 1: '98.57%', 2: '98.61%', 3: '99.59%'}\n",
      "Val Loss: 7.539555, Val Acc: 87.38%, Val-Class-Acc: {0: '88.04%', 1: '77.05%', 2: '89.58%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 189/200, Train Loss: 0.083574, Train-Class-Acc: {0: '99.59%', 1: '98.87%', 2: '99.31%', 3: '99.49%'}\n",
      "Val Loss: 6.629303, Val Acc: 87.38%, Val-Class-Acc: {0: '83.15%', 1: '85.25%', 2: '85.42%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 190/200, Train Loss: 0.103249, Train-Class-Acc: {0: '99.05%', 1: '99.08%', 2: '99.48%', 3: '99.59%'}\n",
      "Val Loss: 7.361970, Val Acc: 87.75%, Val-Class-Acc: {0: '88.59%', 1: '81.15%', 2: '81.94%', 3: '97.13%'}, LR: 0.001000\n",
      "Epoch 191/200, Train Loss: 0.144669, Train-Class-Acc: {0: '98.91%', 1: '99.08%', 2: '99.13%', 3: '99.69%'}\n",
      "Val Loss: 6.241499, Val Acc: 87.50%, Val-Class-Acc: {0: '86.41%', 1: '82.38%', 2: '83.33%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 192/200, Train Loss: 0.096365, Train-Class-Acc: {0: '99.46%', 1: '99.18%', 2: '99.83%', 3: '99.69%'}\n",
      "Val Loss: 7.343896, Val Acc: 87.25%, Val-Class-Acc: {0: '87.50%', 1: '84.02%', 2: '85.42%', 3: '91.39%'}, LR: 0.001000\n",
      "Epoch 193/200, Train Loss: 0.073487, Train-Class-Acc: {0: '99.59%', 1: '99.39%', 2: '99.83%', 3: '99.80%'}\n",
      "Val Loss: 7.278176, Val Acc: 86.76%, Val-Class-Acc: {0: '84.78%', 1: '80.74%', 2: '84.72%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 194/200, Train Loss: 0.109150, Train-Class-Acc: {0: '99.73%', 1: '99.59%', 2: '99.31%', 3: '99.49%'}\n",
      "Val Loss: 7.177629, Val Acc: 86.15%, Val-Class-Acc: {0: '85.87%', 1: '77.87%', 2: '86.11%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 195/200, Train Loss: 0.076616, Train-Class-Acc: {0: '99.18%', 1: '99.18%', 2: '99.13%', 3: '99.80%'}\n",
      "Val Loss: 7.248072, Val Acc: 87.38%, Val-Class-Acc: {0: '85.87%', 1: '82.38%', 2: '86.11%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 196/200, Train Loss: 0.046220, Train-Class-Acc: {0: '99.32%', 1: '99.49%', 2: '99.65%', 3: '100.00%'}\n",
      "Val Loss: 7.112717, Val Acc: 86.89%, Val-Class-Acc: {0: '83.70%', 1: '79.10%', 2: '90.28%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 197/200, Train Loss: 0.059066, Train-Class-Acc: {0: '99.46%', 1: '99.49%', 2: '99.65%', 3: '99.69%'}\n",
      "Val Loss: 7.532784, Val Acc: 86.89%, Val-Class-Acc: {0: '85.87%', 1: '79.92%', 2: '84.72%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 198/200, Train Loss: 0.123691, Train-Class-Acc: {0: '98.77%', 1: '98.57%', 2: '99.31%', 3: '99.59%'}\n",
      "Val Loss: 6.975246, Val Acc: 86.27%, Val-Class-Acc: {0: '77.17%', 1: '82.38%', 2: '89.58%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 199/200, Train Loss: 0.056024, Train-Class-Acc: {0: '99.59%', 1: '99.80%', 2: '99.31%', 3: '99.80%'}\n",
      "Val Loss: 6.515736, Val Acc: 87.01%, Val-Class-Acc: {0: '84.24%', 1: '82.79%', 2: '86.81%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 200/200, Train Loss: 0.096660, Train-Class-Acc: {0: '99.46%', 1: '99.39%', 2: '99.48%', 3: '99.49%'}\n",
      "Val Loss: 7.775250, Val Acc: 86.64%, Val-Class-Acc: {0: '91.85%', 1: '79.10%', 2: '82.64%', 3: '92.62%'}, LR: 0.001000\n",
      "\n",
      "üèÜ Best model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_best.pth (Val Accuracy: 88.97%)\n",
      "\n",
      "üìå Final model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_final.pth\n",
      "\n",
      "üéØ Top 5 Best Models:\n",
      "Epoch 177, Train Loss: 0.103620, Train-Acc: {0: '99.05%', 1: '99.59%', 2: '99.48%', 3: '99.80%'},\n",
      "Val Loss: 6.790907, Val Acc: 88.97%, Val-Acc: {0: '90.22%', 1: '81.15%', 2: '90.28%', 3: '95.08%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_177.pth\n",
      "Epoch 147, Train Loss: 0.036136, Train-Acc: {0: '99.32%', 1: '99.39%', 2: '99.65%', 3: '99.69%'},\n",
      "Val Loss: 5.980588, Val Acc: 88.97%, Val-Acc: {0: '90.22%', 1: '81.97%', 2: '88.19%', 3: '95.49%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_147.pth\n",
      "Epoch 108, Train Loss: 0.067752, Train-Acc: {0: '99.05%', 1: '99.08%', 2: '99.48%', 3: '99.59%'},\n",
      "Val Loss: 6.007310, Val Acc: 88.60%, Val-Acc: {0: '93.48%', 1: '78.28%', 2: '85.42%', 3: '97.13%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_108.pth\n",
      "Epoch 107, Train Loss: 0.150992, Train-Acc: {0: '98.09%', 1: '98.67%', 2: '98.61%', 3: '99.49%'},\n",
      "Val Loss: 5.305538, Val Acc: 88.60%, Val-Acc: {0: '89.13%', 1: '83.20%', 2: '85.42%', 3: '95.49%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_107.pth\n",
      "Epoch 65, Train Loss: 0.332922, Train-Acc: {0: '97.82%', 1: '97.34%', 2: '97.92%', 3: '98.98%'},\n",
      "Val Loss: 5.250357, Val Acc: 88.36%, Val-Acc: {0: '91.85%', 1: '80.74%', 2: '83.33%', 3: '96.31%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/ResNet18_1D_LoRA_epoch_65.pth\n",
      "\n",
      "üß† Model Summary:\n",
      "Total Parameters: 3,889,796\n",
      "Model Size (float32): 14.84 MB\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "Number of LoRA adapters: 8\n",
      "Number of LoRA groups: 1\n",
      "Total Training Time: 323.71 seconds\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "---\n",
      "### Period 2 (alpha = 0.0, similarity_threshold = 0.99)\n",
      "+ ##### Total training time: 323.71 seconds\n",
      "+ ##### Model: ResNet18_1D_LoRA\n",
      "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2'*\n",
      "+ ##### Best Epoch: 177\n",
      "#### __Val Accuracy: 88.97%__\n",
      "#### __Val-Class-Acc: {0: '90.22%', 1: '81.15%', 2: '90.28%', 3: '95.08%'}__\n",
      "#### __Total Parameters: 3,889,796__\n",
      "#### __Model Size (float32): 14.84 MB__\n",
      "#### __Number of LoRA adapters: 8__\n",
      "#### __Number of LoRA groups: 1__\n",
      "\n",
      "Saved class features to: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/class_features.pkl\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå Period 2: DynEx-CLoRA Training (ECG)\n",
    "# ================================\n",
    "period = 2\n",
    "\n",
    "# ==== Paths ====\n",
    "stop_signal_file = os.path.join(BASE_DIR, \"stop_training.txt\")\n",
    "model_saving_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v3\", f\"Period_{period}\")\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# ==== Load Period 2 Data ====\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{period}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{period}.npy\"))\n",
    "X_val   = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val   = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "# ==== Device ====\n",
    "device = auto_select_cuda_device()\n",
    "\n",
    "# ==== ËºâÂÖ• Period 1 ÁöÑ class features ====\n",
    "prev_folder = os.path.join(BASE_DIR, \"ResNet18_Selection\", \"ResNet18_big_inplane_v1\")\n",
    "class_features_path = os.path.join(prev_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, \"rb\") as f:\n",
    "    class_features_dict = pickle.load(f)\n",
    "print(f\"‚úÖ Loaded class features from: {class_features_path}\")\n",
    "\n",
    "# ==== ËºâÂÖ• Period 1 È†êË®ìÁ∑¥Ê®°Âûã ====\n",
    "prev_model_path = os.path.join(prev_folder, \"ResNet18_big_inplane_1D_best.pth\")\n",
    "checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "\n",
    "input_channels = X_train.shape[2]\n",
    "output_size = len(np.unique(y_train))\n",
    "\n",
    "# ==== Âª∫Á´ã teacher modelÔºàoutput_size Ë¶ÅÊâ£ÊéâÊñ∞È°ûÂà•Êï∏Ôºâ\n",
    "teacher_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size - 2).to(device)\n",
    "teacher_model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "teacher_model.eval()\n",
    "\n",
    "# ==== Âª∫Á´ã student model ====\n",
    "model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size).to(device)\n",
    "\n",
    "# ==== Ê†πÊìö LoRA Êï∏ÈáèÂêåÊ≠• adapter ÁµêÊßã ====\n",
    "num_lora_groups = checkpoint.get(\"num_lora_groups\", 0)\n",
    "print(f\"üîÑ Number of LoRA groups: {num_lora_groups}\")\n",
    "related_labels = checkpoint.get(\"related_labels\", {\"base\": [0, 1]})\n",
    "for _ in range(num_lora_groups):\n",
    "    model.add_lora_adapter()\n",
    "\n",
    "# ==== Ë§áË£Ω shared Ê¨äÈáçÔºàÊéíÈô§ fc / lora_adapterÔºâ====\n",
    "model_dict = model.state_dict()\n",
    "prev_state_dict = checkpoint[\"model_state_dict\"]\n",
    "filtered_dict = {\n",
    "    k: v for k, v in prev_state_dict.items()\n",
    "    if k in model_dict and model_dict[k].shape == v.shape and k not in [\"fc.weight\", \"fc.bias\"]\n",
    "}\n",
    "model.load_state_dict(filtered_dict, strict=False)\n",
    "for k in model_dict:\n",
    "    if k not in filtered_dict:\n",
    "        print(f\"üîç Not loaded: {k}, shape={model_dict[k].shape}\")\n",
    "print(\"‚úÖ Loaded shared weights from Period 1 (excluding FC only)\")\n",
    "\n",
    "# ==== Ë®ìÁ∑¥ÂèÉÊï∏ ====\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "alpha = 0.0 # No distillation\n",
    "similarity_threshold = 0.99\n",
    "stable_classes = [0]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# ==== ÈñãÂßãË®ìÁ∑¥ ====\n",
    "train_with_dynex_clora_ecg(\n",
    "    model=model,\n",
    "    teacher_model=None,\n",
    "    output_size=output_size,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    alpha=alpha,\n",
    "    model_saving_folder=model_saving_folder,\n",
    "    model_name=\"ResNet18_1D_LoRA\",\n",
    "    stop_signal_file=stop_signal_file,\n",
    "    scheduler=scheduler,\n",
    "    period=period,\n",
    "    stable_classes=stable_classes,\n",
    "    similarity_threshold=similarity_threshold,\n",
    "    class_features_dict=class_features_dict,\n",
    "    related_labels=related_labels,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ==== Cleanup ====\n",
    "del X_train, y_train, X_val, y_val\n",
    "del model, teacher_model, checkpoint, model_dict, prev_state_dict, filtered_dict\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad6ab5d",
   "metadata": {},
   "source": [
    "### Period 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1816e981",
   "metadata": {},
   "source": [
    "#### ‚úîÔ∏è v3 no distillation, all freeze, th = 0.85\n",
    "##### Period 3 (alpha = 0.0, similarity_threshold = 0.85)\n",
    "+ ##### Total training time: 507.01 seconds\n",
    "+ ##### Model: ResNet18_1D_LoRA\n",
    "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3'*\n",
    "+ ##### Best Epoch: 97\n",
    "##### __Val Accuracy: 88.68%__\n",
    "##### __Val-Class-Acc: {0: '82.61%', 1: '83.58%', 2: '89.58%', 3: '94.67%', 4: '82.50%', 5: '93.11%'}__\n",
    "##### __Total Parameters: 3,891,846__\n",
    "##### __Model Size (float32): 14.85 MB__\n",
    "##### __Number of LoRA adapters: 8__\n",
    "##### __Number of LoRA groups: 1__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90039f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 2\n",
      "    - Memory Used    : 517 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ Loaded class features from: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/class_features.pkl\n",
      "üîÑ Number of LoRA groups: 1\n",
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n",
      "üîç Not loaded: fc.weight, shape=torch.Size([6, 1024])\n",
      "üîç Not loaded: fc.bias, shape=torch.Size([6])\n",
      "‚úÖ Loaded shared weights from Period 2 (excluding FC & LoRA)\n",
      "\n",
      "üöÄ 'train_with_dynex_clora_ecg' started for Period 3\n",
      "\n",
      "‚úÖ Removed existing folder: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1514991/3593941278.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Data Overview:\n",
      "X_train: torch.Size([5120, 5000, 12]), y_train: torch.Size([5120])\n",
      "X_val: torch.Size([1281, 5000, 12]), y_val: torch.Size([1281])\n",
      "\n",
      "üîé Similarity Analysis:\n",
      "  Similarity threshold: 0.8500\n",
      "  Existing classes: [0, 1, 2, 3]\n",
      "  Current classes: [0, 1, 2, 3, 4, 5]\n",
      "  New classes: [4, 5]\n",
      "\n",
      "üìä Similarity Scores:\n",
      "  New Class 4:\n",
      "    - Existing Class 2: 0.8701\n",
      "    - Existing Class 0: 0.8682\n",
      "    - Existing Class 1: 0.8643\n",
      "    - Existing Class 3: 0.8612\n",
      "  New Class 5:\n",
      "    - Existing Class 1: 0.8901\n",
      "    - Existing Class 3: 0.8689\n",
      "    - Existing Class 2: 0.8598\n",
      "    - Existing Class 0: 0.8348\n",
      "\n",
      "  Average similarity: 0.8567, Std: 0.0213\n",
      "\n",
      "üß© Managing LoRA adapters for 2 new classes...\n",
      "üîÑ New Class 4 is similar to Class 2 (sim=0.8701) ‚Üí Added to adapter '0'\n",
      "üîÑ New Class 4 is similar to Class 0 (sim=0.8682) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 5 is similar to Class 1 (sim=0.8901) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 5 is similar to Class 2 (sim=0.8598) ‚Üí Added to adapter '0'\n",
      "\n",
      "üîç Checking stability of existing classes...\n",
      "  Class 0 similarity with itself: 0.8693\n",
      "  Class 1 similarity with itself: 0.8585\n",
      "  Class 2 similarity with itself: 0.8699\n",
      "  Class 3 similarity with itself: 0.8763\n",
      "\n",
      "üîí Default: Freezing ALL model parameters\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå fc.weight                                          | shape=[6, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[6]\n",
      "\n",
      "üîì Unfreezing selected adapters (by group):\n",
      "  - Adapter Group #0 (all blocks) (classes: [2, 4, 5])\n",
      "  - Base layers (classes: [0, 1, 3, 4, 5])\n",
      "\n",
      "üîç Frozen Parameters: (After Unfreeze specific adapter groups)\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå fc.weight                                          | shape=[6, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[6]\n",
      "\n",
      "üîç Frozen Parameters (After Unfreeze newly added adapter groups):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå fc.weight                                          | shape=[6, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[6]\n",
      "\n",
      "üîç Frozen Parameters (After unfreeze the final layer):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "\n",
      "üìã Related Labels Summary:\n",
      "  - Base network: Classes [0, 1, 3, 4, 5]\n",
      "  - Adapter #0: Classes [2, 4, 5]\n",
      "\n",
      "üîß Trainable Parameter Status:\n",
      "  ‚úÖ layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ fc.weight                                          | shape=[6, 1024]\n",
      "  ‚úÖ fc.bias                                            | shape=[6]\n",
      "trainable_count: 26\n",
      "\n",
      "üìä Parameter Statistics:\n",
      "  - Total parameters: 3,891,846\n",
      "  - Trainable parameters: 2,125,830 (54.62%)\n",
      "  - Frozen parameters: 1,766,016 (45.38%)\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "\n",
      "üß† Parameters currently controlled by the optimizer: (78)\n",
      "  ‚õî conv1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer1.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.0.conv2.weight\n",
      "  ‚õî layer1.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer1.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.conv2.weight\n",
      "  ‚õî layer1.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer2.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.conv2.weight\n",
      "  ‚õî layer2.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer2.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.conv2.weight\n",
      "  ‚õî layer2.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer3.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.conv2.weight\n",
      "  ‚õî layer3.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer3.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.conv2.weight\n",
      "  ‚õî layer3.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer4.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.conv2.weight\n",
      "  ‚õî layer4.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer4.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.conv2.weight\n",
      "  ‚õî layer4.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ fc.weight\n",
      "  ‚úÖ fc.bias\n",
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/200, Train Loss: 6.000931, Train-Class-Acc: {0: '65.40%', 1: '59.84%', 2: '70.54%', 3: '79.61%', 4: '30.38%', 5: '81.69%'}\n",
      "Val Loss: 2.815443, Val Acc: 84.00%, Val-Class-Acc: {0: '77.17%', 1: '79.40%', 2: '87.50%', 3: '91.80%', 4: '75.00%', 5: '86.23%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_1.pth\n",
      "Epoch 2/200, Train Loss: 2.824689, Train-Class-Acc: {0: '77.38%', 1: '73.22%', 2: '82.32%', 3: '89.04%', 4: '64.56%', 5: '86.70%'}\n",
      "Val Loss: 2.246254, Val Acc: 85.17%, Val-Class-Acc: {0: '90.76%', 1: '70.45%', 2: '87.50%', 3: '92.21%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_2.pth\n",
      "Epoch 3/200, Train Loss: 2.137692, Train-Class-Acc: {0: '78.75%', 1: '77.19%', 2: '85.27%', 3: '91.19%', 5: '89.69%', 4: '68.99%'}\n",
      "Val Loss: 2.063400, Val Acc: 86.10%, Val-Class-Acc: {0: '81.52%', 1: '82.39%', 2: '88.89%', 3: '92.21%', 4: '80.00%', 5: '87.43%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_3.pth\n",
      "Epoch 4/200, Train Loss: 1.605079, Train-Class-Acc: {0: '84.74%', 1: '81.08%', 2: '89.25%', 3: '92.93%', 4: '74.05%', 5: '90.43%'}\n",
      "Val Loss: 2.250813, Val Acc: 87.04%, Val-Class-Acc: {0: '86.41%', 1: '78.51%', 2: '86.11%', 3: '91.80%', 4: '85.00%', 5: '93.11%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_4.pth\n",
      "Epoch 5/200, Train Loss: 1.495465, Train-Class-Acc: {0: '84.20%', 1: '82.05%', 2: '91.51%', 3: '93.65%', 4: '78.48%', 5: '91.48%'}\n",
      "Val Loss: 2.555622, Val Acc: 86.81%, Val-Class-Acc: {0: '86.96%', 1: '77.91%', 2: '88.89%', 3: '86.89%', 4: '85.00%', 5: '94.91%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_5.pth\n",
      "Epoch 6/200, Train Loss: 1.272977, Train-Class-Acc: {0: '87.74%', 1: '85.56%', 2: '90.29%', 3: '92.52%', 4: '81.65%', 5: '90.81%'}\n",
      "Val Loss: 1.852787, Val Acc: 86.34%, Val-Class-Acc: {0: '74.46%', 1: '84.48%', 2: '86.81%', 3: '92.62%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_1.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_6.pth\n",
      "Epoch 7/200, Train Loss: 0.901061, Train-Class-Acc: {0: '88.96%', 1: '86.09%', 2: '91.85%', 3: '95.18%', 5: '93.72%', 4: '82.28%'}\n",
      "Val Loss: 1.795207, Val Acc: 86.89%, Val-Class-Acc: {0: '85.87%', 1: '84.18%', 2: '86.81%', 3: '90.57%', 4: '82.50%', 5: '88.02%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_2.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_7.pth\n",
      "Epoch 8/200, Train Loss: 0.699958, Train-Class-Acc: {0: '88.96%', 1: '86.46%', 2: '93.93%', 3: '96.21%', 4: '87.97%', 5: '93.72%'}\n",
      "Val Loss: 2.196494, Val Acc: 86.65%, Val-Class-Acc: {0: '80.43%', 1: '78.51%', 2: '87.50%', 3: '94.26%', 4: '85.00%', 5: '92.51%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_3.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_8.pth\n",
      "Epoch 9/200, Train Loss: 0.754402, Train-Class-Acc: {0: '91.42%', 1: '89.45%', 2: '92.03%', 3: '96.72%', 4: '89.87%', 5: '94.10%'}\n",
      "Val Loss: 1.979461, Val Acc: 87.35%, Val-Class-Acc: {0: '79.89%', 1: '83.58%', 2: '90.28%', 3: '90.98%', 4: '85.00%', 5: '91.62%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_6.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_9.pth\n",
      "Epoch 10/200, Train Loss: 0.715990, Train-Class-Acc: {0: '91.28%', 1: '89.83%', 2: '95.15%', 3: '96.72%', 4: '89.87%', 5: '94.77%'}\n",
      "Val Loss: 2.291164, Val Acc: 87.59%, Val-Class-Acc: {0: '78.80%', 1: '85.07%', 2: '85.42%', 3: '95.08%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_8.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_10.pth\n",
      "Epoch 11/200, Train Loss: 0.719902, Train-Class-Acc: {0: '92.37%', 1: '90.28%', 2: '94.45%', 3: '95.18%', 5: '95.59%', 4: '89.24%'}\n",
      "Val Loss: 1.827232, Val Acc: 88.45%, Val-Class-Acc: {0: '95.11%', 1: '79.70%', 2: '88.19%', 3: '90.57%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_5.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_11.pth\n",
      "Epoch 12/200, Train Loss: 0.513974, Train-Class-Acc: {0: '92.23%', 1: '92.52%', 2: '95.49%', 3: '96.41%', 4: '90.51%', 5: '95.81%'}\n",
      "Val Loss: 2.136860, Val Acc: 87.67%, Val-Class-Acc: {0: '92.93%', 1: '77.31%', 2: '88.89%', 3: '95.49%', 4: '80.00%', 5: '89.82%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_7.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_12.pth\n",
      "Epoch 13/200, Train Loss: 0.474922, Train-Class-Acc: {0: '93.32%', 1: '91.32%', 2: '95.49%', 3: '96.72%', 4: '91.14%', 5: '95.96%'}\n",
      "Val Loss: 1.911204, Val Acc: 87.59%, Val-Class-Acc: {0: '88.04%', 1: '77.61%', 2: '88.19%', 3: '93.44%', 4: '85.00%', 5: '93.11%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_4.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_13.pth\n",
      "Epoch 14/200, Train Loss: 0.355539, Train-Class-Acc: {0: '94.01%', 1: '93.04%', 2: '97.23%', 3: '97.64%', 4: '93.04%', 5: '95.81%'}\n",
      "Val Loss: 2.124520, Val Acc: 87.67%, Val-Class-Acc: {0: '86.96%', 1: '81.79%', 2: '88.89%', 3: '92.62%', 4: '80.00%', 5: '90.72%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_9.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_14.pth\n",
      "Epoch 15/200, Train Loss: 0.276719, Train-Class-Acc: {0: '94.69%', 1: '92.82%', 2: '97.23%', 3: '97.64%', 4: '89.87%', 5: '97.01%'}\n",
      "Val Loss: 2.158952, Val Acc: 87.35%, Val-Class-Acc: {0: '83.70%', 1: '82.09%', 2: '89.58%', 3: '93.85%', 4: '80.00%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 16/200, Train Loss: 0.299252, Train-Class-Acc: {0: '94.96%', 1: '94.69%', 2: '97.23%', 3: '98.26%', 5: '97.31%', 4: '94.30%'}\n",
      "Val Loss: 2.590420, Val Acc: 87.28%, Val-Class-Acc: {0: '85.87%', 1: '80.30%', 2: '88.19%', 3: '92.21%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 17/200, Train Loss: 0.372328, Train-Class-Acc: {0: '94.96%', 1: '93.27%', 2: '95.84%', 3: '98.36%', 4: '93.04%', 5: '96.79%'}\n",
      "Val Loss: 2.808290, Val Acc: 86.89%, Val-Class-Acc: {0: '89.13%', 1: '79.40%', 2: '82.64%', 3: '91.39%', 4: '80.00%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 18/200, Train Loss: 0.343032, Train-Class-Acc: {0: '95.91%', 1: '94.99%', 2: '95.84%', 3: '97.95%', 4: '92.41%', 5: '97.09%'}\n",
      "Val Loss: 2.458633, Val Acc: 87.51%, Val-Class-Acc: {0: '77.72%', 1: '84.48%', 2: '89.58%', 3: '93.44%', 4: '82.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 19/200, Train Loss: 0.254129, Train-Class-Acc: {0: '95.78%', 1: '94.91%', 2: '97.05%', 3: '97.44%', 4: '95.57%', 5: '97.31%'}\n",
      "Val Loss: 2.303306, Val Acc: 87.98%, Val-Class-Acc: {0: '85.33%', 1: '82.69%', 2: '88.19%', 3: '94.26%', 4: '85.00%', 5: '90.42%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_10.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_19.pth\n",
      "Epoch 20/200, Train Loss: 0.246953, Train-Class-Acc: {0: '96.59%', 1: '95.74%', 2: '98.79%', 3: '98.57%', 4: '94.94%', 5: '97.61%'}\n",
      "Val Loss: 2.930883, Val Acc: 86.89%, Val-Class-Acc: {0: '81.52%', 1: '79.40%', 2: '90.28%', 3: '94.67%', 4: '85.00%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 21/200, Train Loss: 0.399125, Train-Class-Acc: {0: '95.50%', 1: '94.47%', 2: '97.23%', 3: '97.95%', 4: '92.41%', 5: '97.38%'}\n",
      "Val Loss: 2.897085, Val Acc: 86.49%, Val-Class-Acc: {0: '73.37%', 1: '87.76%', 2: '88.19%', 3: '93.44%', 4: '85.00%', 5: '86.83%'}, LR: 0.001000\n",
      "Epoch 22/200, Train Loss: 0.281652, Train-Class-Acc: {0: '95.91%', 1: '95.21%', 2: '96.88%', 3: '98.57%', 4: '93.67%', 5: '97.76%'}\n",
      "Val Loss: 2.800768, Val Acc: 88.06%, Val-Class-Acc: {0: '92.39%', 1: '78.51%', 2: '90.97%', 3: '93.03%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_13.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_22.pth\n",
      "Epoch 23/200, Train Loss: 0.228577, Train-Class-Acc: {0: '96.87%', 1: '95.06%', 2: '97.57%', 3: '98.16%', 4: '94.94%', 5: '97.16%'}\n",
      "Val Loss: 2.395069, Val Acc: 87.43%, Val-Class-Acc: {0: '83.70%', 1: '80.60%', 2: '87.50%', 3: '93.44%', 4: '85.00%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 24/200, Train Loss: 0.162307, Train-Class-Acc: {0: '96.32%', 1: '96.19%', 2: '98.44%', 3: '98.67%', 4: '95.57%', 5: '98.21%'}\n",
      "Val Loss: 2.766213, Val Acc: 87.59%, Val-Class-Acc: {0: '87.50%', 1: '79.70%', 2: '90.28%', 3: '93.85%', 4: '85.00%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 25/200, Train Loss: 0.259986, Train-Class-Acc: {0: '96.32%', 1: '96.34%', 2: '97.92%', 3: '98.77%', 4: '97.47%', 5: '97.83%'}\n",
      "Val Loss: 2.593712, Val Acc: 87.12%, Val-Class-Acc: {0: '92.93%', 1: '77.61%', 2: '87.50%', 3: '92.62%', 4: '85.00%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 26/200, Train Loss: 0.353472, Train-Class-Acc: {0: '96.32%', 1: '95.44%', 2: '97.05%', 3: '97.95%', 4: '93.67%', 5: '97.61%'}\n",
      "Val Loss: 2.711248, Val Acc: 87.59%, Val-Class-Acc: {0: '82.61%', 1: '82.39%', 2: '88.89%', 3: '92.62%', 4: '82.50%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 27/200, Train Loss: 0.255431, Train-Class-Acc: {0: '97.14%', 1: '94.99%', 2: '96.88%', 3: '98.16%', 4: '94.30%', 5: '97.91%'}\n",
      "Val Loss: 2.784571, Val Acc: 87.43%, Val-Class-Acc: {0: '87.50%', 1: '78.21%', 2: '90.28%', 3: '94.26%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 28/200, Train Loss: 0.264763, Train-Class-Acc: {0: '96.32%', 1: '95.59%', 2: '97.40%', 3: '97.54%', 5: '97.68%', 4: '93.67%'}\n",
      "Val Loss: 2.862640, Val Acc: 87.35%, Val-Class-Acc: {0: '81.52%', 1: '82.69%', 2: '93.06%', 3: '91.39%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 29/200, Train Loss: 0.206228, Train-Class-Acc: {0: '95.64%', 1: '95.74%', 2: '97.40%', 3: '98.16%', 4: '94.30%', 5: '97.83%'}\n",
      "Val Loss: 2.584085, Val Acc: 88.29%, Val-Class-Acc: {0: '94.57%', 1: '76.72%', 2: '90.97%', 3: '92.62%', 4: '85.00%', 5: '92.51%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_12.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_29.pth\n",
      "Epoch 30/200, Train Loss: 0.170747, Train-Class-Acc: {0: '97.41%', 1: '96.41%', 2: '97.75%', 3: '98.77%', 4: '98.10%', 5: '98.13%'}\n",
      "Val Loss: 2.563787, Val Acc: 87.59%, Val-Class-Acc: {0: '91.30%', 1: '77.61%', 2: '91.67%', 3: '95.49%', 4: '85.00%', 5: '88.32%'}, LR: 0.001000\n",
      "Epoch 31/200, Train Loss: 0.241544, Train-Class-Acc: {0: '96.73%', 1: '96.78%', 2: '98.61%', 3: '98.26%', 5: '98.58%', 4: '96.84%'}\n",
      "Val Loss: 3.273341, Val Acc: 86.65%, Val-Class-Acc: {0: '76.09%', 1: '84.18%', 2: '86.11%', 3: '95.49%', 4: '85.00%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 32/200, Train Loss: 0.401717, Train-Class-Acc: {0: '96.05%', 1: '94.39%', 2: '97.23%', 3: '98.05%', 4: '93.04%', 5: '96.79%'}\n",
      "Val Loss: 2.644498, Val Acc: 87.28%, Val-Class-Acc: {0: '86.96%', 1: '77.91%', 2: '88.89%', 3: '95.08%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 33/200, Train Loss: 0.235549, Train-Class-Acc: {0: '96.46%', 1: '97.01%', 2: '97.05%', 3: '98.46%', 4: '96.20%', 5: '98.58%'}\n",
      "Val Loss: 3.136582, Val Acc: 87.12%, Val-Class-Acc: {0: '82.07%', 1: '81.49%', 2: '88.89%', 3: '90.98%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 34/200, Train Loss: 0.214157, Train-Class-Acc: {0: '96.19%', 1: '95.81%', 2: '97.75%', 3: '99.28%', 4: '97.47%', 5: '98.58%'}\n",
      "Val Loss: 3.004376, Val Acc: 86.81%, Val-Class-Acc: {0: '89.13%', 1: '77.01%', 2: '90.28%', 3: '94.26%', 4: '82.50%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 35/200, Train Loss: 0.113321, Train-Class-Acc: {0: '97.82%', 1: '97.08%', 2: '98.79%', 3: '99.28%', 4: '97.47%', 5: '98.80%'}\n",
      "Val Loss: 3.212018, Val Acc: 87.35%, Val-Class-Acc: {0: '78.26%', 1: '84.48%', 2: '87.50%', 3: '92.62%', 4: '82.50%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 36/200, Train Loss: 0.111141, Train-Class-Acc: {0: '98.09%', 1: '97.31%', 2: '99.13%', 3: '99.59%', 4: '96.84%', 5: '98.88%'}\n",
      "Val Loss: 3.463616, Val Acc: 87.28%, Val-Class-Acc: {0: '74.46%', 1: '86.87%', 2: '84.72%', 3: '92.62%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 37/200, Train Loss: 0.154858, Train-Class-Acc: {0: '98.37%', 1: '97.53%', 2: '98.27%', 3: '98.77%', 4: '96.84%', 5: '98.58%'}\n",
      "Val Loss: 3.143176, Val Acc: 86.73%, Val-Class-Acc: {0: '77.17%', 1: '82.39%', 2: '89.58%', 3: '94.67%', 4: '80.00%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 38/200, Train Loss: 0.125477, Train-Class-Acc: {0: '98.23%', 1: '97.61%', 2: '98.44%', 3: '99.39%', 4: '97.47%', 5: '99.03%'}\n",
      "Val Loss: 3.357906, Val Acc: 87.43%, Val-Class-Acc: {0: '93.48%', 1: '73.43%', 2: '88.19%', 3: '91.80%', 4: '82.50%', 5: '95.21%'}, LR: 0.001000\n",
      "Epoch 39/200, Train Loss: 0.056562, Train-Class-Acc: {0: '98.77%', 1: '98.35%', 2: '98.61%', 3: '99.39%', 4: '98.73%', 5: '99.63%'}\n",
      "Val Loss: 3.253632, Val Acc: 88.06%, Val-Class-Acc: {0: '87.50%', 1: '80.30%', 2: '92.36%', 3: '95.49%', 4: '82.50%', 5: '89.52%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_14.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_39.pth\n",
      "Epoch 40/200, Train Loss: 0.375748, Train-Class-Acc: {0: '97.55%', 1: '96.11%', 2: '96.71%', 3: '97.64%', 4: '93.67%', 5: '97.16%'}\n",
      "Val Loss: 3.464403, Val Acc: 87.12%, Val-Class-Acc: {0: '80.98%', 1: '85.37%', 2: '82.64%', 3: '89.75%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 41/200, Train Loss: 0.176238, Train-Class-Acc: {0: '96.87%', 1: '96.56%', 2: '97.75%', 3: '98.57%', 5: '98.36%', 4: '96.20%'}\n",
      "Val Loss: 3.552697, Val Acc: 87.51%, Val-Class-Acc: {0: '84.78%', 1: '81.19%', 2: '90.28%', 3: '95.08%', 4: '82.50%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 42/200, Train Loss: 0.128997, Train-Class-Acc: {0: '98.64%', 1: '97.16%', 2: '97.57%', 3: '98.77%', 4: '97.47%', 5: '98.80%'}\n",
      "Val Loss: 3.150248, Val Acc: 87.90%, Val-Class-Acc: {0: '85.33%', 1: '82.39%', 2: '86.81%', 3: '93.03%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 43/200, Train Loss: 0.090709, Train-Class-Acc: {0: '98.37%', 1: '97.46%', 2: '98.61%', 3: '98.87%', 5: '98.88%', 4: '96.84%'}\n",
      "Val Loss: 2.978591, Val Acc: 87.04%, Val-Class-Acc: {0: '75.00%', 1: '85.07%', 2: '88.19%', 3: '92.62%', 4: '85.00%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 44/200, Train Loss: 0.078387, Train-Class-Acc: {0: '98.37%', 1: '98.20%', 2: '98.96%', 3: '99.49%', 4: '96.20%', 5: '98.88%'}\n",
      "Val Loss: 2.853389, Val Acc: 87.43%, Val-Class-Acc: {0: '85.33%', 1: '82.39%', 2: '86.11%', 3: '90.98%', 4: '85.00%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 45/200, Train Loss: 0.407701, Train-Class-Acc: {0: '96.59%', 1: '95.66%', 2: '98.61%', 3: '97.34%', 5: '96.86%', 4: '98.10%'}\n",
      "Val Loss: 3.568856, Val Acc: 85.09%, Val-Class-Acc: {0: '65.22%', 1: '84.18%', 2: '90.28%', 3: '93.03%', 4: '82.50%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 46/200, Train Loss: 0.198455, Train-Class-Acc: {0: '97.55%', 1: '96.19%', 2: '97.92%', 3: '98.26%', 4: '95.57%', 5: '98.51%'}\n",
      "Val Loss: 3.332925, Val Acc: 86.57%, Val-Class-Acc: {0: '78.80%', 1: '79.40%', 2: '90.28%', 3: '94.67%', 4: '80.00%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 47/200, Train Loss: 0.106807, Train-Class-Acc: {0: '98.37%', 1: '97.83%', 2: '98.79%', 3: '98.87%', 5: '98.95%', 4: '95.57%'}\n",
      "Val Loss: 3.215805, Val Acc: 87.74%, Val-Class-Acc: {0: '85.33%', 1: '82.99%', 2: '88.89%', 3: '90.98%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 48/200, Train Loss: 0.130627, Train-Class-Acc: {0: '98.09%', 1: '97.83%', 2: '98.44%', 3: '99.28%', 4: '97.47%', 5: '98.80%'}\n",
      "Val Loss: 4.084178, Val Acc: 87.04%, Val-Class-Acc: {0: '82.61%', 1: '80.00%', 2: '88.89%', 3: '93.85%', 4: '82.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 49/200, Train Loss: 0.080012, Train-Class-Acc: {0: '98.09%', 1: '97.91%', 2: '98.61%', 3: '99.49%', 4: '97.47%', 5: '99.33%'}\n",
      "Val Loss: 3.702719, Val Acc: 87.82%, Val-Class-Acc: {0: '91.30%', 1: '80.30%', 2: '90.97%', 3: '90.98%', 4: '80.00%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 50/200, Train Loss: 0.063109, Train-Class-Acc: {0: '98.77%', 1: '98.35%', 2: '99.31%', 3: '99.49%', 4: '97.47%', 5: '99.18%'}\n",
      "Val Loss: 3.213442, Val Acc: 87.59%, Val-Class-Acc: {0: '85.87%', 1: '80.30%', 2: '90.97%', 3: '92.62%', 4: '82.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 51/200, Train Loss: 0.056812, Train-Class-Acc: {0: '98.77%', 1: '98.88%', 2: '98.96%', 3: '99.18%', 4: '98.73%', 5: '99.63%'}\n",
      "Val Loss: 3.340781, Val Acc: 86.96%, Val-Class-Acc: {0: '83.70%', 1: '80.30%', 2: '86.81%', 3: '95.08%', 4: '85.00%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 52/200, Train Loss: 0.094800, Train-Class-Acc: {0: '98.77%', 1: '97.76%', 2: '98.61%', 3: '98.98%', 4: '99.37%', 5: '98.95%'}\n",
      "Val Loss: 3.368125, Val Acc: 87.51%, Val-Class-Acc: {0: '88.59%', 1: '77.31%', 2: '87.50%', 3: '94.26%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 53/200, Train Loss: 0.077296, Train-Class-Acc: {0: '97.68%', 1: '98.06%', 2: '98.79%', 3: '99.69%', 4: '97.47%', 5: '99.10%'}\n",
      "Val Loss: 3.580527, Val Acc: 86.49%, Val-Class-Acc: {0: '70.65%', 1: '83.28%', 2: '90.28%', 3: '94.26%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 54/200, Train Loss: 0.081489, Train-Class-Acc: {0: '98.50%', 1: '98.20%', 2: '98.61%', 3: '99.39%', 4: '97.47%', 5: '99.40%'}\n",
      "Val Loss: 3.539639, Val Acc: 88.37%, Val-Class-Acc: {0: '82.61%', 1: '83.58%', 2: '84.72%', 3: '95.08%', 4: '85.00%', 5: '93.41%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_19.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_54.pth\n",
      "Epoch 55/200, Train Loss: 0.078764, Train-Class-Acc: {0: '98.64%', 1: '97.83%', 2: '99.13%', 3: '99.49%', 4: '98.10%', 5: '99.10%'}\n",
      "Val Loss: 3.341393, Val Acc: 87.98%, Val-Class-Acc: {0: '92.39%', 1: '79.40%', 2: '87.50%', 3: '93.44%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 56/200, Train Loss: 0.077882, Train-Class-Acc: {0: '98.37%', 1: '98.35%', 2: '99.48%', 3: '99.39%', 4: '97.47%', 5: '99.18%'}\n",
      "Val Loss: 3.246910, Val Acc: 88.37%, Val-Class-Acc: {0: '79.35%', 1: '84.18%', 2: '88.89%', 3: '92.21%', 4: '90.00%', 5: '94.31%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_22.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_56.pth\n",
      "Epoch 57/200, Train Loss: 0.082542, Train-Class-Acc: {0: '97.82%', 1: '97.91%', 2: '99.13%', 3: '99.39%', 4: '94.94%', 5: '99.40%'}\n",
      "Val Loss: 3.718921, Val Acc: 87.59%, Val-Class-Acc: {0: '85.87%', 1: '81.19%', 2: '88.19%', 3: '94.26%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 58/200, Train Loss: 0.051218, Train-Class-Acc: {0: '98.50%', 1: '98.35%', 2: '99.13%', 3: '99.90%', 4: '98.73%', 5: '99.48%'}\n",
      "Val Loss: 3.768907, Val Acc: 87.04%, Val-Class-Acc: {0: '85.33%', 1: '81.19%', 2: '88.19%', 3: '93.44%', 4: '82.50%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 59/200, Train Loss: 0.048789, Train-Class-Acc: {0: '98.91%', 1: '98.35%', 2: '99.13%', 3: '99.28%', 5: '99.55%', 4: '98.10%'}\n",
      "Val Loss: 3.498246, Val Acc: 86.96%, Val-Class-Acc: {0: '80.98%', 1: '80.00%', 2: '88.19%', 3: '93.44%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 60/200, Train Loss: 0.053728, Train-Class-Acc: {0: '99.05%', 1: '98.95%', 2: '99.31%', 3: '99.69%', 5: '99.03%', 4: '98.73%'}\n",
      "Val Loss: 3.609336, Val Acc: 86.81%, Val-Class-Acc: {0: '84.78%', 1: '78.21%', 2: '89.58%', 3: '90.57%', 4: '85.00%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 61/200, Train Loss: 0.131578, Train-Class-Acc: {0: '97.82%', 1: '97.76%', 2: '98.61%', 3: '98.98%', 4: '96.84%', 5: '98.88%'}\n",
      "Val Loss: 3.730495, Val Acc: 86.57%, Val-Class-Acc: {0: '80.98%', 1: '79.40%', 2: '86.81%', 3: '94.26%', 4: '85.00%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 62/200, Train Loss: 0.063341, Train-Class-Acc: {0: '98.50%', 1: '98.80%', 2: '98.96%', 3: '99.49%', 4: '98.73%', 5: '99.18%'}\n",
      "Val Loss: 3.891065, Val Acc: 87.04%, Val-Class-Acc: {0: '89.13%', 1: '79.10%', 2: '86.81%', 3: '92.21%', 4: '82.50%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 63/200, Train Loss: 0.213243, Train-Class-Acc: {0: '97.96%', 1: '97.01%', 2: '98.27%', 3: '98.57%', 4: '97.47%', 5: '98.36%'}\n",
      "Val Loss: 3.223723, Val Acc: 87.43%, Val-Class-Acc: {0: '85.33%', 1: '80.00%', 2: '88.89%', 3: '94.26%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 64/200, Train Loss: 0.191451, Train-Class-Acc: {0: '97.14%', 1: '97.53%', 2: '98.09%', 3: '98.67%', 4: '98.10%', 5: '98.88%'}\n",
      "Val Loss: 3.297533, Val Acc: 87.43%, Val-Class-Acc: {0: '88.04%', 1: '78.21%', 2: '90.97%', 3: '94.26%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 65/200, Train Loss: 0.162779, Train-Class-Acc: {0: '97.82%', 1: '97.83%', 2: '97.92%', 3: '99.08%', 4: '99.37%', 5: '99.18%'}\n",
      "Val Loss: 3.465101, Val Acc: 87.12%, Val-Class-Acc: {0: '83.15%', 1: '82.69%', 2: '90.97%', 3: '90.57%', 4: '82.50%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 66/200, Train Loss: 0.287414, Train-Class-Acc: {0: '95.23%', 1: '96.11%', 2: '98.61%', 3: '98.26%', 4: '95.57%', 5: '98.36%'}\n",
      "Val Loss: 3.278013, Val Acc: 86.89%, Val-Class-Acc: {0: '89.67%', 1: '78.81%', 2: '88.89%', 3: '93.85%', 4: '82.50%', 5: '88.02%'}, LR: 0.001000\n",
      "Epoch 67/200, Train Loss: 0.167314, Train-Class-Acc: {0: '98.37%', 1: '97.61%', 2: '98.44%', 3: '98.57%', 4: '97.47%', 5: '98.58%'}\n",
      "Val Loss: 2.917295, Val Acc: 85.71%, Val-Class-Acc: {0: '78.26%', 1: '82.39%', 2: '88.89%', 3: '88.52%', 4: '82.50%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 68/200, Train Loss: 0.174090, Train-Class-Acc: {0: '97.82%', 1: '97.46%', 2: '98.09%', 3: '98.57%', 4: '95.57%', 5: '98.95%'}\n",
      "Val Loss: 4.239233, Val Acc: 86.57%, Val-Class-Acc: {0: '90.76%', 1: '77.01%', 2: '89.58%', 3: '92.21%', 4: '80.00%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 69/200, Train Loss: 0.072584, Train-Class-Acc: {0: '98.64%', 1: '98.20%', 2: '98.96%', 3: '99.39%', 4: '98.73%', 5: '99.25%'}\n",
      "Val Loss: 3.536479, Val Acc: 87.20%, Val-Class-Acc: {0: '86.41%', 1: '80.30%', 2: '90.28%', 3: '92.62%', 4: '80.00%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 70/200, Train Loss: 0.061148, Train-Class-Acc: {0: '99.18%', 1: '98.73%', 2: '99.31%', 3: '99.59%', 5: '99.18%', 4: '96.84%'}\n",
      "Val Loss: 3.921881, Val Acc: 87.59%, Val-Class-Acc: {0: '82.07%', 1: '84.48%', 2: '88.19%', 3: '91.39%', 4: '82.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 71/200, Train Loss: 0.063095, Train-Class-Acc: {0: '98.77%', 1: '98.80%', 2: '99.31%', 3: '99.80%', 5: '99.48%', 4: '95.57%'}\n",
      "Val Loss: 4.118681, Val Acc: 87.67%, Val-Class-Acc: {0: '82.07%', 1: '81.19%', 2: '87.50%', 3: '92.62%', 4: '82.50%', 5: '94.31%'}, LR: 0.001000\n",
      "Epoch 72/200, Train Loss: 0.063317, Train-Class-Acc: {0: '98.64%', 1: '98.35%', 2: '98.96%', 3: '99.39%', 4: '97.47%', 5: '99.48%'}\n",
      "Val Loss: 4.008275, Val Acc: 88.37%, Val-Class-Acc: {0: '84.78%', 1: '82.39%', 2: '87.50%', 3: '94.67%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_39.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_72.pth\n",
      "Epoch 73/200, Train Loss: 0.091287, Train-Class-Acc: {0: '98.64%', 1: '98.13%', 2: '98.96%', 3: '99.39%', 4: '98.73%', 5: '99.40%'}\n",
      "Val Loss: 3.005444, Val Acc: 87.51%, Val-Class-Acc: {0: '81.52%', 1: '83.28%', 2: '88.89%', 3: '90.98%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 74/200, Train Loss: 0.089493, Train-Class-Acc: {0: '98.64%', 1: '98.20%', 2: '98.79%', 3: '99.28%', 4: '99.37%', 5: '99.33%'}\n",
      "Val Loss: 3.300980, Val Acc: 87.98%, Val-Class-Acc: {0: '86.41%', 1: '80.60%', 2: '91.67%', 3: '92.62%', 4: '82.50%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 75/200, Train Loss: 0.089105, Train-Class-Acc: {0: '98.50%', 1: '98.88%', 2: '99.13%', 3: '99.28%', 4: '98.73%', 5: '99.18%'}\n",
      "Val Loss: 2.993602, Val Acc: 87.74%, Val-Class-Acc: {0: '77.17%', 1: '83.58%', 2: '90.28%', 3: '93.85%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 76/200, Train Loss: 0.057992, Train-Class-Acc: {0: '99.46%', 1: '98.73%', 2: '99.13%', 3: '99.59%', 4: '98.73%', 5: '99.25%'}\n",
      "Val Loss: 3.870414, Val Acc: 87.67%, Val-Class-Acc: {0: '88.04%', 1: '80.30%', 2: '84.72%', 3: '91.39%', 4: '82.50%', 5: '94.01%'}, LR: 0.001000\n",
      "Epoch 77/200, Train Loss: 0.052086, Train-Class-Acc: {0: '99.32%', 1: '98.95%', 2: '99.48%', 3: '99.59%', 4: '98.73%', 5: '99.48%'}\n",
      "Val Loss: 3.752074, Val Acc: 87.43%, Val-Class-Acc: {0: '80.98%', 1: '82.99%', 2: '88.19%', 3: '92.21%', 4: '80.00%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 78/200, Train Loss: 0.018945, Train-Class-Acc: {0: '99.32%', 1: '99.55%', 2: '99.83%', 3: '99.69%', 4: '99.37%', 5: '99.85%'}\n",
      "Val Loss: 3.636183, Val Acc: 87.35%, Val-Class-Acc: {0: '87.50%', 1: '80.60%', 2: '87.50%', 3: '94.26%', 4: '82.50%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 79/200, Train Loss: 0.017295, Train-Class-Acc: {0: '99.18%', 1: '99.10%', 2: '99.48%', 3: '99.80%', 4: '100.00%', 5: '99.93%'}\n",
      "Val Loss: 3.739788, Val Acc: 88.21%, Val-Class-Acc: {0: '83.70%', 1: '82.39%', 2: '90.97%', 3: '91.39%', 4: '82.50%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 80/200, Train Loss: 0.028095, Train-Class-Acc: {0: '99.18%', 1: '99.25%', 2: '99.48%', 3: '99.49%', 4: '99.37%', 5: '99.55%'}\n",
      "Val Loss: 3.690403, Val Acc: 87.20%, Val-Class-Acc: {0: '83.70%', 1: '77.91%', 2: '89.58%', 3: '96.31%', 4: '80.00%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 81/200, Train Loss: 0.025964, Train-Class-Acc: {0: '99.46%', 1: '99.25%', 2: '99.65%', 3: '99.80%', 4: '100.00%', 5: '99.55%'}\n",
      "Val Loss: 4.066569, Val Acc: 88.13%, Val-Class-Acc: {0: '91.30%', 1: '79.40%', 2: '91.67%', 3: '91.39%', 4: '80.00%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 82/200, Train Loss: 0.128761, Train-Class-Acc: {0: '98.64%', 1: '98.06%', 2: '97.57%', 3: '99.18%', 4: '97.47%', 5: '99.18%'}\n",
      "Val Loss: 4.307263, Val Acc: 87.43%, Val-Class-Acc: {0: '86.96%', 1: '78.81%', 2: '91.67%', 3: '91.39%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 83/200, Train Loss: 0.211816, Train-Class-Acc: {0: '97.28%', 1: '96.48%', 2: '98.27%', 3: '98.36%', 4: '94.94%', 5: '98.28%'}\n",
      "Val Loss: 4.099188, Val Acc: 86.18%, Val-Class-Acc: {0: '76.09%', 1: '81.49%', 2: '89.58%', 3: '89.34%', 4: '80.00%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 84/200, Train Loss: 0.398524, Train-Class-Acc: {0: '96.05%', 1: '95.89%', 2: '96.71%', 3: '97.44%', 5: '97.38%', 4: '95.57%'}\n",
      "Val Loss: 3.815009, Val Acc: 86.89%, Val-Class-Acc: {0: '80.98%', 1: '82.09%', 2: '91.67%', 3: '91.39%', 4: '82.50%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 85/200, Train Loss: 0.131087, Train-Class-Acc: {0: '98.37%', 1: '97.53%', 2: '97.92%', 3: '98.57%', 4: '96.84%', 5: '98.80%'}\n",
      "Val Loss: 3.600885, Val Acc: 87.20%, Val-Class-Acc: {0: '85.33%', 1: '77.31%', 2: '89.58%', 3: '91.80%', 4: '85.00%', 5: '94.01%'}, LR: 0.001000\n",
      "Epoch 86/200, Train Loss: 0.239696, Train-Class-Acc: {0: '98.37%', 1: '98.35%', 2: '98.61%', 3: '98.16%', 4: '96.84%', 5: '98.51%'}\n",
      "Val Loss: 4.048239, Val Acc: 86.73%, Val-Class-Acc: {0: '86.41%', 1: '75.22%', 2: '88.89%', 3: '90.16%', 4: '82.50%', 5: '95.51%'}, LR: 0.001000\n",
      "Epoch 87/200, Train Loss: 0.201602, Train-Class-Acc: {0: '97.82%', 1: '97.68%', 2: '98.44%', 3: '97.85%', 4: '98.10%', 5: '99.18%'}\n",
      "Val Loss: 4.138655, Val Acc: 86.49%, Val-Class-Acc: {0: '86.96%', 1: '78.21%', 2: '91.67%', 3: '86.48%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 88/200, Train Loss: 0.263524, Train-Class-Acc: {0: '98.23%', 1: '97.61%', 2: '97.92%', 3: '99.08%', 4: '98.73%', 5: '98.65%'}\n",
      "Val Loss: 4.732650, Val Acc: 87.04%, Val-Class-Acc: {0: '83.15%', 1: '85.37%', 2: '85.42%', 3: '88.93%', 4: '82.50%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 89/200, Train Loss: 0.096821, Train-Class-Acc: {0: '98.37%', 1: '98.06%', 2: '99.13%', 3: '99.49%', 4: '98.10%', 5: '98.80%'}\n",
      "Val Loss: 4.116618, Val Acc: 86.96%, Val-Class-Acc: {0: '81.52%', 1: '84.48%', 2: '86.11%', 3: '88.52%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 90/200, Train Loss: 0.080385, Train-Class-Acc: {0: '98.50%', 1: '98.35%', 2: '99.13%', 3: '99.39%', 5: '99.10%', 4: '96.84%'}\n",
      "Val Loss: 3.722790, Val Acc: 86.96%, Val-Class-Acc: {0: '84.24%', 1: '77.91%', 2: '90.97%', 3: '94.26%', 4: '85.00%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 91/200, Train Loss: 0.053876, Train-Class-Acc: {0: '98.91%', 1: '98.50%', 2: '99.31%', 3: '99.69%', 4: '99.37%', 5: '99.48%'}\n",
      "Val Loss: 3.767237, Val Acc: 86.96%, Val-Class-Acc: {0: '83.15%', 1: '80.30%', 2: '88.19%', 3: '90.98%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 92/200, Train Loss: 0.074430, Train-Class-Acc: {0: '98.09%', 1: '98.58%', 2: '99.48%', 3: '99.59%', 4: '98.73%', 5: '99.40%'}\n",
      "Val Loss: 3.511767, Val Acc: 86.57%, Val-Class-Acc: {0: '76.09%', 1: '82.99%', 2: '82.64%', 3: '91.80%', 4: '82.50%', 5: '94.31%'}, LR: 0.001000\n",
      "Epoch 93/200, Train Loss: 0.030831, Train-Class-Acc: {0: '99.18%', 1: '99.33%', 2: '98.96%', 3: '99.49%', 4: '98.10%', 5: '99.70%'}\n",
      "Val Loss: 3.663627, Val Acc: 86.89%, Val-Class-Acc: {0: '90.76%', 1: '74.93%', 2: '89.58%', 3: '92.21%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 94/200, Train Loss: 0.060738, Train-Class-Acc: {0: '98.77%', 1: '98.73%', 2: '99.83%', 3: '99.59%', 4: '97.47%', 5: '99.40%'}\n",
      "Val Loss: 3.398221, Val Acc: 87.04%, Val-Class-Acc: {0: '86.41%', 1: '78.81%', 2: '82.64%', 3: '93.03%', 4: '82.50%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 95/200, Train Loss: 0.061947, Train-Class-Acc: {0: '99.05%', 1: '98.65%', 2: '99.65%', 3: '99.59%', 4: '97.47%', 5: '99.55%'}\n",
      "Val Loss: 3.167985, Val Acc: 87.51%, Val-Class-Acc: {0: '84.78%', 1: '83.28%', 2: '86.81%', 3: '90.57%', 4: '82.50%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 96/200, Train Loss: 0.024384, Train-Class-Acc: {0: '99.18%', 1: '99.10%', 2: '99.48%', 3: '99.59%', 5: '99.78%', 4: '99.37%'}\n",
      "Val Loss: 3.478166, Val Acc: 88.13%, Val-Class-Acc: {0: '80.98%', 1: '83.88%', 2: '89.58%', 3: '91.39%', 4: '82.50%', 5: '94.01%'}, LR: 0.001000\n",
      "Epoch 97/200, Train Loss: 0.035700, Train-Class-Acc: {0: '99.32%', 1: '99.10%', 2: '99.65%', 3: '99.90%', 4: '98.73%', 5: '99.70%'}\n",
      "Val Loss: 3.780299, Val Acc: 88.68%, Val-Class-Acc: {0: '82.61%', 1: '83.58%', 2: '89.58%', 3: '94.67%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_29.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_97.pth\n",
      "Epoch 98/200, Train Loss: 0.021369, Train-Class-Acc: {0: '99.18%', 1: '99.40%', 2: '99.65%', 3: '99.80%', 4: '99.37%', 5: '99.85%'}\n",
      "Val Loss: 3.916077, Val Acc: 88.06%, Val-Class-Acc: {0: '83.15%', 1: '81.49%', 2: '88.89%', 3: '93.44%', 4: '85.00%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 99/200, Train Loss: 0.124283, Train-Class-Acc: {0: '99.18%', 1: '98.35%', 2: '98.61%', 3: '99.28%', 4: '98.73%', 5: '99.10%'}\n",
      "Val Loss: 4.914538, Val Acc: 87.82%, Val-Class-Acc: {0: '86.96%', 1: '78.51%', 2: '85.42%', 3: '93.85%', 4: '82.50%', 5: '94.91%'}, LR: 0.001000\n",
      "Epoch 100/200, Train Loss: 0.065721, Train-Class-Acc: {0: '99.46%', 1: '98.65%', 2: '99.65%', 3: '99.39%', 4: '96.20%', 5: '99.10%'}\n",
      "Val Loss: 4.427477, Val Acc: 87.98%, Val-Class-Acc: {0: '85.33%', 1: '78.51%', 2: '90.97%', 3: '93.44%', 4: '87.50%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 101/200, Train Loss: 0.084546, Train-Class-Acc: {0: '98.64%', 1: '99.10%', 2: '99.48%', 3: '99.59%', 4: '98.73%', 5: '99.55%'}\n",
      "Val Loss: 3.882893, Val Acc: 86.89%, Val-Class-Acc: {0: '83.70%', 1: '80.30%', 2: '90.28%', 3: '91.39%', 4: '85.00%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 102/200, Train Loss: 0.099191, Train-Class-Acc: {0: '98.50%', 1: '98.13%', 2: '99.13%', 3: '98.77%', 4: '98.10%', 5: '99.18%'}\n",
      "Val Loss: 3.791972, Val Acc: 86.89%, Val-Class-Acc: {0: '84.78%', 1: '82.69%', 2: '85.42%', 3: '90.57%', 4: '85.00%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 103/200, Train Loss: 0.087983, Train-Class-Acc: {0: '98.64%', 1: '98.50%', 2: '98.44%', 3: '99.39%', 4: '98.73%', 5: '99.48%'}\n",
      "Val Loss: 3.561608, Val Acc: 86.26%, Val-Class-Acc: {0: '85.87%', 1: '78.51%', 2: '87.50%', 3: '93.03%', 4: '85.00%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 104/200, Train Loss: 0.071763, Train-Class-Acc: {0: '99.18%', 1: '98.50%', 2: '99.13%', 3: '99.39%', 4: '98.73%', 5: '99.10%'}\n",
      "Val Loss: 3.371314, Val Acc: 86.18%, Val-Class-Acc: {0: '75.00%', 1: '84.48%', 2: '85.42%', 3: '91.39%', 4: '85.00%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 105/200, Train Loss: 0.047967, Train-Class-Acc: {0: '98.91%', 1: '98.88%', 2: '98.96%', 3: '99.28%', 4: '98.10%', 5: '99.78%'}\n",
      "Val Loss: 3.727436, Val Acc: 86.34%, Val-Class-Acc: {0: '76.63%', 1: '78.81%', 2: '89.58%', 3: '93.85%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 106/200, Train Loss: 0.026256, Train-Class-Acc: {0: '99.32%', 1: '99.18%', 2: '100.00%', 3: '99.59%', 4: '98.73%', 5: '99.55%'}\n",
      "Val Loss: 3.492639, Val Acc: 86.96%, Val-Class-Acc: {0: '76.63%', 1: '83.88%', 2: '84.03%', 3: '89.75%', 4: '82.50%', 5: '95.51%'}, LR: 0.001000\n",
      "Epoch 107/200, Train Loss: 0.021081, Train-Class-Acc: {0: '99.46%', 1: '99.25%', 2: '99.48%', 3: '99.69%', 4: '98.73%', 5: '99.63%'}\n",
      "Val Loss: 3.711168, Val Acc: 86.89%, Val-Class-Acc: {0: '85.33%', 1: '79.10%', 2: '78.47%', 3: '92.62%', 4: '82.50%', 5: '95.51%'}, LR: 0.001000\n",
      "Epoch 108/200, Train Loss: 0.056992, Train-Class-Acc: {0: '99.46%', 1: '98.88%', 2: '98.79%', 3: '99.49%', 5: '99.40%', 4: '99.37%'}\n",
      "Val Loss: 3.829874, Val Acc: 87.90%, Val-Class-Acc: {0: '84.78%', 1: '80.90%', 2: '90.28%', 3: '92.62%', 4: '80.00%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 109/200, Train Loss: 0.019066, Train-Class-Acc: {0: '99.46%', 1: '99.70%', 2: '99.65%', 3: '99.90%', 4: '99.37%', 5: '99.70%'}\n",
      "Val Loss: 3.369762, Val Acc: 87.35%, Val-Class-Acc: {0: '89.67%', 1: '78.51%', 2: '89.58%', 3: '93.03%', 4: '80.00%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 110/200, Train Loss: 0.200918, Train-Class-Acc: {0: '98.64%', 1: '97.98%', 2: '99.13%', 3: '99.39%', 4: '97.47%', 5: '98.65%'}\n",
      "Val Loss: 5.010737, Val Acc: 85.40%, Val-Class-Acc: {0: '80.98%', 1: '74.63%', 2: '90.97%', 3: '94.67%', 4: '82.50%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 111/200, Train Loss: 0.081012, Train-Class-Acc: {0: '98.37%', 1: '97.91%', 2: '98.79%', 3: '99.18%', 4: '98.10%', 5: '99.33%'}\n",
      "Val Loss: 4.699680, Val Acc: 86.96%, Val-Class-Acc: {0: '76.09%', 1: '82.39%', 2: '91.67%', 3: '90.57%', 4: '80.00%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 112/200, Train Loss: 0.061568, Train-Class-Acc: {0: '98.23%', 1: '98.80%', 2: '98.96%', 3: '99.59%', 4: '98.73%', 5: '99.78%'}\n",
      "Val Loss: 4.134166, Val Acc: 85.32%, Val-Class-Acc: {0: '71.20%', 1: '80.90%', 2: '81.94%', 3: '91.80%', 4: '82.50%', 5: '94.61%'}, LR: 0.001000\n",
      "Epoch 113/200, Train Loss: 0.046127, Train-Class-Acc: {0: '98.64%', 1: '98.95%', 2: '99.13%', 3: '99.80%', 4: '98.73%', 5: '99.70%'}\n",
      "Val Loss: 4.305136, Val Acc: 85.95%, Val-Class-Acc: {0: '78.26%', 1: '83.58%', 2: '86.11%', 3: '92.21%', 4: '82.50%', 5: '88.32%'}, LR: 0.001000\n",
      "Epoch 114/200, Train Loss: 0.044626, Train-Class-Acc: {0: '98.37%', 1: '99.10%', 2: '99.13%', 3: '99.80%', 4: '99.37%', 5: '99.40%'}\n",
      "Val Loss: 3.910485, Val Acc: 86.89%, Val-Class-Acc: {0: '84.24%', 1: '83.28%', 2: '82.64%', 3: '92.21%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 115/200, Train Loss: 0.135560, Train-Class-Acc: {0: '98.23%', 1: '98.88%', 2: '98.44%', 3: '99.39%', 4: '98.10%', 5: '99.48%'}\n",
      "Val Loss: 4.793784, Val Acc: 84.70%, Val-Class-Acc: {0: '62.50%', 1: '86.27%', 2: '88.89%', 3: '93.44%', 4: '82.50%', 5: '87.43%'}, LR: 0.001000\n",
      "Epoch 116/200, Train Loss: 0.233013, Train-Class-Acc: {0: '98.64%', 1: '98.20%', 2: '98.61%', 3: '98.67%', 4: '96.84%', 5: '99.48%'}\n",
      "Val Loss: 4.309613, Val Acc: 85.95%, Val-Class-Acc: {0: '82.07%', 1: '77.61%', 2: '87.50%', 3: '96.31%', 4: '87.50%', 5: '88.02%'}, LR: 0.001000\n",
      "Epoch 117/200, Train Loss: 0.356988, Train-Class-Acc: {0: '97.00%', 1: '96.48%', 2: '98.96%', 3: '97.75%', 4: '96.84%', 5: '97.83%'}\n",
      "Val Loss: 3.123887, Val Acc: 85.71%, Val-Class-Acc: {0: '75.00%', 1: '82.69%', 2: '88.89%', 3: '91.80%', 4: '80.00%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 118/200, Train Loss: 0.132647, Train-Class-Acc: {0: '97.68%', 1: '97.83%', 2: '98.09%', 3: '99.08%', 4: '98.10%', 5: '99.33%'}\n",
      "Val Loss: 3.586010, Val Acc: 86.42%, Val-Class-Acc: {0: '85.33%', 1: '77.01%', 2: '88.19%', 3: '93.85%', 4: '82.50%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 119/200, Train Loss: 0.078596, Train-Class-Acc: {0: '98.77%', 1: '98.65%', 2: '98.61%', 3: '99.49%', 4: '98.73%', 5: '99.33%'}\n",
      "Val Loss: 3.701879, Val Acc: 86.26%, Val-Class-Acc: {0: '85.87%', 1: '74.63%', 2: '86.81%', 3: '93.85%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 120/200, Train Loss: 0.026314, Train-Class-Acc: {0: '99.46%', 1: '99.33%', 2: '99.83%', 3: '99.59%', 4: '99.37%', 5: '99.78%'}\n",
      "Val Loss: 3.616095, Val Acc: 86.49%, Val-Class-Acc: {0: '84.24%', 1: '78.51%', 2: '86.11%', 3: '93.85%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 121/200, Train Loss: 0.028183, Train-Class-Acc: {0: '99.18%', 1: '99.48%', 2: '99.65%', 3: '99.49%', 4: '98.73%', 5: '99.63%'}\n",
      "Val Loss: 3.709125, Val Acc: 86.96%, Val-Class-Acc: {0: '80.43%', 1: '81.49%', 2: '85.42%', 3: '93.85%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 122/200, Train Loss: 0.038756, Train-Class-Acc: {0: '98.91%', 1: '98.73%', 2: '99.48%', 3: '99.80%', 4: '99.37%', 5: '99.40%'}\n",
      "Val Loss: 4.184815, Val Acc: 86.34%, Val-Class-Acc: {0: '80.98%', 1: '82.09%', 2: '85.42%', 3: '92.62%', 4: '82.50%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 123/200, Train Loss: 0.022007, Train-Class-Acc: {0: '99.46%', 1: '99.40%', 2: '99.31%', 3: '99.80%', 4: '98.73%', 5: '99.78%'}\n",
      "Val Loss: 4.028141, Val Acc: 86.34%, Val-Class-Acc: {0: '79.89%', 1: '78.51%', 2: '88.19%', 3: '93.85%', 4: '87.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 124/200, Train Loss: 0.134334, Train-Class-Acc: {0: '98.77%', 1: '98.50%', 2: '99.13%', 3: '99.49%', 4: '98.73%', 5: '99.18%'}\n",
      "Val Loss: 4.529461, Val Acc: 84.86%, Val-Class-Acc: {0: '69.57%', 1: '84.48%', 2: '89.58%', 3: '94.26%', 4: '82.50%', 5: '85.03%'}, LR: 0.001000\n",
      "Epoch 125/200, Train Loss: 0.121700, Train-Class-Acc: {0: '98.77%', 1: '98.73%', 2: '98.79%', 3: '99.28%', 5: '98.95%', 4: '98.73%'}\n",
      "Val Loss: 4.036606, Val Acc: 87.28%, Val-Class-Acc: {0: '79.89%', 1: '82.69%', 2: '88.19%', 3: '92.21%', 4: '80.00%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 126/200, Train Loss: 0.161364, Train-Class-Acc: {0: '98.91%', 1: '98.73%', 2: '99.65%', 3: '99.18%', 4: '96.84%', 5: '98.73%'}\n",
      "Val Loss: 6.058512, Val Acc: 85.56%, Val-Class-Acc: {0: '67.39%', 1: '77.31%', 2: '89.58%', 3: '95.90%', 4: '82.50%', 5: '94.91%'}, LR: 0.001000\n",
      "Epoch 127/200, Train Loss: 0.151817, Train-Class-Acc: {0: '97.55%', 1: '97.16%', 2: '99.31%', 3: '99.18%', 5: '99.18%', 4: '97.47%'}\n",
      "Val Loss: 4.176589, Val Acc: 85.40%, Val-Class-Acc: {0: '73.91%', 1: '80.90%', 2: '84.03%', 3: '91.80%', 4: '85.00%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 128/200, Train Loss: 0.233210, Train-Class-Acc: {0: '98.09%', 1: '98.06%', 2: '98.09%', 3: '98.77%', 4: '99.37%', 5: '98.95%'}\n",
      "Val Loss: 4.253573, Val Acc: 86.18%, Val-Class-Acc: {0: '86.96%', 1: '76.72%', 2: '84.72%', 3: '92.62%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 129/200, Train Loss: 0.068242, Train-Class-Acc: {0: '98.91%', 1: '98.43%', 2: '98.96%', 3: '99.49%', 4: '96.84%', 5: '99.40%'}\n",
      "Val Loss: 3.864411, Val Acc: 86.49%, Val-Class-Acc: {0: '90.22%', 1: '78.81%', 2: '82.64%', 3: '91.80%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 130/200, Train Loss: 0.034364, Train-Class-Acc: {0: '98.91%', 1: '99.03%', 2: '99.31%', 3: '99.80%', 4: '98.73%', 5: '99.63%'}\n",
      "Val Loss: 3.716734, Val Acc: 86.49%, Val-Class-Acc: {0: '79.89%', 1: '81.79%', 2: '82.64%', 3: '92.21%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 131/200, Train Loss: 0.030122, Train-Class-Acc: {0: '99.32%', 1: '99.40%', 2: '99.65%', 3: '99.69%', 4: '100.00%', 5: '99.70%'}\n",
      "Val Loss: 3.706205, Val Acc: 87.12%, Val-Class-Acc: {0: '81.52%', 1: '83.28%', 2: '86.11%', 3: '92.21%', 4: '82.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 132/200, Train Loss: 0.026209, Train-Class-Acc: {0: '99.73%', 1: '99.25%', 2: '99.65%', 3: '99.69%', 5: '99.70%', 4: '99.37%'}\n",
      "Val Loss: 3.603452, Val Acc: 86.03%, Val-Class-Acc: {0: '76.63%', 1: '80.90%', 2: '81.94%', 3: '93.03%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 133/200, Train Loss: 0.044739, Train-Class-Acc: {0: '99.18%', 1: '98.95%', 2: '100.00%', 3: '99.49%', 4: '100.00%', 5: '99.33%'}\n",
      "Val Loss: 3.409177, Val Acc: 86.81%, Val-Class-Acc: {0: '83.70%', 1: '78.81%', 2: '88.19%', 3: '92.21%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 134/200, Train Loss: 0.021800, Train-Class-Acc: {0: '99.59%', 1: '99.63%', 2: '99.65%', 3: '99.59%', 4: '98.73%', 5: '99.93%'}\n",
      "Val Loss: 3.552503, Val Acc: 86.81%, Val-Class-Acc: {0: '80.98%', 1: '82.09%', 2: '85.42%', 3: '89.34%', 4: '82.50%', 5: '94.01%'}, LR: 0.001000\n",
      "Epoch 135/200, Train Loss: 0.099716, Train-Class-Acc: {0: '98.50%', 1: '98.58%', 2: '98.79%', 3: '98.98%', 4: '99.37%', 5: '99.48%'}\n",
      "Val Loss: 3.775273, Val Acc: 87.20%, Val-Class-Acc: {0: '85.33%', 1: '80.30%', 2: '87.50%', 3: '90.57%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 136/200, Train Loss: 0.041062, Train-Class-Acc: {0: '99.18%', 1: '99.18%', 2: '100.00%', 3: '99.69%', 4: '99.37%', 5: '99.78%'}\n",
      "Val Loss: 3.637002, Val Acc: 87.59%, Val-Class-Acc: {0: '83.70%', 1: '83.28%', 2: '84.03%', 3: '91.80%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 137/200, Train Loss: 0.068409, Train-Class-Acc: {0: '99.05%', 1: '99.10%', 2: '98.61%', 3: '99.90%', 4: '100.00%', 5: '99.25%'}\n",
      "Val Loss: 4.401182, Val Acc: 87.04%, Val-Class-Acc: {0: '77.72%', 1: '83.58%', 2: '88.19%', 3: '93.44%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 138/200, Train Loss: 0.017425, Train-Class-Acc: {0: '99.73%', 1: '99.25%', 2: '99.83%', 3: '99.69%', 4: '98.73%', 5: '99.78%'}\n",
      "Val Loss: 4.116535, Val Acc: 86.34%, Val-Class-Acc: {0: '90.22%', 1: '75.22%', 2: '88.19%', 3: '93.85%', 4: '85.00%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 139/200, Train Loss: 0.069010, Train-Class-Acc: {0: '99.32%', 1: '99.25%', 2: '99.48%', 3: '99.49%', 4: '99.37%', 5: '99.18%'}\n",
      "Val Loss: 4.100034, Val Acc: 87.67%, Val-Class-Acc: {0: '92.39%', 1: '76.42%', 2: '88.89%', 3: '91.80%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 140/200, Train Loss: 0.056545, Train-Class-Acc: {0: '99.32%', 1: '98.80%', 2: '98.96%', 3: '99.69%', 4: '100.00%', 5: '99.48%'}\n",
      "Val Loss: 4.254750, Val Acc: 86.26%, Val-Class-Acc: {0: '89.13%', 1: '76.12%', 2: '91.67%', 3: '91.39%', 4: '80.00%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 141/200, Train Loss: 0.025088, Train-Class-Acc: {0: '99.46%', 1: '99.25%', 2: '99.48%', 3: '99.90%', 4: '98.73%', 5: '99.63%'}\n",
      "Val Loss: 4.050599, Val Acc: 87.67%, Val-Class-Acc: {0: '82.61%', 1: '82.09%', 2: '86.81%', 3: '91.39%', 4: '82.50%', 5: '94.31%'}, LR: 0.001000\n",
      "Epoch 142/200, Train Loss: 0.011825, Train-Class-Acc: {0: '99.32%', 1: '99.48%', 2: '99.48%', 3: '99.90%', 4: '100.00%', 5: '99.93%'}\n",
      "Val Loss: 4.026223, Val Acc: 87.20%, Val-Class-Acc: {0: '82.61%', 1: '79.70%', 2: '92.36%', 3: '89.75%', 4: '82.50%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 143/200, Train Loss: 0.058122, Train-Class-Acc: {0: '98.77%', 1: '99.40%', 2: '99.83%', 3: '99.69%', 4: '100.00%', 5: '99.63%'}\n",
      "Val Loss: 4.052127, Val Acc: 86.03%, Val-Class-Acc: {0: '74.46%', 1: '80.30%', 2: '91.67%', 3: '92.62%', 4: '82.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 144/200, Train Loss: 0.050860, Train-Class-Acc: {0: '98.77%', 1: '99.03%', 2: '99.31%', 3: '99.69%', 5: '99.40%', 4: '98.73%'}\n",
      "Val Loss: 3.974143, Val Acc: 86.34%, Val-Class-Acc: {0: '87.50%', 1: '74.33%', 2: '87.50%', 3: '92.21%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 145/200, Train Loss: 0.044273, Train-Class-Acc: {0: '99.59%', 1: '99.33%', 2: '99.31%', 3: '99.39%', 4: '98.73%', 5: '99.78%'}\n",
      "Val Loss: 4.747752, Val Acc: 86.49%, Val-Class-Acc: {0: '74.46%', 1: '81.19%', 2: '89.58%', 3: '92.21%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 146/200, Train Loss: 0.118964, Train-Class-Acc: {0: '98.91%', 1: '98.58%', 2: '99.31%', 3: '99.08%', 4: '99.37%', 5: '99.25%'}\n",
      "Val Loss: 4.000573, Val Acc: 86.57%, Val-Class-Acc: {0: '76.63%', 1: '83.88%', 2: '82.64%', 3: '91.80%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 147/200, Train Loss: 0.058630, Train-Class-Acc: {0: '98.91%', 1: '98.73%', 2: '99.31%', 3: '99.59%', 4: '98.73%', 5: '99.63%'}\n",
      "Val Loss: 3.589358, Val Acc: 86.10%, Val-Class-Acc: {0: '82.61%', 1: '80.60%', 2: '86.81%', 3: '90.16%', 4: '90.00%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 148/200, Train Loss: 0.135211, Train-Class-Acc: {0: '98.50%', 1: '98.50%', 2: '99.48%', 3: '98.67%', 4: '98.10%', 5: '99.33%'}\n",
      "Val Loss: 5.750124, Val Acc: 83.45%, Val-Class-Acc: {0: '75.00%', 1: '81.49%', 2: '87.50%', 3: '77.46%', 4: '85.00%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 149/200, Train Loss: 0.131363, Train-Class-Acc: {0: '97.28%', 1: '98.28%', 2: '98.27%', 3: '98.77%', 4: '96.20%', 5: '99.55%'}\n",
      "Val Loss: 4.362528, Val Acc: 87.04%, Val-Class-Acc: {0: '85.87%', 1: '78.21%', 2: '87.50%', 3: '91.80%', 4: '80.00%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 150/200, Train Loss: 0.076194, Train-Class-Acc: {0: '99.18%', 1: '99.03%', 2: '99.31%', 3: '99.49%', 4: '96.84%', 5: '99.63%'}\n",
      "Val Loss: 3.436659, Val Acc: 86.49%, Val-Class-Acc: {0: '80.98%', 1: '80.90%', 2: '84.72%', 3: '92.62%', 4: '85.00%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 151/200, Train Loss: 0.064960, Train-Class-Acc: {0: '98.91%', 1: '98.58%', 2: '99.13%', 3: '99.49%', 4: '96.20%', 5: '99.33%'}\n",
      "Val Loss: 3.769065, Val Acc: 86.73%, Val-Class-Acc: {0: '86.96%', 1: '78.81%', 2: '86.11%', 3: '93.44%', 4: '87.50%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 152/200, Train Loss: 0.032041, Train-Class-Acc: {0: '99.46%', 1: '99.18%', 2: '99.65%', 3: '99.80%', 4: '99.37%', 5: '99.55%'}\n",
      "Val Loss: 4.592701, Val Acc: 86.65%, Val-Class-Acc: {0: '78.80%', 1: '82.39%', 2: '89.58%', 3: '92.62%', 4: '82.50%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 153/200, Train Loss: 0.034590, Train-Class-Acc: {0: '99.05%', 1: '99.25%', 2: '99.83%', 3: '99.69%', 4: '99.37%', 5: '99.55%'}\n",
      "Val Loss: 4.571808, Val Acc: 86.26%, Val-Class-Acc: {0: '84.24%', 1: '79.10%', 2: '88.89%', 3: '91.39%', 4: '82.50%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 154/200, Train Loss: 0.031551, Train-Class-Acc: {0: '98.91%', 1: '98.95%', 2: '99.65%', 3: '99.90%', 4: '99.37%', 5: '99.78%'}\n",
      "Val Loss: 4.958727, Val Acc: 86.34%, Val-Class-Acc: {0: '78.26%', 1: '82.39%', 2: '82.64%', 3: '92.21%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 155/200, Train Loss: 0.024141, Train-Class-Acc: {0: '99.32%', 1: '99.40%', 2: '99.48%', 3: '99.49%', 4: '99.37%', 5: '99.85%'}\n",
      "Val Loss: 4.383665, Val Acc: 86.26%, Val-Class-Acc: {0: '85.33%', 1: '79.40%', 2: '86.81%', 3: '93.03%', 4: '80.00%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 156/200, Train Loss: 0.031473, Train-Class-Acc: {0: '99.59%', 1: '99.33%', 2: '99.48%', 3: '99.69%', 4: '96.84%', 5: '99.78%'}\n",
      "Val Loss: 4.734879, Val Acc: 86.57%, Val-Class-Acc: {0: '80.98%', 1: '78.51%', 2: '89.58%', 3: '93.44%', 4: '82.50%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 157/200, Train Loss: 0.054470, Train-Class-Acc: {0: '98.91%', 1: '98.80%', 2: '99.13%', 3: '99.59%', 4: '97.47%', 5: '99.48%'}\n",
      "Val Loss: 4.440932, Val Acc: 86.57%, Val-Class-Acc: {0: '76.63%', 1: '79.10%', 2: '88.19%', 3: '93.44%', 4: '82.50%', 5: '94.31%'}, LR: 0.001000\n",
      "Epoch 158/200, Train Loss: 0.099660, Train-Class-Acc: {0: '98.91%', 1: '99.18%', 2: '99.31%', 3: '99.59%', 4: '100.00%', 5: '99.55%'}\n",
      "Val Loss: 5.411955, Val Acc: 85.79%, Val-Class-Acc: {0: '83.15%', 1: '73.13%', 2: '89.58%', 3: '93.03%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 159/200, Train Loss: 0.057852, Train-Class-Acc: {0: '98.91%', 1: '98.95%', 2: '99.48%', 3: '99.08%', 4: '98.73%', 5: '99.70%'}\n",
      "Val Loss: 4.296103, Val Acc: 86.42%, Val-Class-Acc: {0: '86.96%', 1: '73.73%', 2: '90.97%', 3: '92.21%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 160/200, Train Loss: 0.024135, Train-Class-Acc: {0: '99.05%', 1: '99.40%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '99.85%'}\n",
      "Val Loss: 4.094567, Val Acc: 86.65%, Val-Class-Acc: {0: '78.26%', 1: '83.88%', 2: '85.42%', 3: '90.16%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 161/200, Train Loss: 0.017978, Train-Class-Acc: {0: '99.59%', 1: '99.33%', 2: '99.83%', 3: '99.69%', 4: '99.37%', 5: '99.78%'}\n",
      "Val Loss: 4.202456, Val Acc: 87.04%, Val-Class-Acc: {0: '86.41%', 1: '77.91%', 2: '86.81%', 3: '93.85%', 4: '85.00%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 162/200, Train Loss: 0.014762, Train-Class-Acc: {0: '99.86%', 1: '99.85%', 2: '99.31%', 3: '99.69%', 4: '99.37%', 5: '99.85%'}\n",
      "Val Loss: 4.417145, Val Acc: 86.57%, Val-Class-Acc: {0: '84.24%', 1: '76.12%', 2: '88.89%', 3: '93.44%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 163/200, Train Loss: 0.019176, Train-Class-Acc: {0: '99.18%', 1: '99.33%', 2: '99.65%', 3: '100.00%', 4: '98.73%', 5: '99.93%'}\n",
      "Val Loss: 4.145725, Val Acc: 86.49%, Val-Class-Acc: {0: '73.91%', 1: '82.99%', 2: '86.81%', 3: '93.44%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 164/200, Train Loss: 0.072361, Train-Class-Acc: {0: '99.05%', 1: '99.10%', 2: '98.61%', 3: '99.80%', 4: '99.37%', 5: '99.55%'}\n",
      "Val Loss: 5.469487, Val Acc: 85.01%, Val-Class-Acc: {0: '74.46%', 1: '79.40%', 2: '79.86%', 3: '89.34%', 4: '80.00%', 5: '96.11%'}, LR: 0.001000\n",
      "Epoch 165/200, Train Loss: 0.085699, Train-Class-Acc: {0: '98.37%', 1: '98.65%', 2: '98.27%', 3: '99.49%', 4: '98.73%', 5: '99.55%'}\n",
      "Val Loss: 3.796017, Val Acc: 86.03%, Val-Class-Acc: {0: '83.15%', 1: '80.60%', 2: '84.72%', 3: '92.62%', 4: '82.50%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 166/200, Train Loss: 0.059432, Train-Class-Acc: {0: '98.64%', 1: '98.80%', 2: '99.48%', 3: '99.49%', 4: '100.00%', 5: '99.25%'}\n",
      "Val Loss: 4.648497, Val Acc: 86.49%, Val-Class-Acc: {0: '74.46%', 1: '80.60%', 2: '88.19%', 3: '93.03%', 4: '82.50%', 5: '94.01%'}, LR: 0.001000\n",
      "Epoch 167/200, Train Loss: 0.147888, Train-Class-Acc: {0: '99.05%', 1: '99.18%', 2: '99.31%', 3: '99.08%', 4: '97.47%', 5: '99.40%'}\n",
      "Val Loss: 5.502644, Val Acc: 86.10%, Val-Class-Acc: {0: '82.61%', 1: '79.40%', 2: '88.89%', 3: '85.66%', 4: '85.00%', 5: '94.01%'}, LR: 0.001000\n",
      "Epoch 168/200, Train Loss: 0.278933, Train-Class-Acc: {0: '98.50%', 1: '98.28%', 2: '98.96%', 3: '98.46%', 4: '96.84%', 5: '98.88%'}\n",
      "Val Loss: 4.673829, Val Acc: 86.26%, Val-Class-Acc: {0: '80.43%', 1: '82.69%', 2: '87.50%', 3: '89.75%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 169/200, Train Loss: 0.223069, Train-Class-Acc: {0: '97.68%', 1: '97.53%', 2: '98.27%', 3: '98.87%', 4: '96.84%', 5: '98.51%'}\n",
      "Val Loss: 5.390892, Val Acc: 86.81%, Val-Class-Acc: {0: '86.41%', 1: '77.91%', 2: '86.81%', 3: '89.75%', 4: '82.50%', 5: '94.31%'}, LR: 0.001000\n",
      "Epoch 170/200, Train Loss: 0.097637, Train-Class-Acc: {0: '97.82%', 1: '97.98%', 2: '99.31%', 3: '99.39%', 4: '98.73%', 5: '99.03%'}\n",
      "Val Loss: 4.342232, Val Acc: 85.71%, Val-Class-Acc: {0: '80.43%', 1: '76.72%', 2: '84.03%', 3: '93.03%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 171/200, Train Loss: 0.057252, Train-Class-Acc: {0: '99.05%', 1: '98.73%', 2: '98.96%', 3: '99.80%', 4: '98.73%', 5: '99.25%'}\n",
      "Val Loss: 4.546860, Val Acc: 85.71%, Val-Class-Acc: {0: '79.35%', 1: '79.40%', 2: '81.94%', 3: '91.80%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 172/200, Train Loss: 0.153140, Train-Class-Acc: {0: '99.32%', 1: '99.10%', 2: '99.31%', 3: '99.08%', 4: '98.10%', 5: '99.40%'}\n",
      "Val Loss: 4.619758, Val Acc: 85.87%, Val-Class-Acc: {0: '82.61%', 1: '77.01%', 2: '86.11%', 3: '93.03%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 173/200, Train Loss: 0.060197, Train-Class-Acc: {0: '99.18%', 1: '98.65%', 2: '98.61%', 3: '99.69%', 4: '98.10%', 5: '99.33%'}\n",
      "Val Loss: 3.928506, Val Acc: 86.49%, Val-Class-Acc: {0: '80.98%', 1: '79.70%', 2: '88.89%', 3: '92.62%', 4: '82.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 174/200, Train Loss: 0.021480, Train-Class-Acc: {0: '99.86%', 1: '99.63%', 2: '99.83%', 3: '99.80%', 5: '99.78%', 4: '100.00%'}\n",
      "Val Loss: 4.190759, Val Acc: 86.89%, Val-Class-Acc: {0: '79.89%', 1: '82.39%', 2: '84.72%', 3: '92.21%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 175/200, Train Loss: 0.023834, Train-Class-Acc: {0: '99.59%', 1: '99.40%', 2: '99.65%', 3: '99.90%', 4: '100.00%', 5: '99.85%'}\n",
      "Val Loss: 5.693252, Val Acc: 86.65%, Val-Class-Acc: {0: '76.09%', 1: '83.28%', 2: '87.50%', 3: '86.89%', 4: '82.50%', 5: '95.81%'}, LR: 0.001000\n",
      "Epoch 176/200, Train Loss: 0.069540, Train-Class-Acc: {0: '98.77%', 1: '99.18%', 2: '99.48%', 3: '99.59%', 4: '98.10%', 5: '99.55%'}\n",
      "Val Loss: 4.605065, Val Acc: 86.96%, Val-Class-Acc: {0: '81.52%', 1: '81.49%', 2: '84.72%', 3: '90.98%', 4: '82.50%', 5: '94.01%'}, LR: 0.001000\n",
      "Epoch 177/200, Train Loss: 0.026293, Train-Class-Acc: {0: '99.05%', 1: '99.10%', 2: '99.65%', 3: '99.69%', 4: '99.37%', 5: '100.00%'}\n",
      "Val Loss: 4.663507, Val Acc: 87.67%, Val-Class-Acc: {0: '85.87%', 1: '79.10%', 2: '87.50%', 3: '91.39%', 4: '82.50%', 5: '95.21%'}, LR: 0.001000\n",
      "Epoch 178/200, Train Loss: 0.010078, Train-Class-Acc: {0: '99.46%', 1: '99.70%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '99.85%'}\n",
      "Val Loss: 4.638520, Val Acc: 87.74%, Val-Class-Acc: {0: '86.41%', 1: '79.10%', 2: '86.81%', 3: '92.21%', 4: '82.50%', 5: '94.91%'}, LR: 0.001000\n",
      "Epoch 179/200, Train Loss: 0.047335, Train-Class-Acc: {0: '99.46%', 1: '99.40%', 2: '99.65%', 3: '99.49%', 4: '100.00%', 5: '99.78%'}\n",
      "Val Loss: 4.640449, Val Acc: 86.34%, Val-Class-Acc: {0: '75.54%', 1: '79.40%', 2: '90.28%', 3: '92.62%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 180/200, Train Loss: 0.018558, Train-Class-Acc: {0: '99.18%', 1: '99.25%', 2: '99.83%', 3: '99.90%', 4: '99.37%', 5: '99.78%'}\n",
      "Val Loss: 4.387915, Val Acc: 86.73%, Val-Class-Acc: {0: '90.22%', 1: '77.31%', 2: '86.81%', 3: '89.75%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 181/200, Train Loss: 0.034242, Train-Class-Acc: {0: '99.59%', 1: '99.25%', 2: '99.83%', 3: '99.80%', 4: '99.37%', 5: '99.70%'}\n",
      "Val Loss: 4.360206, Val Acc: 87.35%, Val-Class-Acc: {0: '85.33%', 1: '82.69%', 2: '87.50%', 3: '88.52%', 4: '77.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 182/200, Train Loss: 0.009011, Train-Class-Acc: {0: '99.73%', 1: '99.63%', 2: '99.83%', 3: '99.90%', 4: '99.37%', 5: '99.93%'}\n",
      "Val Loss: 4.578261, Val Acc: 86.42%, Val-Class-Acc: {0: '84.78%', 1: '78.81%', 2: '84.72%', 3: '87.70%', 4: '82.50%', 5: '95.21%'}, LR: 0.001000\n",
      "Epoch 183/200, Train Loss: 0.018764, Train-Class-Acc: {0: '99.59%', 1: '99.55%', 2: '99.83%', 3: '99.69%', 5: '99.85%', 4: '99.37%'}\n",
      "Val Loss: 4.581482, Val Acc: 86.03%, Val-Class-Acc: {0: '78.26%', 1: '80.00%', 2: '88.89%', 3: '92.21%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 184/200, Train Loss: 0.015082, Train-Class-Acc: {0: '99.46%', 1: '99.70%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '99.93%'}\n",
      "Val Loss: 4.258976, Val Acc: 87.04%, Val-Class-Acc: {0: '86.96%', 1: '77.01%', 2: '87.50%', 3: '91.39%', 4: '85.00%', 5: '94.01%'}, LR: 0.001000\n",
      "Epoch 185/200, Train Loss: 0.007862, Train-Class-Acc: {0: '99.73%', 1: '99.63%', 2: '100.00%', 3: '99.90%', 4: '100.00%', 5: '99.78%'}\n",
      "Val Loss: 4.374251, Val Acc: 86.81%, Val-Class-Acc: {0: '83.15%', 1: '79.40%', 2: '88.89%', 3: '91.80%', 4: '85.00%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 186/200, Train Loss: 0.010232, Train-Class-Acc: {0: '99.59%', 1: '99.63%', 2: '99.83%', 3: '100.00%', 5: '99.93%', 4: '100.00%'}\n",
      "Val Loss: 4.066497, Val Acc: 86.65%, Val-Class-Acc: {0: '79.89%', 1: '80.00%', 2: '88.19%', 3: '91.39%', 4: '85.00%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 187/200, Train Loss: 0.001381, Train-Class-Acc: {0: '99.86%', 1: '99.85%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '99.93%'}\n",
      "Val Loss: 4.377166, Val Acc: 87.51%, Val-Class-Acc: {0: '85.87%', 1: '80.90%', 2: '86.11%', 3: '90.57%', 4: '85.00%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 188/200, Train Loss: 0.013979, Train-Class-Acc: {0: '99.59%', 1: '99.85%', 2: '99.31%', 3: '99.90%', 4: '100.00%', 5: '99.78%'}\n",
      "Val Loss: 3.994495, Val Acc: 86.81%, Val-Class-Acc: {0: '80.98%', 1: '81.19%', 2: '86.81%', 3: '90.98%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 189/200, Train Loss: 0.028513, Train-Class-Acc: {0: '99.59%', 1: '99.63%', 2: '99.48%', 3: '99.69%', 4: '98.10%', 5: '99.70%'}\n",
      "Val Loss: 4.624367, Val Acc: 86.73%, Val-Class-Acc: {0: '84.78%', 1: '79.10%', 2: '83.33%', 3: '89.75%', 4: '82.50%', 5: '95.21%'}, LR: 0.001000\n",
      "Epoch 190/200, Train Loss: 0.059504, Train-Class-Acc: {0: '99.73%', 1: '99.40%', 2: '99.31%', 3: '99.49%', 4: '99.37%', 5: '99.48%'}\n",
      "Val Loss: 4.207720, Val Acc: 86.10%, Val-Class-Acc: {0: '85.87%', 1: '78.51%', 2: '82.64%', 3: '89.75%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 191/200, Train Loss: 0.048284, Train-Class-Acc: {0: '98.37%', 1: '98.95%', 2: '99.65%', 3: '99.69%', 4: '98.73%', 5: '99.63%'}\n",
      "Val Loss: 3.770878, Val Acc: 86.26%, Val-Class-Acc: {0: '85.33%', 1: '80.90%', 2: '82.64%', 3: '91.80%', 4: '80.00%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 192/200, Train Loss: 0.024036, Train-Class-Acc: {0: '99.05%', 1: '99.55%', 2: '99.48%', 3: '99.90%', 4: '99.37%', 5: '99.40%'}\n",
      "Val Loss: 3.791044, Val Acc: 85.79%, Val-Class-Acc: {0: '75.00%', 1: '82.09%', 2: '84.03%', 3: '89.34%', 4: '82.50%', 5: '94.01%'}, LR: 0.001000\n",
      "Epoch 193/200, Train Loss: 0.177027, Train-Class-Acc: {0: '96.19%', 1: '96.78%', 2: '99.13%', 3: '99.59%', 4: '98.73%', 5: '98.73%'}\n",
      "Val Loss: 5.054566, Val Acc: 85.09%, Val-Class-Acc: {0: '88.59%', 1: '73.43%', 2: '81.94%', 3: '87.30%', 4: '82.50%', 5: '94.91%'}, LR: 0.001000\n",
      "Epoch 194/200, Train Loss: 0.065503, Train-Class-Acc: {0: '98.09%', 1: '98.35%', 2: '99.31%', 3: '99.18%', 4: '99.37%', 5: '99.48%'}\n",
      "Val Loss: 4.045420, Val Acc: 86.65%, Val-Class-Acc: {0: '90.22%', 1: '76.42%', 2: '86.11%', 3: '90.57%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 195/200, Train Loss: 0.057196, Train-Class-Acc: {0: '99.46%', 1: '99.18%', 2: '99.48%', 3: '99.69%', 4: '98.73%', 5: '99.55%'}\n",
      "Val Loss: 4.075286, Val Acc: 87.28%, Val-Class-Acc: {0: '83.70%', 1: '81.79%', 2: '86.81%', 3: '93.44%', 4: '80.00%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 196/200, Train Loss: 0.038413, Train-Class-Acc: {0: '99.05%', 1: '99.10%', 2: '99.48%', 3: '99.80%', 4: '99.37%', 5: '99.63%'}\n",
      "Val Loss: 3.943155, Val Acc: 86.26%, Val-Class-Acc: {0: '75.00%', 1: '84.48%', 2: '83.33%', 3: '92.21%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 197/200, Train Loss: 0.024057, Train-Class-Acc: {0: '99.18%', 1: '99.40%', 2: '99.31%', 3: '99.80%', 4: '98.73%', 5: '99.63%'}\n",
      "Val Loss: 3.641946, Val Acc: 86.34%, Val-Class-Acc: {0: '75.00%', 1: '82.69%', 2: '89.58%', 3: '91.80%', 4: '82.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 198/200, Train Loss: 0.014368, Train-Class-Acc: {0: '99.73%', 1: '99.55%', 2: '100.00%', 3: '99.90%', 4: '99.37%', 5: '99.85%'}\n",
      "Val Loss: 3.673522, Val Acc: 87.04%, Val-Class-Acc: {0: '85.87%', 1: '82.09%', 2: '85.42%', 3: '90.57%', 4: '82.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 199/200, Train Loss: 0.006787, Train-Class-Acc: {0: '99.59%', 1: '99.70%', 2: '99.65%', 3: '99.80%', 4: '99.37%', 5: '100.00%'}\n",
      "Val Loss: 3.698702, Val Acc: 86.73%, Val-Class-Acc: {0: '85.87%', 1: '78.51%', 2: '88.89%', 3: '90.98%', 4: '85.00%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 200/200, Train Loss: 0.007861, Train-Class-Acc: {0: '99.86%', 1: '99.85%', 2: '99.83%', 3: '99.80%', 4: '100.00%', 5: '99.93%'}\n",
      "Val Loss: 3.932620, Val Acc: 86.73%, Val-Class-Acc: {0: '86.96%', 1: '78.21%', 2: '88.89%', 3: '90.98%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "\n",
      "üèÜ Best model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_best.pth (Val Accuracy: 88.68%)\n",
      "\n",
      "üìå Final model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_final.pth\n",
      "\n",
      "üéØ Top 5 Best Models:\n",
      "Epoch 97, Train Loss: 0.035700, Train-Acc: {0: '99.32%', 1: '99.10%', 2: '99.65%', 3: '99.90%', 4: '98.73%', 5: '99.70%'},\n",
      "Val Loss: 3.780299, Val Acc: 88.68%, Val-Acc: {0: '82.61%', 1: '83.58%', 2: '89.58%', 3: '94.67%', 4: '82.50%', 5: '93.11%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_97.pth\n",
      "Epoch 11, Train Loss: 0.719902, Train-Acc: {0: '92.37%', 1: '90.28%', 2: '94.45%', 3: '95.18%', 5: '95.59%', 4: '89.24%'},\n",
      "Val Loss: 1.827232, Val Acc: 88.45%, Val-Acc: {0: '95.11%', 1: '79.70%', 2: '88.19%', 3: '90.57%', 4: '82.50%', 5: '92.81%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_11.pth\n",
      "Epoch 72, Train Loss: 0.063317, Train-Acc: {0: '98.64%', 1: '98.35%', 2: '98.96%', 3: '99.39%', 4: '97.47%', 5: '99.48%'},\n",
      "Val Loss: 4.008275, Val Acc: 88.37%, Val-Acc: {0: '84.78%', 1: '82.39%', 2: '87.50%', 3: '94.67%', 4: '82.50%', 5: '92.81%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_72.pth\n",
      "Epoch 56, Train Loss: 0.077882, Train-Acc: {0: '98.37%', 1: '98.35%', 2: '99.48%', 3: '99.39%', 4: '97.47%', 5: '99.18%'},\n",
      "Val Loss: 3.246910, Val Acc: 88.37%, Val-Acc: {0: '79.35%', 1: '84.18%', 2: '88.89%', 3: '92.21%', 4: '90.00%', 5: '94.31%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_56.pth\n",
      "Epoch 54, Train Loss: 0.081489, Train-Acc: {0: '98.50%', 1: '98.20%', 2: '98.61%', 3: '99.39%', 4: '97.47%', 5: '99.40%'},\n",
      "Val Loss: 3.539639, Val Acc: 88.37%, Val-Acc: {0: '82.61%', 1: '83.58%', 2: '84.72%', 3: '95.08%', 4: '85.00%', 5: '93.41%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_54.pth\n",
      "\n",
      "üß† Model Summary:\n",
      "Total Parameters: 3,891,846\n",
      "Model Size (float32): 14.85 MB\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "Number of LoRA adapters: 8\n",
      "Number of LoRA groups: 1\n",
      "Total Training Time: 507.01 seconds\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "---\n",
      "### Period 3 (alpha = 0.0, similarity_threshold = 0.85)\n",
      "+ ##### Total training time: 507.01 seconds\n",
      "+ ##### Model: ResNet18_1D_LoRA\n",
      "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3'*\n",
      "+ ##### Best Epoch: 97\n",
      "#### __Val Accuracy: 88.68%__\n",
      "#### __Val-Class-Acc: {0: '82.61%', 1: '83.58%', 2: '89.58%', 3: '94.67%', 4: '82.50%', 5: '93.11%'}__\n",
      "#### __Total Parameters: 3,891,846__\n",
      "#### __Model Size (float32): 14.85 MB__\n",
      "#### __Number of LoRA adapters: 8__\n",
      "#### __Number of LoRA groups: 1__\n",
      "\n",
      "Saved class features to: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/class_features.pkl\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå Period 3: DynEx-CLoRA Training (ECG)\n",
    "# ================================\n",
    "period = 3\n",
    "\n",
    "# ==== Paths ====\n",
    "stop_signal_file = os.path.join(BASE_DIR, \"stop_training.txt\")\n",
    "model_saving_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v3\", f\"Period_{period}\")\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# ==== Load Period 3 Data ====\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{period}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{period}.npy\"))\n",
    "X_val   = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val   = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "# ==== Device ====\n",
    "device = auto_select_cuda_device()\n",
    "\n",
    "# ==== Load Period 2 Features ====\n",
    "prev_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v3\", \"Period_2\")\n",
    "class_features_path = os.path.join(prev_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, \"rb\") as f:\n",
    "    class_features_dict = pickle.load(f)\n",
    "print(f\"‚úÖ Loaded class features from: {class_features_path}\")\n",
    "\n",
    "# ==== Load Period 2 Checkpoint ====\n",
    "prev_model_path = os.path.join(prev_folder, \"ResNet18_1D_LoRA_best.pth\")\n",
    "checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "\n",
    "input_channels = X_train.shape[2]\n",
    "output_size = len(np.unique(y_train))\n",
    "\n",
    "# ==== Build Teacher Model ====\n",
    "teacher_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size - 2).to(device)\n",
    "teacher_model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "teacher_model.eval()\n",
    "\n",
    "# ==== Build Student Model ====\n",
    "model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size).to(device)\n",
    "\n",
    "# ==== Sync Adapter Structure ====\n",
    "num_lora_groups = checkpoint.get(\"num_lora_groups\", 0)\n",
    "print(f\"üîÑ Number of LoRA groups: {num_lora_groups}\")\n",
    "related_labels = checkpoint.get(\"related_labels\", {\"base\": [0, 1]})\n",
    "for _ in range(num_lora_groups):\n",
    "    model.add_lora_adapter()\n",
    "\n",
    "# ==== Load Previous Weights (excluding FC and LoRA) ====\n",
    "model_dict = model.state_dict()\n",
    "prev_state_dict = checkpoint[\"model_state_dict\"]\n",
    "filtered_dict = {\n",
    "    k: v for k, v in prev_state_dict.items()\n",
    "    if k in model_dict and model_dict[k].shape == v.shape and k not in [\"fc.weight\", \"fc.bias\"]\n",
    "}\n",
    "model.load_state_dict(filtered_dict, strict=False)\n",
    "for k in model_dict:\n",
    "    if k not in filtered_dict:\n",
    "        print(f\"üîç Not loaded: {k}, shape={model_dict[k].shape}\")\n",
    "print(\"‚úÖ Loaded shared weights from Period 2 (excluding FC only)\")\n",
    "\n",
    "# ==== Training Configuration ====\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "alpha = 0.0 # No distillation\n",
    "similarity_threshold = 0.85\n",
    "stable_classes = [0, 2, 3]  # Ê†πÊìö Period 2 class ÁµêÊûú\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# ==== Start Training ====\n",
    "train_with_dynex_clora_ecg(\n",
    "    model=model,\n",
    "    teacher_model=None,\n",
    "    output_size=output_size,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    alpha=alpha,\n",
    "    model_saving_folder=model_saving_folder,\n",
    "    model_name=\"ResNet18_1D_LoRA\",\n",
    "    stop_signal_file=stop_signal_file,\n",
    "    scheduler=scheduler,\n",
    "    period=period,\n",
    "    stable_classes=stable_classes,\n",
    "    similarity_threshold=similarity_threshold,\n",
    "    class_features_dict=class_features_dict,\n",
    "    related_labels=related_labels,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ==== Cleanup ====\n",
    "del X_train, y_train, X_val, y_val\n",
    "del model, teacher_model, checkpoint, model_dict, prev_state_dict, filtered_dict\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7741385",
   "metadata": {},
   "source": [
    "#### v4 no distillation, all freeze, th = 0.89\n",
    "##### Period 3 (alpha = 0.0, similarity_threshold = 0.89)\n",
    "+ ##### Total training time: 512.69 seconds\n",
    "+ ##### Model: ResNet18_1D_LoRA\n",
    "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3'*\n",
    "+ ##### Best Epoch: 13\n",
    "##### __Val Accuracy: 88.52%__\n",
    "##### __Val-Class-Acc: {0: '87.50%', 1: '83.28%', 2: '87.50%', 3: '92.62%', 4: '82.50%', 5: '92.51%'}__\n",
    "##### __Total Parameters: 3,922,566__\n",
    "##### __Model Size (float32): 14.96 MB__\n",
    "##### __Number of LoRA adapters: 16__\n",
    "##### __Number of LoRA groups: 2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "96256217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 2\n",
      "    - Memory Used    : 589 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ Loaded class features from: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/class_features.pkl\n",
      "üîÑ Number of LoRA groups: 1\n",
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n",
      "üîç Not loaded: fc.weight, shape=torch.Size([6, 1024])\n",
      "üîç Not loaded: fc.bias, shape=torch.Size([6])\n",
      "‚úÖ Loaded shared weights from Period 3 (excluding FC only)\n",
      "\n",
      "üöÄ 'train_with_dynex_clora_ecg' started for Period 3\n",
      "\n",
      "‚úÖ Removed existing folder: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1514991/2716122054.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Data Overview:\n",
      "X_train: torch.Size([5120, 5000, 12]), y_train: torch.Size([5120])\n",
      "X_val: torch.Size([1281, 5000, 12]), y_val: torch.Size([1281])\n",
      "\n",
      "üîé Similarity Analysis:\n",
      "  Similarity threshold: 0.8900\n",
      "  Existing classes: [0, 1, 2, 3]\n",
      "  Current classes: [0, 1, 2, 3, 4, 5]\n",
      "  New classes: [4, 5]\n",
      "\n",
      "üìä Similarity Scores:\n",
      "  New Class 4:\n",
      "    - Existing Class 2: 0.8701\n",
      "    - Existing Class 0: 0.8682\n",
      "    - Existing Class 1: 0.8643\n",
      "    - Existing Class 3: 0.8612\n",
      "  New Class 5:\n",
      "    - Existing Class 1: 0.8901\n",
      "    - Existing Class 3: 0.8689\n",
      "    - Existing Class 2: 0.8598\n",
      "    - Existing Class 0: 0.8348\n",
      "\n",
      "  Average similarity: 0.8567, Std: 0.0213\n",
      "\n",
      "üß© Managing LoRA adapters for 2 new classes...\n",
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n",
      "‚ûï New Class 4 is not similar to any existing class ‚Üí Created new adapter group #1\n",
      "üîÑ New Class 5 is similar to Class 1 (sim=0.8901) ‚Üí Added to adapter 'base'\n",
      "\n",
      "üîç Checking stability of existing classes...\n",
      "  Class 0 similarity with itself: 0.8693\n",
      "‚ö†Ô∏è Class 0 has drifted (self-sim=0.8693) ‚Üí Unfreezing adapter 'base'\n",
      "  Class 1 similarity with itself: 0.8585\n",
      "‚ö†Ô∏è Class 1 has drifted (self-sim=0.8585) ‚Üí Unfreezing adapter 'base'\n",
      "  Class 2 similarity with itself: 0.8699\n",
      "‚ö†Ô∏è Class 2 has drifted (self-sim=0.8699) ‚Üí Unfreezing adapter '0'\n",
      "  Class 3 similarity with itself: 0.8763\n",
      "‚ö†Ô∏è Class 3 has drifted (self-sim=0.8763) ‚Üí Unfreezing adapter 'base'\n",
      "\n",
      "üîí Default: Freezing ALL model parameters\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.0.lora_adapters.1.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.1.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.lora_adapters.1.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.1.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.0.lora_adapters.1.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.1.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.lora_adapters.1.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.1.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.0.lora_adapters.1.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.1.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.lora_adapters.1.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.1.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.0.lora_adapters.1.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.1.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.lora_adapters.1.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.1.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå fc.weight                                          | shape=[6, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[6]\n",
      "\n",
      "üîì Unfreezing selected adapters (by group):\n",
      "  - Adapter Group #0 (all blocks) (classes: [2])\n",
      "  - Base layers (classes: [0, 1, 3, 5])\n",
      "\n",
      "üîç Frozen Parameters: (After Unfreeze specific adapter groups)\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.lora_adapters.1.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.1.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.lora_adapters.1.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.1.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.0.lora_adapters.1.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.1.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.lora_adapters.1.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.1.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.0.lora_adapters.1.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.1.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.lora_adapters.1.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.1.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.0.lora_adapters.1.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.1.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.lora_adapters.1.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.1.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå fc.weight                                          | shape=[6, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[6]\n",
      "  - New adapter group #1 (classes: [4])\n",
      "\n",
      "üîç Frozen Parameters (After Unfreeze newly added adapter groups):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå fc.weight                                          | shape=[6, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[6]\n",
      "\n",
      "üîç Frozen Parameters (After unfreeze the final layer):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "\n",
      "üìã Related Labels Summary:\n",
      "  - Base network: Classes [0, 1, 3, 5]\n",
      "  - Adapter #0: Classes [2]\n",
      "  - Adapter #1: Classes [4]\n",
      "\n",
      "üîß Trainable Parameter Status:\n",
      "  ‚úÖ layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer1.0.lora_adapters.1.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.0.lora_adapters.1.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer1.1.lora_adapters.1.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.1.lora_adapters.1.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer2.0.lora_adapters.1.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.0.lora_adapters.1.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer2.1.lora_adapters.1.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.1.lora_adapters.1.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer3.0.lora_adapters.1.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.0.lora_adapters.1.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer3.1.lora_adapters.1.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.1.lora_adapters.1.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ layer4.0.lora_adapters.1.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.0.lora_adapters.1.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ layer4.1.lora_adapters.1.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.1.lora_adapters.1.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ fc.weight                                          | shape=[6, 1024]\n",
      "  ‚úÖ fc.bias                                            | shape=[6]\n",
      "trainable_count: 42\n",
      "\n",
      "üìä Parameter Statistics:\n",
      "  - Total parameters: 3,922,566\n",
      "  - Trainable parameters: 2,156,550 (54.98%)\n",
      "  - Frozen parameters: 1,766,016 (45.02%)\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "\n",
      "üß† Parameters currently controlled by the optimizer: (94)\n",
      "  ‚õî conv1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer1.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.0.conv2.weight\n",
      "  ‚õî layer1.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer1.0.lora_adapters.1.lora_A\n",
      "  ‚úÖ layer1.0.lora_adapters.1.lora_B\n",
      "  ‚õî layer1.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.conv2.weight\n",
      "  ‚õî layer1.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer1.1.lora_adapters.1.lora_A\n",
      "  ‚úÖ layer1.1.lora_adapters.1.lora_B\n",
      "  ‚õî layer2.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.conv2.weight\n",
      "  ‚õî layer2.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer2.0.lora_adapters.1.lora_A\n",
      "  ‚úÖ layer2.0.lora_adapters.1.lora_B\n",
      "  ‚õî layer2.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.conv2.weight\n",
      "  ‚õî layer2.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer2.1.lora_adapters.1.lora_A\n",
      "  ‚úÖ layer2.1.lora_adapters.1.lora_B\n",
      "  ‚õî layer3.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.conv2.weight\n",
      "  ‚õî layer3.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer3.0.lora_adapters.1.lora_A\n",
      "  ‚úÖ layer3.0.lora_adapters.1.lora_B\n",
      "  ‚õî layer3.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.conv2.weight\n",
      "  ‚õî layer3.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer3.1.lora_adapters.1.lora_A\n",
      "  ‚úÖ layer3.1.lora_adapters.1.lora_B\n",
      "  ‚õî layer4.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.conv2.weight\n",
      "  ‚õî layer4.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer4.0.lora_adapters.1.lora_A\n",
      "  ‚úÖ layer4.0.lora_adapters.1.lora_B\n",
      "  ‚õî layer4.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.conv2.weight\n",
      "  ‚õî layer4.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer4.1.lora_adapters.1.lora_A\n",
      "  ‚úÖ layer4.1.lora_adapters.1.lora_B\n",
      "  ‚úÖ fc.weight\n",
      "  ‚úÖ fc.bias\n",
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/200, Train Loss: 8.528680, Train-Class-Acc: {0: '59.67%', 1: '57.29%', 2: '63.78%', 3: '70.08%', 4: '33.54%', 5: '76.01%'}\n",
      "Val Loss: 2.339197, Val Acc: 84.54%, Val-Class-Acc: {0: '85.33%', 1: '76.42%', 2: '90.97%', 3: '89.34%', 4: '72.50%', 5: '87.43%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_1.pth\n",
      "Epoch 2/200, Train Loss: 3.139760, Train-Class-Acc: {0: '72.34%', 1: '70.31%', 2: '83.88%', 3: '87.60%', 4: '65.19%', 5: '85.35%'}\n",
      "Val Loss: 2.390895, Val Acc: 85.25%, Val-Class-Acc: {0: '82.61%', 1: '81.49%', 2: '86.81%', 3: '89.34%', 4: '80.00%', 5: '87.43%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_2.pth\n",
      "Epoch 3/200, Train Loss: 2.277654, Train-Class-Acc: {0: '77.79%', 1: '77.79%', 2: '85.44%', 3: '89.75%', 4: '68.99%', 5: '87.00%'}\n",
      "Val Loss: 2.384477, Val Acc: 85.25%, Val-Class-Acc: {0: '77.72%', 1: '82.39%', 2: '88.89%', 3: '94.26%', 4: '82.50%', 5: '84.43%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_3.pth\n",
      "Epoch 4/200, Train Loss: 1.704381, Train-Class-Acc: {0: '83.65%', 1: '80.03%', 2: '88.73%', 3: '91.80%', 4: '70.89%', 5: '89.39%'}\n",
      "Val Loss: 2.288966, Val Acc: 87.98%, Val-Class-Acc: {0: '86.96%', 1: '81.79%', 2: '85.42%', 3: '95.90%', 4: '72.50%', 5: '91.92%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_4.pth\n",
      "Epoch 5/200, Train Loss: 1.481951, Train-Class-Acc: {0: '83.79%', 1: '82.80%', 2: '89.08%', 3: '93.34%', 4: '82.28%', 5: '90.73%'}\n",
      "Val Loss: 1.950789, Val Acc: 86.57%, Val-Class-Acc: {0: '92.39%', 1: '78.51%', 2: '88.89%', 3: '88.93%', 4: '80.00%', 5: '89.52%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_5.pth\n",
      "Epoch 6/200, Train Loss: 1.392528, Train-Class-Acc: {0: '86.38%', 1: '83.47%', 2: '92.20%', 3: '93.44%', 4: '81.65%', 5: '91.78%'}\n",
      "Val Loss: 2.106941, Val Acc: 87.90%, Val-Class-Acc: {0: '83.70%', 1: '82.39%', 2: '90.28%', 3: '93.44%', 4: '80.00%', 5: '91.62%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_1.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_6.pth\n",
      "Epoch 7/200, Train Loss: 1.002305, Train-Class-Acc: {0: '87.87%', 1: '87.14%', 2: '91.33%', 3: '94.36%', 4: '84.18%', 5: '92.97%'}\n",
      "Val Loss: 2.475762, Val Acc: 86.42%, Val-Class-Acc: {0: '87.50%', 1: '81.79%', 2: '88.89%', 3: '85.66%', 4: '80.00%', 5: '90.72%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_2.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_7.pth\n",
      "Epoch 8/200, Train Loss: 0.937016, Train-Class-Acc: {0: '88.28%', 1: '86.99%', 2: '92.55%', 3: '94.88%', 4: '84.81%', 5: '93.12%'}\n",
      "Val Loss: 2.160319, Val Acc: 87.59%, Val-Class-Acc: {0: '83.15%', 1: '84.78%', 2: '88.19%', 3: '91.39%', 4: '80.00%', 5: '90.72%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_3.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_8.pth\n",
      "Epoch 9/200, Train Loss: 0.649468, Train-Class-Acc: {0: '91.14%', 1: '89.15%', 2: '95.15%', 3: '96.21%', 4: '91.14%', 5: '94.39%'}\n",
      "Val Loss: 1.871958, Val Acc: 87.82%, Val-Class-Acc: {0: '88.04%', 1: '80.00%', 2: '88.89%', 3: '96.72%', 4: '82.50%', 5: '89.22%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_7.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_9.pth\n",
      "Epoch 10/200, Train Loss: 0.646935, Train-Class-Acc: {0: '92.92%', 1: '91.17%', 2: '93.59%', 3: '95.29%', 4: '87.97%', 5: '94.84%'}\n",
      "Val Loss: 1.969844, Val Acc: 88.45%, Val-Class-Acc: {0: '86.96%', 1: '80.30%', 2: '92.36%', 3: '95.90%', 4: '87.50%', 5: '90.42%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_5.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_10.pth\n",
      "Epoch 11/200, Train Loss: 0.457265, Train-Class-Acc: {0: '91.96%', 1: '91.32%', 2: '96.19%', 3: '97.03%', 4: '87.34%', 5: '95.74%'}\n",
      "Val Loss: 1.993443, Val Acc: 87.35%, Val-Class-Acc: {0: '82.61%', 1: '83.28%', 2: '87.50%', 3: '94.26%', 4: '80.00%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 12/200, Train Loss: 0.584485, Train-Class-Acc: {0: '94.14%', 1: '91.85%', 2: '94.45%', 3: '97.34%', 4: '90.51%', 5: '96.04%'}\n",
      "Val Loss: 2.853748, Val Acc: 87.04%, Val-Class-Acc: {0: '83.15%', 1: '80.30%', 2: '87.50%', 3: '95.08%', 4: '85.00%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 13/200, Train Loss: 0.493073, Train-Class-Acc: {0: '93.32%', 1: '92.00%', 2: '95.84%', 3: '96.62%', 4: '88.61%', 5: '95.52%'}\n",
      "Val Loss: 2.414317, Val Acc: 88.52%, Val-Class-Acc: {0: '87.50%', 1: '83.28%', 2: '87.50%', 3: '92.62%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_8.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_13.pth\n",
      "Epoch 14/200, Train Loss: 0.617321, Train-Class-Acc: {0: '94.01%', 1: '92.82%', 2: '95.84%', 3: '97.03%', 4: '89.87%', 5: '96.49%'}\n",
      "Val Loss: 3.178976, Val Acc: 88.21%, Val-Class-Acc: {0: '86.96%', 1: '83.88%', 2: '88.19%', 3: '90.98%', 4: '82.50%', 5: '91.92%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_9.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_14.pth\n",
      "Epoch 15/200, Train Loss: 0.453971, Train-Class-Acc: {0: '94.14%', 1: '93.12%', 2: '96.19%', 3: '97.64%', 4: '93.04%', 5: '96.41%'}\n",
      "Val Loss: 2.689313, Val Acc: 86.89%, Val-Class-Acc: {0: '86.41%', 1: '80.00%', 2: '81.94%', 3: '90.98%', 4: '80.00%', 5: '94.01%'}, LR: 0.001000\n",
      "Epoch 16/200, Train Loss: 0.429441, Train-Class-Acc: {0: '94.69%', 1: '93.42%', 2: '94.97%', 3: '97.03%', 4: '93.04%', 5: '96.64%'}\n",
      "Val Loss: 2.800077, Val Acc: 87.74%, Val-Class-Acc: {0: '86.41%', 1: '80.30%', 2: '90.97%', 3: '93.03%', 4: '82.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 17/200, Train Loss: 0.621466, Train-Class-Acc: {0: '95.10%', 1: '93.42%', 2: '96.88%', 3: '97.03%', 4: '93.67%', 5: '96.41%'}\n",
      "Val Loss: 2.627391, Val Acc: 87.35%, Val-Class-Acc: {0: '83.15%', 1: '78.81%', 2: '91.67%', 3: '96.72%', 4: '82.50%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 18/200, Train Loss: 0.594225, Train-Class-Acc: {0: '93.32%', 1: '92.97%', 2: '94.80%', 3: '96.62%', 4: '88.61%', 5: '96.19%'}\n",
      "Val Loss: 2.189302, Val Acc: 87.98%, Val-Class-Acc: {0: '88.04%', 1: '82.69%', 2: '86.81%', 3: '93.44%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_6.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_18.pth\n",
      "Epoch 19/200, Train Loss: 0.304866, Train-Class-Acc: {0: '95.64%', 1: '94.24%', 2: '96.88%', 3: '98.36%', 4: '91.14%', 5: '97.38%'}\n",
      "Val Loss: 2.972219, Val Acc: 87.98%, Val-Class-Acc: {0: '89.13%', 1: '81.79%', 2: '84.72%', 3: '92.21%', 4: '85.00%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 20/200, Train Loss: 0.318785, Train-Class-Acc: {0: '94.96%', 1: '94.99%', 2: '96.71%', 3: '97.64%', 4: '93.67%', 5: '98.06%'}\n",
      "Val Loss: 2.582119, Val Acc: 87.74%, Val-Class-Acc: {0: '89.13%', 1: '78.81%', 2: '86.81%', 3: '92.62%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 21/200, Train Loss: 0.207526, Train-Class-Acc: {0: '96.46%', 1: '95.14%', 2: '96.19%', 3: '98.57%', 5: '97.53%', 4: '93.04%'}\n",
      "Val Loss: 2.625103, Val Acc: 87.98%, Val-Class-Acc: {0: '85.87%', 1: '81.19%', 2: '89.58%', 3: '93.44%', 4: '80.00%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 22/200, Train Loss: 0.395053, Train-Class-Acc: {0: '96.19%', 1: '94.99%', 2: '97.57%', 3: '98.16%', 4: '94.94%', 5: '96.49%'}\n",
      "Val Loss: 2.669452, Val Acc: 87.12%, Val-Class-Acc: {0: '90.22%', 1: '79.70%', 2: '90.97%', 3: '90.98%', 4: '82.50%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 23/200, Train Loss: 0.230517, Train-Class-Acc: {0: '95.64%', 1: '95.21%', 2: '97.05%', 3: '98.67%', 4: '93.04%', 5: '97.98%'}\n",
      "Val Loss: 2.643339, Val Acc: 87.28%, Val-Class-Acc: {0: '92.39%', 1: '79.40%', 2: '85.42%', 3: '93.44%', 4: '85.00%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 24/200, Train Loss: 0.173242, Train-Class-Acc: {0: '96.46%', 1: '95.59%', 2: '98.61%', 3: '98.77%', 4: '91.77%', 5: '97.61%'}\n",
      "Val Loss: 2.749226, Val Acc: 88.37%, Val-Class-Acc: {0: '89.13%', 1: '80.60%', 2: '90.97%', 3: '95.08%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_4.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_24.pth\n",
      "Epoch 25/200, Train Loss: 0.253749, Train-Class-Acc: {0: '96.59%', 1: '95.44%', 2: '98.27%', 3: '98.26%', 4: '96.20%', 5: '98.21%'}\n",
      "Val Loss: 3.391153, Val Acc: 87.28%, Val-Class-Acc: {0: '86.96%', 1: '80.00%', 2: '88.89%', 3: '91.39%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 26/200, Train Loss: 0.271731, Train-Class-Acc: {0: '94.41%', 1: '95.36%', 2: '97.75%', 3: '98.36%', 4: '95.57%', 5: '97.83%'}\n",
      "Val Loss: 3.026561, Val Acc: 87.04%, Val-Class-Acc: {0: '91.30%', 1: '76.42%', 2: '88.19%', 3: '95.49%', 4: '80.00%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 27/200, Train Loss: 0.170623, Train-Class-Acc: {0: '96.73%', 1: '95.81%', 2: '98.61%', 3: '98.98%', 4: '93.67%', 5: '97.98%'}\n",
      "Val Loss: 3.166263, Val Acc: 86.65%, Val-Class-Acc: {0: '79.35%', 1: '85.07%', 2: '85.42%', 3: '88.11%', 4: '85.00%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 28/200, Train Loss: 0.193947, Train-Class-Acc: {0: '96.87%', 1: '96.63%', 2: '98.44%', 3: '98.46%', 4: '93.67%', 5: '98.21%'}\n",
      "Val Loss: 2.946401, Val Acc: 87.51%, Val-Class-Acc: {0: '83.70%', 1: '82.69%', 2: '86.81%', 3: '94.67%', 4: '82.50%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 29/200, Train Loss: 0.157011, Train-Class-Acc: {0: '97.41%', 1: '97.16%', 2: '98.44%', 3: '98.67%', 4: '95.57%', 5: '98.06%'}\n",
      "Val Loss: 3.168905, Val Acc: 87.35%, Val-Class-Acc: {0: '89.13%', 1: '76.12%', 2: '86.81%', 3: '91.80%', 4: '85.00%', 5: '94.91%'}, LR: 0.001000\n",
      "Epoch 30/200, Train Loss: 0.287230, Train-Class-Acc: {0: '94.82%', 1: '94.39%', 2: '97.40%', 3: '97.75%', 4: '97.47%', 5: '97.76%'}\n",
      "Val Loss: 2.497272, Val Acc: 87.74%, Val-Class-Acc: {0: '87.50%', 1: '82.99%', 2: '88.19%', 3: '93.85%', 4: '82.50%', 5: '88.62%'}, LR: 0.001000\n",
      "Epoch 31/200, Train Loss: 0.137998, Train-Class-Acc: {0: '97.00%', 1: '96.71%', 2: '98.79%', 3: '98.87%', 4: '94.30%', 5: '98.51%'}\n",
      "Val Loss: 3.576073, Val Acc: 86.49%, Val-Class-Acc: {0: '91.30%', 1: '75.52%', 2: '85.42%', 3: '90.98%', 4: '80.00%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 32/200, Train Loss: 0.162396, Train-Class-Acc: {0: '97.28%', 1: '96.93%', 2: '97.92%', 3: '98.36%', 5: '98.65%', 4: '95.57%'}\n",
      "Val Loss: 2.869842, Val Acc: 86.26%, Val-Class-Acc: {0: '72.28%', 1: '85.97%', 2: '86.81%', 3: '90.16%', 4: '85.00%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 33/200, Train Loss: 0.292474, Train-Class-Acc: {0: '97.41%', 1: '96.78%', 2: '97.57%', 3: '97.34%', 5: '98.21%', 4: '96.20%'}\n",
      "Val Loss: 3.931542, Val Acc: 86.57%, Val-Class-Acc: {0: '90.22%', 1: '77.31%', 2: '86.81%', 3: '86.89%', 4: '82.50%', 5: '94.01%'}, LR: 0.001000\n",
      "Epoch 34/200, Train Loss: 0.225142, Train-Class-Acc: {0: '95.78%', 1: '95.66%', 2: '97.23%', 3: '98.77%', 4: '96.20%', 5: '98.06%'}\n",
      "Val Loss: 3.029113, Val Acc: 86.57%, Val-Class-Acc: {0: '75.54%', 1: '82.99%', 2: '90.28%', 3: '90.98%', 4: '82.50%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 35/200, Train Loss: 0.144902, Train-Class-Acc: {0: '97.41%', 1: '96.78%', 2: '98.61%', 3: '98.98%', 4: '98.10%', 5: '98.65%'}\n",
      "Val Loss: 2.816966, Val Acc: 87.51%, Val-Class-Acc: {0: '90.76%', 1: '81.79%', 2: '86.11%', 3: '92.62%', 4: '85.00%', 5: '88.62%'}, LR: 0.001000\n",
      "Epoch 36/200, Train Loss: 0.078268, Train-Class-Acc: {0: '97.96%', 1: '97.76%', 2: '98.79%', 3: '99.18%', 4: '97.47%', 5: '98.80%'}\n",
      "Val Loss: 2.650809, Val Acc: 88.13%, Val-Class-Acc: {0: '88.04%', 1: '81.19%', 2: '91.67%', 3: '92.62%', 4: '85.00%', 5: '90.72%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_18.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_36.pth\n",
      "Epoch 37/200, Train Loss: 0.101751, Train-Class-Acc: {0: '97.28%', 1: '96.71%', 2: '97.92%', 3: '99.28%', 4: '96.84%', 5: '98.65%'}\n",
      "Val Loss: 3.050251, Val Acc: 87.51%, Val-Class-Acc: {0: '87.50%', 1: '76.72%', 2: '90.28%', 3: '94.67%', 4: '80.00%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 38/200, Train Loss: 0.080440, Train-Class-Acc: {0: '97.55%', 1: '97.83%', 2: '99.31%', 3: '99.08%', 4: '98.10%', 5: '99.40%'}\n",
      "Val Loss: 3.012866, Val Acc: 87.74%, Val-Class-Acc: {0: '91.85%', 1: '79.40%', 2: '86.81%', 3: '93.03%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 39/200, Train Loss: 0.099217, Train-Class-Acc: {0: '98.23%', 1: '97.91%', 2: '97.40%', 3: '99.08%', 4: '97.47%', 5: '98.80%'}\n",
      "Val Loss: 2.926864, Val Acc: 88.21%, Val-Class-Acc: {0: '91.30%', 1: '79.10%', 2: '90.97%', 3: '93.85%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_36.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_39.pth\n",
      "Epoch 40/200, Train Loss: 0.145621, Train-Class-Acc: {0: '98.09%', 1: '97.16%', 2: '98.96%', 3: '98.77%', 5: '98.95%', 4: '98.73%'}\n",
      "Val Loss: 3.219650, Val Acc: 87.59%, Val-Class-Acc: {0: '84.24%', 1: '80.60%', 2: '84.72%', 3: '95.08%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 41/200, Train Loss: 0.124046, Train-Class-Acc: {0: '97.82%', 1: '97.23%', 2: '98.61%', 3: '98.77%', 4: '96.84%', 5: '98.73%'}\n",
      "Val Loss: 2.965786, Val Acc: 87.51%, Val-Class-Acc: {0: '90.76%', 1: '76.12%', 2: '88.89%', 3: '94.26%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 42/200, Train Loss: 0.245904, Train-Class-Acc: {0: '98.09%', 1: '96.78%', 2: '97.05%', 3: '98.57%', 4: '93.67%', 5: '97.61%'}\n",
      "Val Loss: 3.403070, Val Acc: 86.73%, Val-Class-Acc: {0: '92.39%', 1: '79.70%', 2: '86.11%', 3: '88.93%', 4: '82.50%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 43/200, Train Loss: 0.140930, Train-Class-Acc: {0: '97.00%', 1: '97.01%', 2: '98.79%', 3: '98.67%', 4: '95.57%', 5: '98.65%'}\n",
      "Val Loss: 3.355038, Val Acc: 86.42%, Val-Class-Acc: {0: '82.61%', 1: '80.00%', 2: '89.58%', 3: '91.80%', 4: '80.00%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 44/200, Train Loss: 0.209288, Train-Class-Acc: {0: '97.55%', 1: '96.86%', 2: '97.92%', 3: '98.98%', 4: '96.20%', 5: '98.51%'}\n",
      "Val Loss: 3.821189, Val Acc: 86.96%, Val-Class-Acc: {0: '86.41%', 1: '80.00%', 2: '89.58%', 3: '91.80%', 4: '82.50%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 45/200, Train Loss: 0.186092, Train-Class-Acc: {0: '98.77%', 1: '97.68%', 2: '97.92%', 3: '98.77%', 5: '98.51%', 4: '95.57%'}\n",
      "Val Loss: 3.410339, Val Acc: 85.64%, Val-Class-Acc: {0: '73.91%', 1: '80.90%', 2: '90.28%', 3: '94.26%', 4: '85.00%', 5: '88.62%'}, LR: 0.001000\n",
      "Epoch 46/200, Train Loss: 0.173642, Train-Class-Acc: {0: '97.68%', 1: '96.93%', 2: '97.92%', 3: '98.26%', 4: '97.47%', 5: '98.73%'}\n",
      "Val Loss: 3.168024, Val Acc: 86.57%, Val-Class-Acc: {0: '79.89%', 1: '79.10%', 2: '88.89%', 3: '91.80%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 47/200, Train Loss: 0.103862, Train-Class-Acc: {0: '97.82%', 1: '97.91%', 2: '98.96%', 3: '99.18%', 5: '98.95%', 4: '98.10%'}\n",
      "Val Loss: 3.431644, Val Acc: 86.42%, Val-Class-Acc: {0: '83.15%', 1: '82.69%', 2: '85.42%', 3: '93.44%', 4: '82.50%', 5: '87.72%'}, LR: 0.001000\n",
      "Epoch 48/200, Train Loss: 0.082405, Train-Class-Acc: {0: '98.37%', 1: '97.91%', 2: '98.96%', 3: '99.39%', 4: '98.73%', 5: '99.33%'}\n",
      "Val Loss: 3.342321, Val Acc: 87.04%, Val-Class-Acc: {0: '88.04%', 1: '79.70%', 2: '87.50%', 3: '93.03%', 4: '85.00%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 49/200, Train Loss: 0.092976, Train-Class-Acc: {0: '98.09%', 1: '98.06%', 2: '99.13%', 3: '98.98%', 4: '97.47%', 5: '99.10%'}\n",
      "Val Loss: 3.585670, Val Acc: 86.89%, Val-Class-Acc: {0: '77.72%', 1: '80.90%', 2: '88.89%', 3: '92.21%', 4: '85.00%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 50/200, Train Loss: 0.077004, Train-Class-Acc: {0: '98.77%', 1: '98.50%', 2: '99.31%', 3: '98.77%', 4: '96.20%', 5: '98.95%'}\n",
      "Val Loss: 3.471604, Val Acc: 86.42%, Val-Class-Acc: {0: '80.43%', 1: '82.09%', 2: '86.81%', 3: '93.03%', 4: '85.00%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 51/200, Train Loss: 0.063433, Train-Class-Acc: {0: '98.37%', 1: '98.35%', 2: '98.96%', 3: '99.49%', 4: '98.10%', 5: '99.40%'}\n",
      "Val Loss: 3.119896, Val Acc: 86.89%, Val-Class-Acc: {0: '90.22%', 1: '76.42%', 2: '88.89%', 3: '92.62%', 4: '85.00%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 52/200, Train Loss: 0.068598, Train-Class-Acc: {0: '98.50%', 1: '98.73%', 2: '100.00%', 3: '99.49%', 4: '96.20%', 5: '99.03%'}\n",
      "Val Loss: 3.357583, Val Acc: 87.20%, Val-Class-Acc: {0: '84.24%', 1: '80.90%', 2: '87.50%', 3: '94.67%', 4: '85.00%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 53/200, Train Loss: 0.092103, Train-Class-Acc: {0: '97.68%', 1: '98.35%', 2: '98.79%', 3: '99.49%', 4: '97.47%', 5: '99.10%'}\n",
      "Val Loss: 2.920405, Val Acc: 87.90%, Val-Class-Acc: {0: '85.87%', 1: '78.81%', 2: '86.11%', 3: '96.31%', 4: '85.00%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 54/200, Train Loss: 0.081543, Train-Class-Acc: {0: '98.37%', 1: '97.98%', 2: '99.13%', 3: '99.59%', 4: '98.10%', 5: '99.03%'}\n",
      "Val Loss: 4.005616, Val Acc: 87.12%, Val-Class-Acc: {0: '83.15%', 1: '79.10%', 2: '87.50%', 3: '94.26%', 4: '85.00%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 55/200, Train Loss: 0.411297, Train-Class-Acc: {0: '96.05%', 1: '96.04%', 2: '96.53%', 3: '98.36%', 5: '98.06%', 4: '94.30%'}\n",
      "Val Loss: 3.798348, Val Acc: 85.87%, Val-Class-Acc: {0: '89.13%', 1: '75.22%', 2: '88.89%', 3: '94.67%', 4: '77.50%', 5: '88.02%'}, LR: 0.001000\n",
      "Epoch 56/200, Train Loss: 0.322093, Train-Class-Acc: {0: '94.82%', 1: '95.74%', 2: '97.57%', 3: '98.16%', 4: '97.47%', 5: '97.83%'}\n",
      "Val Loss: 3.484497, Val Acc: 86.81%, Val-Class-Acc: {0: '87.50%', 1: '77.31%', 2: '90.28%', 3: '94.67%', 4: '85.00%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 57/200, Train Loss: 0.272909, Train-Class-Acc: {0: '97.68%', 1: '96.71%', 2: '97.92%', 3: '98.67%', 5: '98.13%', 4: '96.84%'}\n",
      "Val Loss: 5.138058, Val Acc: 86.03%, Val-Class-Acc: {0: '83.70%', 1: '74.33%', 2: '88.89%', 3: '93.03%', 4: '85.00%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 58/200, Train Loss: 0.270279, Train-Class-Acc: {0: '96.46%', 1: '96.26%', 2: '98.27%', 3: '98.77%', 4: '97.47%', 5: '97.98%'}\n",
      "Val Loss: 4.228956, Val Acc: 86.34%, Val-Class-Acc: {0: '80.98%', 1: '78.21%', 2: '88.19%', 3: '93.85%', 4: '85.00%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 59/200, Train Loss: 0.143193, Train-Class-Acc: {0: '98.09%', 1: '97.23%', 2: '98.96%', 3: '98.36%', 5: '98.65%', 4: '97.47%'}\n",
      "Val Loss: 3.597527, Val Acc: 87.67%, Val-Class-Acc: {0: '84.78%', 1: '80.90%', 2: '89.58%', 3: '91.39%', 4: '85.00%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 60/200, Train Loss: 0.093252, Train-Class-Acc: {0: '99.18%', 1: '98.28%', 2: '98.44%', 3: '99.18%', 4: '95.57%', 5: '99.33%'}\n",
      "Val Loss: 3.623569, Val Acc: 87.43%, Val-Class-Acc: {0: '76.09%', 1: '82.69%', 2: '88.89%', 3: '94.26%', 4: '80.00%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 61/200, Train Loss: 0.046633, Train-Class-Acc: {0: '98.50%', 1: '98.88%', 2: '99.48%', 3: '99.69%', 4: '98.73%', 5: '99.48%'}\n",
      "Val Loss: 3.427457, Val Acc: 86.57%, Val-Class-Acc: {0: '83.15%', 1: '80.00%', 2: '86.11%', 3: '94.67%', 4: '82.50%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 62/200, Train Loss: 0.042915, Train-Class-Acc: {0: '98.91%', 1: '98.58%', 2: '99.13%', 3: '99.69%', 4: '98.73%', 5: '99.40%'}\n",
      "Val Loss: 3.855375, Val Acc: 85.95%, Val-Class-Acc: {0: '75.54%', 1: '83.88%', 2: '90.28%', 3: '92.62%', 4: '82.50%', 5: '87.43%'}, LR: 0.001000\n",
      "Epoch 63/200, Train Loss: 0.096467, Train-Class-Acc: {0: '98.09%', 1: '98.65%', 2: '99.31%', 3: '99.18%', 4: '96.84%', 5: '99.18%'}\n",
      "Val Loss: 3.476970, Val Acc: 87.04%, Val-Class-Acc: {0: '89.13%', 1: '79.10%', 2: '87.50%', 3: '94.67%', 4: '85.00%', 5: '88.32%'}, LR: 0.001000\n",
      "Epoch 64/200, Train Loss: 0.097632, Train-Class-Acc: {0: '97.82%', 1: '97.98%', 2: '98.61%', 3: '98.87%', 4: '96.84%', 5: '99.03%'}\n",
      "Val Loss: 3.968694, Val Acc: 86.18%, Val-Class-Acc: {0: '83.70%', 1: '78.51%', 2: '86.81%', 3: '92.62%', 4: '85.00%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 65/200, Train Loss: 0.090576, Train-Class-Acc: {0: '98.77%', 1: '98.20%', 2: '98.61%', 3: '99.49%', 4: '98.73%', 5: '99.48%'}\n",
      "Val Loss: 3.872728, Val Acc: 85.48%, Val-Class-Acc: {0: '69.57%', 1: '84.18%', 2: '87.50%', 3: '93.03%', 4: '80.00%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 66/200, Train Loss: 0.139060, Train-Class-Acc: {0: '98.50%', 1: '98.20%', 2: '98.44%', 3: '98.98%', 4: '97.47%', 5: '98.95%'}\n",
      "Val Loss: 3.112369, Val Acc: 86.96%, Val-Class-Acc: {0: '84.78%', 1: '82.69%', 2: '86.11%', 3: '90.16%', 4: '85.00%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 67/200, Train Loss: 0.067635, Train-Class-Acc: {0: '98.91%', 1: '98.73%', 2: '98.61%', 3: '99.28%', 4: '99.37%', 5: '99.18%'}\n",
      "Val Loss: 3.716172, Val Acc: 85.95%, Val-Class-Acc: {0: '86.96%', 1: '77.31%', 2: '85.42%', 3: '91.80%', 4: '85.00%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 68/200, Train Loss: 0.094067, Train-Class-Acc: {0: '98.50%', 1: '97.98%', 2: '98.44%', 3: '99.59%', 5: '99.40%', 4: '95.57%'}\n",
      "Val Loss: 3.560255, Val Acc: 86.34%, Val-Class-Acc: {0: '88.59%', 1: '79.10%', 2: '85.42%', 3: '91.39%', 4: '85.00%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 69/200, Train Loss: 0.087946, Train-Class-Acc: {0: '98.37%', 1: '98.50%', 2: '99.13%', 3: '99.28%', 4: '98.10%', 5: '99.10%'}\n",
      "Val Loss: 3.514721, Val Acc: 86.10%, Val-Class-Acc: {0: '80.98%', 1: '82.39%', 2: '86.81%', 3: '90.98%', 4: '82.50%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 70/200, Train Loss: 0.079629, Train-Class-Acc: {0: '98.64%', 1: '98.13%', 2: '98.44%', 3: '98.77%', 4: '98.10%', 5: '99.63%'}\n",
      "Val Loss: 3.548819, Val Acc: 87.43%, Val-Class-Acc: {0: '83.15%', 1: '81.79%', 2: '87.50%', 3: '93.44%', 4: '85.00%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 71/200, Train Loss: 0.127272, Train-Class-Acc: {0: '98.77%', 1: '98.58%', 2: '98.96%', 3: '99.39%', 4: '98.10%', 5: '98.88%'}\n",
      "Val Loss: 4.207560, Val Acc: 87.51%, Val-Class-Acc: {0: '84.78%', 1: '80.90%', 2: '89.58%', 3: '93.44%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 72/200, Train Loss: 0.095663, Train-Class-Acc: {0: '98.23%', 1: '98.06%', 2: '98.61%', 3: '99.08%', 5: '98.95%', 4: '98.10%'}\n",
      "Val Loss: 3.676099, Val Acc: 85.64%, Val-Class-Acc: {0: '84.78%', 1: '81.49%', 2: '86.11%', 3: '90.16%', 4: '82.50%', 5: '87.13%'}, LR: 0.001000\n",
      "Epoch 73/200, Train Loss: 0.120491, Train-Class-Acc: {0: '98.50%', 1: '97.83%', 2: '98.79%', 3: '99.49%', 5: '98.95%', 4: '98.10%'}\n",
      "Val Loss: 4.505872, Val Acc: 86.10%, Val-Class-Acc: {0: '72.28%', 1: '83.58%', 2: '87.50%', 3: '94.26%', 4: '82.50%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 74/200, Train Loss: 0.072752, Train-Class-Acc: {0: '98.77%', 1: '98.28%', 2: '98.96%', 3: '99.18%', 4: '98.73%', 5: '99.55%'}\n",
      "Val Loss: 3.562680, Val Acc: 86.89%, Val-Class-Acc: {0: '81.52%', 1: '82.09%', 2: '89.58%', 3: '91.80%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 75/200, Train Loss: 0.057014, Train-Class-Acc: {0: '98.77%', 1: '98.80%', 2: '99.13%', 3: '99.49%', 4: '96.84%', 5: '99.63%'}\n",
      "Val Loss: 3.871182, Val Acc: 86.10%, Val-Class-Acc: {0: '84.78%', 1: '80.60%', 2: '83.33%', 3: '90.57%', 4: '82.50%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 76/200, Train Loss: 0.215898, Train-Class-Acc: {0: '97.68%', 1: '98.06%', 2: '99.31%', 3: '98.98%', 4: '96.20%', 5: '98.51%'}\n",
      "Val Loss: 4.622737, Val Acc: 85.25%, Val-Class-Acc: {0: '78.26%', 1: '82.39%', 2: '81.94%', 3: '94.26%', 4: '82.50%', 5: '87.13%'}, LR: 0.001000\n",
      "Epoch 77/200, Train Loss: 0.197825, Train-Class-Acc: {0: '97.55%', 1: '96.41%', 2: '97.40%', 3: '97.95%', 4: '94.94%', 5: '98.65%'}\n",
      "Val Loss: 4.156189, Val Acc: 86.89%, Val-Class-Acc: {0: '84.78%', 1: '80.00%', 2: '87.50%', 3: '91.39%', 4: '80.00%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 78/200, Train Loss: 0.130651, Train-Class-Acc: {0: '96.73%', 1: '97.01%', 2: '98.61%', 3: '98.98%', 4: '96.84%', 5: '99.18%'}\n",
      "Val Loss: 3.475642, Val Acc: 86.89%, Val-Class-Acc: {0: '84.78%', 1: '81.79%', 2: '83.33%', 3: '92.62%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 79/200, Train Loss: 0.106207, Train-Class-Acc: {0: '98.50%', 1: '97.91%', 2: '99.13%', 3: '98.98%', 4: '98.10%', 5: '99.25%'}\n",
      "Val Loss: 3.593148, Val Acc: 86.81%, Val-Class-Acc: {0: '79.89%', 1: '81.49%', 2: '82.64%', 3: '93.85%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 80/200, Train Loss: 0.106959, Train-Class-Acc: {0: '98.91%', 1: '98.43%', 2: '98.96%', 3: '98.98%', 4: '97.47%', 5: '99.33%'}\n",
      "Val Loss: 3.785460, Val Acc: 86.10%, Val-Class-Acc: {0: '85.87%', 1: '77.91%', 2: '83.33%', 3: '89.75%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 81/200, Train Loss: 0.067493, Train-Class-Acc: {0: '98.91%', 1: '98.95%', 2: '98.61%', 3: '99.49%', 4: '98.73%', 5: '99.25%'}\n",
      "Val Loss: 3.521652, Val Acc: 86.03%, Val-Class-Acc: {0: '88.04%', 1: '77.01%', 2: '90.28%', 3: '92.62%', 4: '80.00%', 5: '88.02%'}, LR: 0.001000\n",
      "Epoch 82/200, Train Loss: 0.055982, Train-Class-Acc: {0: '98.77%', 1: '99.03%', 2: '99.31%', 3: '99.49%', 4: '99.37%', 5: '99.18%'}\n",
      "Val Loss: 4.132668, Val Acc: 85.40%, Val-Class-Acc: {0: '78.80%', 1: '80.60%', 2: '86.11%', 3: '90.98%', 4: '80.00%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 83/200, Train Loss: 0.042030, Train-Class-Acc: {0: '98.37%', 1: '99.03%', 2: '99.48%', 3: '99.69%', 4: '98.73%', 5: '99.55%'}\n",
      "Val Loss: 3.871671, Val Acc: 86.57%, Val-Class-Acc: {0: '80.98%', 1: '81.49%', 2: '85.42%', 3: '92.62%', 4: '82.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 84/200, Train Loss: 0.036006, Train-Class-Acc: {0: '99.32%', 1: '99.03%', 2: '99.48%', 3: '99.80%', 4: '98.73%', 5: '99.55%'}\n",
      "Val Loss: 4.603607, Val Acc: 86.42%, Val-Class-Acc: {0: '77.17%', 1: '83.28%', 2: '86.81%', 3: '92.62%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 85/200, Train Loss: 0.053543, Train-Class-Acc: {0: '99.73%', 1: '99.03%', 2: '99.48%', 3: '99.59%', 4: '98.73%', 5: '99.48%'}\n",
      "Val Loss: 4.098205, Val Acc: 87.67%, Val-Class-Acc: {0: '85.87%', 1: '80.00%', 2: '86.11%', 3: '91.80%', 4: '80.00%', 5: '94.91%'}, LR: 0.001000\n",
      "Epoch 86/200, Train Loss: 0.241453, Train-Class-Acc: {0: '99.18%', 1: '98.73%', 2: '99.13%', 3: '99.39%', 4: '96.20%', 5: '99.18%'}\n",
      "Val Loss: 5.436530, Val Acc: 85.56%, Val-Class-Acc: {0: '85.87%', 1: '76.72%', 2: '86.11%', 3: '88.93%', 4: '82.50%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 87/200, Train Loss: 0.317473, Train-Class-Acc: {0: '96.46%', 1: '96.11%', 2: '97.40%', 3: '98.05%', 4: '96.84%', 5: '97.98%'}\n",
      "Val Loss: 3.885440, Val Acc: 85.48%, Val-Class-Acc: {0: '75.00%', 1: '83.28%', 2: '86.81%', 3: '95.49%', 4: '82.50%', 5: '85.93%'}, LR: 0.001000\n",
      "Epoch 88/200, Train Loss: 0.245390, Train-Class-Acc: {0: '96.05%', 1: '96.78%', 2: '97.40%', 3: '98.16%', 4: '98.73%', 5: '98.43%'}\n",
      "Val Loss: 4.106187, Val Acc: 86.10%, Val-Class-Acc: {0: '82.61%', 1: '81.79%', 2: '88.89%', 3: '86.89%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 89/200, Train Loss: 0.145334, Train-Class-Acc: {0: '98.91%', 1: '99.10%', 2: '99.31%', 3: '99.18%', 4: '99.37%', 5: '99.33%'}\n",
      "Val Loss: 4.730478, Val Acc: 87.67%, Val-Class-Acc: {0: '89.67%', 1: '79.40%', 2: '86.81%', 3: '90.57%', 4: '82.50%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 90/200, Train Loss: 0.132357, Train-Class-Acc: {0: '98.91%', 1: '98.65%', 2: '98.61%', 3: '98.77%', 4: '97.47%', 5: '99.33%'}\n",
      "Val Loss: 4.479760, Val Acc: 86.96%, Val-Class-Acc: {0: '76.63%', 1: '84.18%', 2: '82.64%', 3: '94.26%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 91/200, Train Loss: 0.247551, Train-Class-Acc: {0: '97.68%', 1: '97.68%', 2: '98.44%', 3: '99.28%', 4: '100.00%', 5: '98.80%'}\n",
      "Val Loss: 3.399684, Val Acc: 84.54%, Val-Class-Acc: {0: '61.96%', 1: '87.46%', 2: '90.28%', 3: '92.62%', 4: '80.00%', 5: '86.23%'}, LR: 0.001000\n",
      "Epoch 92/200, Train Loss: 0.159241, Train-Class-Acc: {0: '97.41%', 1: '97.46%', 2: '99.31%', 3: '99.18%', 4: '98.10%', 5: '99.03%'}\n",
      "Val Loss: 3.752848, Val Acc: 86.18%, Val-Class-Acc: {0: '85.87%', 1: '78.21%', 2: '86.11%', 3: '90.98%', 4: '77.50%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 93/200, Train Loss: 0.091588, Train-Class-Acc: {0: '98.37%', 1: '98.43%', 2: '98.61%', 3: '99.18%', 4: '97.47%', 5: '99.78%'}\n",
      "Val Loss: 3.467743, Val Acc: 86.34%, Val-Class-Acc: {0: '80.43%', 1: '82.69%', 2: '87.50%', 3: '89.34%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 94/200, Train Loss: 0.034388, Train-Class-Acc: {0: '99.46%', 1: '99.18%', 2: '99.65%', 3: '99.59%', 4: '99.37%', 5: '99.93%'}\n",
      "Val Loss: 3.623051, Val Acc: 86.42%, Val-Class-Acc: {0: '78.80%', 1: '83.88%', 2: '83.33%', 3: '93.85%', 4: '82.50%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 95/200, Train Loss: 0.097709, Train-Class-Acc: {0: '98.50%', 1: '98.73%', 2: '98.79%', 3: '99.69%', 4: '99.37%', 5: '99.25%'}\n",
      "Val Loss: 5.204524, Val Acc: 86.34%, Val-Class-Acc: {0: '86.41%', 1: '77.61%', 2: '84.72%', 3: '89.75%', 4: '80.00%', 5: '94.01%'}, LR: 0.001000\n",
      "Epoch 96/200, Train Loss: 0.110087, Train-Class-Acc: {0: '99.18%', 1: '98.50%', 2: '99.31%', 3: '99.08%', 4: '98.73%', 5: '98.80%'}\n",
      "Val Loss: 4.050251, Val Acc: 86.10%, Val-Class-Acc: {0: '88.59%', 1: '71.34%', 2: '90.28%', 3: '93.85%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 97/200, Train Loss: 0.069429, Train-Class-Acc: {0: '99.05%', 1: '98.65%', 2: '98.96%', 3: '99.90%', 4: '97.47%', 5: '99.33%'}\n",
      "Val Loss: 3.725893, Val Acc: 86.57%, Val-Class-Acc: {0: '79.89%', 1: '80.30%', 2: '89.58%', 3: '92.62%', 4: '82.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 98/200, Train Loss: 0.082537, Train-Class-Acc: {0: '99.18%', 1: '98.58%', 2: '99.48%', 3: '99.49%', 4: '98.73%', 5: '99.40%'}\n",
      "Val Loss: 3.957947, Val Acc: 85.48%, Val-Class-Acc: {0: '84.78%', 1: '75.22%', 2: '84.03%', 3: '93.85%', 4: '85.00%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 99/200, Train Loss: 0.037488, Train-Class-Acc: {0: '99.46%', 1: '98.73%', 2: '99.48%', 3: '99.49%', 4: '99.37%', 5: '99.55%'}\n",
      "Val Loss: 4.157483, Val Acc: 85.64%, Val-Class-Acc: {0: '75.54%', 1: '80.90%', 2: '81.94%', 3: '91.80%', 4: '85.00%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 100/200, Train Loss: 0.041453, Train-Class-Acc: {0: '98.77%', 1: '98.80%', 2: '99.83%', 3: '99.59%', 4: '98.10%', 5: '99.63%'}\n",
      "Val Loss: 3.738990, Val Acc: 86.73%, Val-Class-Acc: {0: '75.00%', 1: '84.48%', 2: '88.19%', 3: '94.26%', 4: '82.50%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 101/200, Train Loss: 0.052897, Train-Class-Acc: {0: '99.18%', 1: '99.03%', 2: '99.13%', 3: '99.59%', 4: '98.10%', 5: '99.48%'}\n",
      "Val Loss: 3.921008, Val Acc: 86.73%, Val-Class-Acc: {0: '84.24%', 1: '80.00%', 2: '86.81%', 3: '92.21%', 4: '82.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 102/200, Train Loss: 0.024600, Train-Class-Acc: {0: '98.91%', 1: '99.25%', 2: '98.96%', 3: '99.80%', 4: '99.37%', 5: '99.70%'}\n",
      "Val Loss: 3.960242, Val Acc: 86.57%, Val-Class-Acc: {0: '84.24%', 1: '81.79%', 2: '81.94%', 3: '92.21%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 103/200, Train Loss: 0.024693, Train-Class-Acc: {0: '99.18%', 1: '99.70%', 2: '99.65%', 3: '99.80%', 4: '99.37%', 5: '99.78%'}\n",
      "Val Loss: 3.855509, Val Acc: 86.89%, Val-Class-Acc: {0: '89.13%', 1: '79.10%', 2: '81.94%', 3: '90.57%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 104/200, Train Loss: 0.054223, Train-Class-Acc: {0: '98.77%', 1: '98.88%', 2: '99.13%', 3: '99.49%', 4: '98.10%', 5: '99.78%'}\n",
      "Val Loss: 3.527045, Val Acc: 86.73%, Val-Class-Acc: {0: '83.70%', 1: '82.09%', 2: '81.94%', 3: '92.62%', 4: '82.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 105/200, Train Loss: 0.030710, Train-Class-Acc: {0: '99.18%', 1: '99.40%', 2: '99.31%', 3: '99.80%', 4: '99.37%', 5: '99.48%'}\n",
      "Val Loss: 3.709305, Val Acc: 86.34%, Val-Class-Acc: {0: '82.61%', 1: '81.19%', 2: '85.42%', 3: '92.21%', 4: '82.50%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 106/200, Train Loss: 0.022784, Train-Class-Acc: {0: '99.18%', 1: '99.03%', 2: '99.83%', 3: '99.80%', 4: '100.00%', 5: '99.85%'}\n",
      "Val Loss: 3.928761, Val Acc: 87.43%, Val-Class-Acc: {0: '86.96%', 1: '80.90%', 2: '85.42%', 3: '91.80%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 107/200, Train Loss: 0.023110, Train-Class-Acc: {0: '98.91%', 1: '99.18%', 2: '99.83%', 3: '99.90%', 5: '99.63%', 4: '98.10%'}\n",
      "Val Loss: 3.979153, Val Acc: 86.18%, Val-Class-Acc: {0: '82.07%', 1: '82.09%', 2: '81.94%', 3: '91.80%', 4: '82.50%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 108/200, Train Loss: 0.022912, Train-Class-Acc: {0: '99.59%', 1: '99.63%', 2: '100.00%', 3: '99.39%', 4: '98.73%', 5: '99.70%'}\n",
      "Val Loss: 4.023073, Val Acc: 86.65%, Val-Class-Acc: {0: '89.13%', 1: '75.82%', 2: '84.72%', 3: '93.03%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 109/200, Train Loss: 0.036174, Train-Class-Acc: {0: '99.46%', 1: '98.88%', 2: '99.31%', 3: '99.59%', 4: '98.73%', 5: '99.48%'}\n",
      "Val Loss: 5.200762, Val Acc: 86.34%, Val-Class-Acc: {0: '79.35%', 1: '76.72%', 2: '90.97%', 3: '94.26%', 4: '85.00%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 110/200, Train Loss: 0.142474, Train-Class-Acc: {0: '96.73%', 1: '97.83%', 2: '98.27%', 3: '99.28%', 4: '98.10%', 5: '98.80%'}\n",
      "Val Loss: 4.511487, Val Acc: 86.26%, Val-Class-Acc: {0: '84.24%', 1: '77.61%', 2: '90.97%', 3: '94.67%', 4: '82.50%', 5: '88.32%'}, LR: 0.001000\n",
      "Epoch 111/200, Train Loss: 0.083882, Train-Class-Acc: {0: '98.09%', 1: '98.35%', 2: '98.96%', 3: '99.28%', 4: '98.73%', 5: '99.40%'}\n",
      "Val Loss: 4.020083, Val Acc: 86.57%, Val-Class-Acc: {0: '85.33%', 1: '78.51%', 2: '81.94%', 3: '93.44%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 112/200, Train Loss: 0.127036, Train-Class-Acc: {0: '98.91%', 1: '98.43%', 2: '98.09%', 3: '99.08%', 4: '98.10%', 5: '99.40%'}\n",
      "Val Loss: 3.664982, Val Acc: 85.48%, Val-Class-Acc: {0: '83.15%', 1: '85.07%', 2: '80.56%', 3: '89.34%', 4: '82.50%', 5: '86.83%'}, LR: 0.001000\n",
      "Epoch 113/200, Train Loss: 0.131053, Train-Class-Acc: {0: '98.09%', 1: '97.91%', 2: '99.48%', 3: '99.18%', 4: '98.73%', 5: '98.43%'}\n",
      "Val Loss: 4.775886, Val Acc: 85.25%, Val-Class-Acc: {0: '74.46%', 1: '79.40%', 2: '86.81%', 3: '90.57%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 114/200, Train Loss: 0.086148, Train-Class-Acc: {0: '98.37%', 1: '98.50%', 2: '98.79%', 3: '99.28%', 5: '99.48%', 4: '98.73%'}\n",
      "Val Loss: 4.491082, Val Acc: 85.87%, Val-Class-Acc: {0: '77.17%', 1: '82.39%', 2: '83.33%', 3: '91.39%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 115/200, Train Loss: 0.066272, Train-Class-Acc: {0: '98.77%', 1: '99.03%', 2: '99.31%', 3: '99.69%', 4: '98.73%', 5: '99.10%'}\n",
      "Val Loss: 4.412087, Val Acc: 85.32%, Val-Class-Acc: {0: '86.41%', 1: '79.70%', 2: '81.94%', 3: '92.62%', 4: '82.50%', 5: '86.83%'}, LR: 0.001000\n",
      "Epoch 116/200, Train Loss: 0.057360, Train-Class-Acc: {0: '98.64%', 1: '99.10%', 2: '98.96%', 3: '99.69%', 4: '99.37%', 5: '99.55%'}\n",
      "Val Loss: 4.448273, Val Acc: 86.49%, Val-Class-Acc: {0: '85.87%', 1: '71.94%', 2: '86.11%', 3: '93.03%', 4: '82.50%', 5: '97.31%'}, LR: 0.001000\n",
      "Epoch 117/200, Train Loss: 0.078801, Train-Class-Acc: {0: '98.77%', 1: '98.80%', 2: '98.61%', 3: '99.49%', 4: '98.10%', 5: '99.25%'}\n",
      "Val Loss: 4.501690, Val Acc: 85.09%, Val-Class-Acc: {0: '85.33%', 1: '74.03%', 2: '88.89%', 3: '90.16%', 4: '85.00%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 118/200, Train Loss: 0.146545, Train-Class-Acc: {0: '97.96%', 1: '98.20%', 2: '99.13%', 3: '99.08%', 5: '98.65%', 4: '99.37%'}\n",
      "Val Loss: 4.432171, Val Acc: 85.40%, Val-Class-Acc: {0: '84.24%', 1: '76.12%', 2: '79.86%', 3: '90.98%', 4: '85.00%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 119/200, Train Loss: 0.150102, Train-Class-Acc: {0: '98.91%', 1: '98.06%', 2: '98.96%', 3: '98.57%', 4: '96.84%', 5: '99.03%'}\n",
      "Val Loss: 5.009969, Val Acc: 85.56%, Val-Class-Acc: {0: '78.26%', 1: '76.72%', 2: '85.42%', 3: '90.98%', 4: '82.50%', 5: '94.91%'}, LR: 0.001000\n",
      "Epoch 120/200, Train Loss: 0.176304, Train-Class-Acc: {0: '98.23%', 1: '98.28%', 2: '98.96%', 3: '99.08%', 4: '98.73%', 5: '99.03%'}\n",
      "Val Loss: 5.709738, Val Acc: 85.71%, Val-Class-Acc: {0: '85.33%', 1: '73.43%', 2: '88.19%', 3: '86.89%', 4: '82.50%', 5: '96.71%'}, LR: 0.001000\n",
      "Epoch 121/200, Train Loss: 0.247778, Train-Class-Acc: {0: '97.14%', 1: '97.83%', 2: '98.27%', 3: '98.87%', 4: '96.20%', 5: '98.80%'}\n",
      "Val Loss: 5.012736, Val Acc: 84.70%, Val-Class-Acc: {0: '71.20%', 1: '80.00%', 2: '84.72%', 3: '92.62%', 4: '80.00%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 122/200, Train Loss: 0.209705, Train-Class-Acc: {0: '97.55%', 1: '97.08%', 2: '97.75%', 3: '98.77%', 4: '96.84%', 5: '98.73%'}\n",
      "Val Loss: 4.548594, Val Acc: 85.01%, Val-Class-Acc: {0: '83.70%', 1: '76.12%', 2: '85.42%', 3: '94.26%', 4: '82.50%', 5: '88.02%'}, LR: 0.001000\n",
      "Epoch 123/200, Train Loss: 0.054450, Train-Class-Acc: {0: '98.64%', 1: '98.73%', 2: '99.13%', 3: '99.90%', 5: '99.55%', 4: '96.84%'}\n",
      "Val Loss: 4.307585, Val Acc: 85.40%, Val-Class-Acc: {0: '90.22%', 1: '70.75%', 2: '88.19%', 3: '91.39%', 4: '85.00%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 124/200, Train Loss: 0.142846, Train-Class-Acc: {0: '99.32%', 1: '98.80%', 2: '99.31%', 3: '99.49%', 4: '98.73%', 5: '99.48%'}\n",
      "Val Loss: 5.000756, Val Acc: 85.71%, Val-Class-Acc: {0: '85.33%', 1: '74.33%', 2: '85.42%', 3: '93.03%', 4: '85.00%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 125/200, Train Loss: 0.203599, Train-Class-Acc: {0: '98.77%', 1: '98.35%', 2: '98.96%', 3: '98.26%', 4: '97.47%', 5: '98.73%'}\n",
      "Val Loss: 4.673904, Val Acc: 84.70%, Val-Class-Acc: {0: '73.91%', 1: '80.00%', 2: '88.89%', 3: '93.85%', 4: '80.00%', 5: '87.43%'}, LR: 0.001000\n",
      "Epoch 126/200, Train Loss: 0.358571, Train-Class-Acc: {0: '96.05%', 1: '97.01%', 2: '97.57%', 3: '98.46%', 4: '98.73%', 5: '98.58%'}\n",
      "Val Loss: 3.871758, Val Acc: 85.64%, Val-Class-Acc: {0: '76.63%', 1: '82.99%', 2: '81.25%', 3: '91.80%', 4: '85.00%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 127/200, Train Loss: 0.082724, Train-Class-Acc: {0: '99.05%', 1: '98.35%', 2: '99.31%', 3: '99.08%', 4: '98.10%', 5: '99.40%'}\n",
      "Val Loss: 3.644461, Val Acc: 85.64%, Val-Class-Acc: {0: '85.33%', 1: '75.22%', 2: '86.11%', 3: '93.44%', 4: '85.00%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 128/200, Train Loss: 0.037832, Train-Class-Acc: {0: '99.73%', 1: '99.25%', 2: '99.65%', 3: '99.80%', 4: '99.37%', 5: '99.63%'}\n",
      "Val Loss: 3.976334, Val Acc: 85.87%, Val-Class-Acc: {0: '87.50%', 1: '71.94%', 2: '86.11%', 3: '93.03%', 4: '85.00%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 129/200, Train Loss: 0.081001, Train-Class-Acc: {0: '98.91%', 1: '98.88%', 2: '99.48%', 3: '99.69%', 4: '98.73%', 5: '99.18%'}\n",
      "Val Loss: 4.676443, Val Acc: 85.87%, Val-Class-Acc: {0: '83.15%', 1: '76.42%', 2: '82.64%', 3: '92.21%', 4: '85.00%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 130/200, Train Loss: 0.051212, Train-Class-Acc: {0: '99.32%', 1: '98.95%', 2: '99.31%', 3: '99.59%', 4: '98.10%', 5: '99.40%'}\n",
      "Val Loss: 4.454382, Val Acc: 85.40%, Val-Class-Acc: {0: '86.96%', 1: '73.43%', 2: '86.11%', 3: '92.62%', 4: '85.00%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 131/200, Train Loss: 0.114677, Train-Class-Acc: {0: '99.18%', 1: '98.43%', 2: '98.96%', 3: '99.39%', 5: '99.03%', 4: '98.73%'}\n",
      "Val Loss: 4.938200, Val Acc: 85.01%, Val-Class-Acc: {0: '79.89%', 1: '78.51%', 2: '86.11%', 3: '90.98%', 4: '82.50%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 132/200, Train Loss: 0.067499, Train-Class-Acc: {0: '99.32%', 1: '99.10%', 2: '99.48%', 3: '99.08%', 4: '100.00%', 5: '99.63%'}\n",
      "Val Loss: 4.295174, Val Acc: 86.03%, Val-Class-Acc: {0: '80.43%', 1: '77.01%', 2: '87.50%', 3: '93.85%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 133/200, Train Loss: 0.049997, Train-Class-Acc: {0: '99.32%', 1: '99.48%', 2: '99.48%', 3: '99.69%', 4: '99.37%', 5: '99.40%'}\n",
      "Val Loss: 4.525350, Val Acc: 85.48%, Val-Class-Acc: {0: '88.59%', 1: '72.24%', 2: '86.11%', 3: '92.21%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 134/200, Train Loss: 0.036136, Train-Class-Acc: {0: '99.46%', 1: '99.18%', 2: '99.31%', 3: '100.00%', 4: '98.10%', 5: '99.48%'}\n",
      "Val Loss: 4.380201, Val Acc: 85.32%, Val-Class-Acc: {0: '83.15%', 1: '74.63%', 2: '88.19%', 3: '89.34%', 4: '85.00%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 135/200, Train Loss: 0.049868, Train-Class-Acc: {0: '99.46%', 1: '99.18%', 2: '99.83%', 3: '99.39%', 4: '100.00%', 5: '99.63%'}\n",
      "Val Loss: 4.971186, Val Acc: 86.10%, Val-Class-Acc: {0: '77.17%', 1: '77.31%', 2: '88.89%', 3: '93.85%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 136/200, Train Loss: 0.020463, Train-Class-Acc: {0: '99.46%', 1: '99.55%', 2: '99.83%', 3: '99.80%', 4: '98.73%', 5: '99.70%'}\n",
      "Val Loss: 4.122522, Val Acc: 85.79%, Val-Class-Acc: {0: '87.50%', 1: '75.82%', 2: '88.89%', 3: '93.44%', 4: '85.00%', 5: '88.02%'}, LR: 0.001000\n",
      "Epoch 137/200, Train Loss: 0.030151, Train-Class-Acc: {0: '99.73%', 1: '98.88%', 2: '99.65%', 3: '99.69%', 4: '98.73%', 5: '99.70%'}\n",
      "Val Loss: 4.356102, Val Acc: 84.78%, Val-Class-Acc: {0: '73.37%', 1: '83.28%', 2: '82.64%', 3: '92.62%', 4: '85.00%', 5: '87.72%'}, LR: 0.001000\n",
      "Epoch 138/200, Train Loss: 0.018552, Train-Class-Acc: {0: '99.32%', 1: '99.55%', 2: '99.83%', 3: '99.90%', 4: '99.37%', 5: '99.70%'}\n",
      "Val Loss: 4.132062, Val Acc: 86.10%, Val-Class-Acc: {0: '77.17%', 1: '76.42%', 2: '87.50%', 3: '92.21%', 4: '85.00%', 5: '95.81%'}, LR: 0.001000\n",
      "Epoch 139/200, Train Loss: 0.019366, Train-Class-Acc: {0: '99.59%', 1: '99.40%', 2: '99.48%', 3: '99.90%', 4: '100.00%', 5: '99.93%'}\n",
      "Val Loss: 4.200637, Val Acc: 86.03%, Val-Class-Acc: {0: '86.41%', 1: '79.10%', 2: '81.94%', 3: '89.75%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 140/200, Train Loss: 0.013866, Train-Class-Acc: {0: '99.59%', 1: '99.55%', 2: '99.83%', 3: '99.59%', 4: '99.37%', 5: '99.93%'}\n",
      "Val Loss: 4.037540, Val Acc: 86.18%, Val-Class-Acc: {0: '84.78%', 1: '78.21%', 2: '84.72%', 3: '92.62%', 4: '85.00%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 141/200, Train Loss: 0.015646, Train-Class-Acc: {0: '99.59%', 1: '99.55%', 2: '100.00%', 3: '99.90%', 5: '99.85%', 4: '100.00%'}\n",
      "Val Loss: 4.262477, Val Acc: 86.10%, Val-Class-Acc: {0: '80.43%', 1: '79.40%', 2: '88.19%', 3: '91.39%', 4: '80.00%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 142/200, Train Loss: 0.011481, Train-Class-Acc: {0: '99.32%', 1: '99.55%', 2: '100.00%', 3: '99.80%', 4: '98.73%', 5: '99.93%'}\n",
      "Val Loss: 4.270177, Val Acc: 86.65%, Val-Class-Acc: {0: '78.80%', 1: '81.79%', 2: '89.58%', 3: '91.39%', 4: '80.00%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 143/200, Train Loss: 0.016802, Train-Class-Acc: {0: '99.59%', 1: '99.70%', 2: '99.48%', 3: '99.90%', 4: '99.37%', 5: '99.85%'}\n",
      "Val Loss: 4.540911, Val Acc: 85.95%, Val-Class-Acc: {0: '86.41%', 1: '73.73%', 2: '89.58%', 3: '92.21%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 144/200, Train Loss: 0.056752, Train-Class-Acc: {0: '99.32%', 1: '98.80%', 2: '99.31%', 3: '99.59%', 4: '99.37%', 5: '99.70%'}\n",
      "Val Loss: 4.073650, Val Acc: 86.49%, Val-Class-Acc: {0: '83.70%', 1: '81.79%', 2: '85.42%', 3: '90.16%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 145/200, Train Loss: 0.014627, Train-Class-Acc: {0: '99.86%', 1: '99.78%', 2: '99.83%', 3: '99.69%', 5: '99.85%', 4: '98.10%'}\n",
      "Val Loss: 3.706773, Val Acc: 85.95%, Val-Class-Acc: {0: '86.96%', 1: '76.42%', 2: '86.81%', 3: '90.98%', 4: '85.00%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 146/200, Train Loss: 0.007824, Train-Class-Acc: {0: '99.73%', 1: '99.55%', 2: '100.00%', 3: '99.80%', 4: '99.37%', 5: '99.78%'}\n",
      "Val Loss: 3.663494, Val Acc: 86.34%, Val-Class-Acc: {0: '84.78%', 1: '79.70%', 2: '85.42%', 3: '90.98%', 4: '85.00%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 147/200, Train Loss: 0.033951, Train-Class-Acc: {0: '99.18%', 1: '99.10%', 2: '99.31%', 3: '99.59%', 4: '98.73%', 5: '99.78%'}\n",
      "Val Loss: 3.988354, Val Acc: 86.49%, Val-Class-Acc: {0: '77.17%', 1: '79.70%', 2: '88.19%', 3: '92.21%', 4: '85.00%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 148/200, Train Loss: 0.002475, Train-Class-Acc: {0: '99.46%', 1: '99.85%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '99.93%'}\n",
      "Val Loss: 4.058980, Val Acc: 85.71%, Val-Class-Acc: {0: '77.72%', 1: '79.70%', 2: '84.72%', 3: '91.80%', 4: '85.00%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 149/200, Train Loss: 0.149015, Train-Class-Acc: {0: '98.09%', 1: '98.06%', 2: '99.48%', 3: '99.69%', 4: '98.73%', 5: '99.40%'}\n",
      "Val Loss: 5.598792, Val Acc: 85.48%, Val-Class-Acc: {0: '90.22%', 1: '68.66%', 2: '88.19%', 3: '93.44%', 4: '80.00%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 150/200, Train Loss: 0.058957, Train-Class-Acc: {0: '98.50%', 1: '99.03%', 2: '98.79%', 3: '99.69%', 4: '94.94%', 5: '99.63%'}\n",
      "Val Loss: 4.567059, Val Acc: 85.95%, Val-Class-Acc: {0: '88.59%', 1: '73.43%', 2: '84.03%', 3: '93.03%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 151/200, Train Loss: 0.064123, Train-Class-Acc: {0: '98.91%', 1: '98.43%', 2: '99.31%', 3: '100.00%', 4: '98.73%', 5: '99.40%'}\n",
      "Val Loss: 4.290758, Val Acc: 85.25%, Val-Class-Acc: {0: '71.74%', 1: '80.00%', 2: '88.89%', 3: '91.39%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 152/200, Train Loss: 0.031151, Train-Class-Acc: {0: '98.77%', 1: '99.18%', 2: '99.65%', 3: '99.80%', 5: '99.78%', 4: '99.37%'}\n",
      "Val Loss: 4.416260, Val Acc: 85.01%, Val-Class-Acc: {0: '77.72%', 1: '78.51%', 2: '86.11%', 3: '90.57%', 4: '85.00%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 153/200, Train Loss: 0.135593, Train-Class-Acc: {0: '98.64%', 1: '98.73%', 2: '98.96%', 3: '99.59%', 4: '98.73%', 5: '99.48%'}\n",
      "Val Loss: 5.277649, Val Acc: 85.32%, Val-Class-Acc: {0: '72.28%', 1: '76.42%', 2: '90.28%', 3: '92.62%', 4: '82.50%', 5: '94.31%'}, LR: 0.001000\n",
      "Epoch 154/200, Train Loss: 0.081825, Train-Class-Acc: {0: '98.91%', 1: '98.88%', 2: '98.27%', 3: '99.18%', 4: '98.73%', 5: '99.33%'}\n",
      "Val Loss: 4.178160, Val Acc: 85.95%, Val-Class-Acc: {0: '85.33%', 1: '76.72%', 2: '84.72%', 3: '93.44%', 4: '85.00%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 155/200, Train Loss: 0.042570, Train-Class-Acc: {0: '99.05%', 1: '99.18%', 2: '98.79%', 3: '99.49%', 4: '100.00%', 5: '99.48%'}\n",
      "Val Loss: 4.485180, Val Acc: 85.87%, Val-Class-Acc: {0: '84.24%', 1: '80.30%', 2: '84.72%', 3: '87.30%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 156/200, Train Loss: 0.040032, Train-Class-Acc: {0: '98.77%', 1: '99.40%', 2: '98.96%', 3: '99.69%', 4: '98.73%', 5: '99.93%'}\n",
      "Val Loss: 4.385244, Val Acc: 85.01%, Val-Class-Acc: {0: '82.07%', 1: '76.42%', 2: '84.03%', 3: '89.75%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 157/200, Train Loss: 0.026959, Train-Class-Acc: {0: '98.91%', 1: '99.25%', 2: '99.48%', 3: '99.69%', 4: '98.10%', 5: '99.85%'}\n",
      "Val Loss: 4.477815, Val Acc: 85.09%, Val-Class-Acc: {0: '77.17%', 1: '78.51%', 2: '86.11%', 3: '91.39%', 4: '85.00%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 158/200, Train Loss: 0.033498, Train-Class-Acc: {0: '99.05%', 1: '99.25%', 2: '99.65%', 3: '99.69%', 4: '98.73%', 5: '99.63%'}\n",
      "Val Loss: 4.516224, Val Acc: 84.54%, Val-Class-Acc: {0: '83.15%', 1: '74.33%', 2: '84.03%', 3: '89.75%', 4: '85.00%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 159/200, Train Loss: 0.025766, Train-Class-Acc: {0: '99.32%', 1: '99.48%', 2: '99.65%', 3: '99.59%', 4: '100.00%', 5: '99.85%'}\n",
      "Val Loss: 4.256301, Val Acc: 85.01%, Val-Class-Acc: {0: '83.70%', 1: '74.63%', 2: '86.11%', 3: '93.44%', 4: '80.00%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 160/200, Train Loss: 0.047978, Train-Class-Acc: {0: '99.18%', 1: '99.10%', 2: '98.96%', 3: '99.18%', 4: '100.00%', 5: '99.48%'}\n",
      "Val Loss: 4.403974, Val Acc: 85.64%, Val-Class-Acc: {0: '79.89%', 1: '77.91%', 2: '86.11%', 3: '92.62%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 161/200, Train Loss: 0.028358, Train-Class-Acc: {0: '99.59%', 1: '99.55%', 2: '99.48%', 3: '99.80%', 4: '99.37%', 5: '99.78%'}\n",
      "Val Loss: 4.769897, Val Acc: 85.09%, Val-Class-Acc: {0: '74.46%', 1: '77.61%', 2: '87.50%', 3: '93.44%', 4: '77.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 162/200, Train Loss: 0.189126, Train-Class-Acc: {0: '97.55%', 1: '97.38%', 2: '97.92%', 3: '98.77%', 5: '98.58%', 4: '94.30%'}\n",
      "Val Loss: 4.629305, Val Acc: 85.25%, Val-Class-Acc: {0: '84.24%', 1: '79.40%', 2: '84.72%', 3: '91.39%', 4: '82.50%', 5: '87.72%'}, LR: 0.001000\n",
      "Epoch 163/200, Train Loss: 0.091319, Train-Class-Acc: {0: '98.91%', 1: '98.35%', 2: '99.31%', 3: '99.39%', 4: '97.47%', 5: '99.10%'}\n",
      "Val Loss: 4.377219, Val Acc: 85.01%, Val-Class-Acc: {0: '82.61%', 1: '74.03%', 2: '86.81%', 3: '90.57%', 4: '85.00%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 164/200, Train Loss: 0.081298, Train-Class-Acc: {0: '99.05%', 1: '98.65%', 2: '99.31%', 3: '98.57%', 4: '96.20%', 5: '99.25%'}\n",
      "Val Loss: 4.181737, Val Acc: 85.32%, Val-Class-Acc: {0: '77.72%', 1: '79.40%', 2: '83.33%', 3: '93.03%', 4: '85.00%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 165/200, Train Loss: 0.065139, Train-Class-Acc: {0: '99.05%', 1: '98.73%', 2: '99.13%', 3: '99.90%', 4: '97.47%', 5: '99.55%'}\n",
      "Val Loss: 4.415993, Val Acc: 85.40%, Val-Class-Acc: {0: '73.37%', 1: '82.39%', 2: '84.72%', 3: '93.03%', 4: '85.00%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 166/200, Train Loss: 0.113234, Train-Class-Acc: {0: '98.37%', 1: '98.65%', 2: '99.31%', 3: '99.28%', 4: '98.73%', 5: '99.25%'}\n",
      "Val Loss: 4.317761, Val Acc: 85.56%, Val-Class-Acc: {0: '78.80%', 1: '77.31%', 2: '80.56%', 3: '92.62%', 4: '85.00%', 5: '94.61%'}, LR: 0.001000\n",
      "Epoch 167/200, Train Loss: 0.231758, Train-Class-Acc: {0: '98.37%', 1: '97.98%', 2: '99.13%', 3: '98.05%', 4: '98.73%', 5: '98.95%'}\n",
      "Val Loss: 5.243996, Val Acc: 85.01%, Val-Class-Acc: {0: '83.70%', 1: '74.93%', 2: '86.81%', 3: '90.16%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 168/200, Train Loss: 0.116502, Train-Class-Acc: {0: '98.91%', 1: '98.73%', 2: '99.13%', 3: '98.87%', 5: '99.18%', 4: '97.47%'}\n",
      "Val Loss: 5.133431, Val Acc: 85.01%, Val-Class-Acc: {0: '71.74%', 1: '83.28%', 2: '88.89%', 3: '91.39%', 4: '82.50%', 5: '88.02%'}, LR: 0.001000\n",
      "Epoch 169/200, Train Loss: 0.054402, Train-Class-Acc: {0: '98.91%', 1: '98.95%', 2: '99.31%', 3: '99.80%', 4: '98.10%', 5: '99.63%'}\n",
      "Val Loss: 4.602051, Val Acc: 84.23%, Val-Class-Acc: {0: '77.72%', 1: '80.90%', 2: '81.25%', 3: '88.11%', 4: '82.50%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 170/200, Train Loss: 0.012530, Train-Class-Acc: {0: '99.05%', 1: '99.55%', 2: '99.83%', 3: '99.90%', 4: '99.37%', 5: '99.93%'}\n",
      "Val Loss: 4.362235, Val Acc: 85.87%, Val-Class-Acc: {0: '83.15%', 1: '78.81%', 2: '84.72%', 3: '91.80%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 171/200, Train Loss: 0.014535, Train-Class-Acc: {0: '99.59%', 1: '99.55%', 2: '99.65%', 3: '100.00%', 4: '99.37%', 5: '99.55%'}\n",
      "Val Loss: 4.365457, Val Acc: 85.64%, Val-Class-Acc: {0: '88.59%', 1: '75.52%', 2: '88.19%', 3: '88.11%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 172/200, Train Loss: 0.016572, Train-Class-Acc: {0: '99.59%', 1: '99.63%', 2: '99.65%', 3: '99.80%', 4: '100.00%', 5: '99.70%'}\n",
      "Val Loss: 4.667810, Val Acc: 84.93%, Val-Class-Acc: {0: '77.72%', 1: '78.81%', 2: '84.03%', 3: '90.16%', 4: '82.50%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 173/200, Train Loss: 0.018333, Train-Class-Acc: {0: '99.46%', 1: '99.63%', 2: '99.48%', 3: '100.00%', 4: '99.37%', 5: '99.85%'}\n",
      "Val Loss: 4.746007, Val Acc: 85.79%, Val-Class-Acc: {0: '82.61%', 1: '79.40%', 2: '85.42%', 3: '91.39%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 174/200, Train Loss: 0.013834, Train-Class-Acc: {0: '99.86%', 1: '99.70%', 2: '99.65%', 3: '99.80%', 4: '99.37%', 5: '99.93%'}\n",
      "Val Loss: 4.681459, Val Acc: 85.56%, Val-Class-Acc: {0: '69.57%', 1: '82.69%', 2: '81.25%', 3: '92.62%', 4: '82.50%', 5: '94.31%'}, LR: 0.001000\n",
      "Epoch 175/200, Train Loss: 0.016616, Train-Class-Acc: {0: '99.32%', 1: '99.40%', 2: '99.48%', 3: '99.69%', 4: '99.37%', 5: '99.93%'}\n",
      "Val Loss: 4.586345, Val Acc: 85.64%, Val-Class-Acc: {0: '76.63%', 1: '78.51%', 2: '85.42%', 3: '91.80%', 4: '82.50%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 176/200, Train Loss: 0.011369, Train-Class-Acc: {0: '99.86%', 1: '99.55%', 2: '99.83%', 3: '99.90%', 4: '98.10%', 5: '100.00%'}\n",
      "Val Loss: 4.749659, Val Acc: 86.03%, Val-Class-Acc: {0: '79.89%', 1: '78.21%', 2: '84.03%', 3: '91.80%', 4: '82.50%', 5: '94.31%'}, LR: 0.001000\n",
      "Epoch 177/200, Train Loss: 0.029116, Train-Class-Acc: {0: '99.18%', 1: '99.40%', 2: '99.83%', 3: '99.80%', 4: '100.00%', 5: '99.63%'}\n",
      "Val Loss: 5.020482, Val Acc: 85.79%, Val-Class-Acc: {0: '79.35%', 1: '76.42%', 2: '86.11%', 3: '93.44%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 178/200, Train Loss: 0.045332, Train-Class-Acc: {0: '99.05%', 1: '99.10%', 2: '99.65%', 3: '99.49%', 4: '99.37%', 5: '99.70%'}\n",
      "Val Loss: 6.786520, Val Acc: 85.01%, Val-Class-Acc: {0: '84.24%', 1: '73.13%', 2: '83.33%', 3: '91.80%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 179/200, Train Loss: 0.121763, Train-Class-Acc: {0: '99.05%', 1: '98.95%', 2: '99.48%', 3: '99.08%', 4: '98.73%', 5: '99.18%'}\n",
      "Val Loss: 6.566157, Val Acc: 84.54%, Val-Class-Acc: {0: '83.15%', 1: '74.93%', 2: '81.25%', 3: '87.70%', 4: '82.50%', 5: '94.31%'}, LR: 0.001000\n",
      "Epoch 180/200, Train Loss: 0.169438, Train-Class-Acc: {0: '98.37%', 1: '98.13%', 2: '98.27%', 3: '98.67%', 4: '98.73%', 5: '99.48%'}\n",
      "Val Loss: 5.121005, Val Acc: 83.92%, Val-Class-Acc: {0: '90.76%', 1: '65.37%', 2: '85.42%', 3: '93.03%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 181/200, Train Loss: 0.084124, Train-Class-Acc: {0: '98.50%', 1: '98.35%', 2: '99.13%', 3: '99.08%', 4: '99.37%', 5: '99.63%'}\n",
      "Val Loss: 4.761901, Val Acc: 84.54%, Val-Class-Acc: {0: '69.57%', 1: '82.69%', 2: '81.94%', 3: '93.44%', 4: '82.50%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 182/200, Train Loss: 0.070457, Train-Class-Acc: {0: '98.50%', 1: '98.80%', 2: '98.96%', 3: '99.59%', 4: '97.47%', 5: '99.55%'}\n",
      "Val Loss: 4.477177, Val Acc: 85.40%, Val-Class-Acc: {0: '79.35%', 1: '77.91%', 2: '86.11%', 3: '90.98%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 183/200, Train Loss: 0.057591, Train-Class-Acc: {0: '98.91%', 1: '98.58%', 2: '99.31%', 3: '99.49%', 4: '98.10%', 5: '99.48%'}\n",
      "Val Loss: 4.259079, Val Acc: 85.17%, Val-Class-Acc: {0: '84.78%', 1: '73.43%', 2: '84.03%', 3: '93.85%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 184/200, Train Loss: 0.072415, Train-Class-Acc: {0: '99.18%', 1: '99.03%', 2: '98.61%', 3: '99.49%', 5: '99.85%', 4: '98.73%'}\n",
      "Val Loss: 4.885307, Val Acc: 85.40%, Val-Class-Acc: {0: '71.74%', 1: '82.09%', 2: '87.50%', 3: '92.62%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 185/200, Train Loss: 0.063235, Train-Class-Acc: {0: '99.59%', 1: '99.55%', 2: '99.13%', 3: '99.69%', 4: '99.37%', 5: '99.63%'}\n",
      "Val Loss: 4.588841, Val Acc: 84.93%, Val-Class-Acc: {0: '77.72%', 1: '79.40%', 2: '91.67%', 3: '93.03%', 4: '82.50%', 5: '85.93%'}, LR: 0.001000\n",
      "Epoch 186/200, Train Loss: 0.124262, Train-Class-Acc: {0: '98.77%', 1: '98.58%', 2: '97.92%', 3: '99.28%', 4: '97.47%', 5: '99.18%'}\n",
      "Val Loss: 5.741828, Val Acc: 85.71%, Val-Class-Acc: {0: '79.35%', 1: '77.91%', 2: '83.33%', 3: '93.85%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 187/200, Train Loss: 0.057083, Train-Class-Acc: {0: '98.09%', 1: '98.73%', 2: '99.31%', 3: '99.59%', 4: '98.73%', 5: '99.33%'}\n",
      "Val Loss: 4.665485, Val Acc: 84.70%, Val-Class-Acc: {0: '83.70%', 1: '74.93%', 2: '84.03%', 3: '91.80%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 188/200, Train Loss: 0.037589, Train-Class-Acc: {0: '99.46%', 1: '99.18%', 2: '99.65%', 3: '99.80%', 5: '99.70%', 4: '98.73%'}\n",
      "Val Loss: 3.987461, Val Acc: 85.87%, Val-Class-Acc: {0: '79.89%', 1: '82.99%', 2: '83.33%', 3: '89.75%', 4: '85.00%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 189/200, Train Loss: 0.028294, Train-Class-Acc: {0: '98.23%', 1: '99.33%', 2: '99.83%', 3: '99.80%', 4: '100.00%', 5: '99.78%'}\n",
      "Val Loss: 4.721766, Val Acc: 86.03%, Val-Class-Acc: {0: '88.04%', 1: '75.82%', 2: '82.64%', 3: '88.11%', 4: '82.50%', 5: '95.51%'}, LR: 0.001000\n",
      "Epoch 190/200, Train Loss: 0.040458, Train-Class-Acc: {0: '99.59%', 1: '99.18%', 2: '99.65%', 3: '99.39%', 5: '99.48%', 4: '99.37%'}\n",
      "Val Loss: 5.214644, Val Acc: 85.01%, Val-Class-Acc: {0: '78.26%', 1: '76.72%', 2: '87.50%', 3: '90.57%', 4: '85.00%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 191/200, Train Loss: 0.029001, Train-Class-Acc: {0: '98.77%', 1: '99.18%', 2: '99.48%', 3: '99.49%', 4: '98.73%', 5: '99.70%'}\n",
      "Val Loss: 4.821905, Val Acc: 85.64%, Val-Class-Acc: {0: '83.70%', 1: '79.40%', 2: '79.86%', 3: '90.98%', 4: '85.00%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 192/200, Train Loss: 0.011248, Train-Class-Acc: {0: '99.86%', 1: '99.85%', 2: '99.31%', 3: '99.90%', 4: '100.00%', 5: '99.78%'}\n",
      "Val Loss: 4.592400, Val Acc: 85.95%, Val-Class-Acc: {0: '82.07%', 1: '76.42%', 2: '86.81%', 3: '92.21%', 4: '85.00%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 193/200, Train Loss: 0.043612, Train-Class-Acc: {0: '99.46%', 1: '99.33%', 2: '99.83%', 3: '99.59%', 4: '100.00%', 5: '99.78%'}\n",
      "Val Loss: 5.518627, Val Acc: 85.71%, Val-Class-Acc: {0: '82.07%', 1: '73.13%', 2: '84.03%', 3: '91.39%', 4: '85.00%', 5: '97.01%'}, LR: 0.001000\n",
      "Epoch 194/200, Train Loss: 0.090189, Train-Class-Acc: {0: '99.18%', 1: '98.65%', 2: '99.13%', 3: '99.49%', 5: '99.10%', 4: '98.73%'}\n",
      "Val Loss: 5.038662, Val Acc: 84.62%, Val-Class-Acc: {0: '81.52%', 1: '76.72%', 2: '84.03%', 3: '91.39%', 4: '85.00%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 195/200, Train Loss: 0.065696, Train-Class-Acc: {0: '98.23%', 1: '98.88%', 2: '99.31%', 3: '99.49%', 4: '99.37%', 5: '99.33%'}\n",
      "Val Loss: 4.635849, Val Acc: 85.95%, Val-Class-Acc: {0: '83.70%', 1: '76.12%', 2: '87.50%', 3: '90.16%', 4: '82.50%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 196/200, Train Loss: 0.108861, Train-Class-Acc: {0: '98.77%', 1: '97.98%', 2: '98.61%', 3: '98.57%', 4: '97.47%', 5: '99.40%'}\n",
      "Val Loss: 4.613204, Val Acc: 85.79%, Val-Class-Acc: {0: '80.98%', 1: '80.90%', 2: '84.72%', 3: '89.34%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 197/200, Train Loss: 0.044889, Train-Class-Acc: {0: '99.18%', 1: '99.03%', 2: '99.31%', 3: '99.90%', 4: '100.00%', 5: '99.70%'}\n",
      "Val Loss: 4.509867, Val Acc: 86.18%, Val-Class-Acc: {0: '79.35%', 1: '78.81%', 2: '85.42%', 3: '90.98%', 4: '82.50%', 5: '94.61%'}, LR: 0.001000\n",
      "Epoch 198/200, Train Loss: 0.025771, Train-Class-Acc: {0: '99.32%', 1: '99.33%', 2: '99.65%', 3: '99.80%', 4: '99.37%', 5: '99.85%'}\n",
      "Val Loss: 4.379676, Val Acc: 85.95%, Val-Class-Acc: {0: '76.63%', 1: '83.58%', 2: '82.64%', 3: '90.16%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 199/200, Train Loss: 0.035275, Train-Class-Acc: {0: '99.05%', 1: '99.33%', 2: '99.65%', 3: '99.69%', 4: '99.37%', 5: '99.63%'}\n",
      "Val Loss: 4.770652, Val Acc: 85.95%, Val-Class-Acc: {0: '75.54%', 1: '82.09%', 2: '87.50%', 3: '92.62%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 200/200, Train Loss: 0.042414, Train-Class-Acc: {0: '99.05%', 1: '99.18%', 2: '99.31%', 3: '99.90%', 4: '98.73%', 5: '99.78%'}\n",
      "Val Loss: 5.160045, Val Acc: 85.17%, Val-Class-Acc: {0: '81.52%', 1: '80.90%', 2: '86.11%', 3: '86.48%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "\n",
      "üèÜ Best model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_best.pth (Val Accuracy: 88.52%)\n",
      "\n",
      "üìå Final model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_final.pth\n",
      "\n",
      "üéØ Top 5 Best Models:\n",
      "Epoch 13, Train Loss: 0.493073, Train-Acc: {0: '93.32%', 1: '92.00%', 2: '95.84%', 3: '96.62%', 4: '88.61%', 5: '95.52%'},\n",
      "Val Loss: 2.414317, Val Acc: 88.52%, Val-Acc: {0: '87.50%', 1: '83.28%', 2: '87.50%', 3: '92.62%', 4: '82.50%', 5: '92.51%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_13.pth\n",
      "Epoch 10, Train Loss: 0.646935, Train-Acc: {0: '92.92%', 1: '91.17%', 2: '93.59%', 3: '95.29%', 4: '87.97%', 5: '94.84%'},\n",
      "Val Loss: 1.969844, Val Acc: 88.45%, Val-Acc: {0: '86.96%', 1: '80.30%', 2: '92.36%', 3: '95.90%', 4: '87.50%', 5: '90.42%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_10.pth\n",
      "Epoch 24, Train Loss: 0.173242, Train-Acc: {0: '96.46%', 1: '95.59%', 2: '98.61%', 3: '98.77%', 4: '91.77%', 5: '97.61%'},\n",
      "Val Loss: 2.749226, Val Acc: 88.37%, Val-Acc: {0: '89.13%', 1: '80.60%', 2: '90.97%', 3: '95.08%', 4: '82.50%', 5: '90.42%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_24.pth\n",
      "Epoch 39, Train Loss: 0.099217, Train-Acc: {0: '98.23%', 1: '97.91%', 2: '97.40%', 3: '99.08%', 4: '97.47%', 5: '98.80%'},\n",
      "Val Loss: 2.926864, Val Acc: 88.21%, Val-Acc: {0: '91.30%', 1: '79.10%', 2: '90.97%', 3: '93.85%', 4: '82.50%', 5: '91.02%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_39.pth\n",
      "Epoch 14, Train Loss: 0.617321, Train-Acc: {0: '94.01%', 1: '92.82%', 2: '95.84%', 3: '97.03%', 4: '89.87%', 5: '96.49%'},\n",
      "Val Loss: 3.178976, Val Acc: 88.21%, Val-Acc: {0: '86.96%', 1: '83.88%', 2: '88.19%', 3: '90.98%', 4: '82.50%', 5: '91.92%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/ResNet18_1D_LoRA_epoch_14.pth\n",
      "\n",
      "üß† Model Summary:\n",
      "Total Parameters: 3,922,566\n",
      "Model Size (float32): 14.96 MB\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 16\n",
      "  - BasicBlocks with adapters: 8\n",
      "Number of LoRA adapters: 16\n",
      "Number of LoRA groups: 2\n",
      "Total Training Time: 512.69 seconds\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 16\n",
      "  - BasicBlocks with adapters: 8\n",
      "---\n",
      "### Period 3 (alpha = 0.0, similarity_threshold = 0.89)\n",
      "+ ##### Total training time: 512.69 seconds\n",
      "+ ##### Model: ResNet18_1D_LoRA\n",
      "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3'*\n",
      "+ ##### Best Epoch: 13\n",
      "#### __Val Accuracy: 88.52%__\n",
      "#### __Val-Class-Acc: {0: '87.50%', 1: '83.28%', 2: '87.50%', 3: '92.62%', 4: '82.50%', 5: '92.51%'}__\n",
      "#### __Total Parameters: 3,922,566__\n",
      "#### __Model Size (float32): 14.96 MB__\n",
      "#### __Number of LoRA adapters: 16__\n",
      "#### __Number of LoRA groups: 2__\n",
      "\n",
      "Saved class features to: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_3/class_features.pkl\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå Period 3: DynEx-CLoRA Training (ECG)\n",
    "# ================================\n",
    "period = 3\n",
    "\n",
    "# ==== Paths ====\n",
    "stop_signal_file = os.path.join(BASE_DIR, \"stop_training.txt\")\n",
    "model_saving_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v4\", f\"Period_{period}\")\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# ==== Load Period 3 Data ====\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{period}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{period}.npy\"))\n",
    "X_val   = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val   = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "# ==== Device ====\n",
    "device = auto_select_cuda_device()\n",
    "\n",
    "# ==== Load Period 2 Features ====\n",
    "prev_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v3\", \"Period_2\")\n",
    "class_features_path = os.path.join(prev_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, \"rb\") as f:\n",
    "    class_features_dict = pickle.load(f)\n",
    "print(f\"‚úÖ Loaded class features from: {class_features_path}\")\n",
    "\n",
    "# ==== Load Period 2 Checkpoint ====\n",
    "prev_model_path = os.path.join(prev_folder, \"ResNet18_1D_LoRA_best.pth\")\n",
    "checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "\n",
    "input_channels = X_train.shape[2]\n",
    "output_size = len(np.unique(y_train))\n",
    "\n",
    "# ==== Build Teacher Model ====\n",
    "teacher_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size - 2).to(device)\n",
    "teacher_model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "teacher_model.eval()\n",
    "\n",
    "# ==== Build Student Model ====\n",
    "model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size).to(device)\n",
    "\n",
    "# ==== Sync Adapter Structure ====\n",
    "num_lora_groups = checkpoint.get(\"num_lora_groups\", 0)\n",
    "print(f\"üîÑ Number of LoRA groups: {num_lora_groups}\")\n",
    "related_labels = checkpoint.get(\"related_labels\", {\"base\": [0, 1]})\n",
    "for _ in range(num_lora_groups):\n",
    "    model.add_lora_adapter()\n",
    "\n",
    "# ==== Load Previous Weights (excluding FC and LoRA) ====\n",
    "model_dict = model.state_dict()\n",
    "prev_state_dict = checkpoint[\"model_state_dict\"]\n",
    "filtered_dict = {\n",
    "    k: v for k, v in prev_state_dict.items()\n",
    "    if k in model_dict and model_dict[k].shape == v.shape and k not in [\"fc.weight\", \"fc.bias\"]\n",
    "}\n",
    "model.load_state_dict(filtered_dict, strict=False)\n",
    "for k in model_dict:\n",
    "    if k not in filtered_dict:\n",
    "        print(f\"üîç Not loaded: {k}, shape={model_dict[k].shape}\")\n",
    "print(\"‚úÖ Loaded shared weights from Period 2 (excluding FC only)\")\n",
    "\n",
    "# ==== Training Configuration ====\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "alpha = 0.0 # No distillation\n",
    "similarity_threshold = 0.89\n",
    "stable_classes = [0, 2, 3]  # Ê†πÊìö Period 2 class ÁµêÊûú\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# ==== Start Training ====\n",
    "train_with_dynex_clora_ecg(\n",
    "    model=model,\n",
    "    teacher_model=None,\n",
    "    output_size=output_size,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    alpha=alpha,\n",
    "    model_saving_folder=model_saving_folder,\n",
    "    model_name=\"ResNet18_1D_LoRA\",\n",
    "    stop_signal_file=stop_signal_file,\n",
    "    scheduler=scheduler,\n",
    "    period=period,\n",
    "    stable_classes=stable_classes,\n",
    "    similarity_threshold=similarity_threshold,\n",
    "    class_features_dict=class_features_dict,\n",
    "    related_labels=related_labels,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ==== Cleanup ====\n",
    "del X_train, y_train, X_val, y_val\n",
    "del model, teacher_model, checkpoint, model_dict, prev_state_dict, filtered_dict\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4f5737",
   "metadata": {},
   "source": [
    "#### v5 no distillation, all freeze, th = 0.92\n",
    "##### Period 3 (alpha = 0.0, similarity_threshold = 0.92)\n",
    "+ ##### Total training time: 552.50 seconds\n",
    "+ ##### Model: ResNet18_1D_LoRA\n",
    "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3'*\n",
    "+ ##### Best Epoch: 22\n",
    "##### __Val Accuracy: 88.13%__\n",
    "##### __Val-Class-Acc: {0: '86.96%', 1: '81.79%', 2: '87.50%', 3: '92.21%', 4: '82.50%', 5: '93.11%'}__\n",
    "##### __Total Parameters: 3,953,286__\n",
    "##### __Model Size (float32): 15.08 MB__\n",
    "##### __Number of LoRA adapters: 24__\n",
    "##### __Number of LoRA groups: 3__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b29f55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 0\n",
      "    - Memory Used    : 18 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ Loaded class features from: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_2/class_features.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1775968/2500211532.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Number of LoRA groups: 1\n",
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n",
      "üîç Not loaded: fc.weight, shape=torch.Size([6, 1024])\n",
      "üîç Not loaded: fc.bias, shape=torch.Size([6])\n",
      "‚úÖ Loaded shared weights from Period 2 (excluding FC only)\n",
      "\n",
      "üöÄ 'train_with_dynex_clora_ecg' started for Period 3\n",
      "\n",
      "‚úÖ Removed existing folder: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3\n",
      "\n",
      "‚úÖ Data Overview:\n",
      "X_train: torch.Size([5120, 5000, 12]), y_train: torch.Size([5120])\n",
      "X_val: torch.Size([1281, 5000, 12]), y_val: torch.Size([1281])\n",
      "\n",
      "üîé Similarity Analysis:\n",
      "  Similarity threshold: 0.9200\n",
      "  Existing classes: [0, 1, 2, 3]\n",
      "  Current classes: [0, 1, 2, 3, 4, 5]\n",
      "  New classes: [4, 5]\n",
      "\n",
      "üìä Similarity Scores:\n",
      "  New Class 4:\n",
      "    - Existing Class 2: 0.8701\n",
      "    - Existing Class 0: 0.8682\n",
      "    - Existing Class 1: 0.8643\n",
      "    - Existing Class 3: 0.8612\n",
      "  New Class 5:\n",
      "    - Existing Class 1: 0.8901\n",
      "    - Existing Class 3: 0.8689\n",
      "    - Existing Class 2: 0.8598\n",
      "    - Existing Class 0: 0.8348\n",
      "\n",
      "  Average similarity: 0.8567, Std: 0.0213\n",
      "\n",
      "üß© Managing LoRA adapters for 2 new classes...\n",
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n",
      "‚ûï New Class 4 is not similar to any existing class ‚Üí Created new adapter group #1\n",
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n",
      "‚ûï New Class 5 is not similar to any existing class ‚Üí Created new adapter group #2\n",
      "\n",
      "üîç Checking stability of existing classes...\n",
      "  Class 0 similarity with itself: 0.8693\n",
      "‚ö†Ô∏è Class 0 has drifted (self-sim=0.8693) ‚Üí Unfreezing adapter 'base'\n",
      "  Class 1 similarity with itself: 0.8585\n",
      "‚ö†Ô∏è Class 1 has drifted (self-sim=0.8585) ‚Üí Unfreezing adapter 'base'\n",
      "  Class 2 similarity with itself: 0.8699\n",
      "‚ö†Ô∏è Class 2 has drifted (self-sim=0.8699) ‚Üí Unfreezing adapter '0'\n",
      "  Class 3 similarity with itself: 0.8763\n",
      "‚ö†Ô∏è Class 3 has drifted (self-sim=0.8763) ‚Üí Unfreezing adapter 'base'\n",
      "\n",
      "üîí Default: Freezing ALL model parameters\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.0.lora_adapters.1.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.1.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.0.lora_adapters.2.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.2.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.lora_adapters.1.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.1.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.lora_adapters.2.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.2.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.0.lora_adapters.1.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.1.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.0.lora_adapters.2.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.2.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.lora_adapters.1.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.1.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.lora_adapters.2.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.2.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.0.lora_adapters.1.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.1.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.0.lora_adapters.2.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.2.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.lora_adapters.1.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.1.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.lora_adapters.2.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.2.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.0.lora_adapters.1.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.1.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.0.lora_adapters.2.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.2.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.lora_adapters.1.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.1.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.lora_adapters.2.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.2.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå fc.weight                                          | shape=[6, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[6]\n",
      "\n",
      "üîì Unfreezing selected adapters (by group):\n",
      "  - Adapter Group #0 (all blocks) (classes: [2])\n",
      "  - Base layers (classes: [0, 1, 3])\n",
      "\n",
      "üîç Frozen Parameters: (After Unfreeze specific adapter groups)\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.lora_adapters.1.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.1.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.0.lora_adapters.2.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.2.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.lora_adapters.1.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.1.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.lora_adapters.2.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.2.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.0.lora_adapters.1.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.1.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.0.lora_adapters.2.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.2.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.lora_adapters.1.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.1.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.lora_adapters.2.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.2.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.0.lora_adapters.1.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.1.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.0.lora_adapters.2.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.2.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.lora_adapters.1.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.1.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.lora_adapters.2.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.2.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.0.lora_adapters.1.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.1.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.0.lora_adapters.2.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.2.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.lora_adapters.1.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.1.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.lora_adapters.2.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.2.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå fc.weight                                          | shape=[6, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[6]\n",
      "  - New adapter group #1 (classes: [4])\n",
      "  - New adapter group #2 (classes: [5])\n",
      "\n",
      "üîç Frozen Parameters (After Unfreeze newly added adapter groups):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå fc.weight                                          | shape=[6, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[6]\n",
      "\n",
      "üîç Frozen Parameters (After unfreeze the final layer):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "\n",
      "üìã Related Labels Summary:\n",
      "  - Base network: Classes [0, 1, 3]\n",
      "  - Adapter #0: Classes [2]\n",
      "  - Adapter #1: Classes [4]\n",
      "  - Adapter #2: Classes [5]\n",
      "\n",
      "üîß Trainable Parameter Status:\n",
      "  ‚úÖ layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer1.0.lora_adapters.1.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.0.lora_adapters.1.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer1.0.lora_adapters.2.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.0.lora_adapters.2.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer1.1.lora_adapters.1.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.1.lora_adapters.1.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer1.1.lora_adapters.2.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.1.lora_adapters.2.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer2.0.lora_adapters.1.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.0.lora_adapters.1.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer2.0.lora_adapters.2.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.0.lora_adapters.2.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer2.1.lora_adapters.1.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.1.lora_adapters.1.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer2.1.lora_adapters.2.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.1.lora_adapters.2.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer3.0.lora_adapters.1.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.0.lora_adapters.1.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer3.0.lora_adapters.2.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.0.lora_adapters.2.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer3.1.lora_adapters.1.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.1.lora_adapters.1.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer3.1.lora_adapters.2.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.1.lora_adapters.2.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ layer4.0.lora_adapters.1.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.0.lora_adapters.1.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ layer4.0.lora_adapters.2.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.0.lora_adapters.2.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ layer4.1.lora_adapters.1.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.1.lora_adapters.1.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ layer4.1.lora_adapters.2.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.1.lora_adapters.2.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ fc.weight                                          | shape=[6, 1024]\n",
      "  ‚úÖ fc.bias                                            | shape=[6]\n",
      "trainable_count: 58\n",
      "\n",
      "üìä Parameter Statistics:\n",
      "  - Total parameters: 3,953,286\n",
      "  - Trainable parameters: 2,187,270 (55.33%)\n",
      "  - Frozen parameters: 1,766,016 (44.67%)\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "\n",
      "üß† Parameters currently controlled by the optimizer: (110)\n",
      "  ‚õî conv1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer1.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.0.conv2.weight\n",
      "  ‚õî layer1.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer1.0.lora_adapters.1.lora_A\n",
      "  ‚úÖ layer1.0.lora_adapters.1.lora_B\n",
      "  ‚úÖ layer1.0.lora_adapters.2.lora_A\n",
      "  ‚úÖ layer1.0.lora_adapters.2.lora_B\n",
      "  ‚õî layer1.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.conv2.weight\n",
      "  ‚õî layer1.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer1.1.lora_adapters.1.lora_A\n",
      "  ‚úÖ layer1.1.lora_adapters.1.lora_B\n",
      "  ‚úÖ layer1.1.lora_adapters.2.lora_A\n",
      "  ‚úÖ layer1.1.lora_adapters.2.lora_B\n",
      "  ‚õî layer2.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.conv2.weight\n",
      "  ‚õî layer2.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer2.0.lora_adapters.1.lora_A\n",
      "  ‚úÖ layer2.0.lora_adapters.1.lora_B\n",
      "  ‚úÖ layer2.0.lora_adapters.2.lora_A\n",
      "  ‚úÖ layer2.0.lora_adapters.2.lora_B\n",
      "  ‚õî layer2.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.conv2.weight\n",
      "  ‚õî layer2.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer2.1.lora_adapters.1.lora_A\n",
      "  ‚úÖ layer2.1.lora_adapters.1.lora_B\n",
      "  ‚úÖ layer2.1.lora_adapters.2.lora_A\n",
      "  ‚úÖ layer2.1.lora_adapters.2.lora_B\n",
      "  ‚õî layer3.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.conv2.weight\n",
      "  ‚õî layer3.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer3.0.lora_adapters.1.lora_A\n",
      "  ‚úÖ layer3.0.lora_adapters.1.lora_B\n",
      "  ‚úÖ layer3.0.lora_adapters.2.lora_A\n",
      "  ‚úÖ layer3.0.lora_adapters.2.lora_B\n",
      "  ‚õî layer3.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.conv2.weight\n",
      "  ‚õî layer3.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer3.1.lora_adapters.1.lora_A\n",
      "  ‚úÖ layer3.1.lora_adapters.1.lora_B\n",
      "  ‚úÖ layer3.1.lora_adapters.2.lora_A\n",
      "  ‚úÖ layer3.1.lora_adapters.2.lora_B\n",
      "  ‚õî layer4.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.conv2.weight\n",
      "  ‚õî layer4.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer4.0.lora_adapters.1.lora_A\n",
      "  ‚úÖ layer4.0.lora_adapters.1.lora_B\n",
      "  ‚úÖ layer4.0.lora_adapters.2.lora_A\n",
      "  ‚úÖ layer4.0.lora_adapters.2.lora_B\n",
      "  ‚õî layer4.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.conv2.weight\n",
      "  ‚õî layer4.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ layer4.1.lora_adapters.1.lora_A\n",
      "  ‚úÖ layer4.1.lora_adapters.1.lora_B\n",
      "  ‚úÖ layer4.1.lora_adapters.2.lora_A\n",
      "  ‚úÖ layer4.1.lora_adapters.2.lora_B\n",
      "  ‚úÖ fc.weight\n",
      "  ‚úÖ fc.bias\n",
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/200, Train Loss: 8.368740, Train-Class-Acc: {0: '55.99%', 1: '60.28%', 2: '65.34%', 3: '71.21%', 5: '78.62%', 4: '37.34%'}\n",
      "Val Loss: 2.852901, Val Acc: 84.15%, Val-Class-Acc: {0: '78.26%', 1: '81.49%', 2: '84.72%', 3: '87.70%', 4: '80.00%', 5: '87.72%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_1.pth\n",
      "Epoch 2/200, Train Loss: 3.084975, Train-Class-Acc: {0: '76.16%', 1: '73.30%', 2: '83.19%', 3: '88.22%', 4: '69.62%', 5: '86.70%'}\n",
      "Val Loss: 2.336497, Val Acc: 85.87%, Val-Class-Acc: {0: '84.24%', 1: '80.00%', 2: '89.58%', 3: '91.80%', 4: '82.50%', 5: '87.13%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_2.pth\n",
      "Epoch 3/200, Train Loss: 2.130898, Train-Class-Acc: {0: '78.75%', 1: '78.09%', 2: '86.66%', 3: '91.09%', 4: '70.25%', 5: '87.52%'}\n",
      "Val Loss: 2.468403, Val Acc: 85.56%, Val-Class-Acc: {0: '86.96%', 1: '77.61%', 2: '86.81%', 3: '84.43%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_3.pth\n",
      "Epoch 4/200, Train Loss: 1.705314, Train-Class-Acc: {0: '81.88%', 1: '79.96%', 2: '88.04%', 3: '91.91%', 4: '82.28%', 5: '91.78%'}\n",
      "Val Loss: 1.941870, Val Acc: 87.04%, Val-Class-Acc: {0: '85.87%', 1: '80.90%', 2: '88.19%', 3: '93.85%', 4: '82.50%', 5: '88.92%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_4.pth\n",
      "Epoch 5/200, Train Loss: 1.539011, Train-Class-Acc: {0: '84.60%', 1: '82.42%', 2: '89.25%', 3: '93.55%', 4: '77.22%', 5: '90.43%'}\n",
      "Val Loss: 2.378871, Val Acc: 85.71%, Val-Class-Acc: {0: '82.07%', 1: '79.40%', 2: '91.67%', 3: '93.44%', 4: '85.00%', 5: '85.93%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_5.pth\n",
      "Epoch 6/200, Train Loss: 1.305062, Train-Class-Acc: {0: '86.78%', 1: '83.99%', 2: '91.85%', 3: '93.65%', 4: '80.38%', 5: '91.93%'}\n",
      "Val Loss: 2.637847, Val Acc: 86.89%, Val-Class-Acc: {0: '83.15%', 1: '80.90%', 2: '90.97%', 3: '88.93%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_1.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_6.pth\n",
      "Epoch 7/200, Train Loss: 0.950556, Train-Class-Acc: {0: '89.78%', 1: '86.54%', 2: '91.33%', 3: '95.08%', 4: '89.24%', 5: '92.60%'}\n",
      "Val Loss: 2.077667, Val Acc: 87.04%, Val-Class-Acc: {0: '84.24%', 1: '80.60%', 2: '88.19%', 3: '93.85%', 4: '80.00%', 5: '90.42%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_3.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_7.pth\n",
      "Epoch 8/200, Train Loss: 0.745585, Train-Class-Acc: {0: '90.46%', 1: '88.71%', 2: '93.07%', 3: '95.80%', 5: '94.17%', 4: '86.71%'}\n",
      "Val Loss: 2.295989, Val Acc: 86.89%, Val-Class-Acc: {0: '88.59%', 1: '76.12%', 2: '90.28%', 3: '93.03%', 4: '85.00%', 5: '91.02%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_5.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_8.pth\n",
      "Epoch 9/200, Train Loss: 0.648369, Train-Class-Acc: {0: '90.33%', 1: '89.98%', 2: '94.80%', 3: '95.70%', 4: '86.08%', 5: '94.92%'}\n",
      "Val Loss: 2.620615, Val Acc: 87.51%, Val-Class-Acc: {0: '88.59%', 1: '76.72%', 2: '88.89%', 3: '92.21%', 4: '85.00%', 5: '94.01%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_2.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_9.pth\n",
      "Epoch 10/200, Train Loss: 0.621304, Train-Class-Acc: {0: '90.33%', 1: '90.13%', 2: '95.32%', 3: '96.62%', 5: '95.37%', 4: '89.87%'}\n",
      "Val Loss: 2.435788, Val Acc: 87.43%, Val-Class-Acc: {0: '90.22%', 1: '77.91%', 2: '86.11%', 3: '90.57%', 4: '82.50%', 5: '94.31%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_6.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_10.pth\n",
      "Epoch 11/200, Train Loss: 0.543590, Train-Class-Acc: {0: '93.19%', 1: '91.70%', 2: '94.28%', 3: '96.82%', 4: '88.61%', 5: '95.67%'}\n",
      "Val Loss: 2.427737, Val Acc: 87.74%, Val-Class-Acc: {0: '84.78%', 1: '80.90%', 2: '87.50%', 3: '91.80%', 4: '85.00%', 5: '93.71%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_8.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_11.pth\n",
      "Epoch 12/200, Train Loss: 0.623957, Train-Class-Acc: {0: '92.37%', 1: '90.73%', 2: '95.15%', 3: '96.82%', 4: '91.77%', 5: '95.52%'}\n",
      "Val Loss: 2.089336, Val Acc: 86.57%, Val-Class-Acc: {0: '82.61%', 1: '82.39%', 2: '89.58%', 3: '90.98%', 4: '85.00%', 5: '88.62%'}, LR: 0.001000\n",
      "Epoch 13/200, Train Loss: 0.548493, Train-Class-Acc: {0: '92.51%', 1: '91.55%', 2: '95.49%', 3: '97.23%', 5: '94.62%', 4: '87.97%'}\n",
      "Val Loss: 2.295171, Val Acc: 87.43%, Val-Class-Acc: {0: '90.76%', 1: '77.61%', 2: '88.89%', 3: '90.98%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_4.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_13.pth\n",
      "Epoch 14/200, Train Loss: 0.378729, Train-Class-Acc: {0: '93.73%', 1: '92.74%', 2: '95.15%', 3: '96.82%', 4: '92.41%', 5: '97.31%'}\n",
      "Val Loss: 2.505129, Val Acc: 88.06%, Val-Class-Acc: {0: '88.59%', 1: '78.51%', 2: '88.19%', 3: '93.85%', 4: '82.50%', 5: '93.71%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_7.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_14.pth\n",
      "Epoch 15/200, Train Loss: 0.538164, Train-Class-Acc: {0: '94.14%', 1: '92.30%', 2: '96.36%', 3: '96.82%', 4: '88.61%', 5: '96.56%'}\n",
      "Val Loss: 2.800902, Val Acc: 87.35%, Val-Class-Acc: {0: '83.70%', 1: '81.19%', 2: '86.81%', 3: '93.85%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 16/200, Train Loss: 0.351962, Train-Class-Acc: {0: '94.41%', 1: '94.02%', 2: '97.23%', 3: '97.13%', 4: '94.94%', 5: '96.79%'}\n",
      "Val Loss: 2.201158, Val Acc: 85.87%, Val-Class-Acc: {0: '74.46%', 1: '83.28%', 2: '88.19%', 3: '93.03%', 4: '82.50%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 17/200, Train Loss: 0.382683, Train-Class-Acc: {0: '94.96%', 1: '93.72%', 2: '96.71%', 3: '97.64%', 4: '93.04%', 5: '96.64%'}\n",
      "Val Loss: 2.645516, Val Acc: 87.90%, Val-Class-Acc: {0: '92.39%', 1: '77.01%', 2: '85.42%', 3: '92.21%', 4: '82.50%', 5: '94.91%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_10.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_17.pth\n",
      "Epoch 18/200, Train Loss: 0.279530, Train-Class-Acc: {0: '96.32%', 1: '94.02%', 2: '96.01%', 3: '98.36%', 5: '96.86%', 4: '93.04%'}\n",
      "Val Loss: 2.314789, Val Acc: 87.59%, Val-Class-Acc: {0: '85.33%', 1: '79.70%', 2: '89.58%', 3: '95.49%', 4: '80.00%', 5: '91.02%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_13.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_18.pth\n",
      "Epoch 19/200, Train Loss: 0.301545, Train-Class-Acc: {0: '95.91%', 1: '95.44%', 2: '97.57%', 3: '98.16%', 4: '91.14%', 5: '97.61%'}\n",
      "Val Loss: 2.342511, Val Acc: 86.03%, Val-Class-Acc: {0: '84.78%', 1: '79.70%', 2: '86.81%', 3: '96.31%', 4: '82.50%', 5: '85.63%'}, LR: 0.001000\n",
      "Epoch 20/200, Train Loss: 0.484040, Train-Class-Acc: {0: '94.41%', 1: '93.72%', 2: '95.84%', 3: '97.54%', 4: '92.41%', 5: '96.64%'}\n",
      "Val Loss: 2.248609, Val Acc: 87.04%, Val-Class-Acc: {0: '80.43%', 1: '82.99%', 2: '84.03%', 3: '93.44%', 4: '82.50%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 21/200, Train Loss: 0.247793, Train-Class-Acc: {0: '94.82%', 1: '95.21%', 2: '96.71%', 3: '98.77%', 4: '96.20%', 5: '97.68%'}\n",
      "Val Loss: 2.621934, Val Acc: 87.74%, Val-Class-Acc: {0: '92.93%', 1: '78.21%', 2: '85.42%', 3: '92.62%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_9.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_21.pth\n",
      "Epoch 22/200, Train Loss: 0.255767, Train-Class-Acc: {0: '95.50%', 1: '95.59%', 2: '97.75%', 3: '98.16%', 4: '95.57%', 5: '97.68%'}\n",
      "Val Loss: 2.469048, Val Acc: 88.13%, Val-Class-Acc: {0: '86.96%', 1: '81.79%', 2: '87.50%', 3: '92.21%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_18.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_22.pth\n",
      "Epoch 23/200, Train Loss: 0.255605, Train-Class-Acc: {0: '97.68%', 1: '95.89%', 2: '97.92%', 3: '98.46%', 4: '94.94%', 5: '97.53%'}\n",
      "Val Loss: 2.632334, Val Acc: 85.64%, Val-Class-Acc: {0: '78.26%', 1: '79.70%', 2: '87.50%', 3: '93.85%', 4: '82.50%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 24/200, Train Loss: 0.263642, Train-Class-Acc: {0: '95.78%', 1: '94.61%', 2: '98.27%', 3: '98.46%', 4: '94.30%', 5: '97.68%'}\n",
      "Val Loss: 2.510422, Val Acc: 87.20%, Val-Class-Acc: {0: '85.33%', 1: '76.72%', 2: '88.19%', 3: '93.85%', 4: '82.50%', 5: '94.01%'}, LR: 0.001000\n",
      "Epoch 25/200, Train Loss: 0.164598, Train-Class-Acc: {0: '96.87%', 1: '96.41%', 2: '97.75%', 3: '98.36%', 4: '93.67%', 5: '98.51%'}\n",
      "Val Loss: 2.541451, Val Acc: 87.12%, Val-Class-Acc: {0: '77.72%', 1: '82.39%', 2: '89.58%', 3: '92.21%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 26/200, Train Loss: 0.157777, Train-Class-Acc: {0: '96.32%', 1: '96.78%', 2: '98.79%', 3: '98.67%', 4: '94.94%', 5: '98.65%'}\n",
      "Val Loss: 2.429051, Val Acc: 87.67%, Val-Class-Acc: {0: '88.04%', 1: '77.91%', 2: '90.28%', 3: '95.49%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 27/200, Train Loss: 0.131956, Train-Class-Acc: {0: '97.96%', 1: '97.16%', 2: '97.57%', 3: '98.77%', 4: '96.84%', 5: '98.65%'}\n",
      "Val Loss: 2.667630, Val Acc: 87.28%, Val-Class-Acc: {0: '88.04%', 1: '76.72%', 2: '90.28%', 3: '93.44%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 28/200, Train Loss: 0.122526, Train-Class-Acc: {0: '97.14%', 1: '97.16%', 2: '98.09%', 3: '99.39%', 4: '96.84%', 5: '98.80%'}\n",
      "Val Loss: 2.904637, Val Acc: 87.28%, Val-Class-Acc: {0: '85.33%', 1: '78.81%', 2: '88.89%', 3: '95.49%', 4: '80.00%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 29/200, Train Loss: 0.219600, Train-Class-Acc: {0: '97.55%', 1: '96.56%', 2: '97.57%', 3: '97.85%', 4: '96.20%', 5: '98.43%'}\n",
      "Val Loss: 4.093976, Val Acc: 85.40%, Val-Class-Acc: {0: '94.02%', 1: '72.84%', 2: '86.81%', 3: '90.98%', 4: '82.50%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 30/200, Train Loss: 0.311927, Train-Class-Acc: {0: '95.91%', 1: '95.29%', 2: '97.92%', 3: '98.16%', 4: '95.57%', 5: '97.46%'}\n",
      "Val Loss: 2.768805, Val Acc: 87.04%, Val-Class-Acc: {0: '78.80%', 1: '82.69%', 2: '84.72%', 3: '94.26%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 31/200, Train Loss: 0.146449, Train-Class-Acc: {0: '96.87%', 1: '96.93%', 2: '97.92%', 3: '97.95%', 4: '96.84%', 5: '98.80%'}\n",
      "Val Loss: 2.577895, Val Acc: 86.57%, Val-Class-Acc: {0: '85.87%', 1: '76.72%', 2: '92.36%', 3: '95.08%', 4: '85.00%', 5: '88.32%'}, LR: 0.001000\n",
      "Epoch 32/200, Train Loss: 0.119124, Train-Class-Acc: {0: '96.87%', 1: '97.16%', 2: '98.27%', 3: '98.98%', 4: '96.84%', 5: '98.80%'}\n",
      "Val Loss: 2.661923, Val Acc: 87.74%, Val-Class-Acc: {0: '83.70%', 1: '79.40%', 2: '90.97%', 3: '95.90%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 33/200, Train Loss: 0.102744, Train-Class-Acc: {0: '97.68%', 1: '98.13%', 2: '99.83%', 3: '99.18%', 4: '97.47%', 5: '98.65%'}\n",
      "Val Loss: 2.827208, Val Acc: 86.89%, Val-Class-Acc: {0: '85.33%', 1: '77.01%', 2: '90.28%', 3: '95.49%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 34/200, Train Loss: 0.149587, Train-Class-Acc: {0: '97.14%', 1: '96.93%', 2: '98.27%', 3: '98.77%', 4: '96.84%', 5: '98.51%'}\n",
      "Val Loss: 2.676904, Val Acc: 86.26%, Val-Class-Acc: {0: '75.00%', 1: '79.70%', 2: '89.58%', 3: '91.39%', 4: '82.50%', 5: '94.31%'}, LR: 0.001000\n",
      "Epoch 35/200, Train Loss: 0.176963, Train-Class-Acc: {0: '96.19%', 1: '96.34%', 2: '98.44%', 3: '98.77%', 5: '98.65%', 4: '96.20%'}\n",
      "Val Loss: 3.337756, Val Acc: 87.35%, Val-Class-Acc: {0: '89.67%', 1: '77.91%', 2: '88.89%', 3: '89.75%', 4: '85.00%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 36/200, Train Loss: 0.138374, Train-Class-Acc: {0: '97.28%', 1: '97.23%', 2: '98.44%', 3: '98.57%', 4: '96.84%', 5: '98.51%'}\n",
      "Val Loss: 3.371912, Val Acc: 86.73%, Val-Class-Acc: {0: '83.70%', 1: '80.90%', 2: '88.19%', 3: '91.39%', 4: '82.50%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 37/200, Train Loss: 0.181558, Train-Class-Acc: {0: '98.64%', 1: '96.93%', 2: '97.92%', 3: '98.67%', 4: '94.30%', 5: '98.43%'}\n",
      "Val Loss: 3.563557, Val Acc: 86.81%, Val-Class-Acc: {0: '80.43%', 1: '82.09%', 2: '90.97%', 3: '90.16%', 4: '82.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 38/200, Train Loss: 0.143380, Train-Class-Acc: {0: '98.23%', 1: '97.08%', 2: '98.79%', 3: '98.36%', 5: '98.13%', 4: '98.73%'}\n",
      "Val Loss: 3.566191, Val Acc: 86.49%, Val-Class-Acc: {0: '79.35%', 1: '78.51%', 2: '86.81%', 3: '93.85%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 39/200, Train Loss: 0.379581, Train-Class-Acc: {0: '97.96%', 1: '95.51%', 2: '97.40%', 3: '97.75%', 4: '93.04%', 5: '97.68%'}\n",
      "Val Loss: 3.310487, Val Acc: 86.81%, Val-Class-Acc: {0: '91.85%', 1: '79.40%', 2: '83.33%', 3: '92.21%', 4: '80.00%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 40/200, Train Loss: 0.207243, Train-Class-Acc: {0: '96.59%', 1: '96.04%', 2: '97.05%', 3: '98.46%', 4: '96.84%', 5: '98.21%'}\n",
      "Val Loss: 3.996652, Val Acc: 87.67%, Val-Class-Acc: {0: '82.61%', 1: '80.60%', 2: '89.58%', 3: '91.39%', 4: '82.50%', 5: '94.61%'}, LR: 0.001000\n",
      "Epoch 41/200, Train Loss: 0.171210, Train-Class-Acc: {0: '96.59%', 1: '96.71%', 2: '98.44%', 3: '98.98%', 4: '96.20%', 5: '98.43%'}\n",
      "Val Loss: 2.678893, Val Acc: 86.42%, Val-Class-Acc: {0: '85.33%', 1: '82.99%', 2: '80.56%', 3: '90.98%', 4: '82.50%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 42/200, Train Loss: 0.183723, Train-Class-Acc: {0: '97.96%', 1: '97.31%', 2: '97.57%', 3: '98.98%', 4: '96.84%', 5: '98.13%'}\n",
      "Val Loss: 3.091565, Val Acc: 86.81%, Val-Class-Acc: {0: '79.89%', 1: '78.81%', 2: '88.19%', 3: '94.67%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 43/200, Train Loss: 0.250906, Train-Class-Acc: {0: '97.55%', 1: '97.16%', 2: '97.92%', 3: '97.95%', 4: '97.47%', 5: '98.43%'}\n",
      "Val Loss: 5.811944, Val Acc: 85.01%, Val-Class-Acc: {0: '90.22%', 1: '77.61%', 2: '79.17%', 3: '84.43%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 44/200, Train Loss: 0.331012, Train-Class-Acc: {0: '95.23%', 1: '95.59%', 2: '97.05%', 3: '97.64%', 4: '96.20%', 5: '98.13%'}\n",
      "Val Loss: 4.101194, Val Acc: 87.28%, Val-Class-Acc: {0: '82.07%', 1: '81.49%', 2: '84.72%', 3: '93.03%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 45/200, Train Loss: 0.135470, Train-Class-Acc: {0: '97.00%', 1: '96.71%', 2: '97.40%', 3: '98.67%', 4: '96.20%', 5: '98.88%'}\n",
      "Val Loss: 3.108682, Val Acc: 86.89%, Val-Class-Acc: {0: '82.07%', 1: '82.69%', 2: '88.19%', 3: '90.98%', 4: '82.50%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 46/200, Train Loss: 0.164923, Train-Class-Acc: {0: '98.09%', 1: '97.38%', 2: '97.40%', 3: '99.39%', 4: '97.47%', 5: '98.28%'}\n",
      "Val Loss: 3.654424, Val Acc: 87.04%, Val-Class-Acc: {0: '85.87%', 1: '77.91%', 2: '90.97%', 3: '95.08%', 4: '82.50%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 47/200, Train Loss: 0.131525, Train-Class-Acc: {0: '97.28%', 1: '97.38%', 2: '98.61%', 3: '99.18%', 4: '98.10%', 5: '98.58%'}\n",
      "Val Loss: 3.903151, Val Acc: 86.65%, Val-Class-Acc: {0: '81.52%', 1: '82.99%', 2: '84.03%', 3: '89.75%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 48/200, Train Loss: 0.127920, Train-Class-Acc: {0: '97.55%', 1: '97.23%', 2: '98.09%', 3: '98.57%', 4: '96.20%', 5: '98.73%'}\n",
      "Val Loss: 3.594321, Val Acc: 87.59%, Val-Class-Acc: {0: '91.85%', 1: '75.52%', 2: '88.19%', 3: '95.08%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 49/200, Train Loss: 0.129996, Train-Class-Acc: {0: '96.87%', 1: '97.31%', 2: '98.79%', 3: '99.18%', 4: '98.10%', 5: '99.33%'}\n",
      "Val Loss: 3.570336, Val Acc: 87.28%, Val-Class-Acc: {0: '78.26%', 1: '84.48%', 2: '86.81%', 3: '90.98%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 50/200, Train Loss: 0.055522, Train-Class-Acc: {0: '98.77%', 1: '98.73%', 2: '99.48%', 3: '99.28%', 5: '99.25%', 4: '99.37%'}\n",
      "Val Loss: 3.400447, Val Acc: 87.28%, Val-Class-Acc: {0: '83.70%', 1: '80.30%', 2: '88.19%', 3: '92.21%', 4: '80.00%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 51/200, Train Loss: 0.052292, Train-Class-Acc: {0: '98.91%', 1: '98.73%', 2: '99.48%', 3: '99.59%', 4: '98.73%', 5: '99.03%'}\n",
      "Val Loss: 3.263813, Val Acc: 86.03%, Val-Class-Acc: {0: '77.17%', 1: '81.49%', 2: '88.89%', 3: '94.67%', 4: '80.00%', 5: '88.62%'}, LR: 0.001000\n",
      "Epoch 52/200, Train Loss: 0.094932, Train-Class-Acc: {0: '98.23%', 1: '97.38%', 2: '99.13%', 3: '99.28%', 4: '97.47%', 5: '98.88%'}\n",
      "Val Loss: 3.594827, Val Acc: 87.04%, Val-Class-Acc: {0: '84.78%', 1: '78.21%', 2: '87.50%', 3: '94.26%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 53/200, Train Loss: 0.102799, Train-Class-Acc: {0: '97.82%', 1: '98.06%', 2: '98.79%', 3: '99.28%', 4: '98.73%', 5: '99.48%'}\n",
      "Val Loss: 3.626322, Val Acc: 86.57%, Val-Class-Acc: {0: '89.13%', 1: '81.19%', 2: '78.47%', 3: '94.26%', 4: '82.50%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 54/200, Train Loss: 0.208272, Train-Class-Acc: {0: '98.91%', 1: '97.83%', 2: '98.44%', 3: '98.05%', 4: '96.84%', 5: '98.58%'}\n",
      "Val Loss: 4.175878, Val Acc: 85.87%, Val-Class-Acc: {0: '92.39%', 1: '73.43%', 2: '88.19%', 3: '89.75%', 4: '82.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 55/200, Train Loss: 0.127416, Train-Class-Acc: {0: '98.37%', 1: '97.53%', 2: '98.44%', 3: '98.87%', 4: '98.10%', 5: '99.03%'}\n",
      "Val Loss: 3.231891, Val Acc: 86.26%, Val-Class-Acc: {0: '84.78%', 1: '81.19%', 2: '88.19%', 3: '90.16%', 4: '82.50%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 56/200, Train Loss: 0.116522, Train-Class-Acc: {0: '98.09%', 1: '97.91%', 2: '98.27%', 3: '99.39%', 4: '98.73%', 5: '98.95%'}\n",
      "Val Loss: 3.636840, Val Acc: 86.57%, Val-Class-Acc: {0: '84.24%', 1: '78.51%', 2: '83.33%', 3: '92.21%', 4: '82.50%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 57/200, Train Loss: 0.138499, Train-Class-Acc: {0: '98.91%', 1: '98.13%', 2: '98.96%', 3: '98.57%', 4: '98.10%', 5: '98.65%'}\n",
      "Val Loss: 3.744370, Val Acc: 85.79%, Val-Class-Acc: {0: '81.52%', 1: '77.91%', 2: '84.03%', 3: '95.90%', 4: '82.50%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 58/200, Train Loss: 0.103121, Train-Class-Acc: {0: '97.82%', 1: '97.83%', 2: '98.61%', 3: '98.87%', 5: '99.25%', 4: '95.57%'}\n",
      "Val Loss: 3.450305, Val Acc: 87.04%, Val-Class-Acc: {0: '87.50%', 1: '77.01%', 2: '88.19%', 3: '93.03%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 59/200, Train Loss: 0.084487, Train-Class-Acc: {0: '98.50%', 1: '98.35%', 2: '98.96%', 3: '99.18%', 4: '99.37%', 5: '99.70%'}\n",
      "Val Loss: 3.103074, Val Acc: 87.43%, Val-Class-Acc: {0: '85.87%', 1: '78.81%', 2: '92.36%', 3: '93.44%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 60/200, Train Loss: 0.068260, Train-Class-Acc: {0: '99.05%', 1: '98.58%', 2: '98.44%', 3: '99.49%', 4: '97.47%', 5: '99.48%'}\n",
      "Val Loss: 3.360498, Val Acc: 87.51%, Val-Class-Acc: {0: '86.41%', 1: '78.81%', 2: '89.58%', 3: '93.44%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 61/200, Train Loss: 0.126041, Train-Class-Acc: {0: '97.00%', 1: '98.06%', 2: '98.96%', 3: '99.39%', 4: '97.47%', 5: '99.10%'}\n",
      "Val Loss: 4.131728, Val Acc: 87.12%, Val-Class-Acc: {0: '84.78%', 1: '81.19%', 2: '86.11%', 3: '92.62%', 4: '82.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 62/200, Train Loss: 0.064871, Train-Class-Acc: {0: '98.64%', 1: '98.28%', 2: '99.31%', 3: '99.69%', 4: '97.47%', 5: '99.10%'}\n",
      "Val Loss: 3.554425, Val Acc: 87.28%, Val-Class-Acc: {0: '79.89%', 1: '81.49%', 2: '91.67%', 3: '91.80%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 63/200, Train Loss: 0.092837, Train-Class-Acc: {0: '98.50%', 1: '98.13%', 2: '99.13%', 3: '99.80%', 5: '98.88%', 4: '96.84%'}\n",
      "Val Loss: 2.981192, Val Acc: 86.96%, Val-Class-Acc: {0: '84.78%', 1: '79.70%', 2: '90.97%', 3: '94.26%', 4: '82.50%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 64/200, Train Loss: 0.050228, Train-Class-Acc: {0: '98.64%', 1: '98.20%', 2: '98.96%', 3: '99.59%', 4: '99.37%', 5: '99.33%'}\n",
      "Val Loss: 3.675845, Val Acc: 87.04%, Val-Class-Acc: {0: '87.50%', 1: '78.21%', 2: '81.25%', 3: '92.62%', 4: '82.50%', 5: '94.61%'}, LR: 0.001000\n",
      "Epoch 65/200, Train Loss: 0.071639, Train-Class-Acc: {0: '98.23%', 1: '98.43%', 2: '98.44%', 3: '99.28%', 4: '97.47%', 5: '99.48%'}\n",
      "Val Loss: 3.330338, Val Acc: 86.18%, Val-Class-Acc: {0: '77.17%', 1: '80.60%', 2: '88.89%', 3: '92.21%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 66/200, Train Loss: 0.034326, Train-Class-Acc: {0: '99.32%', 1: '99.10%', 2: '99.13%', 3: '99.69%', 4: '97.47%', 5: '99.63%'}\n",
      "Val Loss: 3.478296, Val Acc: 87.20%, Val-Class-Acc: {0: '81.52%', 1: '80.60%', 2: '90.28%', 3: '92.21%', 4: '85.00%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 67/200, Train Loss: 0.043883, Train-Class-Acc: {0: '98.91%', 1: '99.03%', 2: '99.48%', 3: '99.69%', 5: '99.33%', 4: '98.73%'}\n",
      "Val Loss: 4.457766, Val Acc: 86.42%, Val-Class-Acc: {0: '90.22%', 1: '74.63%', 2: '77.08%', 3: '95.90%', 4: '82.50%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 68/200, Train Loss: 0.221271, Train-Class-Acc: {0: '94.55%', 1: '96.11%', 2: '97.75%', 3: '98.46%', 4: '97.47%', 5: '98.73%'}\n",
      "Val Loss: 3.632525, Val Acc: 86.42%, Val-Class-Acc: {0: '72.28%', 1: '82.09%', 2: '84.72%', 3: '92.21%', 4: '85.00%', 5: '95.21%'}, LR: 0.001000\n",
      "Epoch 69/200, Train Loss: 0.156436, Train-Class-Acc: {0: '97.96%', 1: '97.38%', 2: '97.75%', 3: '99.28%', 4: '98.10%', 5: '98.58%'}\n",
      "Val Loss: 3.482593, Val Acc: 87.98%, Val-Class-Acc: {0: '85.87%', 1: '77.91%', 2: '89.58%', 3: '94.26%', 4: '85.00%', 5: '94.31%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_11.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_69.pth\n",
      "Epoch 70/200, Train Loss: 0.051501, Train-Class-Acc: {0: '98.91%', 1: '98.73%', 2: '99.31%', 3: '99.28%', 4: '98.73%', 5: '99.48%'}\n",
      "Val Loss: 3.698562, Val Acc: 86.26%, Val-Class-Acc: {0: '79.35%', 1: '81.79%', 2: '83.33%', 3: '94.26%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 71/200, Train Loss: 0.076700, Train-Class-Acc: {0: '98.77%', 1: '98.28%', 2: '99.13%', 3: '98.98%', 4: '98.10%', 5: '99.40%'}\n",
      "Val Loss: 3.499023, Val Acc: 87.51%, Val-Class-Acc: {0: '78.80%', 1: '82.99%', 2: '86.11%', 3: '93.44%', 4: '82.50%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 72/200, Train Loss: 0.051712, Train-Class-Acc: {0: '98.37%', 1: '98.80%', 2: '99.48%', 3: '99.80%', 4: '98.10%', 5: '99.10%'}\n",
      "Val Loss: 3.654659, Val Acc: 87.20%, Val-Class-Acc: {0: '83.70%', 1: '80.90%', 2: '90.28%', 3: '95.08%', 4: '82.50%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 73/200, Train Loss: 0.091704, Train-Class-Acc: {0: '98.77%', 1: '98.73%', 2: '98.79%', 3: '99.28%', 4: '98.73%', 5: '99.18%'}\n",
      "Val Loss: 4.191612, Val Acc: 85.71%, Val-Class-Acc: {0: '88.04%', 1: '75.82%', 2: '88.19%', 3: '94.26%', 4: '82.50%', 5: '87.43%'}, LR: 0.001000\n",
      "Epoch 74/200, Train Loss: 0.257408, Train-Class-Acc: {0: '97.14%', 1: '97.01%', 2: '98.27%', 3: '97.75%', 5: '98.28%', 4: '95.57%'}\n",
      "Val Loss: 3.794883, Val Acc: 84.93%, Val-Class-Acc: {0: '71.20%', 1: '82.09%', 2: '90.97%', 3: '89.34%', 4: '82.50%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 75/200, Train Loss: 0.117065, Train-Class-Acc: {0: '97.28%', 1: '96.78%', 2: '98.96%', 3: '98.57%', 4: '96.84%', 5: '99.03%'}\n",
      "Val Loss: 3.538262, Val Acc: 86.42%, Val-Class-Acc: {0: '85.87%', 1: '77.91%', 2: '88.89%', 3: '91.80%', 4: '85.00%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 76/200, Train Loss: 0.189193, Train-Class-Acc: {0: '98.09%', 1: '98.06%', 2: '98.79%', 3: '98.98%', 4: '96.84%', 5: '98.51%'}\n",
      "Val Loss: 4.474330, Val Acc: 83.84%, Val-Class-Acc: {0: '79.89%', 1: '78.81%', 2: '84.03%', 3: '81.15%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 77/200, Train Loss: 0.408749, Train-Class-Acc: {0: '95.64%', 1: '95.44%', 2: '97.23%', 3: '97.34%', 5: '97.23%', 4: '95.57%'}\n",
      "Val Loss: 4.610503, Val Acc: 86.26%, Val-Class-Acc: {0: '87.50%', 1: '71.04%', 2: '88.19%', 3: '95.49%', 4: '82.50%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 78/200, Train Loss: 0.181899, Train-Class-Acc: {0: '98.91%', 1: '97.16%', 2: '98.44%', 3: '98.77%', 5: '98.80%', 4: '98.10%'}\n",
      "Val Loss: 3.353450, Val Acc: 86.57%, Val-Class-Acc: {0: '76.09%', 1: '80.90%', 2: '86.81%', 3: '94.67%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 79/200, Train Loss: 0.076786, Train-Class-Acc: {0: '97.82%', 1: '97.98%', 2: '98.96%', 3: '99.39%', 4: '97.47%', 5: '99.48%'}\n",
      "Val Loss: 3.220137, Val Acc: 85.71%, Val-Class-Acc: {0: '80.98%', 1: '80.00%', 2: '82.64%', 3: '91.80%', 4: '85.00%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 80/200, Train Loss: 0.056252, Train-Class-Acc: {0: '98.91%', 1: '98.73%', 2: '99.48%', 3: '99.28%', 4: '99.37%', 5: '99.40%'}\n",
      "Val Loss: 3.690583, Val Acc: 86.34%, Val-Class-Acc: {0: '79.35%', 1: '80.60%', 2: '82.64%', 3: '94.67%', 4: '82.50%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 81/200, Train Loss: 0.035591, Train-Class-Acc: {0: '98.91%', 1: '99.25%', 2: '99.65%', 3: '99.90%', 4: '98.10%', 5: '99.70%'}\n",
      "Val Loss: 3.410257, Val Acc: 85.09%, Val-Class-Acc: {0: '75.54%', 1: '81.49%', 2: '81.25%', 3: '92.62%', 4: '85.00%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 82/200, Train Loss: 0.032215, Train-Class-Acc: {0: '99.18%', 1: '99.25%', 2: '99.31%', 3: '99.69%', 4: '99.37%', 5: '99.48%'}\n",
      "Val Loss: 3.856233, Val Acc: 86.42%, Val-Class-Acc: {0: '90.76%', 1: '74.03%', 2: '83.33%', 3: '94.26%', 4: '85.00%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 83/200, Train Loss: 0.034420, Train-Class-Acc: {0: '99.05%', 1: '99.33%', 2: '99.65%', 3: '99.39%', 5: '99.63%', 4: '100.00%'}\n",
      "Val Loss: 3.384880, Val Acc: 86.49%, Val-Class-Acc: {0: '88.04%', 1: '77.31%', 2: '85.42%', 3: '94.26%', 4: '85.00%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 84/200, Train Loss: 0.028719, Train-Class-Acc: {0: '99.59%', 1: '99.18%', 2: '99.48%', 3: '99.39%', 4: '100.00%', 5: '99.70%'}\n",
      "Val Loss: 3.289401, Val Acc: 87.12%, Val-Class-Acc: {0: '83.70%', 1: '81.19%', 2: '82.64%', 3: '93.44%', 4: '85.00%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 85/200, Train Loss: 0.021902, Train-Class-Acc: {0: '99.59%', 1: '99.03%', 2: '99.83%', 3: '99.80%', 4: '98.73%', 5: '99.55%'}\n",
      "Val Loss: 3.313647, Val Acc: 87.35%, Val-Class-Acc: {0: '82.07%', 1: '78.81%', 2: '83.33%', 3: '94.26%', 4: '85.00%', 5: '95.81%'}, LR: 0.001000\n",
      "Epoch 86/200, Train Loss: 0.046380, Train-Class-Acc: {0: '99.18%', 1: '98.65%', 2: '99.13%', 3: '99.90%', 4: '100.00%', 5: '99.40%'}\n",
      "Val Loss: 3.329191, Val Acc: 87.28%, Val-Class-Acc: {0: '84.78%', 1: '78.51%', 2: '84.72%', 3: '95.08%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 87/200, Train Loss: 0.037129, Train-Class-Acc: {0: '99.32%', 1: '99.18%', 2: '98.96%', 3: '99.69%', 4: '98.10%', 5: '99.48%'}\n",
      "Val Loss: 3.598854, Val Acc: 86.89%, Val-Class-Acc: {0: '86.41%', 1: '74.33%', 2: '84.03%', 3: '94.26%', 4: '85.00%', 5: '95.81%'}, LR: 0.001000\n",
      "Epoch 88/200, Train Loss: 0.043798, Train-Class-Acc: {0: '99.59%', 1: '99.18%', 2: '99.13%', 3: '99.59%', 4: '98.10%', 5: '99.70%'}\n",
      "Val Loss: 3.522787, Val Acc: 87.28%, Val-Class-Acc: {0: '87.50%', 1: '77.91%', 2: '86.11%', 3: '94.26%', 4: '85.00%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 89/200, Train Loss: 0.033171, Train-Class-Acc: {0: '99.18%', 1: '98.80%', 2: '99.31%', 3: '99.80%', 4: '99.37%', 5: '99.70%'}\n",
      "Val Loss: 3.832163, Val Acc: 87.43%, Val-Class-Acc: {0: '84.78%', 1: '81.79%', 2: '86.11%', 3: '90.57%', 4: '85.00%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 90/200, Train Loss: 0.193957, Train-Class-Acc: {0: '98.77%', 1: '98.28%', 2: '98.96%', 3: '98.98%', 4: '98.73%', 5: '98.88%'}\n",
      "Val Loss: 5.040012, Val Acc: 85.32%, Val-Class-Acc: {0: '85.87%', 1: '76.42%', 2: '84.03%', 3: '89.75%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 91/200, Train Loss: 0.209523, Train-Class-Acc: {0: '97.14%', 1: '97.31%', 2: '97.92%', 3: '98.67%', 5: '98.73%', 4: '98.10%'}\n",
      "Val Loss: 3.736148, Val Acc: 86.49%, Val-Class-Acc: {0: '79.89%', 1: '81.79%', 2: '79.86%', 3: '93.03%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 92/200, Train Loss: 0.240087, Train-Class-Acc: {0: '97.41%', 1: '96.86%', 2: '98.27%', 3: '98.57%', 4: '96.84%', 5: '98.51%'}\n",
      "Val Loss: 4.062750, Val Acc: 86.81%, Val-Class-Acc: {0: '85.87%', 1: '77.31%', 2: '84.72%', 3: '92.62%', 4: '85.00%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 93/200, Train Loss: 0.245993, Train-Class-Acc: {0: '96.19%', 1: '97.61%', 2: '98.27%', 3: '98.57%', 4: '98.10%', 5: '98.73%'}\n",
      "Val Loss: 3.059559, Val Acc: 85.25%, Val-Class-Acc: {0: '82.07%', 1: '80.60%', 2: '81.94%', 3: '93.85%', 4: '82.50%', 5: '87.13%'}, LR: 0.001000\n",
      "Epoch 94/200, Train Loss: 0.389803, Train-Class-Acc: {0: '97.00%', 1: '97.01%', 2: '98.27%', 3: '97.54%', 4: '96.20%', 5: '97.76%'}\n",
      "Val Loss: 3.551581, Val Acc: 85.32%, Val-Class-Acc: {0: '81.52%', 1: '79.10%', 2: '82.64%', 3: '88.93%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 95/200, Train Loss: 0.159150, Train-Class-Acc: {0: '98.23%', 1: '97.31%', 2: '98.27%', 3: '98.77%', 5: '98.58%', 4: '97.47%'}\n",
      "Val Loss: 4.519370, Val Acc: 84.93%, Val-Class-Acc: {0: '69.57%', 1: '76.42%', 2: '88.89%', 3: '94.26%', 4: '82.50%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 96/200, Train Loss: 0.049761, Train-Class-Acc: {0: '98.77%', 1: '99.10%', 2: '98.96%', 3: '99.39%', 4: '98.73%', 5: '99.55%'}\n",
      "Val Loss: 4.016904, Val Acc: 86.81%, Val-Class-Acc: {0: '82.61%', 1: '79.40%', 2: '86.11%', 3: '92.62%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 97/200, Train Loss: 0.019510, Train-Class-Acc: {0: '99.05%', 1: '99.33%', 2: '99.65%', 3: '99.90%', 4: '99.37%', 5: '99.70%'}\n",
      "Val Loss: 3.934460, Val Acc: 86.03%, Val-Class-Acc: {0: '75.54%', 1: '80.60%', 2: '87.50%', 3: '93.44%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 98/200, Train Loss: 0.453688, Train-Class-Acc: {0: '95.78%', 1: '96.26%', 2: '97.57%', 3: '98.26%', 4: '96.84%', 5: '98.13%'}\n",
      "Val Loss: 4.190983, Val Acc: 86.65%, Val-Class-Acc: {0: '88.04%', 1: '74.33%', 2: '85.42%', 3: '94.26%', 4: '82.50%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 99/200, Train Loss: 0.158807, Train-Class-Acc: {0: '98.37%', 1: '97.61%', 2: '98.09%', 3: '98.67%', 4: '98.10%', 5: '99.03%'}\n",
      "Val Loss: 3.528125, Val Acc: 85.25%, Val-Class-Acc: {0: '80.98%', 1: '81.49%', 2: '83.33%', 3: '90.98%', 4: '82.50%', 5: '88.32%'}, LR: 0.001000\n",
      "Epoch 100/200, Train Loss: 0.133278, Train-Class-Acc: {0: '98.09%', 1: '98.06%', 2: '99.65%', 3: '99.28%', 4: '98.73%', 5: '99.03%'}\n",
      "Val Loss: 3.274905, Val Acc: 86.18%, Val-Class-Acc: {0: '86.41%', 1: '80.00%', 2: '88.19%', 3: '89.34%', 4: '80.00%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 101/200, Train Loss: 0.128814, Train-Class-Acc: {0: '98.09%', 1: '97.83%', 2: '99.13%', 3: '99.28%', 4: '96.84%', 5: '97.98%'}\n",
      "Val Loss: 3.820141, Val Acc: 86.57%, Val-Class-Acc: {0: '78.80%', 1: '82.09%', 2: '83.33%', 3: '90.98%', 4: '82.50%', 5: '94.01%'}, LR: 0.001000\n",
      "Epoch 102/200, Train Loss: 0.066616, Train-Class-Acc: {0: '98.64%', 1: '98.80%', 2: '98.96%', 3: '99.49%', 4: '98.10%', 5: '99.48%'}\n",
      "Val Loss: 3.630593, Val Acc: 86.18%, Val-Class-Acc: {0: '81.52%', 1: '80.00%', 2: '81.25%', 3: '93.85%', 4: '82.50%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 103/200, Train Loss: 0.035446, Train-Class-Acc: {0: '99.32%', 1: '98.88%', 2: '98.96%', 3: '99.59%', 5: '99.85%', 4: '98.10%'}\n",
      "Val Loss: 3.989121, Val Acc: 86.57%, Val-Class-Acc: {0: '75.00%', 1: '83.58%', 2: '85.42%', 3: '93.03%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 104/200, Train Loss: 0.046035, Train-Class-Acc: {0: '99.18%', 1: '99.40%', 2: '99.48%', 3: '99.69%', 4: '97.47%', 5: '99.33%'}\n",
      "Val Loss: 4.315409, Val Acc: 86.96%, Val-Class-Acc: {0: '84.24%', 1: '83.58%', 2: '87.50%', 3: '88.11%', 4: '82.50%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 105/200, Train Loss: 0.063203, Train-Class-Acc: {0: '98.50%', 1: '98.95%', 2: '99.65%', 3: '99.69%', 4: '99.37%', 5: '99.25%'}\n",
      "Val Loss: 4.418002, Val Acc: 86.18%, Val-Class-Acc: {0: '75.54%', 1: '80.00%', 2: '84.72%', 3: '95.08%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 106/200, Train Loss: 0.042819, Train-Class-Acc: {0: '98.91%', 1: '99.10%', 2: '99.65%', 3: '99.59%', 5: '99.63%', 4: '100.00%'}\n",
      "Val Loss: 4.729153, Val Acc: 86.42%, Val-Class-Acc: {0: '86.96%', 1: '76.42%', 2: '79.86%', 3: '94.26%', 4: '82.50%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 107/200, Train Loss: 0.042748, Train-Class-Acc: {0: '99.18%', 1: '99.03%', 2: '99.13%', 3: '99.59%', 4: '99.37%', 5: '99.63%'}\n",
      "Val Loss: 4.113275, Val Acc: 86.73%, Val-Class-Acc: {0: '86.41%', 1: '76.72%', 2: '85.42%', 3: '94.26%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 108/200, Train Loss: 0.017793, Train-Class-Acc: {0: '99.46%', 1: '99.33%', 2: '99.65%', 3: '99.80%', 4: '98.10%', 5: '99.70%'}\n",
      "Val Loss: 4.452609, Val Acc: 86.18%, Val-Class-Acc: {0: '85.87%', 1: '78.21%', 2: '87.50%', 3: '94.67%', 4: '82.50%', 5: '88.02%'}, LR: 0.001000\n",
      "Epoch 109/200, Train Loss: 0.025513, Train-Class-Acc: {0: '99.46%', 1: '99.40%', 2: '99.83%', 3: '99.80%', 4: '96.84%', 5: '99.85%'}\n",
      "Val Loss: 3.858576, Val Acc: 86.26%, Val-Class-Acc: {0: '73.91%', 1: '81.49%', 2: '85.42%', 3: '94.67%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 110/200, Train Loss: 0.022444, Train-Class-Acc: {0: '99.18%', 1: '99.40%', 2: '99.65%', 3: '99.90%', 4: '100.00%', 5: '99.93%'}\n",
      "Val Loss: 4.165293, Val Acc: 86.18%, Val-Class-Acc: {0: '86.41%', 1: '76.42%', 2: '84.72%', 3: '95.49%', 4: '82.50%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 111/200, Train Loss: 0.024497, Train-Class-Acc: {0: '99.46%', 1: '99.63%', 2: '99.65%', 3: '99.80%', 4: '100.00%', 5: '99.85%'}\n",
      "Val Loss: 3.987111, Val Acc: 86.42%, Val-Class-Acc: {0: '81.52%', 1: '77.91%', 2: '89.58%', 3: '91.80%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 112/200, Train Loss: 0.018921, Train-Class-Acc: {0: '99.46%', 1: '99.33%', 2: '99.83%', 3: '99.90%', 4: '98.73%', 5: '99.70%'}\n",
      "Val Loss: 3.932855, Val Acc: 86.34%, Val-Class-Acc: {0: '84.78%', 1: '79.40%', 2: '86.81%', 3: '89.34%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 113/200, Train Loss: 0.049620, Train-Class-Acc: {0: '99.18%', 1: '99.40%', 2: '99.13%', 3: '99.69%', 4: '100.00%', 5: '99.48%'}\n",
      "Val Loss: 3.525698, Val Acc: 86.18%, Val-Class-Acc: {0: '86.96%', 1: '78.81%', 2: '88.89%', 3: '91.39%', 4: '82.50%', 5: '88.62%'}, LR: 0.001000\n",
      "Epoch 114/200, Train Loss: 0.028346, Train-Class-Acc: {0: '99.59%', 1: '99.10%', 2: '99.48%', 3: '99.49%', 4: '97.47%', 5: '99.85%'}\n",
      "Val Loss: 4.289113, Val Acc: 86.18%, Val-Class-Acc: {0: '75.54%', 1: '80.60%', 2: '87.50%', 3: '93.85%', 4: '82.50%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 115/200, Train Loss: 0.021657, Train-Class-Acc: {0: '99.46%', 1: '99.40%', 2: '99.83%', 3: '100.00%', 4: '99.37%', 5: '99.70%'}\n",
      "Val Loss: 4.218404, Val Acc: 87.35%, Val-Class-Acc: {0: '82.07%', 1: '79.10%', 2: '86.81%', 3: '94.67%', 4: '82.50%', 5: '94.01%'}, LR: 0.001000\n",
      "Epoch 116/200, Train Loss: 0.059896, Train-Class-Acc: {0: '98.91%', 1: '99.10%', 2: '98.44%', 3: '99.49%', 4: '99.37%', 5: '99.55%'}\n",
      "Val Loss: 3.952049, Val Acc: 86.89%, Val-Class-Acc: {0: '84.24%', 1: '77.61%', 2: '90.28%', 3: '93.85%', 4: '85.00%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 117/200, Train Loss: 0.031334, Train-Class-Acc: {0: '98.91%', 1: '99.03%', 2: '99.31%', 3: '99.80%', 5: '99.78%', 4: '98.73%'}\n",
      "Val Loss: 3.832066, Val Acc: 86.96%, Val-Class-Acc: {0: '86.41%', 1: '79.10%', 2: '84.03%', 3: '95.08%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 118/200, Train Loss: 0.018480, Train-Class-Acc: {0: '99.73%', 1: '99.48%', 2: '99.48%', 3: '99.59%', 4: '99.37%', 5: '100.00%'}\n",
      "Val Loss: 3.724838, Val Acc: 86.96%, Val-Class-Acc: {0: '83.15%', 1: '80.30%', 2: '86.81%', 3: '94.67%', 4: '85.00%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 119/200, Train Loss: 0.111058, Train-Class-Acc: {0: '99.32%', 1: '98.88%', 2: '99.13%', 3: '98.98%', 4: '98.10%', 5: '99.18%'}\n",
      "Val Loss: 3.444372, Val Acc: 86.73%, Val-Class-Acc: {0: '85.33%', 1: '77.91%', 2: '86.11%', 3: '93.44%', 4: '87.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 120/200, Train Loss: 0.129962, Train-Class-Acc: {0: '97.96%', 1: '98.06%', 2: '98.61%', 3: '99.18%', 4: '98.10%', 5: '99.18%'}\n",
      "Val Loss: 3.698318, Val Acc: 86.73%, Val-Class-Acc: {0: '83.70%', 1: '80.90%', 2: '86.81%', 3: '90.16%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 121/200, Train Loss: 0.076106, Train-Class-Acc: {0: '98.23%', 1: '98.88%', 2: '99.31%', 3: '99.28%', 4: '99.37%', 5: '99.55%'}\n",
      "Val Loss: 4.480174, Val Acc: 84.47%, Val-Class-Acc: {0: '71.74%', 1: '78.51%', 2: '86.11%', 3: '94.67%', 4: '82.50%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 122/200, Train Loss: 0.079419, Train-Class-Acc: {0: '98.23%', 1: '98.20%', 2: '98.61%', 3: '99.49%', 4: '99.37%', 5: '99.55%'}\n",
      "Val Loss: 4.298218, Val Acc: 85.79%, Val-Class-Acc: {0: '78.26%', 1: '79.10%', 2: '90.28%', 3: '92.21%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 123/200, Train Loss: 0.064779, Train-Class-Acc: {0: '99.05%', 1: '98.50%', 2: '99.48%', 3: '99.49%', 4: '97.47%', 5: '99.55%'}\n",
      "Val Loss: 3.960493, Val Acc: 85.48%, Val-Class-Acc: {0: '73.37%', 1: '82.69%', 2: '88.19%', 3: '91.80%', 4: '82.50%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 124/200, Train Loss: 0.046017, Train-Class-Acc: {0: '98.77%', 1: '99.33%', 2: '99.13%', 3: '99.80%', 4: '100.00%', 5: '99.33%'}\n",
      "Val Loss: 3.748743, Val Acc: 86.34%, Val-Class-Acc: {0: '73.91%', 1: '79.70%', 2: '86.81%', 3: '92.62%', 4: '82.50%', 5: '95.51%'}, LR: 0.001000\n",
      "Epoch 125/200, Train Loss: 0.032770, Train-Class-Acc: {0: '98.50%', 1: '98.95%', 2: '99.83%', 3: '99.80%', 4: '100.00%', 5: '99.55%'}\n",
      "Val Loss: 3.786961, Val Acc: 86.57%, Val-Class-Acc: {0: '86.96%', 1: '76.72%', 2: '84.03%', 3: '91.39%', 4: '82.50%', 5: '94.31%'}, LR: 0.001000\n",
      "Epoch 126/200, Train Loss: 0.096473, Train-Class-Acc: {0: '98.37%', 1: '98.88%', 2: '98.61%', 3: '99.28%', 4: '99.37%', 5: '99.78%'}\n",
      "Val Loss: 3.886245, Val Acc: 86.42%, Val-Class-Acc: {0: '82.07%', 1: '79.40%', 2: '89.58%', 3: '92.62%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 127/200, Train Loss: 0.275554, Train-Class-Acc: {0: '98.64%', 1: '97.31%', 2: '98.09%', 3: '98.36%', 4: '97.47%', 5: '98.43%'}\n",
      "Val Loss: 4.502764, Val Acc: 85.64%, Val-Class-Acc: {0: '80.98%', 1: '77.01%', 2: '84.03%', 3: '93.44%', 4: '80.00%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 128/200, Train Loss: 0.184367, Train-Class-Acc: {0: '97.41%', 1: '97.61%', 2: '98.79%', 3: '98.77%', 4: '96.84%', 5: '98.43%'}\n",
      "Val Loss: 4.180949, Val Acc: 86.89%, Val-Class-Acc: {0: '82.07%', 1: '79.70%', 2: '86.81%', 3: '92.62%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 129/200, Train Loss: 0.075815, Train-Class-Acc: {0: '98.77%', 1: '98.73%', 2: '98.79%', 3: '99.59%', 4: '98.73%', 5: '99.48%'}\n",
      "Val Loss: 4.135000, Val Acc: 86.81%, Val-Class-Acc: {0: '90.22%', 1: '76.42%', 2: '88.19%', 3: '93.03%', 4: '80.00%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 130/200, Train Loss: 0.077548, Train-Class-Acc: {0: '98.50%', 1: '98.73%', 2: '99.31%', 3: '99.69%', 5: '99.10%', 4: '98.10%'}\n",
      "Val Loss: 3.996474, Val Acc: 86.89%, Val-Class-Acc: {0: '83.70%', 1: '76.42%', 2: '86.81%', 3: '91.80%', 4: '82.50%', 5: '96.11%'}, LR: 0.001000\n",
      "Epoch 131/200, Train Loss: 0.052839, Train-Class-Acc: {0: '98.91%', 1: '99.03%', 2: '99.48%', 3: '99.80%', 4: '99.37%', 5: '99.63%'}\n",
      "Val Loss: 3.484459, Val Acc: 86.96%, Val-Class-Acc: {0: '80.98%', 1: '83.28%', 2: '88.89%', 3: '91.39%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 132/200, Train Loss: 0.223616, Train-Class-Acc: {0: '97.82%', 1: '97.76%', 2: '98.79%', 3: '99.28%', 4: '98.10%', 5: '98.58%'}\n",
      "Val Loss: 5.170837, Val Acc: 85.32%, Val-Class-Acc: {0: '76.09%', 1: '78.21%', 2: '83.33%', 3: '87.70%', 4: '80.00%', 5: '97.31%'}, LR: 0.001000\n",
      "Epoch 133/200, Train Loss: 0.172114, Train-Class-Acc: {0: '97.68%', 1: '97.16%', 2: '98.44%', 3: '98.26%', 4: '94.94%', 5: '98.36%'}\n",
      "Val Loss: 3.977074, Val Acc: 86.03%, Val-Class-Acc: {0: '83.70%', 1: '77.31%', 2: '80.56%', 3: '92.21%', 4: '82.50%', 5: '94.31%'}, LR: 0.001000\n",
      "Epoch 134/200, Train Loss: 0.067379, Train-Class-Acc: {0: '99.18%', 1: '98.35%', 2: '99.48%', 3: '99.49%', 4: '98.73%', 5: '99.40%'}\n",
      "Val Loss: 3.920511, Val Acc: 87.20%, Val-Class-Acc: {0: '80.98%', 1: '77.01%', 2: '89.58%', 3: '93.85%', 4: '82.50%', 5: '95.51%'}, LR: 0.001000\n",
      "Epoch 135/200, Train Loss: 0.065194, Train-Class-Acc: {0: '98.50%', 1: '98.73%', 2: '98.79%', 3: '99.59%', 4: '99.37%', 5: '99.85%'}\n",
      "Val Loss: 3.660622, Val Acc: 86.65%, Val-Class-Acc: {0: '78.26%', 1: '79.40%', 2: '89.58%', 3: '93.44%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 136/200, Train Loss: 0.033450, Train-Class-Acc: {0: '99.18%', 1: '99.25%', 2: '99.65%', 3: '99.80%', 4: '99.37%', 5: '99.48%'}\n",
      "Val Loss: 3.435496, Val Acc: 87.35%, Val-Class-Acc: {0: '87.50%', 1: '78.51%', 2: '86.11%', 3: '93.85%', 4: '85.00%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 137/200, Train Loss: 0.063324, Train-Class-Acc: {0: '99.18%', 1: '99.10%', 2: '99.48%', 3: '99.08%', 4: '98.73%', 5: '99.25%'}\n",
      "Val Loss: 4.311930, Val Acc: 86.10%, Val-Class-Acc: {0: '82.07%', 1: '78.21%', 2: '89.58%', 3: '92.62%', 4: '87.50%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 138/200, Train Loss: 0.075115, Train-Class-Acc: {0: '98.09%', 1: '98.43%', 2: '99.65%', 3: '99.59%', 4: '98.73%', 5: '99.63%'}\n",
      "Val Loss: 4.290777, Val Acc: 85.71%, Val-Class-Acc: {0: '88.04%', 1: '74.63%', 2: '86.11%', 3: '92.21%', 4: '85.00%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 139/200, Train Loss: 0.082481, Train-Class-Acc: {0: '99.18%', 1: '98.65%', 2: '98.96%', 3: '99.08%', 4: '98.10%', 5: '99.33%'}\n",
      "Val Loss: 3.738681, Val Acc: 85.64%, Val-Class-Acc: {0: '84.78%', 1: '78.51%', 2: '80.56%', 3: '94.26%', 4: '87.50%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 140/200, Train Loss: 0.037272, Train-Class-Acc: {0: '99.46%', 1: '99.10%', 2: '99.48%', 3: '99.49%', 5: '99.70%', 4: '99.37%'}\n",
      "Val Loss: 3.938639, Val Acc: 86.42%, Val-Class-Acc: {0: '81.52%', 1: '80.00%', 2: '87.50%', 3: '91.80%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 141/200, Train Loss: 0.024533, Train-Class-Acc: {0: '99.73%', 1: '99.25%', 2: '99.48%', 3: '99.80%', 4: '99.37%', 5: '99.55%'}\n",
      "Val Loss: 4.015633, Val Acc: 86.81%, Val-Class-Acc: {0: '83.70%', 1: '77.01%', 2: '86.81%', 3: '94.67%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 142/200, Train Loss: 0.013977, Train-Class-Acc: {0: '99.59%', 1: '99.33%', 2: '99.65%', 3: '99.80%', 4: '99.37%', 5: '99.93%'}\n",
      "Val Loss: 3.895432, Val Acc: 87.12%, Val-Class-Acc: {0: '84.78%', 1: '79.10%', 2: '84.72%', 3: '91.39%', 4: '82.50%', 5: '94.91%'}, LR: 0.001000\n",
      "Epoch 143/200, Train Loss: 0.008604, Train-Class-Acc: {0: '99.86%', 1: '99.78%', 2: '99.31%', 3: '99.69%', 5: '99.85%', 4: '99.37%'}\n",
      "Val Loss: 3.617477, Val Acc: 86.34%, Val-Class-Acc: {0: '76.09%', 1: '78.51%', 2: '87.50%', 3: '95.08%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 144/200, Train Loss: 0.012929, Train-Class-Acc: {0: '99.86%', 1: '99.63%', 2: '99.65%', 3: '99.80%', 4: '99.37%', 5: '99.85%'}\n",
      "Val Loss: 3.527591, Val Acc: 87.12%, Val-Class-Acc: {0: '85.33%', 1: '77.61%', 2: '87.50%', 3: '91.80%', 4: '85.00%', 5: '94.31%'}, LR: 0.001000\n",
      "Epoch 145/200, Train Loss: 0.015993, Train-Class-Acc: {0: '99.73%', 1: '99.70%', 2: '99.48%', 3: '99.59%', 5: '99.93%', 4: '100.00%'}\n",
      "Val Loss: 3.382861, Val Acc: 86.42%, Val-Class-Acc: {0: '83.15%', 1: '80.30%', 2: '83.33%', 3: '90.57%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 146/200, Train Loss: 0.013647, Train-Class-Acc: {0: '99.59%', 1: '99.18%', 2: '99.65%', 3: '100.00%', 4: '98.73%', 5: '99.93%'}\n",
      "Val Loss: 3.728522, Val Acc: 86.89%, Val-Class-Acc: {0: '80.43%', 1: '80.90%', 2: '83.33%', 3: '91.39%', 4: '82.50%', 5: '95.21%'}, LR: 0.001000\n",
      "Epoch 147/200, Train Loss: 0.020686, Train-Class-Acc: {0: '99.32%', 1: '99.63%', 2: '100.00%', 3: '100.00%', 5: '99.85%', 4: '100.00%'}\n",
      "Val Loss: 3.928064, Val Acc: 87.20%, Val-Class-Acc: {0: '84.24%', 1: '78.51%', 2: '87.50%', 3: '93.44%', 4: '82.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 148/200, Train Loss: 0.016752, Train-Class-Acc: {0: '99.18%', 1: '99.40%', 2: '99.65%', 3: '100.00%', 4: '98.73%', 5: '99.78%'}\n",
      "Val Loss: 3.573617, Val Acc: 87.04%, Val-Class-Acc: {0: '85.87%', 1: '79.10%', 2: '84.03%', 3: '94.26%', 4: '85.00%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 149/200, Train Loss: 0.057478, Train-Class-Acc: {0: '99.59%', 1: '99.63%', 2: '99.83%', 3: '99.49%', 4: '99.37%', 5: '99.85%'}\n",
      "Val Loss: 3.944359, Val Acc: 85.56%, Val-Class-Acc: {0: '77.17%', 1: '80.60%', 2: '86.11%', 3: '93.85%', 4: '80.00%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 150/200, Train Loss: 0.121864, Train-Class-Acc: {0: '99.05%', 1: '98.50%', 2: '98.09%', 3: '98.77%', 5: '99.33%', 4: '98.10%'}\n",
      "Val Loss: 4.225647, Val Acc: 85.87%, Val-Class-Acc: {0: '81.52%', 1: '77.91%', 2: '92.36%', 3: '91.80%', 4: '82.50%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 151/200, Train Loss: 0.074293, Train-Class-Acc: {0: '98.91%', 1: '98.43%', 2: '98.96%', 3: '99.59%', 4: '98.73%', 5: '99.33%'}\n",
      "Val Loss: 4.258378, Val Acc: 85.25%, Val-Class-Acc: {0: '77.17%', 1: '77.31%', 2: '90.28%', 3: '90.98%', 4: '85.00%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 152/200, Train Loss: 0.053176, Train-Class-Acc: {0: '99.18%', 1: '98.95%', 2: '98.96%', 3: '99.49%', 4: '99.37%', 5: '99.40%'}\n",
      "Val Loss: 4.012251, Val Acc: 86.26%, Val-Class-Acc: {0: '81.52%', 1: '79.40%', 2: '88.19%', 3: '91.80%', 4: '85.00%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 153/200, Train Loss: 0.084747, Train-Class-Acc: {0: '99.18%', 1: '98.95%', 2: '99.48%', 3: '99.39%', 5: '99.40%', 4: '98.73%'}\n",
      "Val Loss: 4.641968, Val Acc: 85.01%, Val-Class-Acc: {0: '85.87%', 1: '74.93%', 2: '83.33%', 3: '93.03%', 4: '80.00%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 154/200, Train Loss: 0.049472, Train-Class-Acc: {0: '99.32%', 1: '98.80%', 2: '99.48%', 3: '99.49%', 5: '99.40%', 4: '97.47%'}\n",
      "Val Loss: 5.241625, Val Acc: 85.48%, Val-Class-Acc: {0: '68.48%', 1: '86.27%', 2: '84.03%', 3: '92.21%', 4: '85.00%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 155/200, Train Loss: 0.032459, Train-Class-Acc: {0: '99.05%', 1: '99.10%', 2: '99.65%', 3: '100.00%', 4: '99.37%', 5: '99.70%'}\n",
      "Val Loss: 4.531783, Val Acc: 86.18%, Val-Class-Acc: {0: '72.83%', 1: '81.49%', 2: '90.28%', 3: '93.44%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 156/200, Train Loss: 0.045734, Train-Class-Acc: {0: '99.05%', 1: '99.40%', 2: '99.65%', 3: '99.59%', 4: '98.73%', 5: '99.70%'}\n",
      "Val Loss: 4.531343, Val Acc: 84.62%, Val-Class-Acc: {0: '75.00%', 1: '80.60%', 2: '82.64%', 3: '92.62%', 4: '80.00%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 157/200, Train Loss: 0.046228, Train-Class-Acc: {0: '99.32%', 1: '99.18%', 2: '99.31%', 3: '99.59%', 4: '97.47%', 5: '99.63%'}\n",
      "Val Loss: 4.296982, Val Acc: 84.70%, Val-Class-Acc: {0: '77.72%', 1: '79.40%', 2: '84.03%', 3: '84.43%', 4: '87.50%', 5: '94.01%'}, LR: 0.001000\n",
      "Epoch 158/200, Train Loss: 0.035309, Train-Class-Acc: {0: '99.46%', 1: '99.63%', 2: '99.48%', 3: '99.39%', 4: '98.10%', 5: '99.48%'}\n",
      "Val Loss: 4.415938, Val Acc: 85.87%, Val-Class-Acc: {0: '78.26%', 1: '80.00%', 2: '87.50%', 3: '92.21%', 4: '85.00%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 159/200, Train Loss: 0.264417, Train-Class-Acc: {0: '98.50%', 1: '98.43%', 2: '97.75%', 3: '98.57%', 4: '100.00%', 5: '98.95%'}\n",
      "Val Loss: 5.105117, Val Acc: 85.71%, Val-Class-Acc: {0: '82.07%', 1: '80.60%', 2: '85.42%', 3: '88.11%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 160/200, Train Loss: 0.182805, Train-Class-Acc: {0: '97.55%', 1: '97.01%', 2: '98.44%', 3: '98.87%', 4: '98.10%', 5: '98.88%'}\n",
      "Val Loss: 3.849493, Val Acc: 85.56%, Val-Class-Acc: {0: '83.15%', 1: '81.79%', 2: '81.94%', 3: '89.75%', 4: '87.50%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 161/200, Train Loss: 0.072321, Train-Class-Acc: {0: '98.37%', 1: '98.35%', 2: '98.79%', 3: '99.59%', 4: '97.47%', 5: '99.18%'}\n",
      "Val Loss: 4.040150, Val Acc: 85.71%, Val-Class-Acc: {0: '77.17%', 1: '81.19%', 2: '83.33%', 3: '87.70%', 4: '85.00%', 5: '94.61%'}, LR: 0.001000\n",
      "Epoch 162/200, Train Loss: 0.029222, Train-Class-Acc: {0: '99.32%', 1: '99.25%', 2: '99.31%', 3: '99.69%', 4: '100.00%', 5: '99.63%'}\n",
      "Val Loss: 4.190317, Val Acc: 86.10%, Val-Class-Acc: {0: '78.80%', 1: '78.21%', 2: '84.03%', 3: '93.44%', 4: '85.00%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 163/200, Train Loss: 0.023952, Train-Class-Acc: {0: '99.32%', 1: '99.48%', 2: '99.65%', 3: '99.80%', 4: '100.00%', 5: '99.78%'}\n",
      "Val Loss: 4.379643, Val Acc: 85.32%, Val-Class-Acc: {0: '84.78%', 1: '73.43%', 2: '82.64%', 3: '93.03%', 4: '87.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 164/200, Train Loss: 0.165281, Train-Class-Acc: {0: '99.05%', 1: '98.80%', 2: '99.31%', 3: '99.28%', 4: '98.10%', 5: '99.03%'}\n",
      "Val Loss: 5.255200, Val Acc: 84.31%, Val-Class-Acc: {0: '79.89%', 1: '82.09%', 2: '77.08%', 3: '85.25%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 165/200, Train Loss: 0.254391, Train-Class-Acc: {0: '97.68%', 1: '97.61%', 2: '98.27%', 3: '98.26%', 5: '98.65%', 4: '98.10%'}\n",
      "Val Loss: 4.354452, Val Acc: 84.15%, Val-Class-Acc: {0: '83.70%', 1: '74.93%', 2: '83.33%', 3: '93.85%', 4: '85.00%', 5: '86.83%'}, LR: 0.001000\n",
      "Epoch 166/200, Train Loss: 0.090912, Train-Class-Acc: {0: '98.37%', 1: '98.73%', 2: '99.48%', 3: '99.49%', 5: '99.48%', 4: '98.73%'}\n",
      "Val Loss: 4.296688, Val Acc: 84.62%, Val-Class-Acc: {0: '78.80%', 1: '77.31%', 2: '82.64%', 3: '93.44%', 4: '82.50%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 167/200, Train Loss: 0.040993, Train-Class-Acc: {0: '99.05%', 1: '98.95%', 2: '99.65%', 3: '99.59%', 5: '99.40%', 4: '99.37%'}\n",
      "Val Loss: 4.532520, Val Acc: 84.54%, Val-Class-Acc: {0: '71.74%', 1: '79.40%', 2: '81.94%', 3: '92.62%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 168/200, Train Loss: 0.070048, Train-Class-Acc: {0: '99.18%', 1: '99.10%', 2: '99.48%', 3: '99.69%', 4: '98.73%', 5: '99.55%'}\n",
      "Val Loss: 4.758340, Val Acc: 85.79%, Val-Class-Acc: {0: '86.41%', 1: '73.73%', 2: '87.50%', 3: '93.44%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 169/200, Train Loss: 0.035483, Train-Class-Acc: {0: '99.18%', 1: '99.33%', 2: '99.65%', 3: '99.69%', 4: '100.00%', 5: '99.70%'}\n",
      "Val Loss: 4.965055, Val Acc: 86.42%, Val-Class-Acc: {0: '77.72%', 1: '80.60%', 2: '85.42%', 3: '93.03%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 170/200, Train Loss: 0.036289, Train-Class-Acc: {0: '99.86%', 1: '99.33%', 2: '99.31%', 3: '99.80%', 4: '98.10%', 5: '99.55%'}\n",
      "Val Loss: 4.959594, Val Acc: 85.71%, Val-Class-Acc: {0: '82.07%', 1: '79.10%', 2: '79.86%', 3: '92.62%', 4: '85.00%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 171/200, Train Loss: 0.041330, Train-Class-Acc: {0: '98.91%', 1: '99.10%', 2: '98.96%', 3: '99.59%', 4: '100.00%', 5: '99.78%'}\n",
      "Val Loss: 5.152099, Val Acc: 85.79%, Val-Class-Acc: {0: '83.15%', 1: '74.93%', 2: '85.42%', 3: '93.85%', 4: '85.00%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 172/200, Train Loss: 0.054605, Train-Class-Acc: {0: '99.05%', 1: '98.95%', 2: '99.48%', 3: '100.00%', 4: '100.00%', 5: '99.63%'}\n",
      "Val Loss: 5.186999, Val Acc: 86.10%, Val-Class-Acc: {0: '83.15%', 1: '73.73%', 2: '89.58%', 3: '91.80%', 4: '85.00%', 5: '94.61%'}, LR: 0.001000\n",
      "Epoch 173/200, Train Loss: 0.031656, Train-Class-Acc: {0: '99.18%', 1: '99.70%', 2: '100.00%', 3: '99.80%', 4: '100.00%', 5: '99.85%'}\n",
      "Val Loss: 4.881233, Val Acc: 87.12%, Val-Class-Acc: {0: '80.43%', 1: '81.79%', 2: '86.81%', 3: '93.85%', 4: '85.00%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 174/200, Train Loss: 0.078481, Train-Class-Acc: {0: '99.32%', 1: '99.03%', 2: '99.48%', 3: '99.18%', 4: '100.00%', 5: '99.48%'}\n",
      "Val Loss: 5.220757, Val Acc: 85.09%, Val-Class-Acc: {0: '68.48%', 1: '83.88%', 2: '86.11%', 3: '92.62%', 4: '85.00%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 175/200, Train Loss: 0.052477, Train-Class-Acc: {0: '99.46%', 1: '99.33%', 2: '99.65%', 3: '99.69%', 4: '98.73%', 5: '99.55%'}\n",
      "Val Loss: 5.339658, Val Acc: 86.18%, Val-Class-Acc: {0: '80.43%', 1: '78.81%', 2: '84.72%', 3: '94.26%', 4: '82.50%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 176/200, Train Loss: 0.056738, Train-Class-Acc: {0: '98.64%', 1: '99.25%', 2: '99.83%', 3: '99.59%', 5: '99.40%', 4: '99.37%'}\n",
      "Val Loss: 4.956465, Val Acc: 86.10%, Val-Class-Acc: {0: '82.07%', 1: '78.21%', 2: '86.11%', 3: '93.85%', 4: '80.00%', 5: '91.32%'}, LR: 0.001000\n",
      "Epoch 177/200, Train Loss: 0.055978, Train-Class-Acc: {0: '99.59%', 1: '98.95%', 2: '99.83%', 3: '99.59%', 4: '97.47%', 5: '99.70%'}\n",
      "Val Loss: 5.765832, Val Acc: 86.26%, Val-Class-Acc: {0: '77.72%', 1: '82.69%', 2: '84.03%', 3: '90.57%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 178/200, Train Loss: 0.054872, Train-Class-Acc: {0: '98.77%', 1: '98.73%', 2: '98.96%', 3: '99.39%', 4: '98.73%', 5: '99.48%'}\n",
      "Val Loss: 5.246279, Val Acc: 85.87%, Val-Class-Acc: {0: '81.52%', 1: '81.79%', 2: '81.94%', 3: '90.57%', 4: '85.00%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 179/200, Train Loss: 0.015074, Train-Class-Acc: {0: '99.18%', 1: '99.40%', 2: '99.65%', 3: '99.90%', 4: '99.37%', 5: '99.85%'}\n",
      "Val Loss: 5.293199, Val Acc: 85.95%, Val-Class-Acc: {0: '77.72%', 1: '77.61%', 2: '88.19%', 3: '90.16%', 4: '85.00%', 5: '94.91%'}, LR: 0.001000\n",
      "Epoch 180/200, Train Loss: 0.090781, Train-Class-Acc: {0: '100.00%', 1: '99.63%', 2: '99.65%', 3: '99.49%', 4: '100.00%', 5: '99.63%'}\n",
      "Val Loss: 5.453555, Val Acc: 85.79%, Val-Class-Acc: {0: '75.54%', 1: '78.21%', 2: '88.19%', 3: '90.16%', 4: '82.50%', 5: '95.21%'}, LR: 0.001000\n",
      "Epoch 181/200, Train Loss: 0.087760, Train-Class-Acc: {0: '98.64%', 1: '98.65%', 2: '98.79%', 3: '99.59%', 4: '97.47%', 5: '99.40%'}\n",
      "Val Loss: 5.112361, Val Acc: 85.56%, Val-Class-Acc: {0: '73.91%', 1: '79.10%', 2: '86.81%', 3: '91.39%', 4: '85.00%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 182/200, Train Loss: 0.052648, Train-Class-Acc: {0: '99.05%', 1: '99.10%', 2: '98.96%', 3: '99.90%', 4: '98.73%', 5: '99.18%'}\n",
      "Val Loss: 4.738405, Val Acc: 86.57%, Val-Class-Acc: {0: '88.04%', 1: '77.61%', 2: '84.72%', 3: '90.57%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 183/200, Train Loss: 0.040190, Train-Class-Acc: {0: '99.32%', 1: '99.10%', 2: '99.83%', 3: '99.59%', 4: '99.37%', 5: '99.63%'}\n",
      "Val Loss: 4.437997, Val Acc: 86.49%, Val-Class-Acc: {0: '85.33%', 1: '76.42%', 2: '85.42%', 3: '91.80%', 4: '82.50%', 5: '94.31%'}, LR: 0.001000\n",
      "Epoch 184/200, Train Loss: 0.037738, Train-Class-Acc: {0: '99.05%', 1: '99.10%', 2: '99.65%', 3: '99.69%', 4: '98.73%', 5: '99.93%'}\n",
      "Val Loss: 4.516632, Val Acc: 86.57%, Val-Class-Acc: {0: '83.70%', 1: '80.90%', 2: '85.42%', 3: '93.03%', 4: '82.50%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 185/200, Train Loss: 0.009606, Train-Class-Acc: {0: '99.73%', 1: '99.48%', 2: '99.48%', 3: '99.90%', 4: '99.37%', 5: '99.85%'}\n",
      "Val Loss: 4.874731, Val Acc: 86.26%, Val-Class-Acc: {0: '80.98%', 1: '77.31%', 2: '89.58%', 3: '92.62%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 186/200, Train Loss: 0.006215, Train-Class-Acc: {0: '100.00%', 1: '99.70%', 2: '99.83%', 3: '99.90%', 4: '99.37%', 5: '100.00%'}\n",
      "Val Loss: 4.747429, Val Acc: 86.49%, Val-Class-Acc: {0: '79.35%', 1: '82.99%', 2: '85.42%', 3: '91.80%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 187/200, Train Loss: 0.076149, Train-Class-Acc: {0: '99.18%', 1: '98.88%', 2: '99.31%', 3: '99.28%', 5: '99.25%', 4: '100.00%'}\n",
      "Val Loss: 4.919136, Val Acc: 85.87%, Val-Class-Acc: {0: '70.65%', 1: '86.27%', 2: '82.64%', 3: '95.90%', 4: '82.50%', 5: '88.32%'}, LR: 0.001000\n",
      "Epoch 188/200, Train Loss: 0.121246, Train-Class-Acc: {0: '98.50%', 1: '98.50%', 2: '98.27%', 3: '98.57%', 4: '96.84%', 5: '99.63%'}\n",
      "Val Loss: 4.689702, Val Acc: 86.65%, Val-Class-Acc: {0: '84.24%', 1: '75.82%', 2: '89.58%', 3: '93.44%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 189/200, Train Loss: 0.053053, Train-Class-Acc: {0: '99.05%', 1: '98.95%', 2: '99.48%', 3: '99.69%', 4: '99.37%', 5: '99.48%'}\n",
      "Val Loss: 3.998297, Val Acc: 87.43%, Val-Class-Acc: {0: '79.35%', 1: '83.88%', 2: '87.50%', 3: '91.80%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 190/200, Train Loss: 0.013142, Train-Class-Acc: {0: '99.59%', 1: '99.70%', 2: '99.65%', 3: '99.69%', 4: '99.37%', 5: '99.85%'}\n",
      "Val Loss: 4.465410, Val Acc: 86.73%, Val-Class-Acc: {0: '89.67%', 1: '73.43%', 2: '88.89%', 3: '94.26%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 191/200, Train Loss: 0.027732, Train-Class-Acc: {0: '99.18%', 1: '99.40%', 2: '99.65%', 3: '100.00%', 4: '99.37%', 5: '99.63%'}\n",
      "Val Loss: 4.143893, Val Acc: 85.95%, Val-Class-Acc: {0: '89.13%', 1: '80.00%', 2: '81.94%', 3: '90.16%', 4: '82.50%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 192/200, Train Loss: 0.010643, Train-Class-Acc: {0: '99.86%', 1: '99.70%', 2: '100.00%', 3: '100.00%', 4: '99.37%', 5: '99.93%'}\n",
      "Val Loss: 4.044583, Val Acc: 85.56%, Val-Class-Acc: {0: '79.89%', 1: '82.99%', 2: '82.64%', 3: '89.75%', 4: '82.50%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 193/200, Train Loss: 0.010686, Train-Class-Acc: {0: '99.86%', 1: '99.63%', 2: '99.83%', 3: '99.80%', 4: '98.10%', 5: '99.85%'}\n",
      "Val Loss: 4.569763, Val Acc: 86.03%, Val-Class-Acc: {0: '85.87%', 1: '74.93%', 2: '84.72%', 3: '93.85%', 4: '82.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 194/200, Train Loss: 0.012697, Train-Class-Acc: {0: '99.73%', 1: '99.40%', 2: '99.48%', 3: '99.59%', 4: '99.37%', 5: '99.93%'}\n",
      "Val Loss: 4.128223, Val Acc: 85.40%, Val-Class-Acc: {0: '85.87%', 1: '79.40%', 2: '78.47%', 3: '90.98%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 195/200, Train Loss: 0.011969, Train-Class-Acc: {0: '99.73%', 1: '99.48%', 2: '99.65%', 3: '99.90%', 4: '100.00%', 5: '99.55%'}\n",
      "Val Loss: 4.285671, Val Acc: 86.34%, Val-Class-Acc: {0: '83.70%', 1: '73.73%', 2: '88.19%', 3: '93.85%', 4: '82.50%', 5: '94.61%'}, LR: 0.001000\n",
      "Epoch 196/200, Train Loss: 0.045563, Train-Class-Acc: {0: '99.46%', 1: '99.55%', 2: '99.65%', 3: '99.90%', 4: '99.37%', 5: '99.78%'}\n",
      "Val Loss: 5.997742, Val Acc: 85.64%, Val-Class-Acc: {0: '74.46%', 1: '80.30%', 2: '90.97%', 3: '91.80%', 4: '82.50%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 197/200, Train Loss: 0.048645, Train-Class-Acc: {0: '99.18%', 1: '99.25%', 2: '99.31%', 3: '99.69%', 4: '98.10%', 5: '99.70%'}\n",
      "Val Loss: 5.389108, Val Acc: 85.79%, Val-Class-Acc: {0: '80.98%', 1: '79.10%', 2: '77.08%', 3: '93.03%', 4: '85.00%', 5: '93.71%'}, LR: 0.001000\n",
      "Epoch 198/200, Train Loss: 0.060713, Train-Class-Acc: {0: '99.18%', 1: '98.80%', 2: '99.31%', 3: '99.39%', 4: '98.73%', 5: '99.63%'}\n",
      "Val Loss: 4.197390, Val Acc: 86.18%, Val-Class-Acc: {0: '83.70%', 1: '79.40%', 2: '85.42%', 3: '89.34%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 199/200, Train Loss: 0.023682, Train-Class-Acc: {0: '99.05%', 1: '99.33%', 2: '99.83%', 3: '99.69%', 4: '99.37%', 5: '99.85%'}\n",
      "Val Loss: 4.151157, Val Acc: 87.20%, Val-Class-Acc: {0: '88.59%', 1: '78.21%', 2: '88.89%', 3: '93.03%', 4: '85.00%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 200/200, Train Loss: 0.107920, Train-Class-Acc: {0: '98.91%', 1: '98.43%', 2: '99.31%', 3: '99.28%', 4: '97.47%', 5: '98.80%'}\n",
      "Val Loss: 4.216237, Val Acc: 85.95%, Val-Class-Acc: {0: '81.52%', 1: '79.10%', 2: '81.25%', 3: '92.62%', 4: '82.50%', 5: '92.81%'}, LR: 0.001000\n",
      "\n",
      "üèÜ Best model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_best.pth (Val Accuracy: 88.13%)\n",
      "\n",
      "üìå Final model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_final.pth\n",
      "\n",
      "üéØ Top 5 Best Models:\n",
      "Epoch 22, Train Loss: 0.255767, Train-Acc: {0: '95.50%', 1: '95.59%', 2: '97.75%', 3: '98.16%', 4: '95.57%', 5: '97.68%'},\n",
      "Val Loss: 2.469048, Val Acc: 88.13%, Val-Acc: {0: '86.96%', 1: '81.79%', 2: '87.50%', 3: '92.21%', 4: '82.50%', 5: '93.11%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_22.pth\n",
      "Epoch 14, Train Loss: 0.378729, Train-Acc: {0: '93.73%', 1: '92.74%', 2: '95.15%', 3: '96.82%', 4: '92.41%', 5: '97.31%'},\n",
      "Val Loss: 2.505129, Val Acc: 88.06%, Val-Acc: {0: '88.59%', 1: '78.51%', 2: '88.19%', 3: '93.85%', 4: '82.50%', 5: '93.71%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_14.pth\n",
      "Epoch 69, Train Loss: 0.156436, Train-Acc: {0: '97.96%', 1: '97.38%', 2: '97.75%', 3: '99.28%', 4: '98.10%', 5: '98.58%'},\n",
      "Val Loss: 3.482593, Val Acc: 87.98%, Val-Acc: {0: '85.87%', 1: '77.91%', 2: '89.58%', 3: '94.26%', 4: '85.00%', 5: '94.31%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_69.pth\n",
      "Epoch 17, Train Loss: 0.382683, Train-Acc: {0: '94.96%', 1: '93.72%', 2: '96.71%', 3: '97.64%', 4: '93.04%', 5: '96.64%'},\n",
      "Val Loss: 2.645516, Val Acc: 87.90%, Val-Acc: {0: '92.39%', 1: '77.01%', 2: '85.42%', 3: '92.21%', 4: '82.50%', 5: '94.91%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_17.pth\n",
      "Epoch 21, Train Loss: 0.247793, Train-Acc: {0: '94.82%', 1: '95.21%', 2: '96.71%', 3: '98.77%', 4: '96.20%', 5: '97.68%'},\n",
      "Val Loss: 2.621934, Val Acc: 87.74%, Val-Acc: {0: '92.93%', 1: '78.21%', 2: '85.42%', 3: '92.62%', 4: '82.50%', 5: '92.51%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/ResNet18_1D_LoRA_epoch_21.pth\n",
      "\n",
      "üß† Model Summary:\n",
      "Total Parameters: 3,953,286\n",
      "Model Size (float32): 15.08 MB\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 24\n",
      "  - BasicBlocks with adapters: 8\n",
      "Number of LoRA adapters: 24\n",
      "Number of LoRA groups: 3\n",
      "Total Training Time: 552.50 seconds\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 24\n",
      "  - BasicBlocks with adapters: 8\n",
      "---\n",
      "### Period 3 (alpha = 0.0, similarity_threshold = 0.92)\n",
      "+ ##### Total training time: 552.50 seconds\n",
      "+ ##### Model: ResNet18_1D_LoRA\n",
      "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3'*\n",
      "+ ##### Best Epoch: 22\n",
      "#### __Val Accuracy: 88.13%__\n",
      "#### __Val-Class-Acc: {0: '86.96%', 1: '81.79%', 2: '87.50%', 3: '92.21%', 4: '82.50%', 5: '93.11%'}__\n",
      "#### __Total Parameters: 3,953,286__\n",
      "#### __Model Size (float32): 15.08 MB__\n",
      "#### __Number of LoRA adapters: 24__\n",
      "#### __Number of LoRA groups: 3__\n",
      "\n",
      "Saved class features to: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_3/class_features.pkl\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå Period 3: DynEx-CLoRA Training (ECG)\n",
    "# ================================\n",
    "period = 3\n",
    "\n",
    "# ==== Paths ====\n",
    "stop_signal_file = os.path.join(BASE_DIR, \"stop_training.txt\")\n",
    "model_saving_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v5\", f\"Period_{period}\")\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# ==== Load Period 3 Data ====\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{period}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{period}.npy\"))\n",
    "X_val   = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val   = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "# ==== Device ====\n",
    "device = auto_select_cuda_device()\n",
    "\n",
    "# ==== Load Period 2 Features ====\n",
    "prev_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v3\", \"Period_2\")\n",
    "class_features_path = os.path.join(prev_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, \"rb\") as f:\n",
    "    class_features_dict = pickle.load(f)\n",
    "print(f\"‚úÖ Loaded class features from: {class_features_path}\")\n",
    "\n",
    "# ==== Load Period 2 Checkpoint ====\n",
    "prev_model_path = os.path.join(prev_folder, \"ResNet18_1D_LoRA_best.pth\")\n",
    "checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "\n",
    "input_channels = X_train.shape[2]\n",
    "output_size = len(np.unique(y_train))\n",
    "\n",
    "# ==== Build Teacher Model ====\n",
    "teacher_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size - 2).to(device)\n",
    "teacher_model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "teacher_model.eval()\n",
    "\n",
    "# ==== Build Student Model ====\n",
    "model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size).to(device)\n",
    "\n",
    "# ==== Sync Adapter Structure ====\n",
    "num_lora_groups = checkpoint.get(\"num_lora_groups\", 0)\n",
    "print(f\"üîÑ Number of LoRA groups: {num_lora_groups}\")\n",
    "related_labels = checkpoint.get(\"related_labels\", {\"base\": [0, 1]})\n",
    "for _ in range(num_lora_groups):\n",
    "    model.add_lora_adapter()\n",
    "\n",
    "# ==== Load Previous Weights (excluding FC and LoRA) ====\n",
    "model_dict = model.state_dict()\n",
    "prev_state_dict = checkpoint[\"model_state_dict\"]\n",
    "filtered_dict = {\n",
    "    k: v for k, v in prev_state_dict.items()\n",
    "    if k in model_dict and model_dict[k].shape == v.shape and k not in [\"fc.weight\", \"fc.bias\"]\n",
    "}\n",
    "model.load_state_dict(filtered_dict, strict=False)\n",
    "for k in model_dict:\n",
    "    if k not in filtered_dict:\n",
    "        print(f\"üîç Not loaded: {k}, shape={model_dict[k].shape}\")\n",
    "print(\"‚úÖ Loaded shared weights from Period 2 (excluding FC only)\")\n",
    "\n",
    "# ==== Training Configuration ====\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "alpha = 0.0 # No distillation\n",
    "similarity_threshold = 0.92\n",
    "stable_classes = [0, 2, 3]  # Ê†πÊìö Period 2 class ÁµêÊûú\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# ==== Start Training ====\n",
    "train_with_dynex_clora_ecg(\n",
    "    model=model,\n",
    "    teacher_model=None,\n",
    "    output_size=output_size,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    alpha=alpha,\n",
    "    model_saving_folder=model_saving_folder,\n",
    "    model_name=\"ResNet18_1D_LoRA\",\n",
    "    stop_signal_file=stop_signal_file,\n",
    "    scheduler=scheduler,\n",
    "    period=period,\n",
    "    stable_classes=stable_classes,\n",
    "    similarity_threshold=similarity_threshold,\n",
    "    class_features_dict=class_features_dict,\n",
    "    related_labels=related_labels,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ==== Cleanup ====\n",
    "del X_train, y_train, X_val, y_val\n",
    "del model, teacher_model, checkpoint, model_dict, prev_state_dict, filtered_dict\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df753cf",
   "metadata": {},
   "source": [
    "### Period 4 (0.65, 0.7, 0.75, 0.79)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10aed81",
   "metadata": {},
   "source": [
    "#### ‚úîÔ∏è v3 no distillation, all freeze, th = 0.65\n",
    "##### Period 4 (alpha = 0.0, similarity_threshold = 0.65)\n",
    "+ ##### Total training time: 574.52 seconds\n",
    "+ ##### Model: ResNet18_1D_LoRA\n",
    "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4'*\n",
    "+ ##### Best Epoch: 44\n",
    "##### __Val Accuracy: 89.37%__\n",
    "##### __Val-Class-Acc: {0: '89.67%', 2: '94.44%', 3: '97.95%', 4: '87.50%', 5: '98.81%', 6: '44.44%', 7: '88.80%', 8: '86.62%', 9: '72.97%'}__\n",
    "##### __Total Parameters: 3,895,946__\n",
    "##### __Model Size (float32): 14.86 MB__\n",
    "##### __Number of LoRA adapters: 8__\n",
    "##### __Number of LoRA groups: 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8d60731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 0\n",
      "    - Memory Used    : 1741 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ Loaded class features from: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/class_features.pkl\n",
      "üîÑ Number of LoRA groups: 1\n",
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n",
      "üîç Not loaded: fc.weight, shape=torch.Size([10, 1024])\n",
      "üîç Not loaded: fc.bias, shape=torch.Size([10])\n",
      "‚úÖ Loaded shared weights from Period 3 (excluding FC only)\n",
      "\n",
      "üöÄ 'train_with_dynex_clora_ecg' started for Period 4\n",
      "\n",
      "‚úÖ Removed existing folder: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1775968/46911881.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Data Overview:\n",
      "X_train: torch.Size([5493, 5000, 12]), y_train: torch.Size([5493])\n",
      "X_val: torch.Size([1374, 5000, 12]), y_val: torch.Size([1374])\n",
      "\n",
      "üîé Similarity Analysis:\n",
      "  Similarity threshold: 0.6500\n",
      "  Existing classes: [0, 1, 2, 3, 4, 5]\n",
      "  Current classes: [0, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  New classes: [6, 7, 8, 9]\n",
      "\n",
      "üìä Similarity Scores:\n",
      "  New Class 6:\n",
      "    - Existing Class 1: 0.9078\n",
      "    - Existing Class 4: 0.9039\n",
      "    - Existing Class 0: 0.8786\n",
      "    - Existing Class 2: 0.8730\n",
      "    - Existing Class 3: 0.8699\n",
      "    - Existing Class 5: 0.8550\n",
      "  New Class 7:\n",
      "    - Existing Class 1: 0.9061\n",
      "    - Existing Class 4: 0.9041\n",
      "    - Existing Class 0: 0.8745\n",
      "    - Existing Class 2: 0.8719\n",
      "    - Existing Class 3: 0.8712\n",
      "    - Existing Class 5: 0.8631\n",
      "  New Class 8:\n",
      "    - Existing Class 1: 0.8973\n",
      "    - Existing Class 4: 0.8882\n",
      "    - Existing Class 0: 0.8746\n",
      "    - Existing Class 2: 0.8499\n",
      "    - Existing Class 3: 0.8435\n",
      "    - Existing Class 5: 0.8316\n",
      "  New Class 9:\n",
      "    - Existing Class 1: 0.8995\n",
      "    - Existing Class 4: 0.8924\n",
      "    - Existing Class 0: 0.8799\n",
      "    - Existing Class 2: 0.8592\n",
      "    - Existing Class 3: 0.8532\n",
      "    - Existing Class 5: 0.8383\n",
      "\n",
      "  Average similarity: 0.8682, Std: 0.0262\n",
      "\n",
      "üß© Managing LoRA adapters for 4 new classes...\n",
      "üîÑ New Class 8 is similar to Class 1 (sim=0.8973) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 8 is similar to Class 2 (sim=0.8499) ‚Üí Added to adapter '0'\n",
      "üîÑ New Class 9 is similar to Class 1 (sim=0.8995) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 9 is similar to Class 2 (sim=0.8592) ‚Üí Added to adapter '0'\n",
      "üîÑ New Class 6 is similar to Class 1 (sim=0.9078) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 6 is similar to Class 2 (sim=0.8730) ‚Üí Added to adapter '0'\n",
      "üîÑ New Class 7 is similar to Class 1 (sim=0.9061) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 7 is similar to Class 2 (sim=0.8719) ‚Üí Added to adapter '0'\n",
      "\n",
      "üîç Checking stability of existing classes...\n",
      "  Class 0 similarity with itself: 0.8814\n",
      "  Class 2 similarity with itself: 0.8816\n",
      "  Class 3 similarity with itself: 0.8985\n",
      "  Class 4 similarity with itself: 0.8817\n",
      "  Class 5 similarity with itself: 0.9125\n",
      "\n",
      "üîí Default: Freezing ALL model parameters\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå fc.weight                                          | shape=[10, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[10]\n",
      "\n",
      "üîì Unfreezing selected adapters (by group):\n",
      "  - Adapter Group #0 (all blocks) (classes: [2, 4, 5, 8, 9, 6, 7])\n",
      "  - Base layers (classes: [0, 1, 3, 4, 5, 8, 9, 6, 7])\n",
      "\n",
      "üîç Frozen Parameters: (After Unfreeze specific adapter groups)\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå fc.weight                                          | shape=[10, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[10]\n",
      "\n",
      "üîç Frozen Parameters (After Unfreeze newly added adapter groups):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå fc.weight                                          | shape=[10, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[10]\n",
      "\n",
      "üîç Frozen Parameters (After unfreeze the final layer):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "\n",
      "üìã Related Labels Summary:\n",
      "  - Base network: Classes [0, 1, 3, 4, 5, 8, 9, 6, 7]\n",
      "  - Adapter #0: Classes [2, 4, 5, 8, 9, 6, 7]\n",
      "\n",
      "üîß Trainable Parameter Status:\n",
      "  ‚úÖ layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ fc.weight                                          | shape=[10, 1024]\n",
      "  ‚úÖ fc.bias                                            | shape=[10]\n",
      "trainable_count: 26\n",
      "\n",
      "üìä Parameter Statistics:\n",
      "  - Total parameters: 3,895,946\n",
      "  - Trainable parameters: 2,129,930 (54.67%)\n",
      "  - Frozen parameters: 1,766,016 (45.33%)\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "\n",
      "üß† Parameters currently controlled by the optimizer: (78)\n",
      "  ‚õî conv1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer1.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.0.conv2.weight\n",
      "  ‚õî layer1.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer1.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.conv2.weight\n",
      "  ‚õî layer1.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer2.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.conv2.weight\n",
      "  ‚õî layer2.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer2.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.conv2.weight\n",
      "  ‚õî layer2.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer3.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.conv2.weight\n",
      "  ‚õî layer3.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer3.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.conv2.weight\n",
      "  ‚õî layer3.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer4.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.conv2.weight\n",
      "  ‚õî layer4.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer4.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.conv2.weight\n",
      "  ‚õî layer4.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ fc.weight\n",
      "  ‚úÖ fc.bias\n",
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/200, Train Loss: 5.426701, Train-Class-Acc: {0: '55.31%', 2: '59.79%', 3: '67.21%', 5: '75.99%', 6: '14.52%', 7: '45.11%', 8: '38.06%', 9: '4.05%', 4: '31.01%'}\n",
      "Val Loss: 1.592952, Val Acc: 83.62%, Val-Class-Acc: {0: '89.67%', 2: '94.44%', 3: '97.54%', 4: '65.00%', 5: '94.33%', 6: '29.63%', 7: '84.80%', 8: '80.89%', 9: '8.11%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_1.pth\n",
      "Epoch 2/200, Train Loss: 1.691024, Train-Class-Acc: {0: '80.11%', 2: '87.00%', 3: '89.45%', 4: '70.25%', 5: '90.88%', 6: '34.56%', 7: '64.47%', 8: '63.54%', 9: '25.68%'}\n",
      "Val Loss: 1.312611, Val Acc: 85.44%, Val-Class-Acc: {0: '89.67%', 2: '93.75%', 3: '97.54%', 4: '80.00%', 5: '95.22%', 6: '33.33%', 7: '84.80%', 8: '84.08%', 9: '29.73%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_2.pth\n",
      "Epoch 3/200, Train Loss: 1.244563, Train-Class-Acc: {0: '85.15%', 2: '89.25%', 3: '93.03%', 4: '76.58%', 5: '91.62%', 6: '42.63%', 7: '66.47%', 8: '70.06%', 9: '41.89%'}\n",
      "Val Loss: 1.154172, Val Acc: 86.24%, Val-Class-Acc: {0: '85.33%', 2: '94.44%', 3: '97.13%', 4: '87.50%', 5: '95.22%', 6: '37.96%', 7: '84.00%', 8: '88.54%', 9: '43.24%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_3.pth\n",
      "Epoch 4/200, Train Loss: 1.018235, Train-Class-Acc: {0: '89.37%', 2: '91.68%', 3: '94.36%', 5: '92.74%', 6: '45.85%', 7: '71.06%', 8: '75.16%', 9: '45.95%', 4: '82.28%'}\n",
      "Val Loss: 1.087185, Val Acc: 86.90%, Val-Class-Acc: {0: '90.76%', 2: '95.14%', 3: '98.36%', 4: '80.00%', 5: '95.22%', 6: '43.52%', 7: '86.40%', 8: '82.80%', 9: '37.84%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_4.pth\n",
      "Epoch 5/200, Train Loss: 0.696574, Train-Class-Acc: {0: '93.19%', 2: '92.72%', 3: '96.00%', 4: '81.65%', 5: '94.54%', 6: '54.61%', 7: '73.25%', 8: '79.14%', 9: '63.51%'}\n",
      "Val Loss: 1.140549, Val Acc: 87.41%, Val-Class-Acc: {0: '91.30%', 2: '95.14%', 3: '98.77%', 4: '87.50%', 5: '95.82%', 6: '47.22%', 7: '81.60%', 8: '83.44%', 9: '40.54%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_5.pth\n",
      "Epoch 6/200, Train Loss: 0.658650, Train-Class-Acc: {0: '93.87%', 2: '93.24%', 3: '96.41%', 5: '95.51%', 6: '61.06%', 7: '76.45%', 8: '81.53%', 9: '63.51%', 4: '87.97%'}\n",
      "Val Loss: 1.223757, Val Acc: 87.63%, Val-Class-Acc: {0: '94.57%', 2: '92.36%', 3: '97.95%', 4: '90.00%', 5: '96.12%', 6: '38.89%', 7: '83.20%', 8: '83.44%', 9: '62.16%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_1.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_6.pth\n",
      "Epoch 7/200, Train Loss: 0.456075, Train-Class-Acc: {0: '94.41%', 2: '95.32%', 3: '96.41%', 4: '89.24%', 5: '96.04%', 6: '65.21%', 7: '81.44%', 8: '83.92%', 9: '70.95%'}\n",
      "Val Loss: 1.153180, Val Acc: 88.06%, Val-Class-Acc: {0: '91.85%', 2: '94.44%', 3: '99.59%', 4: '87.50%', 5: '96.12%', 6: '55.56%', 7: '77.60%', 8: '82.80%', 9: '48.65%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_2.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_7.pth\n",
      "Epoch 8/200, Train Loss: 0.419109, Train-Class-Acc: {0: '94.69%', 2: '94.97%', 3: '97.64%', 4: '91.77%', 5: '97.08%', 6: '71.43%', 7: '83.43%', 8: '84.39%', 9: '66.89%'}\n",
      "Val Loss: 1.190254, Val Acc: 88.21%, Val-Class-Acc: {0: '90.22%', 2: '94.44%', 3: '99.18%', 4: '87.50%', 5: '95.22%', 6: '50.93%', 7: '81.60%', 8: '85.35%', 9: '62.16%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_3.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_8.pth\n",
      "Epoch 9/200, Train Loss: 0.314441, Train-Class-Acc: {0: '95.91%', 2: '95.67%', 3: '97.23%', 4: '92.41%', 5: '97.31%', 6: '73.50%', 7: '83.23%', 8: '86.62%', 9: '76.35%'}\n",
      "Val Loss: 1.295790, Val Acc: 87.77%, Val-Class-Acc: {0: '87.50%', 2: '94.44%', 3: '99.18%', 4: '87.50%', 5: '96.42%', 6: '50.93%', 7: '81.60%', 8: '82.80%', 9: '59.46%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_4.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_9.pth\n",
      "Epoch 10/200, Train Loss: 0.367432, Train-Class-Acc: {0: '96.32%', 2: '95.84%', 3: '98.16%', 4: '91.77%', 5: '97.01%', 6: '75.12%', 7: '85.03%', 8: '86.94%', 9: '74.32%'}\n",
      "Val Loss: 1.066937, Val Acc: 89.08%, Val-Class-Acc: {0: '95.11%', 2: '95.14%', 3: '99.18%', 4: '85.00%', 5: '95.82%', 6: '51.85%', 7: '84.00%', 8: '83.44%', 9: '62.16%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_5.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_10.pth\n",
      "Epoch 11/200, Train Loss: 0.360476, Train-Class-Acc: {0: '95.91%', 2: '97.92%', 3: '97.85%', 4: '91.14%', 5: '97.23%', 6: '77.65%', 7: '86.23%', 8: '90.61%', 9: '81.76%'}\n",
      "Val Loss: 1.192707, Val Acc: 88.06%, Val-Class-Acc: {0: '91.30%', 2: '96.53%', 3: '95.08%', 4: '85.00%', 5: '94.03%', 6: '52.78%', 7: '85.60%', 8: '84.08%', 9: '70.27%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_6.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_11.pth\n",
      "Epoch 12/200, Train Loss: 0.235228, Train-Class-Acc: {0: '97.82%', 2: '96.53%', 3: '97.75%', 5: '98.06%', 6: '79.72%', 7: '85.03%', 8: '90.76%', 9: '81.76%', 4: '92.41%'}\n",
      "Val Loss: 1.468913, Val Acc: 87.85%, Val-Class-Acc: {0: '87.50%', 2: '94.44%', 3: '98.36%', 4: '90.00%', 5: '96.72%', 6: '50.00%', 7: '80.00%', 8: '84.08%', 9: '64.86%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_9.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_12.pth\n",
      "Epoch 13/200, Train Loss: 0.286611, Train-Class-Acc: {0: '96.73%', 2: '97.40%', 3: '97.85%', 4: '96.20%', 5: '97.68%', 6: '80.41%', 7: '89.02%', 8: '89.17%', 9: '83.11%'}\n",
      "Val Loss: 1.362162, Val Acc: 87.70%, Val-Class-Acc: {0: '85.87%', 2: '94.44%', 3: '98.36%', 4: '87.50%', 5: '96.12%', 6: '48.15%', 7: '82.40%', 8: '86.62%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 14/200, Train Loss: 0.343629, Train-Class-Acc: {0: '95.64%', 2: '96.53%', 3: '97.85%', 4: '92.41%', 5: '97.53%', 6: '80.65%', 7: '87.43%', 8: '87.58%', 9: '79.73%'}\n",
      "Val Loss: 1.319689, Val Acc: 88.06%, Val-Class-Acc: {0: '92.93%', 2: '93.06%', 3: '98.77%', 4: '90.00%', 5: '94.33%', 6: '47.22%', 7: '83.20%', 8: '87.90%', 9: '51.35%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_12.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_14.pth\n",
      "Epoch 15/200, Train Loss: 0.190926, Train-Class-Acc: {0: '97.68%', 2: '97.23%', 3: '97.85%', 4: '96.84%', 5: '98.28%', 6: '83.87%', 7: '89.22%', 8: '91.24%', 9: '85.14%'}\n",
      "Val Loss: 1.386095, Val Acc: 87.92%, Val-Class-Acc: {0: '91.30%', 2: '93.75%', 3: '98.77%', 4: '90.00%', 5: '96.12%', 6: '46.30%', 7: '83.20%', 8: '84.08%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 16/200, Train Loss: 0.166545, Train-Class-Acc: {0: '98.50%', 2: '98.96%', 3: '99.28%', 4: '94.94%', 5: '98.43%', 6: '87.79%', 7: '91.82%', 8: '94.11%', 9: '87.84%'}\n",
      "Val Loss: 1.360570, Val Acc: 88.65%, Val-Class-Acc: {0: '92.39%', 2: '95.83%', 3: '98.77%', 4: '85.00%', 5: '96.12%', 6: '56.48%', 7: '82.40%', 8: '80.25%', 9: '62.16%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_7.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_16.pth\n",
      "Epoch 17/200, Train Loss: 0.183200, Train-Class-Acc: {0: '97.96%', 2: '98.44%', 3: '99.18%', 4: '96.84%', 5: '98.13%', 6: '91.47%', 7: '92.22%', 8: '94.75%', 9: '87.16%'}\n",
      "Val Loss: 1.201720, Val Acc: 89.16%, Val-Class-Acc: {0: '91.85%', 2: '94.44%', 3: '99.18%', 4: '90.00%', 5: '94.93%', 6: '57.41%', 7: '88.00%', 8: '80.25%', 9: '70.27%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_11.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_17.pth\n",
      "Epoch 18/200, Train Loss: 0.192572, Train-Class-Acc: {0: '98.91%', 2: '97.75%', 3: '98.46%', 4: '94.30%', 5: '98.35%', 6: '87.10%', 7: '90.62%', 8: '93.63%', 9: '87.16%'}\n",
      "Val Loss: 1.690231, Val Acc: 87.70%, Val-Class-Acc: {0: '92.39%', 2: '94.44%', 3: '99.18%', 4: '90.00%', 5: '93.73%', 6: '50.93%', 7: '82.40%', 8: '79.62%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 19/200, Train Loss: 0.222378, Train-Class-Acc: {0: '99.05%', 2: '98.44%', 3: '97.75%', 4: '96.20%', 5: '97.83%', 6: '87.56%', 7: '91.02%', 8: '95.22%', 9: '89.86%'}\n",
      "Val Loss: 1.624070, Val Acc: 87.77%, Val-Class-Acc: {0: '92.93%', 2: '92.36%', 3: '98.77%', 4: '87.50%', 5: '96.12%', 6: '48.15%', 7: '85.60%', 8: '78.98%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 20/200, Train Loss: 0.124691, Train-Class-Acc: {0: '98.50%', 2: '98.27%', 3: '99.18%', 5: '98.73%', 6: '90.78%', 7: '93.21%', 8: '94.59%', 9: '89.19%', 4: '95.57%'}\n",
      "Val Loss: 1.405636, Val Acc: 88.43%, Val-Class-Acc: {0: '92.39%', 2: '96.53%', 3: '98.77%', 4: '90.00%', 5: '94.03%', 6: '49.07%', 7: '83.20%', 8: '84.08%', 9: '67.57%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_14.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_20.pth\n",
      "Epoch 21/200, Train Loss: 0.122369, Train-Class-Acc: {0: '98.37%', 2: '98.09%', 3: '99.39%', 5: '99.18%', 6: '91.71%', 7: '95.61%', 8: '96.02%', 9: '91.89%', 4: '98.10%'}\n",
      "Val Loss: 1.746147, Val Acc: 87.34%, Val-Class-Acc: {0: '87.50%', 2: '95.83%', 3: '98.77%', 4: '87.50%', 5: '95.22%', 6: '52.78%', 7: '82.40%', 8: '81.53%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 22/200, Train Loss: 0.167499, Train-Class-Acc: {0: '98.09%', 2: '97.05%', 3: '98.46%', 4: '96.84%', 5: '98.50%', 6: '90.09%', 7: '94.01%', 8: '95.06%', 9: '89.86%'}\n",
      "Val Loss: 1.462739, Val Acc: 88.06%, Val-Class-Acc: {0: '92.93%', 2: '94.44%', 3: '98.77%', 4: '87.50%', 5: '96.42%', 6: '42.59%', 7: '77.60%', 8: '87.90%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 23/200, Train Loss: 0.117341, Train-Class-Acc: {0: '98.50%', 2: '98.79%', 3: '98.98%', 4: '94.94%', 5: '99.18%', 6: '91.24%', 7: '94.01%', 8: '94.27%', 9: '89.19%'}\n",
      "Val Loss: 1.634887, Val Acc: 87.63%, Val-Class-Acc: {0: '94.02%', 2: '94.44%', 3: '98.77%', 4: '90.00%', 5: '95.52%', 6: '50.00%', 7: '81.60%', 8: '78.98%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 24/200, Train Loss: 0.077367, Train-Class-Acc: {0: '98.77%', 2: '98.79%', 3: '99.69%', 4: '99.37%', 5: '99.33%', 6: '92.86%', 7: '95.61%', 8: '96.50%', 9: '91.22%'}\n",
      "Val Loss: 1.918579, Val Acc: 88.43%, Val-Class-Acc: {0: '91.85%', 2: '93.75%', 3: '98.77%', 4: '90.00%', 5: '96.72%', 6: '43.52%', 7: '84.80%', 8: '86.62%', 9: '56.76%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_8.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_24.pth\n",
      "Epoch 25/200, Train Loss: 0.157085, Train-Class-Acc: {0: '98.50%', 2: '99.13%', 3: '98.05%', 4: '96.20%', 5: '99.25%', 6: '93.09%', 7: '95.41%', 8: '96.82%', 9: '93.92%'}\n",
      "Val Loss: 1.587639, Val Acc: 88.50%, Val-Class-Acc: {0: '92.39%', 2: '93.06%', 3: '98.36%', 4: '87.50%', 5: '96.12%', 6: '49.07%', 7: '84.80%', 8: '82.80%', 9: '70.27%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_20.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_25.pth\n",
      "Epoch 26/200, Train Loss: 0.174240, Train-Class-Acc: {0: '97.68%', 2: '97.75%', 3: '97.44%', 4: '97.47%', 5: '98.80%', 6: '88.94%', 7: '93.41%', 8: '94.43%', 9: '91.89%'}\n",
      "Val Loss: 1.477813, Val Acc: 87.77%, Val-Class-Acc: {0: '91.30%', 2: '93.75%', 3: '97.13%', 4: '87.50%', 5: '95.82%', 6: '53.70%', 7: '79.20%', 8: '85.35%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 27/200, Train Loss: 0.104962, Train-Class-Acc: {0: '99.05%', 2: '99.13%', 3: '98.98%', 4: '96.20%', 5: '99.18%', 6: '92.17%', 7: '95.41%', 8: '96.82%', 9: '93.24%'}\n",
      "Val Loss: 1.502499, Val Acc: 88.36%, Val-Class-Acc: {0: '88.04%', 2: '94.44%', 3: '98.77%', 4: '90.00%', 5: '95.82%', 6: '50.93%', 7: '82.40%', 8: '87.26%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 28/200, Train Loss: 0.065384, Train-Class-Acc: {0: '98.64%', 2: '98.44%', 3: '98.98%', 4: '98.10%', 5: '99.03%', 6: '94.70%', 7: '97.01%', 8: '97.29%', 9: '95.27%'}\n",
      "Val Loss: 1.447888, Val Acc: 88.06%, Val-Class-Acc: {0: '91.85%', 2: '94.44%', 3: '98.77%', 4: '90.00%', 5: '94.33%', 6: '47.22%', 7: '81.60%', 8: '84.71%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 29/200, Train Loss: 0.049697, Train-Class-Acc: {0: '99.73%', 2: '98.61%', 3: '99.39%', 5: '99.33%', 6: '95.62%', 7: '97.01%', 8: '97.93%', 9: '97.97%', 4: '98.10%'}\n",
      "Val Loss: 1.545426, Val Acc: 87.77%, Val-Class-Acc: {0: '87.50%', 2: '94.44%', 3: '99.18%', 4: '90.00%', 5: '96.12%', 6: '42.59%', 7: '80.80%', 8: '88.54%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 30/200, Train Loss: 0.066969, Train-Class-Acc: {0: '99.18%', 2: '99.48%', 3: '99.18%', 5: '99.40%', 6: '94.47%', 7: '96.41%', 8: '97.61%', 9: '87.84%', 4: '97.47%'}\n",
      "Val Loss: 1.741782, Val Acc: 88.06%, Val-Class-Acc: {0: '91.85%', 2: '93.75%', 3: '99.59%', 4: '85.00%', 5: '95.22%', 6: '45.37%', 7: '81.60%', 8: '84.08%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 31/200, Train Loss: 0.073712, Train-Class-Acc: {0: '98.91%', 2: '98.96%', 3: '99.69%', 4: '100.00%', 5: '98.95%', 6: '94.01%', 7: '96.81%', 8: '96.82%', 9: '95.95%'}\n",
      "Val Loss: 1.837974, Val Acc: 88.14%, Val-Class-Acc: {0: '89.13%', 2: '93.75%', 3: '97.54%', 4: '90.00%', 5: '97.31%', 6: '51.85%', 7: '78.40%', 8: '84.71%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 32/200, Train Loss: 0.053924, Train-Class-Acc: {0: '98.77%', 2: '98.61%', 3: '99.39%', 4: '98.10%', 5: '99.70%', 6: '96.08%', 7: '96.81%', 8: '97.61%', 9: '97.97%'}\n",
      "Val Loss: 1.627953, Val Acc: 88.28%, Val-Class-Acc: {0: '92.93%', 2: '95.14%', 3: '97.95%', 4: '87.50%', 5: '95.52%', 6: '50.00%', 7: '80.00%', 8: '84.08%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 33/200, Train Loss: 0.061481, Train-Class-Acc: {0: '99.05%', 2: '99.13%', 3: '99.69%', 4: '98.73%', 5: '99.40%', 6: '95.85%', 7: '96.81%', 8: '97.29%', 9: '97.30%'}\n",
      "Val Loss: 1.530180, Val Acc: 89.37%, Val-Class-Acc: {0: '91.85%', 2: '93.75%', 3: '99.59%', 4: '90.00%', 5: '95.82%', 6: '51.85%', 7: '84.80%', 8: '85.99%', 9: '72.97%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_24.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_33.pth\n",
      "Epoch 34/200, Train Loss: 0.032982, Train-Class-Acc: {0: '99.59%', 2: '99.65%', 3: '99.80%', 4: '100.00%', 5: '99.55%', 6: '96.77%', 7: '96.61%', 8: '99.04%', 9: '96.62%'}\n",
      "Val Loss: 1.458270, Val Acc: 88.21%, Val-Class-Acc: {0: '90.76%', 2: '90.97%', 3: '97.95%', 4: '87.50%', 5: '96.72%', 6: '50.00%', 7: '85.60%', 8: '85.99%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 35/200, Train Loss: 0.025367, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '99.78%', 6: '97.47%', 7: '97.80%', 8: '99.04%', 9: '95.95%'}\n",
      "Val Loss: 2.079794, Val Acc: 89.16%, Val-Class-Acc: {0: '92.39%', 2: '94.44%', 3: '98.77%', 4: '87.50%', 5: '96.42%', 6: '44.44%', 7: '87.20%', 8: '87.26%', 9: '70.27%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_25.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_35.pth\n",
      "Epoch 36/200, Train Loss: 0.284106, Train-Class-Acc: {0: '96.46%', 2: '98.27%', 3: '98.16%', 4: '98.10%', 5: '98.28%', 6: '89.63%', 7: '93.01%', 8: '94.11%', 9: '92.57%'}\n",
      "Val Loss: 1.927157, Val Acc: 86.03%, Val-Class-Acc: {0: '89.13%', 2: '94.44%', 3: '98.36%', 4: '87.50%', 5: '91.64%', 6: '50.00%', 7: '80.00%', 8: '82.80%', 9: '43.24%'}, LR: 0.001000\n",
      "Epoch 37/200, Train Loss: 0.173930, Train-Class-Acc: {0: '96.73%', 2: '97.23%', 3: '98.57%', 4: '97.47%', 5: '98.50%', 6: '88.94%', 7: '93.81%', 8: '95.22%', 9: '90.54%'}\n",
      "Val Loss: 1.722563, Val Acc: 86.90%, Val-Class-Acc: {0: '91.85%', 2: '89.58%', 3: '96.31%', 4: '85.00%', 5: '95.52%', 6: '50.00%', 7: '82.40%', 8: '82.80%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 38/200, Train Loss: 0.096192, Train-Class-Acc: {0: '99.05%', 2: '98.27%', 3: '98.87%', 4: '96.84%', 5: '99.25%', 6: '93.09%', 7: '94.21%', 8: '96.82%', 9: '92.57%'}\n",
      "Val Loss: 1.782768, Val Acc: 87.55%, Val-Class-Acc: {0: '91.85%', 2: '94.44%', 3: '98.36%', 4: '87.50%', 5: '95.52%', 6: '50.00%', 7: '80.80%', 8: '80.25%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 39/200, Train Loss: 0.036661, Train-Class-Acc: {0: '99.32%', 2: '99.83%', 3: '99.80%', 5: '99.48%', 6: '97.24%', 7: '97.21%', 8: '98.25%', 9: '96.62%', 4: '98.73%'}\n",
      "Val Loss: 1.814270, Val Acc: 87.92%, Val-Class-Acc: {0: '89.67%', 2: '93.06%', 3: '98.36%', 4: '85.00%', 5: '95.52%', 6: '46.30%', 7: '84.80%', 8: '84.08%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 40/200, Train Loss: 0.067141, Train-Class-Acc: {0: '99.18%', 2: '99.31%', 3: '98.98%', 4: '98.10%', 5: '99.10%', 6: '96.77%', 7: '96.21%', 8: '98.89%', 9: '95.95%'}\n",
      "Val Loss: 1.872029, Val Acc: 87.92%, Val-Class-Acc: {0: '91.30%', 2: '94.44%', 3: '98.36%', 4: '90.00%', 5: '97.01%', 6: '50.00%', 7: '78.40%', 8: '82.17%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 41/200, Train Loss: 0.070768, Train-Class-Acc: {0: '98.50%', 2: '98.96%', 3: '98.98%', 4: '97.47%', 5: '99.33%', 6: '95.16%', 7: '97.21%', 8: '96.97%', 9: '95.27%'}\n",
      "Val Loss: 1.445855, Val Acc: 87.34%, Val-Class-Acc: {0: '89.13%', 2: '95.14%', 3: '97.95%', 4: '90.00%', 5: '96.42%', 6: '50.00%', 7: '76.80%', 8: '81.53%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 42/200, Train Loss: 0.076776, Train-Class-Acc: {0: '99.18%', 2: '99.13%', 3: '98.77%', 4: '96.84%', 5: '99.40%', 6: '95.39%', 7: '94.21%', 8: '97.61%', 9: '95.95%'}\n",
      "Val Loss: 1.459462, Val Acc: 87.99%, Val-Class-Acc: {0: '89.13%', 2: '93.75%', 3: '98.77%', 4: '87.50%', 5: '96.12%', 6: '38.89%', 7: '85.60%', 8: '85.99%', 9: '75.68%'}, LR: 0.001000\n",
      "Epoch 43/200, Train Loss: 0.056822, Train-Class-Acc: {0: '99.05%', 2: '99.31%', 3: '99.69%', 5: '99.63%', 6: '95.39%', 7: '96.81%', 8: '98.57%', 9: '93.92%', 4: '96.84%'}\n",
      "Val Loss: 1.479798, Val Acc: 88.65%, Val-Class-Acc: {0: '91.30%', 2: '93.06%', 3: '98.36%', 4: '90.00%', 5: '95.82%', 6: '49.07%', 7: '85.60%', 8: '84.71%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 44/200, Train Loss: 0.069169, Train-Class-Acc: {0: '99.18%', 2: '99.13%', 3: '99.59%', 4: '98.10%', 5: '99.10%', 6: '95.85%', 7: '96.81%', 8: '97.77%', 9: '97.30%'}\n",
      "Val Loss: 1.787247, Val Acc: 89.37%, Val-Class-Acc: {0: '89.67%', 2: '94.44%', 3: '97.95%', 4: '87.50%', 5: '98.81%', 6: '44.44%', 7: '88.80%', 8: '86.62%', 9: '72.97%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_16.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_44.pth\n",
      "Epoch 45/200, Train Loss: 0.068166, Train-Class-Acc: {0: '98.64%', 2: '99.48%', 3: '98.77%', 4: '98.10%', 5: '99.48%', 6: '95.39%', 7: '94.81%', 8: '96.66%', 9: '94.59%'}\n",
      "Val Loss: 1.551629, Val Acc: 88.21%, Val-Class-Acc: {0: '89.13%', 2: '93.06%', 3: '98.77%', 4: '90.00%', 5: '95.82%', 6: '50.00%', 7: '80.00%', 8: '88.54%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 46/200, Train Loss: 0.045773, Train-Class-Acc: {0: '99.46%', 2: '99.65%', 3: '99.28%', 4: '97.47%', 5: '99.63%', 6: '96.08%', 7: '98.00%', 8: '98.09%', 9: '94.59%'}\n",
      "Val Loss: 1.778107, Val Acc: 87.85%, Val-Class-Acc: {0: '89.67%', 2: '93.06%', 3: '98.36%', 4: '90.00%', 5: '95.82%', 6: '48.15%', 7: '84.00%', 8: '81.53%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 47/200, Train Loss: 0.056883, Train-Class-Acc: {0: '99.32%', 2: '99.65%', 3: '99.49%', 4: '98.10%', 5: '99.40%', 6: '95.85%', 7: '97.21%', 8: '98.09%', 9: '96.62%'}\n",
      "Val Loss: 1.539351, Val Acc: 88.06%, Val-Class-Acc: {0: '92.39%', 2: '94.44%', 3: '98.36%', 4: '90.00%', 5: '94.63%', 6: '47.22%', 7: '86.40%', 8: '82.17%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 48/200, Train Loss: 0.114621, Train-Class-Acc: {0: '98.77%', 2: '99.13%', 3: '99.08%', 4: '95.57%', 5: '99.33%', 6: '96.54%', 7: '97.41%', 8: '97.45%', 9: '91.89%'}\n",
      "Val Loss: 1.837463, Val Acc: 88.14%, Val-Class-Acc: {0: '89.67%', 2: '93.75%', 3: '99.59%', 4: '90.00%', 5: '94.63%', 6: '49.07%', 7: '77.60%', 8: '88.54%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 49/200, Train Loss: 0.262273, Train-Class-Acc: {0: '98.64%', 2: '98.27%', 3: '97.95%', 4: '98.10%', 5: '98.35%', 6: '92.40%', 7: '93.01%', 8: '95.22%', 9: '93.24%'}\n",
      "Val Loss: 1.980277, Val Acc: 87.92%, Val-Class-Acc: {0: '90.76%', 2: '94.44%', 3: '95.49%', 4: '85.00%', 5: '95.52%', 6: '49.07%', 7: '88.00%', 8: '84.08%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 50/200, Train Loss: 0.179581, Train-Class-Acc: {0: '98.09%', 2: '97.57%', 3: '98.16%', 4: '98.10%', 5: '98.73%', 6: '91.01%', 7: '94.41%', 8: '96.02%', 9: '92.57%'}\n",
      "Val Loss: 2.108378, Val Acc: 87.70%, Val-Class-Acc: {0: '88.59%', 2: '95.14%', 3: '96.72%', 4: '82.50%', 5: '97.01%', 6: '43.52%', 7: '85.60%', 8: '83.44%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 51/200, Train Loss: 0.118551, Train-Class-Acc: {0: '98.09%', 2: '98.09%', 3: '99.28%', 4: '97.47%', 5: '99.18%', 6: '94.47%', 7: '96.61%', 8: '96.50%', 9: '94.59%'}\n",
      "Val Loss: 1.621771, Val Acc: 88.36%, Val-Class-Acc: {0: '92.39%', 2: '93.75%', 3: '98.36%', 4: '85.00%', 5: '95.52%', 6: '48.15%', 7: '84.00%', 8: '87.26%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 52/200, Train Loss: 0.066424, Train-Class-Acc: {0: '99.05%', 2: '99.48%', 3: '99.39%', 4: '96.84%', 5: '99.33%', 6: '96.54%', 7: '97.41%', 8: '98.57%', 9: '92.57%'}\n",
      "Val Loss: 2.192411, Val Acc: 88.79%, Val-Class-Acc: {0: '92.39%', 2: '91.67%', 3: '97.54%', 4: '87.50%', 5: '97.01%', 6: '54.63%', 7: '81.60%', 8: '85.35%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 53/200, Train Loss: 0.043130, Train-Class-Acc: {0: '99.18%', 2: '99.48%', 3: '99.39%', 4: '99.37%', 5: '99.33%', 6: '97.00%', 7: '97.21%', 8: '97.77%', 9: '95.95%'}\n",
      "Val Loss: 1.955084, Val Acc: 88.28%, Val-Class-Acc: {0: '90.76%', 2: '89.58%', 3: '97.54%', 4: '85.00%', 5: '97.01%', 6: '46.30%', 7: '86.40%', 8: '85.35%', 9: '75.68%'}, LR: 0.001000\n",
      "Epoch 54/200, Train Loss: 0.022950, Train-Class-Acc: {0: '99.59%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '99.78%', 6: '98.62%', 7: '97.80%', 8: '98.09%', 9: '96.62%'}\n",
      "Val Loss: 1.927043, Val Acc: 88.06%, Val-Class-Acc: {0: '90.22%', 2: '92.36%', 3: '97.95%', 4: '87.50%', 5: '96.42%', 6: '44.44%', 7: '84.00%', 8: '86.62%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 55/200, Train Loss: 0.019682, Train-Class-Acc: {0: '99.59%', 2: '99.83%', 3: '99.90%', 4: '98.73%', 5: '99.85%', 6: '97.70%', 7: '98.00%', 8: '98.57%', 9: '97.97%'}\n",
      "Val Loss: 1.800138, Val Acc: 88.36%, Val-Class-Acc: {0: '93.48%', 2: '93.75%', 3: '97.13%', 4: '85.00%', 5: '95.52%', 6: '50.00%', 7: '84.80%', 8: '82.80%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 56/200, Train Loss: 0.044042, Train-Class-Acc: {0: '99.18%', 2: '98.61%', 3: '99.90%', 5: '99.48%', 6: '98.16%', 7: '99.00%', 8: '98.25%', 9: '97.30%', 4: '96.84%'}\n",
      "Val Loss: 1.884367, Val Acc: 88.14%, Val-Class-Acc: {0: '91.30%', 2: '95.14%', 3: '98.36%', 4: '87.50%', 5: '94.33%', 6: '44.44%', 7: '87.20%', 8: '84.71%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 57/200, Train Loss: 0.035598, Train-Class-Acc: {0: '99.73%', 2: '99.31%', 3: '99.80%', 4: '99.37%', 5: '99.40%', 6: '97.24%', 7: '98.00%', 8: '98.25%', 9: '97.30%'}\n",
      "Val Loss: 1.754779, Val Acc: 88.21%, Val-Class-Acc: {0: '85.87%', 2: '95.14%', 3: '98.36%', 4: '90.00%', 5: '95.22%', 6: '54.63%', 7: '84.00%', 8: '84.71%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 58/200, Train Loss: 0.083514, Train-Class-Acc: {0: '98.91%', 2: '97.92%', 3: '99.08%', 4: '98.73%', 5: '99.25%', 6: '95.39%', 7: '97.01%', 8: '98.25%', 9: '95.27%'}\n",
      "Val Loss: 2.135453, Val Acc: 87.34%, Val-Class-Acc: {0: '89.13%', 2: '88.89%', 3: '95.49%', 4: '85.00%', 5: '96.72%', 6: '45.37%', 7: '86.40%', 8: '88.54%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 59/200, Train Loss: 0.030974, Train-Class-Acc: {0: '98.64%', 2: '99.83%', 3: '100.00%', 4: '98.73%', 5: '99.85%', 6: '97.70%', 7: '98.60%', 8: '98.57%', 9: '96.62%'}\n",
      "Val Loss: 1.507592, Val Acc: 87.85%, Val-Class-Acc: {0: '93.48%', 2: '92.36%', 3: '96.31%', 4: '87.50%', 5: '93.73%', 6: '48.15%', 7: '87.20%', 8: '85.35%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 60/200, Train Loss: 0.022343, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '100.00%', 5: '99.70%', 6: '97.47%', 7: '98.20%', 8: '98.89%', 9: '100.00%', 4: '98.10%'}\n",
      "Val Loss: 1.889982, Val Acc: 87.85%, Val-Class-Acc: {0: '85.87%', 2: '93.06%', 3: '97.54%', 4: '90.00%', 5: '96.42%', 6: '48.15%', 7: '85.60%', 8: '86.62%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 61/200, Train Loss: 0.021091, Train-Class-Acc: {0: '99.46%', 2: '99.31%', 3: '99.90%', 4: '99.37%', 5: '99.78%', 6: '97.70%', 7: '99.20%', 8: '99.20%', 9: '100.00%'}\n",
      "Val Loss: 1.908813, Val Acc: 88.36%, Val-Class-Acc: {0: '94.02%', 2: '95.83%', 3: '98.36%', 4: '90.00%', 5: '95.22%', 6: '46.30%', 7: '86.40%', 8: '80.89%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 62/200, Train Loss: 0.019770, Train-Class-Acc: {0: '99.46%', 2: '99.65%', 3: '99.69%', 5: '99.78%', 6: '98.62%', 7: '99.00%', 8: '99.52%', 4: '100.00%', 9: '97.30%'}\n",
      "Val Loss: 2.101992, Val Acc: 88.72%, Val-Class-Acc: {0: '92.93%', 2: '93.75%', 3: '97.54%', 4: '87.50%', 5: '96.42%', 6: '45.37%', 7: '86.40%', 8: '87.26%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 63/200, Train Loss: 0.030762, Train-Class-Acc: {0: '99.32%', 2: '99.65%', 3: '99.90%', 4: '99.37%', 5: '99.63%', 6: '99.08%', 7: '98.60%', 8: '98.89%', 9: '97.30%'}\n",
      "Val Loss: 1.922298, Val Acc: 87.63%, Val-Class-Acc: {0: '86.96%', 2: '93.06%', 3: '96.31%', 4: '75.00%', 5: '94.33%', 6: '50.93%', 7: '86.40%', 8: '89.81%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 64/200, Train Loss: 0.138906, Train-Class-Acc: {0: '99.05%', 2: '99.65%', 3: '99.08%', 4: '98.10%', 5: '99.48%', 6: '96.31%', 7: '98.40%', 8: '98.73%', 9: '97.97%'}\n",
      "Val Loss: 2.286242, Val Acc: 86.17%, Val-Class-Acc: {0: '89.67%', 2: '92.36%', 3: '89.75%', 4: '87.50%', 5: '94.63%', 6: '55.56%', 7: '80.80%', 8: '87.26%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 65/200, Train Loss: 0.345985, Train-Class-Acc: {0: '95.23%', 2: '97.23%', 3: '96.82%', 4: '95.57%', 5: '97.83%', 6: '85.25%', 7: '92.61%', 8: '91.24%', 9: '83.78%'}\n",
      "Val Loss: 1.870429, Val Acc: 86.90%, Val-Class-Acc: {0: '81.52%', 2: '93.75%', 3: '98.77%', 4: '87.50%', 5: '95.52%', 6: '36.11%', 7: '88.80%', 8: '87.90%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 66/200, Train Loss: 0.088130, Train-Class-Acc: {0: '98.37%', 2: '98.27%', 3: '99.18%', 4: '97.47%', 5: '99.25%', 6: '93.55%', 7: '94.81%', 8: '96.02%', 9: '93.24%'}\n",
      "Val Loss: 1.801091, Val Acc: 88.57%, Val-Class-Acc: {0: '90.76%', 2: '92.36%', 3: '98.36%', 4: '87.50%', 5: '97.01%', 6: '50.93%', 7: '84.80%', 8: '83.44%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 67/200, Train Loss: 0.058552, Train-Class-Acc: {0: '99.18%', 2: '99.48%', 3: '99.49%', 4: '99.37%', 5: '99.25%', 6: '95.16%', 7: '96.81%', 8: '97.29%', 9: '95.95%'}\n",
      "Val Loss: 1.770593, Val Acc: 87.77%, Val-Class-Acc: {0: '87.50%', 2: '94.44%', 3: '97.13%', 4: '85.00%', 5: '96.12%', 6: '46.30%', 7: '82.40%', 8: '86.62%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 68/200, Train Loss: 0.022651, Train-Class-Acc: {0: '99.32%', 2: '99.48%', 3: '99.49%', 4: '99.37%', 5: '99.85%', 6: '97.93%', 7: '98.60%', 8: '98.57%', 9: '99.32%'}\n",
      "Val Loss: 1.684847, Val Acc: 88.21%, Val-Class-Acc: {0: '93.48%', 2: '92.36%', 3: '98.77%', 4: '87.50%', 5: '96.42%', 6: '47.22%', 7: '82.40%', 8: '83.44%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 69/200, Train Loss: 0.119234, Train-Class-Acc: {0: '99.32%', 2: '99.48%', 3: '99.69%', 4: '99.37%', 5: '99.18%', 6: '96.31%', 7: '98.00%', 8: '98.73%', 9: '95.95%'}\n",
      "Val Loss: 1.861643, Val Acc: 87.19%, Val-Class-Acc: {0: '88.59%', 2: '93.06%', 3: '97.54%', 4: '87.50%', 5: '97.91%', 6: '34.26%', 7: '88.00%', 8: '85.35%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 70/200, Train Loss: 0.069537, Train-Class-Acc: {0: '98.50%', 2: '99.13%', 3: '99.39%', 4: '100.00%', 5: '99.40%', 6: '93.09%', 7: '97.41%', 8: '96.97%', 9: '95.95%'}\n",
      "Val Loss: 2.370620, Val Acc: 87.63%, Val-Class-Acc: {0: '92.93%', 2: '93.75%', 3: '97.95%', 4: '87.50%', 5: '95.22%', 6: '50.00%', 7: '81.60%', 8: '79.62%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 71/200, Train Loss: 0.039101, Train-Class-Acc: {0: '98.91%', 2: '99.65%', 3: '99.80%', 4: '98.73%', 5: '99.63%', 6: '96.77%', 7: '98.20%', 8: '98.57%', 9: '97.30%'}\n",
      "Val Loss: 2.082405, Val Acc: 88.06%, Val-Class-Acc: {0: '91.85%', 2: '93.75%', 3: '98.36%', 4: '87.50%', 5: '94.93%', 6: '49.07%', 7: '81.60%', 8: '85.99%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 72/200, Train Loss: 0.031783, Train-Class-Acc: {0: '99.59%', 2: '99.65%', 3: '99.90%', 4: '99.37%', 5: '99.70%', 6: '97.93%', 7: '98.60%', 8: '99.04%', 9: '97.97%'}\n",
      "Val Loss: 2.334979, Val Acc: 88.14%, Val-Class-Acc: {0: '90.22%', 2: '93.06%', 3: '96.72%', 4: '87.50%', 5: '97.31%', 6: '49.07%', 7: '81.60%', 8: '86.62%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 73/200, Train Loss: 0.049095, Train-Class-Acc: {0: '99.32%', 2: '99.83%', 3: '99.80%', 4: '98.73%', 5: '99.55%', 6: '97.70%', 7: '98.00%', 8: '98.73%', 9: '97.30%'}\n",
      "Val Loss: 1.431072, Val Acc: 87.85%, Val-Class-Acc: {0: '93.48%', 2: '90.97%', 3: '98.36%', 4: '87.50%', 5: '93.13%', 6: '47.22%', 7: '88.00%', 8: '84.08%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 74/200, Train Loss: 0.029306, Train-Class-Acc: {0: '99.05%', 2: '99.65%', 3: '100.00%', 4: '98.73%', 5: '99.48%', 6: '97.24%', 7: '98.20%', 8: '99.36%', 9: '97.30%'}\n",
      "Val Loss: 1.718446, Val Acc: 87.77%, Val-Class-Acc: {0: '89.67%', 2: '93.75%', 3: '97.54%', 4: '90.00%', 5: '93.43%', 6: '51.85%', 7: '84.80%', 8: '85.35%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 75/200, Train Loss: 0.023245, Train-Class-Acc: {0: '99.59%', 2: '100.00%', 3: '99.69%', 4: '100.00%', 5: '99.55%', 7: '98.40%', 8: '98.41%', 9: '98.65%', 6: '98.16%'}\n",
      "Val Loss: 1.901720, Val Acc: 87.92%, Val-Class-Acc: {0: '89.67%', 2: '91.67%', 3: '97.13%', 4: '90.00%', 5: '95.82%', 6: '41.67%', 7: '88.00%', 8: '88.54%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 76/200, Train Loss: 0.014627, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '100.00%', 4: '98.73%', 5: '99.78%', 6: '99.08%', 7: '98.60%', 8: '99.52%', 9: '97.30%'}\n",
      "Val Loss: 2.004672, Val Acc: 88.28%, Val-Class-Acc: {0: '95.11%', 2: '92.36%', 3: '97.13%', 4: '82.50%', 5: '96.12%', 6: '49.07%', 7: '84.80%', 8: '85.99%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 77/200, Train Loss: 0.133382, Train-Class-Acc: {0: '99.18%', 2: '98.79%', 3: '99.18%', 4: '97.47%', 5: '98.95%', 6: '96.54%', 7: '97.80%', 8: '97.93%', 9: '95.95%'}\n",
      "Val Loss: 2.583070, Val Acc: 84.06%, Val-Class-Acc: {0: '82.07%', 2: '90.97%', 3: '97.95%', 4: '80.00%', 5: '87.16%', 6: '52.78%', 7: '79.20%', 8: '84.08%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 78/200, Train Loss: 0.146498, Train-Class-Acc: {0: '97.82%', 2: '97.40%', 3: '98.05%', 5: '99.10%', 6: '91.94%', 7: '93.61%', 8: '96.66%', 9: '94.59%', 4: '96.84%'}\n",
      "Val Loss: 1.940208, Val Acc: 88.65%, Val-Class-Acc: {0: '87.50%', 2: '93.06%', 3: '96.72%', 4: '87.50%', 5: '95.82%', 6: '52.78%', 7: '85.60%', 8: '89.81%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 79/200, Train Loss: 0.068061, Train-Class-Acc: {0: '98.77%', 2: '98.96%', 3: '99.69%', 4: '98.73%', 5: '99.33%', 6: '95.62%', 7: '97.80%', 8: '97.45%', 9: '91.89%'}\n",
      "Val Loss: 1.935132, Val Acc: 86.39%, Val-Class-Acc: {0: '85.87%', 2: '86.11%', 3: '96.31%', 4: '77.50%', 5: '97.01%', 6: '55.56%', 7: '80.80%', 8: '84.08%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 80/200, Train Loss: 0.036565, Train-Class-Acc: {0: '99.05%', 2: '99.13%', 3: '99.80%', 4: '98.73%', 5: '99.78%', 6: '96.08%', 7: '97.60%', 8: '99.04%', 9: '97.97%'}\n",
      "Val Loss: 1.874081, Val Acc: 88.28%, Val-Class-Acc: {0: '90.22%', 2: '89.58%', 3: '97.54%', 4: '87.50%', 5: '97.31%', 6: '52.78%', 7: '85.60%', 8: '82.17%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 81/200, Train Loss: 0.047274, Train-Class-Acc: {0: '99.18%', 2: '98.96%', 3: '99.59%', 4: '97.47%', 5: '99.78%', 6: '96.54%', 7: '98.20%', 8: '98.41%', 9: '95.27%'}\n",
      "Val Loss: 1.703497, Val Acc: 87.41%, Val-Class-Acc: {0: '91.85%', 2: '89.58%', 3: '95.49%', 4: '87.50%', 5: '95.82%', 6: '57.41%', 7: '84.00%', 8: '78.34%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 82/200, Train Loss: 0.039767, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '99.69%', 4: '99.37%', 5: '99.93%', 6: '97.93%', 7: '98.00%', 8: '98.89%', 9: '97.97%'}\n",
      "Val Loss: 1.649372, Val Acc: 87.41%, Val-Class-Acc: {0: '86.41%', 2: '91.67%', 3: '98.77%', 4: '87.50%', 5: '95.22%', 6: '53.70%', 7: '78.40%', 8: '84.71%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 83/200, Train Loss: 0.108345, Train-Class-Acc: {0: '99.05%', 2: '98.79%', 3: '98.98%', 4: '98.10%', 5: '99.33%', 6: '97.00%', 7: '97.01%', 8: '97.77%', 9: '93.92%'}\n",
      "Val Loss: 2.326417, Val Acc: 86.83%, Val-Class-Acc: {0: '91.30%', 2: '94.44%', 3: '95.49%', 4: '77.50%', 5: '93.13%', 6: '51.85%', 7: '84.80%', 8: '81.53%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 84/200, Train Loss: 0.391845, Train-Class-Acc: {0: '97.96%', 2: '98.44%', 3: '97.54%', 5: '98.35%', 6: '93.09%', 7: '96.41%', 8: '97.77%', 9: '91.22%', 4: '96.84%'}\n",
      "Val Loss: 2.574140, Val Acc: 84.35%, Val-Class-Acc: {0: '85.87%', 2: '93.75%', 3: '90.16%', 4: '77.50%', 5: '94.63%', 6: '45.37%', 7: '82.40%', 8: '81.53%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 85/200, Train Loss: 0.639193, Train-Class-Acc: {0: '95.10%', 2: '94.28%', 3: '97.85%', 5: '96.93%', 6: '85.48%', 7: '90.62%', 8: '91.24%', 9: '83.11%', 4: '97.47%'}\n",
      "Val Loss: 2.023446, Val Acc: 87.26%, Val-Class-Acc: {0: '88.04%', 2: '93.75%', 3: '95.08%', 4: '82.50%', 5: '95.22%', 6: '51.85%', 7: '84.00%', 8: '89.17%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 86/200, Train Loss: 0.092681, Train-Class-Acc: {0: '98.37%', 2: '98.61%', 3: '99.18%', 4: '97.47%', 5: '98.95%', 6: '93.78%', 7: '95.41%', 8: '96.97%', 9: '91.89%'}\n",
      "Val Loss: 2.074280, Val Acc: 87.77%, Val-Class-Acc: {0: '89.13%', 2: '92.36%', 3: '97.95%', 4: '87.50%', 5: '96.72%', 6: '48.15%', 7: '80.00%', 8: '87.90%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 87/200, Train Loss: 0.048596, Train-Class-Acc: {0: '99.05%', 2: '99.48%', 3: '99.80%', 4: '98.10%', 5: '99.63%', 6: '96.77%', 7: '97.60%', 8: '96.97%', 9: '97.97%'}\n",
      "Val Loss: 1.767942, Val Acc: 87.41%, Val-Class-Acc: {0: '87.50%', 2: '93.75%', 3: '97.95%', 4: '90.00%', 5: '95.52%', 6: '54.63%', 7: '81.60%', 8: '82.17%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 88/200, Train Loss: 0.035852, Train-Class-Acc: {0: '99.05%', 2: '99.48%', 3: '99.59%', 4: '100.00%', 5: '99.55%', 6: '97.47%', 7: '97.60%', 8: '98.57%', 9: '93.92%'}\n",
      "Val Loss: 1.757894, Val Acc: 88.36%, Val-Class-Acc: {0: '92.93%', 2: '90.28%', 3: '97.13%', 4: '82.50%', 5: '96.72%', 6: '50.93%', 7: '86.40%', 8: '83.44%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 89/200, Train Loss: 0.031488, Train-Class-Acc: {0: '99.46%', 2: '99.31%', 3: '99.80%', 5: '99.85%', 6: '97.47%', 7: '97.21%', 8: '98.89%', 9: '98.65%', 4: '98.73%'}\n",
      "Val Loss: 2.110624, Val Acc: 88.06%, Val-Class-Acc: {0: '91.85%', 2: '93.06%', 3: '97.54%', 4: '85.00%', 5: '96.12%', 6: '42.59%', 7: '88.80%', 8: '87.26%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 90/200, Train Loss: 0.022486, Train-Class-Acc: {0: '99.86%', 2: '99.13%', 3: '99.90%', 4: '99.37%', 5: '99.70%', 6: '99.08%', 7: '98.40%', 8: '99.04%', 9: '97.30%'}\n",
      "Val Loss: 2.117403, Val Acc: 87.41%, Val-Class-Acc: {0: '87.50%', 2: '84.03%', 3: '97.54%', 4: '90.00%', 5: '96.42%', 6: '53.70%', 7: '82.40%', 8: '85.99%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 91/200, Train Loss: 0.024590, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '99.90%', 4: '99.37%', 5: '99.63%', 6: '97.93%', 7: '98.00%', 8: '98.41%', 9: '97.30%'}\n",
      "Val Loss: 2.008705, Val Acc: 88.28%, Val-Class-Acc: {0: '89.13%', 2: '93.75%', 3: '98.36%', 4: '85.00%', 5: '96.42%', 6: '54.63%', 7: '86.40%', 8: '80.89%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 92/200, Train Loss: 0.019281, Train-Class-Acc: {0: '99.73%', 2: '99.48%', 3: '100.00%', 4: '99.37%', 5: '99.70%', 6: '98.85%', 7: '98.40%', 8: '99.04%', 9: '98.65%'}\n",
      "Val Loss: 2.118849, Val Acc: 87.41%, Val-Class-Acc: {0: '89.13%', 2: '92.36%', 3: '97.13%', 4: '82.50%', 5: '94.93%', 6: '43.52%', 7: '89.60%', 8: '85.99%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 93/200, Train Loss: 0.017338, Train-Class-Acc: {0: '99.59%', 2: '99.83%', 3: '100.00%', 5: '99.70%', 6: '99.31%', 7: '98.60%', 8: '99.20%', 9: '98.65%', 4: '99.37%'}\n",
      "Val Loss: 1.979680, Val Acc: 88.21%, Val-Class-Acc: {0: '86.96%', 2: '93.75%', 3: '98.77%', 4: '87.50%', 5: '95.52%', 6: '46.30%', 7: '86.40%', 8: '87.26%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 94/200, Train Loss: 0.016418, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '99.90%', 4: '100.00%', 5: '99.85%', 6: '99.08%', 7: '99.40%', 8: '99.68%', 9: '98.65%'}\n",
      "Val Loss: 1.746746, Val Acc: 87.63%, Val-Class-Acc: {0: '85.33%', 2: '93.06%', 3: '98.36%', 4: '82.50%', 5: '96.42%', 6: '42.59%', 7: '84.80%', 8: '88.54%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 95/200, Train Loss: 0.029404, Train-Class-Acc: {0: '99.32%', 2: '99.65%', 3: '99.69%', 4: '99.37%', 5: '99.78%', 6: '98.39%', 7: '98.20%', 8: '98.73%', 9: '97.30%'}\n",
      "Val Loss: 2.331574, Val Acc: 87.92%, Val-Class-Acc: {0: '90.22%', 2: '91.67%', 3: '98.77%', 4: '85.00%', 5: '94.03%', 6: '40.74%', 7: '89.60%', 8: '87.90%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 96/200, Train Loss: 0.012698, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '99.90%', 5: '99.70%', 6: '98.85%', 7: '98.80%', 8: '99.52%', 9: '100.00%', 4: '99.37%'}\n",
      "Val Loss: 2.208212, Val Acc: 87.48%, Val-Class-Acc: {0: '81.52%', 2: '90.97%', 3: '97.13%', 4: '90.00%', 5: '96.42%', 6: '54.63%', 7: '86.40%', 8: '85.99%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 97/200, Train Loss: 0.022240, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '99.80%', 4: '99.37%', 5: '99.70%', 6: '98.16%', 7: '99.20%', 8: '99.04%', 9: '97.97%'}\n",
      "Val Loss: 2.007662, Val Acc: 88.36%, Val-Class-Acc: {0: '90.22%', 2: '93.75%', 3: '98.36%', 4: '82.50%', 5: '96.12%', 6: '43.52%', 7: '86.40%', 8: '86.62%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 98/200, Train Loss: 0.010988, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '99.90%', 4: '98.73%', 5: '99.93%', 6: '99.31%', 7: '99.20%', 8: '99.36%', 9: '98.65%'}\n",
      "Val Loss: 2.168434, Val Acc: 87.77%, Val-Class-Acc: {0: '87.50%', 2: '90.97%', 3: '97.95%', 4: '87.50%', 5: '95.82%', 6: '45.37%', 7: '83.20%', 8: '89.81%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 99/200, Train Loss: 0.008257, Train-Class-Acc: {0: '99.59%', 2: '100.00%', 3: '99.90%', 4: '100.00%', 5: '99.93%', 6: '99.31%', 7: '99.40%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 2.093612, Val Acc: 87.99%, Val-Class-Acc: {0: '91.85%', 2: '92.36%', 3: '97.13%', 4: '87.50%', 5: '96.72%', 6: '52.78%', 7: '77.60%', 8: '84.08%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 100/200, Train Loss: 0.008299, Train-Class-Acc: {0: '99.59%', 2: '100.00%', 3: '100.00%', 4: '98.73%', 5: '100.00%', 6: '98.85%', 7: '99.20%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 2.024771, Val Acc: 87.55%, Val-Class-Acc: {0: '86.96%', 2: '91.67%', 3: '97.13%', 4: '82.50%', 5: '95.52%', 6: '52.78%', 7: '88.80%', 8: '82.80%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 101/200, Train Loss: 0.016758, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '99.80%', 4: '99.37%', 5: '99.85%', 6: '99.31%', 7: '99.20%', 8: '99.52%', 9: '98.65%'}\n",
      "Val Loss: 2.286280, Val Acc: 87.05%, Val-Class-Acc: {0: '89.13%', 2: '93.06%', 3: '95.90%', 4: '85.00%', 5: '95.52%', 6: '39.81%', 7: '82.40%', 8: '89.17%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 102/200, Train Loss: 0.032861, Train-Class-Acc: {0: '99.32%', 2: '99.31%', 3: '99.39%', 4: '99.37%', 5: '99.55%', 6: '98.85%', 7: '99.20%', 8: '98.73%', 9: '96.62%'}\n",
      "Val Loss: 2.188534, Val Acc: 87.34%, Val-Class-Acc: {0: '88.59%', 2: '90.28%', 3: '95.49%', 4: '85.00%', 5: '97.31%', 6: '51.85%', 7: '84.00%', 8: '81.53%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 103/200, Train Loss: 0.029014, Train-Class-Acc: {0: '99.32%', 2: '98.96%', 3: '99.80%', 4: '98.73%', 5: '99.63%', 6: '97.70%', 7: '98.60%', 8: '98.73%', 9: '97.97%'}\n",
      "Val Loss: 2.003507, Val Acc: 87.26%, Val-Class-Acc: {0: '89.67%', 2: '92.36%', 3: '96.31%', 4: '77.50%', 5: '94.33%', 6: '45.37%', 7: '86.40%', 8: '88.54%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 104/200, Train Loss: 0.030928, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '99.80%', 4: '99.37%', 5: '99.85%', 6: '99.08%', 7: '99.40%', 8: '99.52%', 9: '98.65%'}\n",
      "Val Loss: 2.126434, Val Acc: 87.41%, Val-Class-Acc: {0: '89.67%', 2: '93.75%', 3: '96.31%', 4: '87.50%', 5: '95.82%', 6: '47.22%', 7: '90.40%', 8: '80.89%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 105/200, Train Loss: 0.050951, Train-Class-Acc: {0: '99.32%', 2: '99.65%', 3: '99.18%', 4: '99.37%', 5: '99.63%', 6: '96.77%', 7: '98.40%', 8: '98.73%', 9: '97.30%'}\n",
      "Val Loss: 1.876787, Val Acc: 86.90%, Val-Class-Acc: {0: '88.04%', 2: '94.44%', 3: '97.95%', 4: '90.00%', 5: '94.93%', 6: '43.52%', 7: '83.20%', 8: '80.89%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 106/200, Train Loss: 0.247343, Train-Class-Acc: {0: '96.73%', 2: '98.44%', 3: '98.05%', 4: '96.84%', 5: '98.28%', 6: '92.17%', 7: '94.61%', 8: '95.86%', 9: '89.19%'}\n",
      "Val Loss: 2.375799, Val Acc: 86.32%, Val-Class-Acc: {0: '85.33%', 2: '92.36%', 3: '98.36%', 4: '82.50%', 5: '94.93%', 6: '42.59%', 7: '83.20%', 8: '84.08%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 107/200, Train Loss: 0.197847, Train-Class-Acc: {0: '98.23%', 2: '98.27%', 3: '97.23%', 4: '97.47%', 5: '98.50%', 6: '93.32%', 7: '95.01%', 8: '96.82%', 9: '94.59%'}\n",
      "Val Loss: 2.119892, Val Acc: 85.88%, Val-Class-Acc: {0: '85.87%', 2: '87.50%', 3: '96.72%', 4: '90.00%', 5: '95.52%', 6: '41.67%', 7: '82.40%', 8: '84.71%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 108/200, Train Loss: 0.226022, Train-Class-Acc: {0: '98.64%', 2: '97.57%', 3: '98.77%', 4: '97.47%', 5: '98.88%', 6: '94.24%', 7: '95.61%', 8: '96.02%', 9: '94.59%'}\n",
      "Val Loss: 2.241977, Val Acc: 86.39%, Val-Class-Acc: {0: '86.96%', 2: '90.97%', 3: '97.95%', 4: '82.50%', 5: '94.03%', 6: '35.19%', 7: '87.20%', 8: '88.54%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 109/200, Train Loss: 0.155260, Train-Class-Acc: {0: '98.64%', 2: '98.09%', 3: '99.69%', 4: '98.73%', 5: '99.03%', 6: '93.32%', 7: '96.21%', 8: '96.82%', 9: '91.89%'}\n",
      "Val Loss: 2.297161, Val Acc: 86.46%, Val-Class-Acc: {0: '83.15%', 2: '95.14%', 3: '96.31%', 4: '80.00%', 5: '93.73%', 6: '47.22%', 7: '86.40%', 8: '82.80%', 9: '75.68%'}, LR: 0.001000\n",
      "Epoch 110/200, Train Loss: 0.035222, Train-Class-Acc: {0: '98.91%', 2: '98.96%', 3: '99.69%', 4: '96.84%', 5: '99.85%', 6: '97.47%', 7: '97.41%', 8: '99.20%', 9: '96.62%'}\n",
      "Val Loss: 2.017397, Val Acc: 86.61%, Val-Class-Acc: {0: '83.70%', 2: '93.06%', 3: '97.13%', 4: '82.50%', 5: '95.22%', 6: '44.44%', 7: '87.20%', 8: '84.71%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 111/200, Train Loss: 0.021449, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '99.69%', 5: '99.78%', 6: '98.62%', 7: '97.80%', 8: '98.89%', 4: '100.00%', 9: '96.62%'}\n",
      "Val Loss: 2.094845, Val Acc: 87.34%, Val-Class-Acc: {0: '84.78%', 2: '93.75%', 3: '97.54%', 4: '90.00%', 5: '97.01%', 6: '39.81%', 7: '84.80%', 8: '87.90%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 112/200, Train Loss: 0.041203, Train-Class-Acc: {0: '99.46%', 2: '99.65%', 3: '99.39%', 4: '99.37%', 5: '99.78%', 6: '98.39%', 7: '99.20%', 8: '99.36%', 9: '98.65%'}\n",
      "Val Loss: 2.063935, Val Acc: 86.68%, Val-Class-Acc: {0: '82.07%', 2: '91.67%', 3: '97.95%', 4: '90.00%', 5: '94.03%', 6: '40.74%', 7: '85.60%', 8: '89.81%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 113/200, Train Loss: 0.028247, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '99.59%', 5: '99.40%', 6: '99.08%', 7: '97.80%', 8: '98.73%', 9: '98.65%', 4: '99.37%'}\n",
      "Val Loss: 2.064537, Val Acc: 87.05%, Val-Class-Acc: {0: '91.30%', 2: '92.36%', 3: '97.54%', 4: '90.00%', 5: '95.52%', 6: '41.67%', 7: '83.20%', 8: '80.89%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 114/200, Train Loss: 0.046302, Train-Class-Acc: {0: '99.18%', 2: '99.13%', 3: '100.00%', 5: '99.78%', 6: '99.08%', 7: '97.80%', 8: '98.89%', 9: '97.30%', 4: '98.10%'}\n",
      "Val Loss: 2.213801, Val Acc: 87.05%, Val-Class-Acc: {0: '90.76%', 2: '94.44%', 3: '97.54%', 4: '87.50%', 5: '93.43%', 6: '38.89%', 7: '84.80%', 8: '87.26%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 115/200, Train Loss: 0.016498, Train-Class-Acc: {0: '99.73%', 2: '99.48%', 3: '99.80%', 4: '98.73%', 5: '99.55%', 6: '98.85%', 7: '99.40%', 8: '99.36%', 9: '98.65%'}\n",
      "Val Loss: 2.096831, Val Acc: 86.54%, Val-Class-Acc: {0: '87.50%', 2: '92.36%', 3: '97.54%', 4: '92.50%', 5: '94.03%', 6: '43.52%', 7: '79.20%', 8: '85.35%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 116/200, Train Loss: 0.023150, Train-Class-Acc: {0: '99.18%', 2: '100.00%', 3: '99.80%', 4: '99.37%', 5: '99.70%', 6: '98.16%', 7: '99.20%', 8: '99.04%', 9: '99.32%'}\n",
      "Val Loss: 2.175455, Val Acc: 86.97%, Val-Class-Acc: {0: '88.04%', 2: '89.58%', 3: '96.31%', 4: '90.00%', 5: '96.72%', 6: '45.37%', 7: '83.20%', 8: '83.44%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 117/200, Train Loss: 0.015880, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '99.69%', 4: '100.00%', 5: '100.00%', 6: '99.31%', 7: '98.60%', 8: '99.36%', 9: '99.32%'}\n",
      "Val Loss: 1.980550, Val Acc: 87.55%, Val-Class-Acc: {0: '90.76%', 2: '91.67%', 3: '97.95%', 4: '90.00%', 5: '97.01%', 6: '40.74%', 7: '80.80%', 8: '87.26%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 118/200, Train Loss: 0.022580, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '99.90%', 4: '99.37%', 5: '99.78%', 6: '98.62%', 7: '99.40%', 8: '99.20%', 9: '99.32%'}\n",
      "Val Loss: 1.869622, Val Acc: 87.99%, Val-Class-Acc: {0: '90.22%', 2: '92.36%', 3: '97.54%', 4: '90.00%', 5: '97.01%', 6: '46.30%', 7: '80.00%', 8: '85.99%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 119/200, Train Loss: 0.101226, Train-Class-Acc: {0: '99.59%', 2: '100.00%', 3: '99.80%', 4: '100.00%', 5: '99.48%', 6: '99.31%', 7: '98.80%', 8: '99.52%', 9: '100.00%'}\n",
      "Val Loss: 3.497644, Val Acc: 85.30%, Val-Class-Acc: {0: '92.39%', 2: '94.44%', 3: '94.26%', 4: '80.00%', 5: '96.72%', 6: '49.07%', 7: '68.00%', 8: '80.25%', 9: '43.24%'}, LR: 0.001000\n",
      "Epoch 120/200, Train Loss: 0.388524, Train-Class-Acc: {0: '97.55%', 2: '97.05%', 3: '95.80%', 4: '98.10%', 5: '97.53%', 6: '91.01%', 7: '94.41%', 8: '94.90%', 9: '91.89%'}\n",
      "Val Loss: 2.010279, Val Acc: 86.54%, Val-Class-Acc: {0: '89.67%', 2: '90.97%', 3: '96.72%', 4: '90.00%', 5: '93.43%', 6: '43.52%', 7: '86.40%', 8: '86.62%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 121/200, Train Loss: 0.056826, Train-Class-Acc: {0: '98.77%', 2: '99.65%', 3: '99.90%', 4: '99.37%', 5: '99.33%', 6: '95.39%', 7: '97.80%', 8: '97.93%', 9: '94.59%'}\n",
      "Val Loss: 2.124925, Val Acc: 87.05%, Val-Class-Acc: {0: '85.87%', 2: '90.97%', 3: '97.13%', 4: '87.50%', 5: '94.93%', 6: '47.22%', 7: '82.40%', 8: '87.26%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 122/200, Train Loss: 0.030981, Train-Class-Acc: {0: '99.18%', 2: '99.48%', 3: '99.59%', 4: '98.10%', 5: '99.63%', 6: '98.85%', 7: '98.60%', 8: '98.73%', 9: '95.95%'}\n",
      "Val Loss: 1.940106, Val Acc: 86.32%, Val-Class-Acc: {0: '83.15%', 2: '92.36%', 3: '96.72%', 4: '90.00%', 5: '93.13%', 6: '48.15%', 7: '84.00%', 8: '85.35%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 123/200, Train Loss: 0.016228, Train-Class-Acc: {0: '99.32%', 2: '99.83%', 3: '99.90%', 4: '99.37%', 5: '99.85%', 6: '99.08%', 7: '99.60%', 8: '99.20%', 9: '99.32%'}\n",
      "Val Loss: 2.093257, Val Acc: 87.34%, Val-Class-Acc: {0: '86.41%', 2: '92.36%', 3: '97.95%', 4: '90.00%', 5: '94.63%', 6: '46.30%', 7: '83.20%', 8: '87.26%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 124/200, Train Loss: 0.023000, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '100.00%', 4: '99.37%', 5: '99.78%', 6: '99.31%', 7: '98.60%', 8: '99.04%', 9: '97.97%'}\n",
      "Val Loss: 2.148397, Val Acc: 88.06%, Val-Class-Acc: {0: '86.96%', 2: '92.36%', 3: '97.95%', 4: '90.00%', 5: '94.63%', 6: '53.70%', 7: '84.00%', 8: '87.26%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 125/200, Train Loss: 0.033267, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '99.80%', 4: '98.73%', 5: '99.55%', 6: '98.16%', 7: '98.20%', 8: '99.04%', 9: '97.97%'}\n",
      "Val Loss: 2.170010, Val Acc: 87.41%, Val-Class-Acc: {0: '89.13%', 2: '89.58%', 3: '97.13%', 4: '87.50%', 5: '94.33%', 6: '44.44%', 7: '88.00%', 8: '86.62%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 126/200, Train Loss: 0.035912, Train-Class-Acc: {0: '99.46%', 2: '98.79%', 3: '99.69%', 4: '98.10%', 5: '99.63%', 6: '98.16%', 7: '99.40%', 8: '99.36%', 9: '97.97%'}\n",
      "Val Loss: 2.046578, Val Acc: 88.21%, Val-Class-Acc: {0: '91.30%', 2: '93.06%', 3: '98.36%', 4: '77.50%', 5: '95.22%', 6: '55.56%', 7: '83.20%', 8: '82.80%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 127/200, Train Loss: 0.020139, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '99.80%', 4: '98.73%', 5: '99.85%', 6: '99.08%', 7: '99.00%', 8: '99.68%', 9: '97.97%'}\n",
      "Val Loss: 1.905750, Val Acc: 88.36%, Val-Class-Acc: {0: '88.04%', 2: '91.67%', 3: '97.13%', 4: '90.00%', 5: '96.42%', 6: '50.00%', 7: '84.80%', 8: '87.26%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 128/200, Train Loss: 0.011448, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '100.00%', 4: '99.37%', 5: '99.85%', 6: '99.08%', 7: '99.00%', 8: '99.36%', 9: '98.65%'}\n",
      "Val Loss: 2.067829, Val Acc: 88.14%, Val-Class-Acc: {0: '93.48%', 2: '90.97%', 3: '97.95%', 4: '90.00%', 5: '95.82%', 6: '47.22%', 7: '85.60%', 8: '84.08%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 129/200, Train Loss: 0.006205, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '99.90%', 4: '100.00%', 5: '99.85%', 6: '98.85%', 7: '99.60%', 8: '99.84%', 9: '99.32%'}\n",
      "Val Loss: 2.173424, Val Acc: 87.85%, Val-Class-Acc: {0: '88.59%', 2: '91.67%', 3: '98.77%', 4: '87.50%', 5: '96.42%', 6: '46.30%', 7: '81.60%', 8: '86.62%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 130/200, Train Loss: 0.007868, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '99.90%', 4: '99.37%', 5: '100.00%', 6: '99.77%', 7: '99.80%', 8: '99.20%', 9: '98.65%'}\n",
      "Val Loss: 2.025792, Val Acc: 88.21%, Val-Class-Acc: {0: '94.57%', 2: '91.67%', 3: '98.77%', 4: '87.50%', 5: '96.42%', 6: '44.44%', 7: '83.20%', 8: '84.71%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 131/200, Train Loss: 0.293076, Train-Class-Acc: {0: '97.96%', 2: '97.57%', 3: '99.39%', 4: '99.37%', 5: '98.73%', 6: '95.16%', 7: '98.60%', 8: '98.09%', 9: '95.95%'}\n",
      "Val Loss: 3.151796, Val Acc: 85.37%, Val-Class-Acc: {0: '86.96%', 2: '91.67%', 3: '98.36%', 4: '85.00%', 5: '94.93%', 6: '46.30%', 7: '72.80%', 8: '83.44%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 132/200, Train Loss: 0.125288, Train-Class-Acc: {0: '97.68%', 2: '97.75%', 3: '98.36%', 4: '97.47%', 5: '98.80%', 6: '94.01%', 7: '97.01%', 8: '96.66%', 9: '93.24%'}\n",
      "Val Loss: 2.035506, Val Acc: 86.61%, Val-Class-Acc: {0: '84.24%', 2: '88.89%', 3: '96.72%', 4: '90.00%', 5: '95.22%', 6: '49.07%', 7: '82.40%', 8: '84.08%', 9: '75.68%'}, LR: 0.001000\n",
      "Epoch 133/200, Train Loss: 0.042817, Train-Class-Acc: {0: '99.05%', 2: '99.65%', 3: '99.90%', 4: '98.73%', 5: '99.63%', 6: '97.00%', 7: '98.20%', 8: '98.25%', 9: '96.62%'}\n",
      "Val Loss: 1.954085, Val Acc: 87.26%, Val-Class-Acc: {0: '86.96%', 2: '90.28%', 3: '96.72%', 4: '87.50%', 5: '95.82%', 6: '52.78%', 7: '83.20%', 8: '82.17%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 134/200, Train Loss: 0.025113, Train-Class-Acc: {0: '99.46%', 2: '99.65%', 3: '100.00%', 4: '99.37%', 5: '99.78%', 6: '98.16%', 7: '98.20%', 8: '99.20%', 9: '97.30%'}\n",
      "Val Loss: 2.033137, Val Acc: 87.26%, Val-Class-Acc: {0: '91.30%', 2: '94.44%', 3: '96.72%', 4: '90.00%', 5: '94.03%', 6: '50.00%', 7: '81.60%', 8: '79.62%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 135/200, Train Loss: 0.043752, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '99.39%', 4: '98.10%', 5: '99.63%', 6: '98.62%', 7: '98.40%', 8: '98.41%', 9: '98.65%'}\n",
      "Val Loss: 2.402766, Val Acc: 86.32%, Val-Class-Acc: {0: '86.96%', 2: '91.67%', 3: '97.54%', 4: '90.00%', 5: '94.33%', 6: '37.96%', 7: '80.80%', 8: '89.81%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 136/200, Train Loss: 0.027563, Train-Class-Acc: {0: '99.18%', 2: '99.83%', 3: '99.69%', 5: '99.78%', 6: '97.70%', 7: '99.40%', 8: '98.73%', 9: '97.97%', 4: '98.73%'}\n",
      "Val Loss: 2.299674, Val Acc: 86.90%, Val-Class-Acc: {0: '84.78%', 2: '90.28%', 3: '96.31%', 4: '90.00%', 5: '96.12%', 6: '50.00%', 7: '80.80%', 8: '86.62%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 137/200, Train Loss: 0.028037, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '99.69%', 4: '98.73%', 5: '99.78%', 6: '98.39%', 7: '99.20%', 8: '99.04%', 9: '97.97%'}\n",
      "Val Loss: 2.107284, Val Acc: 86.68%, Val-Class-Acc: {0: '88.59%', 2: '92.36%', 3: '97.13%', 4: '87.50%', 5: '95.22%', 6: '41.67%', 7: '80.80%', 8: '86.62%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 138/200, Train Loss: 0.009517, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '99.90%', 4: '100.00%', 5: '99.85%', 6: '99.08%', 7: '99.60%', 8: '99.52%', 9: '100.00%'}\n",
      "Val Loss: 2.230749, Val Acc: 86.83%, Val-Class-Acc: {0: '89.67%', 2: '90.97%', 3: '97.95%', 4: '87.50%', 5: '94.93%', 6: '48.15%', 7: '84.80%', 8: '82.80%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 139/200, Train Loss: 0.008689, Train-Class-Acc: {0: '100.00%', 2: '99.65%', 3: '99.80%', 4: '99.37%', 5: '99.85%', 6: '99.54%', 7: '99.80%', 8: '99.68%', 9: '97.97%'}\n",
      "Val Loss: 2.141780, Val Acc: 86.83%, Val-Class-Acc: {0: '85.33%', 2: '88.19%', 3: '97.95%', 4: '82.50%', 5: '96.72%', 6: '44.44%', 7: '83.20%', 8: '86.62%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 140/200, Train Loss: 0.008648, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '99.90%', 4: '100.00%', 5: '99.78%', 6: '99.31%', 7: '99.60%', 8: '99.36%', 9: '99.32%'}\n",
      "Val Loss: 2.438106, Val Acc: 86.97%, Val-Class-Acc: {0: '86.41%', 2: '90.28%', 3: '97.95%', 4: '85.00%', 5: '97.01%', 6: '44.44%', 7: '77.60%', 8: '87.90%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 141/200, Train Loss: 0.006622, Train-Class-Acc: {0: '99.46%', 2: '99.83%', 3: '100.00%', 4: '99.37%', 5: '99.78%', 6: '99.77%', 7: '99.60%', 8: '99.52%', 9: '98.65%'}\n",
      "Val Loss: 2.345806, Val Acc: 86.90%, Val-Class-Acc: {0: '92.93%', 2: '91.67%', 3: '97.95%', 4: '85.00%', 5: '94.33%', 6: '50.93%', 7: '81.60%', 8: '77.07%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 142/200, Train Loss: 0.020947, Train-Class-Acc: {0: '99.86%', 2: '99.65%', 3: '100.00%', 4: '100.00%', 5: '99.55%', 6: '98.85%', 7: '99.20%', 8: '99.36%', 9: '100.00%'}\n",
      "Val Loss: 2.179861, Val Acc: 84.57%, Val-Class-Acc: {0: '80.98%', 2: '90.97%', 3: '97.95%', 4: '77.50%', 5: '92.24%', 6: '36.11%', 7: '80.00%', 8: '90.45%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 143/200, Train Loss: 0.071344, Train-Class-Acc: {0: '99.05%', 2: '97.57%', 3: '99.59%', 5: '99.85%', 6: '97.47%', 7: '97.60%', 8: '98.57%', 9: '99.32%', 4: '98.73%'}\n",
      "Val Loss: 1.946417, Val Acc: 86.32%, Val-Class-Acc: {0: '94.57%', 2: '93.06%', 3: '97.95%', 4: '85.00%', 5: '95.22%', 6: '40.74%', 7: '76.00%', 8: '82.80%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 144/200, Train Loss: 0.015663, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '99.69%', 4: '100.00%', 5: '99.70%', 6: '98.62%', 7: '99.40%', 8: '99.04%', 9: '97.97%'}\n",
      "Val Loss: 1.887132, Val Acc: 87.26%, Val-Class-Acc: {0: '86.96%', 2: '94.44%', 3: '97.13%', 4: '87.50%', 5: '96.42%', 6: '43.52%', 7: '84.80%', 8: '82.80%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 145/200, Train Loss: 0.019211, Train-Class-Acc: {0: '100.00%', 2: '99.83%', 3: '99.80%', 4: '99.37%', 5: '99.78%', 6: '99.31%', 7: '99.00%', 8: '99.36%', 9: '97.97%'}\n",
      "Val Loss: 1.904863, Val Acc: 87.26%, Val-Class-Acc: {0: '86.96%', 2: '94.44%', 3: '96.72%', 4: '90.00%', 5: '96.12%', 6: '53.70%', 7: '80.00%', 8: '80.25%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 146/200, Train Loss: 0.016235, Train-Class-Acc: {0: '100.00%', 2: '99.83%', 3: '99.69%', 4: '100.00%', 5: '99.85%', 6: '99.08%', 7: '99.60%', 8: '99.68%', 9: '97.30%'}\n",
      "Val Loss: 2.233783, Val Acc: 86.97%, Val-Class-Acc: {0: '87.50%', 2: '94.44%', 3: '97.54%', 4: '87.50%', 5: '96.12%', 6: '40.74%', 7: '79.20%', 8: '87.90%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 147/200, Train Loss: 0.009553, Train-Class-Acc: {0: '99.73%', 2: '99.48%', 3: '99.69%', 5: '99.85%', 6: '99.08%', 7: '99.80%', 8: '99.84%', 4: '99.37%', 9: '100.00%'}\n",
      "Val Loss: 2.328452, Val Acc: 87.55%, Val-Class-Acc: {0: '90.22%', 2: '93.06%', 3: '95.90%', 4: '87.50%', 5: '95.52%', 6: '44.44%', 7: '86.40%', 8: '87.26%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 148/200, Train Loss: 0.015342, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '99.39%', 4: '100.00%', 5: '99.63%', 6: '98.85%', 7: '99.00%', 8: '99.04%', 9: '100.00%'}\n",
      "Val Loss: 2.027461, Val Acc: 86.97%, Val-Class-Acc: {0: '86.41%', 2: '90.97%', 3: '97.54%', 4: '82.50%', 5: '96.12%', 6: '45.37%', 7: '88.00%', 8: '82.80%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 149/200, Train Loss: 0.010637, Train-Class-Acc: {0: '99.32%', 2: '99.48%', 3: '100.00%', 4: '99.37%', 5: '100.00%', 6: '99.31%', 7: '99.80%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 1.949444, Val Acc: 86.90%, Val-Class-Acc: {0: '85.33%', 2: '90.97%', 3: '95.49%', 4: '87.50%', 5: '95.52%', 6: '52.78%', 7: '84.00%', 8: '82.17%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 150/200, Train Loss: 0.010318, Train-Class-Acc: {0: '99.86%', 2: '99.65%', 3: '100.00%', 4: '100.00%', 5: '99.85%', 6: '100.00%', 7: '100.00%', 8: '99.84%', 9: '98.65%'}\n",
      "Val Loss: 2.057738, Val Acc: 88.21%, Val-Class-Acc: {0: '90.76%', 2: '93.75%', 3: '96.72%', 4: '90.00%', 5: '96.72%', 6: '43.52%', 7: '85.60%', 8: '82.80%', 9: '81.08%'}, LR: 0.001000\n",
      "Epoch 151/200, Train Loss: 0.010650, Train-Class-Acc: {0: '99.59%', 2: '100.00%', 3: '99.80%', 4: '100.00%', 5: '99.78%', 6: '98.62%', 7: '99.80%', 8: '100.00%', 9: '98.65%'}\n",
      "Val Loss: 2.146062, Val Acc: 86.61%, Val-Class-Acc: {0: '88.59%', 2: '83.33%', 3: '96.31%', 4: '87.50%', 5: '95.22%', 6: '40.74%', 7: '88.00%', 8: '87.26%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 152/200, Train Loss: 0.104414, Train-Class-Acc: {0: '99.59%', 2: '99.13%', 3: '99.39%', 4: '98.73%', 5: '99.25%', 6: '98.39%', 7: '97.80%', 8: '99.04%', 9: '98.65%'}\n",
      "Val Loss: 2.202976, Val Acc: 85.30%, Val-Class-Acc: {0: '88.04%', 2: '90.97%', 3: '97.95%', 4: '80.00%', 5: '89.85%', 6: '33.33%', 7: '88.00%', 8: '89.17%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 153/200, Train Loss: 0.178645, Train-Class-Acc: {0: '96.59%', 2: '98.09%', 3: '98.87%', 4: '96.20%', 5: '99.03%', 6: '93.78%', 7: '94.81%', 8: '94.90%', 9: '91.89%'}\n",
      "Val Loss: 1.950292, Val Acc: 84.50%, Val-Class-Acc: {0: '78.80%', 2: '95.83%', 3: '96.72%', 4: '85.00%', 5: '91.64%', 6: '36.11%', 7: '82.40%', 8: '86.62%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 154/200, Train Loss: 0.054501, Train-Class-Acc: {0: '99.05%', 2: '98.79%', 3: '99.69%', 4: '98.73%', 5: '99.48%', 6: '97.24%', 7: '97.01%', 8: '98.41%', 9: '98.65%'}\n",
      "Val Loss: 2.092315, Val Acc: 86.83%, Val-Class-Acc: {0: '84.78%', 2: '90.28%', 3: '96.72%', 4: '85.00%', 5: '96.42%', 6: '50.93%', 7: '85.60%', 8: '83.44%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 155/200, Train Loss: 0.075347, Train-Class-Acc: {0: '99.59%', 2: '99.13%', 3: '99.49%', 4: '98.10%', 5: '99.33%', 6: '97.47%', 7: '97.21%', 8: '99.20%', 9: '94.59%'}\n",
      "Val Loss: 2.419718, Val Acc: 86.39%, Val-Class-Acc: {0: '83.15%', 2: '91.67%', 3: '95.08%', 4: '85.00%', 5: '96.42%', 6: '41.67%', 7: '82.40%', 8: '86.62%', 9: '78.38%'}, LR: 0.001000\n",
      "Epoch 156/200, Train Loss: 0.096029, Train-Class-Acc: {0: '98.64%', 2: '98.44%', 3: '99.59%', 5: '99.40%', 6: '96.31%', 7: '98.20%', 8: '97.45%', 9: '97.30%', 4: '99.37%'}\n",
      "Val Loss: 2.165177, Val Acc: 85.95%, Val-Class-Acc: {0: '92.39%', 2: '82.64%', 3: '95.49%', 4: '90.00%', 5: '94.63%', 6: '36.11%', 7: '87.20%', 8: '85.99%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 157/200, Train Loss: 0.025987, Train-Class-Acc: {0: '99.59%', 2: '98.96%', 3: '99.90%', 4: '98.73%', 5: '99.85%', 6: '97.70%', 7: '98.80%', 8: '98.73%', 9: '97.30%'}\n",
      "Val Loss: 2.064534, Val Acc: 86.46%, Val-Class-Acc: {0: '89.13%', 2: '93.06%', 3: '96.31%', 4: '82.50%', 5: '94.33%', 6: '36.11%', 7: '86.40%', 8: '86.62%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 158/200, Train Loss: 0.018865, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '100.00%', 4: '98.10%', 5: '99.78%', 6: '98.39%', 7: '98.80%', 8: '99.04%', 9: '99.32%'}\n",
      "Val Loss: 1.940795, Val Acc: 87.05%, Val-Class-Acc: {0: '89.13%', 2: '90.28%', 3: '96.72%', 4: '87.50%', 5: '94.93%', 6: '48.15%', 7: '88.80%', 8: '80.89%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 159/200, Train Loss: 0.010587, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '99.80%', 4: '99.37%', 5: '100.00%', 6: '98.85%', 7: '99.20%', 8: '99.52%', 9: '98.65%'}\n",
      "Val Loss: 1.964772, Val Acc: 87.12%, Val-Class-Acc: {0: '88.04%', 2: '90.28%', 3: '95.90%', 4: '87.50%', 5: '96.42%', 6: '41.67%', 7: '88.80%', 8: '86.62%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 160/200, Train Loss: 0.020982, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '99.80%', 4: '99.37%', 5: '99.85%', 6: '99.08%', 7: '99.40%', 8: '99.04%', 9: '100.00%'}\n",
      "Val Loss: 2.522507, Val Acc: 87.19%, Val-Class-Acc: {0: '85.87%', 2: '91.67%', 3: '97.13%', 4: '87.50%', 5: '95.22%', 6: '42.59%', 7: '88.00%', 8: '86.62%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 161/200, Train Loss: 0.062442, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '99.90%', 4: '98.73%', 5: '99.48%', 6: '99.08%', 7: '98.20%', 8: '99.20%', 9: '98.65%'}\n",
      "Val Loss: 3.672025, Val Acc: 83.33%, Val-Class-Acc: {0: '80.43%', 2: '91.67%', 3: '99.18%', 4: '90.00%', 5: '85.97%', 6: '43.52%', 7: '74.40%', 8: '87.90%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 162/200, Train Loss: 0.430859, Train-Class-Acc: {0: '97.00%', 2: '97.75%', 3: '97.03%', 4: '94.94%', 5: '97.61%', 6: '88.48%', 7: '92.81%', 8: '94.90%', 9: '88.51%'}\n",
      "Val Loss: 2.943844, Val Acc: 85.88%, Val-Class-Acc: {0: '88.59%', 2: '90.28%', 3: '94.67%', 4: '85.00%', 5: '95.22%', 6: '50.00%', 7: '87.20%', 8: '76.43%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 163/200, Train Loss: 0.104932, Train-Class-Acc: {0: '97.82%', 2: '98.27%', 3: '99.39%', 4: '98.73%', 5: '98.73%', 6: '93.55%', 7: '96.61%', 8: '96.82%', 9: '92.57%'}\n",
      "Val Loss: 2.195577, Val Acc: 86.24%, Val-Class-Acc: {0: '88.59%', 2: '88.19%', 3: '95.90%', 4: '87.50%', 5: '96.72%', 6: '36.11%', 7: '85.60%', 8: '86.62%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 164/200, Train Loss: 0.123497, Train-Class-Acc: {0: '98.91%', 2: '98.27%', 3: '98.98%', 4: '98.10%', 5: '99.03%', 6: '97.24%', 7: '96.81%', 8: '98.25%', 9: '94.59%'}\n",
      "Val Loss: 2.560299, Val Acc: 86.90%, Val-Class-Acc: {0: '84.24%', 2: '90.28%', 3: '95.08%', 4: '85.00%', 5: '96.72%', 6: '43.52%', 7: '90.40%', 8: '86.62%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 165/200, Train Loss: 0.051976, Train-Class-Acc: {0: '98.91%', 2: '99.13%', 3: '99.59%', 5: '99.48%', 6: '97.47%', 7: '98.40%', 8: '98.57%', 9: '97.30%', 4: '98.73%'}\n",
      "Val Loss: 2.053580, Val Acc: 87.05%, Val-Class-Acc: {0: '87.50%', 2: '93.75%', 3: '95.08%', 4: '90.00%', 5: '92.84%', 6: '51.85%', 7: '83.20%', 8: '85.99%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 166/200, Train Loss: 0.032370, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '99.59%', 4: '99.37%', 5: '99.63%', 6: '97.93%', 7: '98.60%', 8: '99.04%', 9: '96.62%'}\n",
      "Val Loss: 2.205816, Val Acc: 86.54%, Val-Class-Acc: {0: '83.15%', 2: '88.89%', 3: '95.90%', 4: '87.50%', 5: '94.33%', 6: '46.30%', 7: '88.00%', 8: '87.26%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 167/200, Train Loss: 0.038104, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '99.49%', 4: '98.73%', 5: '99.85%', 6: '97.47%', 7: '98.00%', 8: '97.93%', 9: '98.65%'}\n",
      "Val Loss: 2.391457, Val Acc: 86.03%, Val-Class-Acc: {0: '78.80%', 2: '90.28%', 3: '95.49%', 4: '82.50%', 5: '96.12%', 6: '53.70%', 7: '84.80%', 8: '84.08%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 168/200, Train Loss: 0.014334, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '99.69%', 4: '97.47%', 5: '100.00%', 6: '99.31%', 7: '99.20%', 8: '99.52%', 9: '99.32%'}\n",
      "Val Loss: 2.330056, Val Acc: 86.75%, Val-Class-Acc: {0: '86.41%', 2: '90.28%', 3: '96.31%', 4: '92.50%', 5: '95.52%', 6: '42.59%', 7: '84.00%', 8: '87.90%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 169/200, Train Loss: 0.013704, Train-Class-Acc: {0: '99.73%', 2: '99.48%', 3: '100.00%', 4: '100.00%', 5: '99.93%', 6: '98.62%', 7: '98.80%', 8: '99.36%', 9: '98.65%'}\n",
      "Val Loss: 2.390178, Val Acc: 86.75%, Val-Class-Acc: {0: '86.96%', 2: '93.06%', 3: '94.26%', 4: '85.00%', 5: '96.12%', 6: '37.04%', 7: '88.80%', 8: '88.54%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 170/200, Train Loss: 0.014000, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '99.80%', 4: '99.37%', 5: '99.70%', 6: '99.08%', 7: '99.20%', 8: '99.84%', 9: '98.65%'}\n",
      "Val Loss: 2.218772, Val Acc: 86.97%, Val-Class-Acc: {0: '86.96%', 2: '93.75%', 3: '95.90%', 4: '90.00%', 5: '94.63%', 6: '45.37%', 7: '88.80%', 8: '82.80%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 171/200, Train Loss: 0.007506, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '99.78%', 6: '99.77%', 7: '99.80%', 8: '99.52%', 9: '98.65%'}\n",
      "Val Loss: 2.189210, Val Acc: 87.19%, Val-Class-Acc: {0: '84.78%', 2: '91.67%', 3: '96.72%', 4: '87.50%', 5: '96.42%', 6: '46.30%', 7: '87.20%', 8: '84.71%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 172/200, Train Loss: 0.007922, Train-Class-Acc: {0: '99.05%', 2: '100.00%', 3: '99.80%', 4: '99.37%', 5: '100.00%', 6: '99.54%', 7: '99.60%', 8: '99.84%', 9: '98.65%'}\n",
      "Val Loss: 2.033452, Val Acc: 87.99%, Val-Class-Acc: {0: '85.87%', 2: '90.97%', 3: '95.49%', 4: '92.50%', 5: '96.42%', 6: '50.93%', 7: '88.80%', 8: '84.71%', 9: '75.68%'}, LR: 0.001000\n",
      "Epoch 173/200, Train Loss: 0.009015, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '99.90%', 5: '100.00%', 6: '99.31%', 7: '99.40%', 8: '99.36%', 9: '98.65%', 4: '100.00%'}\n",
      "Val Loss: 2.230103, Val Acc: 87.70%, Val-Class-Acc: {0: '91.30%', 2: '90.97%', 3: '95.90%', 4: '90.00%', 5: '95.52%', 6: '49.07%', 7: '88.80%', 8: '82.17%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 174/200, Train Loss: 0.008591, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '99.90%', 4: '99.37%', 5: '99.85%', 6: '99.08%', 7: '99.40%', 8: '99.84%', 9: '99.32%'}\n",
      "Val Loss: 2.165653, Val Acc: 87.63%, Val-Class-Acc: {0: '90.76%', 2: '91.67%', 3: '96.31%', 4: '90.00%', 5: '95.82%', 6: '46.30%', 7: '88.00%', 8: '84.71%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 175/200, Train Loss: 0.104884, Train-Class-Acc: {0: '99.32%', 2: '98.79%', 3: '99.59%', 4: '100.00%', 5: '99.48%', 6: '98.62%', 7: '99.40%', 8: '98.73%', 9: '97.30%'}\n",
      "Val Loss: 2.451475, Val Acc: 86.54%, Val-Class-Acc: {0: '90.22%', 2: '88.19%', 3: '95.90%', 4: '90.00%', 5: '93.43%', 6: '48.15%', 7: '85.60%', 8: '86.62%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 176/200, Train Loss: 0.025835, Train-Class-Acc: {0: '99.05%', 2: '99.13%', 3: '99.90%', 4: '99.37%', 5: '99.78%', 6: '99.08%', 7: '98.60%', 8: '99.04%', 9: '96.62%'}\n",
      "Val Loss: 2.386505, Val Acc: 87.12%, Val-Class-Acc: {0: '90.76%', 2: '93.06%', 3: '95.08%', 4: '85.00%', 5: '93.73%', 6: '41.67%', 7: '88.80%', 8: '87.26%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 177/200, Train Loss: 0.029576, Train-Class-Acc: {0: '99.86%', 2: '99.48%', 3: '99.49%', 4: '98.10%', 5: '99.63%', 6: '98.62%', 7: '99.40%', 8: '99.52%', 9: '100.00%'}\n",
      "Val Loss: 2.178231, Val Acc: 87.41%, Val-Class-Acc: {0: '90.22%', 2: '91.67%', 3: '97.95%', 4: '82.50%', 5: '96.12%', 6: '42.59%', 7: '86.40%', 8: '84.71%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 178/200, Train Loss: 0.022186, Train-Class-Acc: {0: '99.18%', 2: '99.31%', 3: '99.80%', 4: '99.37%', 5: '99.48%', 6: '98.62%', 7: '99.80%', 8: '99.36%', 9: '97.97%'}\n",
      "Val Loss: 2.114800, Val Acc: 86.39%, Val-Class-Acc: {0: '83.70%', 2: '92.36%', 3: '97.13%', 4: '90.00%', 5: '93.13%', 6: '41.67%', 7: '84.00%', 8: '87.90%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 179/200, Train Loss: 0.012139, Train-Class-Acc: {0: '99.32%', 2: '99.83%', 3: '100.00%', 4: '99.37%', 5: '99.78%', 6: '99.31%', 7: '99.40%', 8: '99.52%', 9: '97.97%'}\n",
      "Val Loss: 2.104467, Val Acc: 86.39%, Val-Class-Acc: {0: '88.04%', 2: '88.19%', 3: '95.90%', 4: '85.00%', 5: '93.73%', 6: '40.74%', 7: '87.20%', 8: '86.62%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 180/200, Train Loss: 0.006597, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '99.80%', 4: '100.00%', 5: '100.00%', 6: '99.08%', 7: '99.40%', 8: '99.52%', 9: '100.00%'}\n",
      "Val Loss: 2.114385, Val Acc: 87.34%, Val-Class-Acc: {0: '90.76%', 2: '91.67%', 3: '95.49%', 4: '85.00%', 5: '94.93%', 6: '41.67%', 7: '86.40%', 8: '88.54%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 181/200, Train Loss: 0.008610, Train-Class-Acc: {0: '100.00%', 2: '99.65%', 3: '99.90%', 4: '98.10%', 5: '99.93%', 6: '99.08%', 7: '99.80%', 8: '99.84%', 9: '99.32%'}\n",
      "Val Loss: 2.435311, Val Acc: 87.48%, Val-Class-Acc: {0: '85.87%', 2: '93.06%', 3: '95.49%', 4: '87.50%', 5: '95.22%', 6: '49.07%', 7: '84.80%', 8: '87.90%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 182/200, Train Loss: 0.005551, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '99.90%', 4: '99.37%', 5: '99.93%', 6: '98.85%', 7: '99.20%', 8: '99.68%', 9: '100.00%'}\n",
      "Val Loss: 2.242475, Val Acc: 88.06%, Val-Class-Acc: {0: '88.59%', 2: '91.67%', 3: '97.13%', 4: '90.00%', 5: '94.93%', 6: '51.85%', 7: '85.60%', 8: '87.26%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 183/200, Train Loss: 0.007593, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '99.90%', 4: '100.00%', 5: '100.00%', 6: '99.54%', 7: '100.00%', 8: '99.52%', 9: '99.32%'}\n",
      "Val Loss: 2.322504, Val Acc: 86.17%, Val-Class-Acc: {0: '79.35%', 2: '92.36%', 3: '93.44%', 4: '90.00%', 5: '94.63%', 6: '48.15%', 7: '87.20%', 8: '87.90%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 184/200, Train Loss: 0.004090, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '100.00%', 5: '99.85%', 6: '99.77%', 7: '99.80%', 8: '99.84%', 9: '98.65%', 4: '100.00%'}\n",
      "Val Loss: 2.403356, Val Acc: 87.70%, Val-Class-Acc: {0: '88.04%', 2: '93.06%', 3: '95.90%', 4: '90.00%', 5: '96.42%', 6: '52.78%', 7: '85.60%', 8: '84.08%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 185/200, Train Loss: 0.006909, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '99.80%', 5: '99.93%', 6: '99.54%', 7: '99.80%', 8: '99.84%', 9: '99.32%', 4: '100.00%'}\n",
      "Val Loss: 2.421063, Val Acc: 86.90%, Val-Class-Acc: {0: '87.50%', 2: '93.06%', 3: '97.54%', 4: '82.50%', 5: '91.94%', 6: '50.00%', 7: '88.00%', 8: '84.71%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 186/200, Train Loss: 0.004762, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '100.00%', 4: '99.37%', 5: '99.85%', 6: '99.31%', 7: '100.00%', 8: '100.00%', 9: '99.32%'}\n",
      "Val Loss: 2.466010, Val Acc: 86.32%, Val-Class-Acc: {0: '86.41%', 2: '92.36%', 3: '96.31%', 4: '87.50%', 5: '95.22%', 6: '36.11%', 7: '87.20%', 8: '85.99%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 187/200, Train Loss: 0.049583, Train-Class-Acc: {0: '99.59%', 2: '99.83%', 3: '99.90%', 4: '98.73%', 5: '99.63%', 6: '99.31%', 7: '99.00%', 8: '99.52%', 9: '98.65%'}\n",
      "Val Loss: 2.695828, Val Acc: 85.88%, Val-Class-Acc: {0: '83.15%', 2: '92.36%', 3: '97.54%', 4: '77.50%', 5: '94.33%', 6: '37.96%', 7: '84.80%', 8: '88.54%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 188/200, Train Loss: 0.010874, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '99.85%', 6: '99.77%', 7: '99.40%', 8: '99.52%', 9: '98.65%'}\n",
      "Val Loss: 3.235831, Val Acc: 86.32%, Val-Class-Acc: {0: '88.04%', 2: '92.36%', 3: '97.95%', 4: '90.00%', 5: '96.12%', 6: '34.26%', 7: '78.40%', 8: '85.99%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 189/200, Train Loss: 0.057955, Train-Class-Acc: {0: '99.32%', 2: '98.96%', 3: '99.39%', 4: '98.73%', 5: '99.55%', 6: '98.16%', 7: '97.80%', 8: '99.04%', 9: '99.32%'}\n",
      "Val Loss: 2.391563, Val Acc: 85.52%, Val-Class-Acc: {0: '80.98%', 2: '92.36%', 3: '97.13%', 4: '77.50%', 5: '96.42%', 6: '38.89%', 7: '80.00%', 8: '85.35%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 190/200, Train Loss: 0.025872, Train-Class-Acc: {0: '99.32%', 2: '99.83%', 3: '99.49%', 4: '99.37%', 5: '99.85%', 6: '98.39%', 7: '99.40%', 8: '98.89%', 9: '97.97%'}\n",
      "Val Loss: 2.394377, Val Acc: 84.93%, Val-Class-Acc: {0: '82.61%', 2: '93.06%', 3: '95.90%', 4: '77.50%', 5: '94.03%', 6: '35.19%', 7: '84.00%', 8: '85.35%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 191/200, Train Loss: 0.014426, Train-Class-Acc: {0: '99.32%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '99.78%', 6: '99.31%', 7: '99.20%', 8: '99.36%', 9: '98.65%'}\n",
      "Val Loss: 2.424405, Val Acc: 85.74%, Val-Class-Acc: {0: '85.33%', 2: '93.06%', 3: '93.85%', 4: '80.00%', 5: '95.52%', 6: '44.44%', 7: '84.00%', 8: '81.53%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 192/200, Train Loss: 0.094382, Train-Class-Acc: {0: '99.18%', 2: '98.44%', 3: '99.39%', 4: '100.00%', 5: '98.88%', 6: '96.77%', 7: '97.21%', 8: '98.25%', 9: '99.32%'}\n",
      "Val Loss: 2.173834, Val Acc: 86.24%, Val-Class-Acc: {0: '84.78%', 2: '87.50%', 3: '96.72%', 4: '85.00%', 5: '94.63%', 6: '48.15%', 7: '88.00%', 8: '85.99%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 193/200, Train Loss: 0.068277, Train-Class-Acc: {0: '98.37%', 2: '98.96%', 3: '99.39%', 4: '96.84%', 5: '99.63%', 6: '98.16%', 7: '97.21%', 8: '98.57%', 9: '95.95%'}\n",
      "Val Loss: 2.392735, Val Acc: 86.03%, Val-Class-Acc: {0: '83.70%', 2: '90.28%', 3: '94.67%', 4: '82.50%', 5: '95.82%', 6: '39.81%', 7: '88.00%', 8: '85.99%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 194/200, Train Loss: 0.022324, Train-Class-Acc: {0: '99.32%', 2: '99.65%', 3: '99.49%', 5: '99.78%', 6: '98.39%', 7: '98.40%', 8: '99.20%', 9: '99.32%', 4: '98.73%'}\n",
      "Val Loss: 2.254048, Val Acc: 85.59%, Val-Class-Acc: {0: '85.33%', 2: '93.75%', 3: '97.13%', 4: '85.00%', 5: '95.52%', 6: '43.52%', 7: '78.40%', 8: '82.17%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 195/200, Train Loss: 0.012005, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '100.00%', 4: '99.37%', 5: '99.85%', 6: '98.39%', 7: '99.60%', 8: '99.20%', 9: '97.30%'}\n",
      "Val Loss: 2.195514, Val Acc: 85.15%, Val-Class-Acc: {0: '83.70%', 2: '90.97%', 3: '94.67%', 4: '82.50%', 5: '93.73%', 6: '48.15%', 7: '79.20%', 8: '85.35%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 196/200, Train Loss: 0.010075, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '99.80%', 4: '99.37%', 5: '99.85%', 6: '99.31%', 7: '99.60%', 8: '99.52%', 9: '98.65%'}\n",
      "Val Loss: 2.331655, Val Acc: 86.61%, Val-Class-Acc: {0: '84.24%', 2: '93.75%', 3: '96.72%', 4: '85.00%', 5: '95.22%', 6: '43.52%', 7: '84.80%', 8: '83.44%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 197/200, Train Loss: 0.005985, Train-Class-Acc: {0: '99.86%', 2: '99.65%', 3: '99.90%', 5: '100.00%', 6: '99.31%', 7: '99.80%', 8: '99.36%', 4: '100.00%', 9: '100.00%'}\n",
      "Val Loss: 2.368323, Val Acc: 85.74%, Val-Class-Acc: {0: '82.61%', 2: '93.75%', 3: '94.67%', 4: '85.00%', 5: '94.33%', 6: '44.44%', 7: '84.00%', 8: '86.62%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 198/200, Train Loss: 0.010459, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '99.80%', 4: '100.00%', 5: '99.85%', 6: '99.31%', 7: '99.60%', 8: '99.68%', 9: '97.30%'}\n",
      "Val Loss: 2.949731, Val Acc: 86.61%, Val-Class-Acc: {0: '87.50%', 2: '93.75%', 3: '93.44%', 4: '85.00%', 5: '96.12%', 6: '42.59%', 7: '87.20%', 8: '87.26%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 199/200, Train Loss: 0.018964, Train-Class-Acc: {0: '99.18%', 2: '99.65%', 3: '99.69%', 4: '100.00%', 5: '99.85%', 6: '98.85%', 7: '99.20%', 8: '99.52%', 9: '99.32%'}\n",
      "Val Loss: 2.341662, Val Acc: 85.59%, Val-Class-Acc: {0: '83.70%', 2: '93.75%', 3: '97.13%', 4: '82.50%', 5: '90.75%', 6: '47.22%', 7: '81.60%', 8: '89.17%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 200/200, Train Loss: 0.049638, Train-Class-Acc: {0: '99.73%', 2: '99.13%', 3: '99.28%', 4: '99.37%', 5: '99.48%', 6: '98.62%', 7: '97.80%', 8: '98.57%', 9: '98.65%'}\n",
      "Val Loss: 2.843619, Val Acc: 85.15%, Val-Class-Acc: {0: '85.33%', 2: '90.97%', 3: '95.08%', 4: '80.00%', 5: '96.72%', 6: '42.59%', 7: '82.40%', 8: '79.62%', 9: '54.05%'}, LR: 0.001000\n",
      "\n",
      "üèÜ Best model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_best.pth (Val Accuracy: 89.37%)\n",
      "\n",
      "üìå Final model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_final.pth\n",
      "\n",
      "üéØ Top 5 Best Models:\n",
      "Epoch 44, Train Loss: 0.069169, Train-Acc: {0: '99.18%', 2: '99.13%', 3: '99.59%', 4: '98.10%', 5: '99.10%', 6: '95.85%', 7: '96.81%', 8: '97.77%', 9: '97.30%'},\n",
      "Val Loss: 1.787247, Val Acc: 89.37%, Val-Acc: {0: '89.67%', 2: '94.44%', 3: '97.95%', 4: '87.50%', 5: '98.81%', 6: '44.44%', 7: '88.80%', 8: '86.62%', 9: '72.97%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_44.pth\n",
      "Epoch 33, Train Loss: 0.061481, Train-Acc: {0: '99.05%', 2: '99.13%', 3: '99.69%', 4: '98.73%', 5: '99.40%', 6: '95.85%', 7: '96.81%', 8: '97.29%', 9: '97.30%'},\n",
      "Val Loss: 1.530180, Val Acc: 89.37%, Val-Acc: {0: '91.85%', 2: '93.75%', 3: '99.59%', 4: '90.00%', 5: '95.82%', 6: '51.85%', 7: '84.80%', 8: '85.99%', 9: '72.97%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_33.pth\n",
      "Epoch 35, Train Loss: 0.025367, Train-Acc: {0: '99.86%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '99.78%', 6: '97.47%', 7: '97.80%', 8: '99.04%', 9: '95.95%'},\n",
      "Val Loss: 2.079794, Val Acc: 89.16%, Val-Acc: {0: '92.39%', 2: '94.44%', 3: '98.77%', 4: '87.50%', 5: '96.42%', 6: '44.44%', 7: '87.20%', 8: '87.26%', 9: '70.27%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_35.pth\n",
      "Epoch 17, Train Loss: 0.183200, Train-Acc: {0: '97.96%', 2: '98.44%', 3: '99.18%', 4: '96.84%', 5: '98.13%', 6: '91.47%', 7: '92.22%', 8: '94.75%', 9: '87.16%'},\n",
      "Val Loss: 1.201720, Val Acc: 89.16%, Val-Acc: {0: '91.85%', 2: '94.44%', 3: '99.18%', 4: '90.00%', 5: '94.93%', 6: '57.41%', 7: '88.00%', 8: '80.25%', 9: '70.27%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_17.pth\n",
      "Epoch 10, Train Loss: 0.367432, Train-Acc: {0: '96.32%', 2: '95.84%', 3: '98.16%', 4: '91.77%', 5: '97.01%', 6: '75.12%', 7: '85.03%', 8: '86.94%', 9: '74.32%'},\n",
      "Val Loss: 1.066937, Val Acc: 89.08%, Val-Acc: {0: '95.11%', 2: '95.14%', 3: '99.18%', 4: '85.00%', 5: '95.82%', 6: '51.85%', 7: '84.00%', 8: '83.44%', 9: '62.16%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_10.pth\n",
      "\n",
      "üß† Model Summary:\n",
      "Total Parameters: 3,895,946\n",
      "Model Size (float32): 14.86 MB\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "Number of LoRA adapters: 8\n",
      "Number of LoRA groups: 1\n",
      "Total Training Time: 574.52 seconds\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "---\n",
      "### Period 4 (alpha = 0.0, similarity_threshold = 0.65)\n",
      "+ ##### Total training time: 574.52 seconds\n",
      "+ ##### Model: ResNet18_1D_LoRA\n",
      "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4'*\n",
      "+ ##### Best Epoch: 44\n",
      "#### __Val Accuracy: 89.37%__\n",
      "#### __Val-Class-Acc: {0: '89.67%', 2: '94.44%', 3: '97.95%', 4: '87.50%', 5: '98.81%', 6: '44.44%', 7: '88.80%', 8: '86.62%', 9: '72.97%'}__\n",
      "#### __Total Parameters: 3,895,946__\n",
      "#### __Model Size (float32): 14.86 MB__\n",
      "#### __Number of LoRA adapters: 8__\n",
      "#### __Number of LoRA groups: 1__\n",
      "\n",
      "Saved class features to: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_4/class_features.pkl\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå Period 4: DynEx-CLoRA Training (ECG)\n",
    "# ================================\n",
    "period = 4\n",
    "\n",
    "# ==== Paths ====\n",
    "stop_signal_file = os.path.join(BASE_DIR, \"stop_training.txt\")\n",
    "model_saving_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v3\", f\"Period_{period}\")\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# ==== Load Period 4 Data ====\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{period}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{period}.npy\"))\n",
    "X_val   = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val   = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "# ==== Device ====\n",
    "device = auto_select_cuda_device()\n",
    "\n",
    "# ==== Load Period 3 Features ====\n",
    "prev_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v3\", \"Period_3\")\n",
    "class_features_path = os.path.join(prev_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, \"rb\") as f:\n",
    "    class_features_dict = pickle.load(f)\n",
    "print(f\"‚úÖ Loaded class features from: {class_features_path}\")\n",
    "\n",
    "# ==== Load Period 3 Checkpoint ====\n",
    "prev_model_path = os.path.join(prev_folder, \"ResNet18_1D_LoRA_best.pth\")\n",
    "checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "\n",
    "input_channels = X_train.shape[2]\n",
    "output_size = int(np.max(y_train)) + 1  # e.g., max=9 ‚Üí output_size=10\n",
    "\n",
    "# ==== Build Teacher Model ====\n",
    "teacher_output_size = len(np.unique(np.load(os.path.join(save_dir, f\"y_train_p{period-1}.npy\"))))\n",
    "teacher_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=teacher_output_size).to(device)\n",
    "teacher_model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "teacher_model.eval()\n",
    "\n",
    "# ==== Build Student Model ====\n",
    "model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size).to(device)\n",
    "\n",
    "# ==== Sync Adapter Structure ====\n",
    "num_lora_groups = checkpoint.get(\"num_lora_groups\", 0)\n",
    "print(f\"üîÑ Number of LoRA groups: {num_lora_groups}\")\n",
    "related_labels = checkpoint.get(\"related_labels\", {\"base\": [0, 1]})\n",
    "for _ in range(num_lora_groups):\n",
    "    model.add_lora_adapter()\n",
    "\n",
    "# ==== Load Shared Weights (excluding FC) ====\n",
    "model_dict = model.state_dict()\n",
    "prev_state_dict = checkpoint[\"model_state_dict\"]\n",
    "filtered_dict = {\n",
    "    k: v for k, v in prev_state_dict.items()\n",
    "    if k in model_dict and model_dict[k].shape == v.shape and k not in [\"fc.weight\", \"fc.bias\"]\n",
    "}\n",
    "model.load_state_dict(filtered_dict, strict=False)\n",
    "for k in model_dict:\n",
    "    if k not in filtered_dict:\n",
    "        print(f\"üîç Not loaded: {k}, shape={model_dict[k].shape}\")\n",
    "print(\"‚úÖ Loaded shared weights from Period 3 (excluding FC only)\")\n",
    "\n",
    "# ==== Training Configuration ====\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "alpha = 0.0 # No distillation\n",
    "similarity_threshold = 0.65\n",
    "stable_classes = [0, 2, 3, 4, 5]  # ‚Üê Ë´ãÊ†πÊìö‰Ω†ÂâçÊúüÈ°ûÂà•Ë™øÊï¥ÔºÅ\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# ==== Start Training ====\n",
    "train_with_dynex_clora_ecg(\n",
    "    model=model,\n",
    "    teacher_model=None,\n",
    "    output_size=output_size,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    alpha=alpha,\n",
    "    model_saving_folder=model_saving_folder,\n",
    "    model_name=\"ResNet18_1D_LoRA\",\n",
    "    stop_signal_file=stop_signal_file,\n",
    "    scheduler=scheduler,\n",
    "    period=period,\n",
    "    stable_classes=stable_classes,\n",
    "    similarity_threshold=similarity_threshold,\n",
    "    class_features_dict=class_features_dict,\n",
    "    related_labels=related_labels,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ==== Cleanup ====\n",
    "del X_train, y_train, X_val, y_val\n",
    "del model, teacher_model, checkpoint, model_dict, prev_state_dict, filtered_dict\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e14f47",
   "metadata": {},
   "source": [
    "#### v4 no distillation, all freeze, th = 0.70\n",
    "##### Period 4 (alpha = 0.0, similarity_threshold = 0.7)\n",
    "+ ##### Total training time: 886.21 seconds\n",
    "+ ##### Model: ResNet18_1D_LoRA\n",
    "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4'*\n",
    "+ ##### Best Epoch: 15\n",
    "##### __Val Accuracy: 88.79%__\n",
    "##### __Val-Class-Acc: {0: '91.85%', 2: '95.14%', 3: '98.77%', 4: '87.50%', 5: '97.01%', 6: '42.59%', 7: '83.20%', 8: '86.62%', 9: '72.97%'}__\n",
    "##### __Total Parameters: 3,895,946__\n",
    "##### __Model Size (float32): 14.86 MB__\n",
    "##### __Number of LoRA adapters: 8__\n",
    "##### __Number of LoRA groups: 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42a97def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 2\n",
      "    - Memory Used    : 2133 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ Loaded class features from: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/class_features.pkl\n",
      "üîÑ Number of LoRA groups: 1\n",
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n",
      "üîç Not loaded: fc.weight, shape=torch.Size([10, 1024])\n",
      "üîç Not loaded: fc.bias, shape=torch.Size([10])\n",
      "‚úÖ Loaded shared weights from Period 3 (excluding FC only)\n",
      "\n",
      "üöÄ 'train_with_dynex_clora_ecg' started for Period 4\n",
      "\n",
      "‚úÖ Removed existing folder: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1775968/3400798578.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Data Overview:\n",
      "X_train: torch.Size([5493, 5000, 12]), y_train: torch.Size([5493])\n",
      "X_val: torch.Size([1374, 5000, 12]), y_val: torch.Size([1374])\n",
      "\n",
      "üîé Similarity Analysis:\n",
      "  Similarity threshold: 0.7000\n",
      "  Existing classes: [0, 1, 2, 3, 4, 5]\n",
      "  Current classes: [0, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  New classes: [6, 7, 8, 9]\n",
      "\n",
      "üìä Similarity Scores:\n",
      "  New Class 6:\n",
      "    - Existing Class 1: 0.9078\n",
      "    - Existing Class 4: 0.9039\n",
      "    - Existing Class 0: 0.8786\n",
      "    - Existing Class 2: 0.8730\n",
      "    - Existing Class 3: 0.8699\n",
      "    - Existing Class 5: 0.8550\n",
      "  New Class 7:\n",
      "    - Existing Class 1: 0.9061\n",
      "    - Existing Class 4: 0.9041\n",
      "    - Existing Class 0: 0.8745\n",
      "    - Existing Class 2: 0.8719\n",
      "    - Existing Class 3: 0.8712\n",
      "    - Existing Class 5: 0.8631\n",
      "  New Class 8:\n",
      "    - Existing Class 1: 0.8973\n",
      "    - Existing Class 4: 0.8882\n",
      "    - Existing Class 0: 0.8746\n",
      "    - Existing Class 2: 0.8499\n",
      "    - Existing Class 3: 0.8435\n",
      "    - Existing Class 5: 0.8316\n",
      "  New Class 9:\n",
      "    - Existing Class 1: 0.8995\n",
      "    - Existing Class 4: 0.8924\n",
      "    - Existing Class 0: 0.8799\n",
      "    - Existing Class 2: 0.8592\n",
      "    - Existing Class 3: 0.8532\n",
      "    - Existing Class 5: 0.8383\n",
      "\n",
      "  Average similarity: 0.8682, Std: 0.0262\n",
      "\n",
      "üß© Managing LoRA adapters for 4 new classes...\n",
      "üîÑ New Class 8 is similar to Class 1 (sim=0.8973) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 8 is similar to Class 2 (sim=0.8499) ‚Üí Added to adapter '0'\n",
      "üîÑ New Class 9 is similar to Class 1 (sim=0.8995) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 9 is similar to Class 2 (sim=0.8592) ‚Üí Added to adapter '0'\n",
      "üîÑ New Class 6 is similar to Class 1 (sim=0.9078) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 6 is similar to Class 2 (sim=0.8730) ‚Üí Added to adapter '0'\n",
      "üîÑ New Class 7 is similar to Class 1 (sim=0.9061) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 7 is similar to Class 2 (sim=0.8719) ‚Üí Added to adapter '0'\n",
      "\n",
      "üîç Checking stability of existing classes...\n",
      "  Class 0 similarity with itself: 0.8814\n",
      "  Class 2 similarity with itself: 0.8816\n",
      "  Class 3 similarity with itself: 0.8985\n",
      "  Class 4 similarity with itself: 0.8817\n",
      "  Class 5 similarity with itself: 0.9125\n",
      "\n",
      "üîí Default: Freezing ALL model parameters\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå fc.weight                                          | shape=[10, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[10]\n",
      "\n",
      "üîì Unfreezing selected adapters (by group):\n",
      "  - Adapter Group #0 (all blocks) (classes: [2, 4, 5, 8, 9, 6, 7])\n",
      "  - Base layers (classes: [0, 1, 3, 4, 5, 8, 9, 6, 7])\n",
      "\n",
      "üîç Frozen Parameters: (After Unfreeze specific adapter groups)\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå fc.weight                                          | shape=[10, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[10]\n",
      "\n",
      "üîç Frozen Parameters (After Unfreeze newly added adapter groups):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå fc.weight                                          | shape=[10, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[10]\n",
      "\n",
      "üîç Frozen Parameters (After unfreeze the final layer):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "\n",
      "üìã Related Labels Summary:\n",
      "  - Base network: Classes [0, 1, 3, 4, 5, 8, 9, 6, 7]\n",
      "  - Adapter #0: Classes [2, 4, 5, 8, 9, 6, 7]\n",
      "\n",
      "üîß Trainable Parameter Status:\n",
      "  ‚úÖ layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ fc.weight                                          | shape=[10, 1024]\n",
      "  ‚úÖ fc.bias                                            | shape=[10]\n",
      "trainable_count: 26\n",
      "\n",
      "üìä Parameter Statistics:\n",
      "  - Total parameters: 3,895,946\n",
      "  - Trainable parameters: 2,129,930 (54.67%)\n",
      "  - Frozen parameters: 1,766,016 (45.33%)\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "\n",
      "üß† Parameters currently controlled by the optimizer: (78)\n",
      "  ‚õî conv1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer1.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.0.conv2.weight\n",
      "  ‚õî layer1.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer1.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.conv2.weight\n",
      "  ‚õî layer1.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer2.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.conv2.weight\n",
      "  ‚õî layer2.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer2.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.conv2.weight\n",
      "  ‚õî layer2.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer3.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.conv2.weight\n",
      "  ‚õî layer3.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer3.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.conv2.weight\n",
      "  ‚õî layer3.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer4.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.conv2.weight\n",
      "  ‚õî layer4.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer4.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.conv2.weight\n",
      "  ‚õî layer4.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ fc.weight\n",
      "  ‚úÖ fc.bias\n",
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/200, Train Loss: 5.774096, Train-Class-Acc: {0: '54.90%', 2: '56.50%', 3: '62.30%', 4: '25.32%', 5: '77.56%', 6: '16.59%', 7: '41.32%', 8: '44.11%', 9: '10.14%'}\n",
      "Val Loss: 1.473135, Val Acc: 82.31%, Val-Class-Acc: {0: '88.59%', 2: '90.28%', 3: '97.54%', 4: '65.00%', 5: '94.63%', 6: '20.37%', 7: '80.80%', 8: '81.53%', 9: '16.22%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_1.pth\n",
      "Epoch 2/200, Train Loss: 1.632167, Train-Class-Acc: {0: '79.02%', 2: '83.36%', 3: '89.65%', 4: '73.42%', 5: '91.17%', 6: '30.18%', 7: '61.08%', 8: '63.69%', 9: '24.32%'}\n",
      "Val Loss: 1.112776, Val Acc: 84.50%, Val-Class-Acc: {0: '89.67%', 2: '94.44%', 3: '97.95%', 4: '67.50%', 5: '92.84%', 6: '35.19%', 7: '80.00%', 8: '80.25%', 9: '51.35%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_2.pth\n",
      "Epoch 3/200, Train Loss: 1.244659, Train-Class-Acc: {0: '85.29%', 2: '88.91%', 3: '91.19%', 4: '75.32%', 5: '91.55%', 6: '43.32%', 7: '64.47%', 8: '69.11%', 9: '43.24%'}\n",
      "Val Loss: 1.179095, Val Acc: 86.32%, Val-Class-Acc: {0: '90.76%', 2: '93.06%', 3: '97.95%', 4: '82.50%', 5: '97.01%', 6: '35.19%', 7: '79.20%', 8: '84.08%', 9: '51.35%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_3.pth\n",
      "Epoch 4/200, Train Loss: 0.911698, Train-Class-Acc: {0: '89.78%', 2: '92.37%', 3: '94.26%', 4: '79.75%', 5: '94.09%', 6: '49.54%', 7: '70.86%', 8: '75.64%', 9: '46.62%'}\n",
      "Val Loss: 1.126526, Val Acc: 86.61%, Val-Class-Acc: {0: '89.67%', 2: '95.83%', 3: '96.72%', 4: '87.50%', 5: '94.93%', 6: '33.33%', 7: '84.00%', 8: '84.71%', 9: '64.86%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_4.pth\n",
      "Epoch 5/200, Train Loss: 0.876547, Train-Class-Acc: {0: '91.14%', 2: '91.85%', 3: '94.16%', 4: '81.65%', 5: '94.39%', 6: '57.37%', 7: '73.25%', 8: '77.55%', 9: '60.14%'}\n",
      "Val Loss: 1.097037, Val Acc: 87.70%, Val-Class-Acc: {0: '90.22%', 2: '95.83%', 3: '97.54%', 4: '82.50%', 5: '94.93%', 6: '39.81%', 7: '85.60%', 8: '89.17%', 9: '59.46%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_5.pth\n",
      "Epoch 6/200, Train Loss: 0.587320, Train-Class-Acc: {0: '93.05%', 2: '94.11%', 3: '95.39%', 4: '89.24%', 5: '95.81%', 6: '62.44%', 7: '75.85%', 8: '82.17%', 9: '58.78%'}\n",
      "Val Loss: 1.348546, Val Acc: 86.97%, Val-Class-Acc: {0: '94.02%', 2: '93.06%', 3: '98.36%', 4: '85.00%', 5: '94.93%', 6: '42.59%', 7: '78.40%', 8: '84.08%', 9: '54.05%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_1.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_6.pth\n",
      "Epoch 7/200, Train Loss: 0.527467, Train-Class-Acc: {0: '93.05%', 2: '94.97%', 3: '96.82%', 4: '85.44%', 5: '96.19%', 6: '63.59%', 7: '77.05%', 8: '83.44%', 9: '60.14%'}\n",
      "Val Loss: 1.560292, Val Acc: 87.34%, Val-Class-Acc: {0: '94.02%', 2: '91.67%', 3: '98.36%', 4: '82.50%', 5: '99.40%', 6: '47.22%', 7: '80.80%', 8: '76.43%', 9: '45.95%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_2.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_7.pth\n",
      "Epoch 8/200, Train Loss: 0.513794, Train-Class-Acc: {0: '93.87%', 2: '94.80%', 3: '96.41%', 5: '96.11%', 6: '67.74%', 7: '82.44%', 8: '83.28%', 9: '70.95%', 4: '84.81%'}\n",
      "Val Loss: 0.943381, Val Acc: 87.12%, Val-Class-Acc: {0: '92.39%', 2: '95.14%', 3: '95.08%', 4: '87.50%', 5: '97.01%', 6: '39.81%', 7: '83.20%', 8: '84.71%', 9: '48.65%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_3.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_8.pth\n",
      "Epoch 9/200, Train Loss: 0.382612, Train-Class-Acc: {0: '95.78%', 2: '95.15%', 3: '96.93%', 4: '91.14%', 5: '97.23%', 6: '73.04%', 7: '83.23%', 8: '87.10%', 9: '66.89%'}\n",
      "Val Loss: 1.076397, Val Acc: 87.26%, Val-Class-Acc: {0: '92.39%', 2: '92.36%', 3: '97.95%', 4: '85.00%', 5: '94.93%', 6: '39.81%', 7: '81.60%', 8: '87.90%', 9: '59.46%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_4.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_9.pth\n",
      "Epoch 10/200, Train Loss: 0.302855, Train-Class-Acc: {0: '95.78%', 2: '96.19%', 3: '98.05%', 4: '88.61%', 5: '97.46%', 6: '76.96%', 7: '84.43%', 8: '90.29%', 9: '79.73%'}\n",
      "Val Loss: 1.427008, Val Acc: 87.85%, Val-Class-Acc: {0: '92.93%', 2: '94.44%', 3: '97.54%', 4: '90.00%', 5: '97.31%', 6: '47.22%', 7: '83.20%', 8: '77.07%', 9: '64.86%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_6.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_10.pth\n",
      "Epoch 11/200, Train Loss: 0.276735, Train-Class-Acc: {0: '96.32%', 2: '97.05%', 3: '97.75%', 4: '92.41%', 5: '97.61%', 6: '76.96%', 7: '86.63%', 8: '88.38%', 9: '78.38%'}\n",
      "Val Loss: 1.268141, Val Acc: 87.92%, Val-Class-Acc: {0: '91.30%', 2: '94.44%', 3: '98.77%', 4: '85.00%', 5: '95.82%', 6: '39.81%', 7: '83.20%', 8: '85.99%', 9: '70.27%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_8.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_11.pth\n",
      "Epoch 12/200, Train Loss: 0.227666, Train-Class-Acc: {0: '96.59%', 2: '97.57%', 3: '98.57%', 4: '95.57%', 5: '98.06%', 6: '84.56%', 7: '88.62%', 8: '92.20%', 9: '81.08%'}\n",
      "Val Loss: 1.444557, Val Acc: 88.14%, Val-Class-Acc: {0: '89.67%', 2: '94.44%', 3: '99.18%', 4: '85.00%', 5: '96.72%', 6: '45.37%', 7: '83.20%', 8: '84.08%', 9: '67.57%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_9.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_12.pth\n",
      "Epoch 13/200, Train Loss: 0.419644, Train-Class-Acc: {0: '96.73%', 2: '97.05%', 3: '98.16%', 4: '91.77%', 5: '97.08%', 6: '83.18%', 8: '90.92%', 7: '86.63%', 9: '75.00%'}\n",
      "Val Loss: 1.592159, Val Acc: 87.41%, Val-Class-Acc: {0: '89.67%', 2: '93.06%', 3: '98.77%', 4: '87.50%', 5: '94.63%', 6: '42.59%', 7: '82.40%', 8: '88.54%', 9: '56.76%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_7.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_13.pth\n",
      "Epoch 14/200, Train Loss: 0.295253, Train-Class-Acc: {0: '96.87%', 2: '97.05%', 3: '97.03%', 4: '93.67%', 5: '97.08%', 6: '78.57%', 7: '88.42%', 8: '90.92%', 9: '83.78%'}\n",
      "Val Loss: 1.424641, Val Acc: 87.41%, Val-Class-Acc: {0: '88.59%', 2: '93.75%', 3: '97.95%', 4: '82.50%', 5: '96.12%', 6: '43.52%', 7: '81.60%', 8: '84.71%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 15/200, Train Loss: 0.232491, Train-Class-Acc: {0: '97.82%', 2: '97.75%', 3: '98.26%', 4: '94.30%', 5: '97.68%', 6: '84.56%', 7: '87.82%', 8: '93.31%', 9: '87.16%'}\n",
      "Val Loss: 1.472040, Val Acc: 88.79%, Val-Class-Acc: {0: '91.85%', 2: '95.14%', 3: '98.77%', 4: '87.50%', 5: '97.01%', 6: '42.59%', 7: '83.20%', 8: '86.62%', 9: '72.97%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_13.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_15.pth\n",
      "Epoch 16/200, Train Loss: 0.172781, Train-Class-Acc: {0: '98.37%', 2: '97.75%', 3: '97.44%', 4: '91.77%', 5: '98.35%', 6: '85.94%', 7: '92.81%', 8: '93.63%', 9: '84.46%'}\n",
      "Val Loss: 1.002920, Val Acc: 87.77%, Val-Class-Acc: {0: '88.59%', 2: '94.44%', 3: '98.77%', 4: '87.50%', 5: '93.13%', 6: '43.52%', 7: '88.80%', 8: '86.62%', 9: '67.57%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_5.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_16.pth\n",
      "Epoch 17/200, Train Loss: 0.151964, Train-Class-Acc: {0: '97.14%', 2: '98.27%', 3: '98.46%', 4: '96.84%', 5: '98.80%', 6: '86.41%', 7: '91.82%', 8: '93.63%', 9: '88.51%'}\n",
      "Val Loss: 1.229336, Val Acc: 87.48%, Val-Class-Acc: {0: '90.76%', 2: '93.06%', 3: '95.90%', 4: '85.00%', 5: '96.12%', 6: '45.37%', 7: '82.40%', 8: '85.35%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 18/200, Train Loss: 0.086396, Train-Class-Acc: {0: '98.50%', 2: '98.61%', 3: '98.67%', 4: '96.20%', 5: '99.18%', 6: '91.71%', 7: '93.01%', 8: '96.50%', 9: '92.57%'}\n",
      "Val Loss: 1.388207, Val Acc: 88.28%, Val-Class-Acc: {0: '92.39%', 2: '93.75%', 3: '98.77%', 4: '90.00%', 5: '97.31%', 6: '43.52%', 7: '84.80%', 8: '82.17%', 9: '62.16%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_16.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_18.pth\n",
      "Epoch 19/200, Train Loss: 0.078465, Train-Class-Acc: {0: '98.77%', 2: '98.61%', 3: '99.49%', 4: '97.47%', 5: '99.18%', 6: '91.94%', 7: '95.01%', 8: '96.02%', 9: '92.57%'}\n",
      "Val Loss: 1.347853, Val Acc: 88.43%, Val-Class-Acc: {0: '88.59%', 2: '93.75%', 3: '98.36%', 4: '87.50%', 5: '97.01%', 6: '44.44%', 7: '86.40%', 8: '87.90%', 9: '62.16%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_10.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_19.pth\n",
      "Epoch 20/200, Train Loss: 0.082493, Train-Class-Acc: {0: '98.91%', 2: '99.31%', 3: '99.49%', 4: '96.84%', 5: '99.18%', 6: '92.86%', 7: '95.01%', 8: '96.66%', 9: '91.22%'}\n",
      "Val Loss: 1.662570, Val Acc: 87.85%, Val-Class-Acc: {0: '90.76%', 2: '94.44%', 3: '98.77%', 4: '82.50%', 5: '92.84%', 6: '45.37%', 7: '84.80%', 8: '87.26%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 21/200, Train Loss: 0.059342, Train-Class-Acc: {0: '98.91%', 2: '99.48%', 3: '99.69%', 4: '98.73%', 5: '99.25%', 6: '93.55%', 7: '96.21%', 8: '96.82%', 9: '94.59%'}\n",
      "Val Loss: 1.423097, Val Acc: 88.28%, Val-Class-Acc: {0: '92.93%', 2: '94.44%', 3: '98.36%', 4: '85.00%', 5: '96.72%', 6: '44.44%', 7: '85.60%', 8: '82.17%', 9: '64.86%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_11.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_21.pth\n",
      "Epoch 22/200, Train Loss: 0.061343, Train-Class-Acc: {0: '99.05%', 2: '99.31%', 3: '99.59%', 5: '99.10%', 6: '93.55%', 7: '96.81%', 8: '97.13%', 9: '96.62%', 4: '98.10%'}\n",
      "Val Loss: 1.422322, Val Acc: 88.21%, Val-Class-Acc: {0: '88.59%', 2: '94.44%', 3: '97.95%', 4: '85.00%', 5: '96.42%', 6: '45.37%', 7: '84.00%', 8: '87.90%', 9: '67.57%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_12.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_22.pth\n",
      "Epoch 23/200, Train Loss: 0.070959, Train-Class-Acc: {0: '98.09%', 2: '98.79%', 3: '99.08%', 5: '99.63%', 6: '94.01%', 7: '95.41%', 8: '96.50%', 9: '96.62%', 4: '99.37%'}\n",
      "Val Loss: 1.371962, Val Acc: 87.70%, Val-Class-Acc: {0: '88.04%', 2: '94.44%', 3: '99.18%', 4: '82.50%', 5: '95.82%', 6: '46.30%', 7: '82.40%', 8: '86.62%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 24/200, Train Loss: 0.081119, Train-Class-Acc: {0: '98.64%', 2: '98.61%', 3: '99.39%', 4: '98.10%', 5: '99.03%', 6: '94.24%', 7: '96.01%', 8: '96.66%', 9: '91.89%'}\n",
      "Val Loss: 1.936297, Val Acc: 87.85%, Val-Class-Acc: {0: '94.02%', 2: '94.44%', 3: '96.31%', 4: '90.00%', 5: '95.52%', 6: '47.22%', 7: '80.80%', 8: '84.71%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 25/200, Train Loss: 0.262459, Train-Class-Acc: {0: '98.91%', 2: '98.61%', 3: '98.77%', 5: '98.65%', 6: '91.71%', 7: '94.61%', 8: '96.50%', 9: '91.22%', 4: '96.84%'}\n",
      "Val Loss: 1.568489, Val Acc: 86.75%, Val-Class-Acc: {0: '88.59%', 2: '90.28%', 3: '98.36%', 4: '90.00%', 5: '93.13%', 6: '45.37%', 7: '85.60%', 8: '85.35%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 26/200, Train Loss: 0.205103, Train-Class-Acc: {0: '97.14%', 2: '95.84%', 3: '98.46%', 4: '97.47%', 5: '98.35%', 6: '86.87%', 7: '93.21%', 8: '94.59%', 9: '93.24%'}\n",
      "Val Loss: 1.606502, Val Acc: 87.85%, Val-Class-Acc: {0: '92.93%', 2: '95.14%', 3: '98.36%', 4: '87.50%', 5: '96.42%', 6: '40.74%', 7: '85.60%', 8: '84.08%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 27/200, Train Loss: 0.106859, Train-Class-Acc: {0: '99.05%', 2: '98.44%', 3: '99.28%', 4: '97.47%', 5: '99.18%', 6: '91.01%', 7: '94.61%', 8: '95.38%', 9: '91.89%'}\n",
      "Val Loss: 1.635546, Val Acc: 88.28%, Val-Class-Acc: {0: '91.30%', 2: '95.14%', 3: '98.36%', 4: '87.50%', 5: '97.01%', 6: '45.37%', 7: '80.80%', 8: '86.62%', 9: '59.46%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_22.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_27.pth\n",
      "Epoch 28/200, Train Loss: 0.083438, Train-Class-Acc: {0: '99.32%', 2: '98.61%', 3: '99.49%', 4: '94.94%', 5: '98.80%', 6: '94.70%', 7: '95.01%', 8: '96.02%', 9: '95.95%'}\n",
      "Val Loss: 1.576818, Val Acc: 87.85%, Val-Class-Acc: {0: '89.67%', 2: '95.14%', 3: '97.95%', 4: '90.00%', 5: '95.82%', 6: '46.30%', 7: '83.20%', 8: '84.08%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 29/200, Train Loss: 0.068806, Train-Class-Acc: {0: '98.77%', 2: '99.48%', 3: '98.87%', 4: '98.73%', 5: '99.33%', 6: '95.39%', 7: '96.41%', 8: '97.93%', 9: '93.24%'}\n",
      "Val Loss: 1.389379, Val Acc: 87.63%, Val-Class-Acc: {0: '89.13%', 2: '95.14%', 3: '98.77%', 4: '80.00%', 5: '95.82%', 6: '42.59%', 7: '81.60%', 8: '87.26%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 30/200, Train Loss: 0.074944, Train-Class-Acc: {0: '98.09%', 2: '98.79%', 3: '99.08%', 4: '99.37%', 5: '99.48%', 6: '94.47%', 7: '95.81%', 8: '96.82%', 9: '96.62%'}\n",
      "Val Loss: 1.532930, Val Acc: 88.28%, Val-Class-Acc: {0: '91.30%', 2: '93.75%', 3: '97.95%', 4: '85.00%', 5: '96.42%', 6: '48.15%', 7: '87.20%', 8: '83.44%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 31/200, Train Loss: 0.116471, Train-Class-Acc: {0: '99.05%', 2: '97.75%', 3: '98.87%', 4: '99.37%', 5: '99.03%', 6: '93.78%', 7: '95.81%', 8: '97.13%', 9: '97.30%'}\n",
      "Val Loss: 1.612870, Val Acc: 87.85%, Val-Class-Acc: {0: '91.85%', 2: '93.06%', 3: '97.54%', 4: '85.00%', 5: '96.42%', 6: '38.89%', 7: '86.40%', 8: '87.26%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 32/200, Train Loss: 0.040185, Train-Class-Acc: {0: '99.05%', 2: '99.83%', 3: '99.80%', 4: '98.10%', 5: '99.78%', 6: '94.93%', 7: '97.41%', 8: '98.25%', 9: '94.59%'}\n",
      "Val Loss: 1.549322, Val Acc: 87.77%, Val-Class-Acc: {0: '89.67%', 2: '92.36%', 3: '98.77%', 4: '87.50%', 5: '94.93%', 6: '45.37%', 7: '82.40%', 8: '87.90%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 33/200, Train Loss: 0.090601, Train-Class-Acc: {0: '99.05%', 2: '98.96%', 3: '99.49%', 4: '98.10%', 5: '99.10%', 6: '93.55%', 7: '95.81%', 8: '97.45%', 9: '93.24%'}\n",
      "Val Loss: 1.664352, Val Acc: 87.92%, Val-Class-Acc: {0: '86.96%', 2: '94.44%', 3: '98.36%', 4: '90.00%', 5: '96.72%', 6: '44.44%', 7: '83.20%', 8: '86.62%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 34/200, Train Loss: 0.126821, Train-Class-Acc: {0: '98.77%', 2: '98.96%', 3: '98.98%', 4: '96.84%', 5: '99.10%', 6: '93.09%', 7: '96.21%', 8: '96.82%', 9: '91.22%'}\n",
      "Val Loss: 1.837069, Val Acc: 86.83%, Val-Class-Acc: {0: '89.67%', 2: '90.28%', 3: '98.36%', 4: '80.00%', 5: '97.91%', 6: '36.11%', 7: '81.60%', 8: '87.90%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 35/200, Train Loss: 0.160696, Train-Class-Acc: {0: '97.00%', 2: '98.27%', 3: '98.67%', 4: '96.20%', 5: '98.50%', 6: '89.86%', 7: '93.21%', 8: '94.11%', 9: '90.54%'}\n",
      "Val Loss: 1.618123, Val Acc: 87.19%, Val-Class-Acc: {0: '85.87%', 2: '93.75%', 3: '95.90%', 4: '90.00%', 5: '97.31%', 6: '42.59%', 7: '83.20%', 8: '89.17%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 36/200, Train Loss: 0.277258, Train-Class-Acc: {0: '96.32%', 2: '97.75%', 3: '97.95%', 4: '94.30%', 5: '98.58%', 6: '86.87%', 7: '92.02%', 8: '93.31%', 9: '85.14%'}\n",
      "Val Loss: 1.743195, Val Acc: 87.48%, Val-Class-Acc: {0: '88.04%', 2: '93.75%', 3: '95.90%', 4: '87.50%', 5: '95.22%', 6: '41.67%', 7: '85.60%', 8: '89.17%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 37/200, Train Loss: 0.173058, Train-Class-Acc: {0: '98.09%', 2: '98.61%', 3: '98.16%', 4: '97.47%', 5: '98.58%', 6: '89.86%', 7: '93.61%', 8: '95.54%', 9: '86.49%'}\n",
      "Val Loss: 1.583805, Val Acc: 86.54%, Val-Class-Acc: {0: '87.50%', 2: '93.06%', 3: '98.77%', 4: '87.50%', 5: '92.54%', 6: '42.59%', 7: '83.20%', 8: '85.99%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 38/200, Train Loss: 0.086553, Train-Class-Acc: {0: '99.46%', 2: '98.61%', 3: '98.98%', 4: '97.47%', 5: '98.73%', 6: '94.01%', 7: '96.61%', 8: '97.13%', 9: '90.54%'}\n",
      "Val Loss: 1.442733, Val Acc: 87.77%, Val-Class-Acc: {0: '83.70%', 2: '93.75%', 3: '98.36%', 4: '85.00%', 5: '96.12%', 6: '48.15%', 7: '84.80%', 8: '88.54%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 39/200, Train Loss: 0.046022, Train-Class-Acc: {0: '98.64%', 2: '99.13%', 3: '99.80%', 4: '96.20%', 5: '99.63%', 6: '95.16%', 7: '96.41%', 8: '97.29%', 9: '95.95%'}\n",
      "Val Loss: 1.623551, Val Acc: 87.99%, Val-Class-Acc: {0: '86.96%', 2: '93.75%', 3: '98.36%', 4: '87.50%', 5: '96.72%', 6: '48.15%', 7: '84.80%', 8: '86.62%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 40/200, Train Loss: 0.040349, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '99.80%', 4: '99.37%', 5: '99.55%', 6: '96.77%', 7: '97.01%', 8: '98.41%', 9: '97.97%'}\n",
      "Val Loss: 1.642435, Val Acc: 88.14%, Val-Class-Acc: {0: '90.22%', 2: '93.75%', 3: '98.36%', 4: '90.00%', 5: '96.42%', 6: '45.37%', 7: '84.00%', 8: '85.35%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 41/200, Train Loss: 0.049241, Train-Class-Acc: {0: '99.18%', 2: '99.13%', 3: '99.59%', 4: '100.00%', 5: '99.25%', 6: '95.85%', 7: '96.21%', 8: '98.09%', 9: '97.97%'}\n",
      "Val Loss: 1.696729, Val Acc: 87.41%, Val-Class-Acc: {0: '85.87%', 2: '94.44%', 3: '96.31%', 4: '87.50%', 5: '94.93%', 6: '47.22%', 7: '82.40%', 8: '87.90%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 42/200, Train Loss: 0.166111, Train-Class-Acc: {0: '98.64%', 2: '98.61%', 3: '98.57%', 4: '97.47%', 5: '99.10%', 6: '94.01%', 7: '94.61%', 8: '97.29%', 9: '89.86%'}\n",
      "Val Loss: 1.873613, Val Acc: 87.26%, Val-Class-Acc: {0: '89.13%', 2: '94.44%', 3: '97.95%', 4: '77.50%', 5: '94.33%', 6: '46.30%', 7: '84.80%', 8: '86.62%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 43/200, Train Loss: 0.072264, Train-Class-Acc: {0: '98.91%', 2: '99.13%', 3: '99.39%', 4: '98.10%', 5: '99.25%', 6: '94.93%', 7: '98.60%', 8: '96.66%', 9: '92.57%'}\n",
      "Val Loss: 1.894699, Val Acc: 86.90%, Val-Class-Acc: {0: '80.43%', 2: '90.97%', 3: '97.54%', 4: '80.00%', 5: '95.82%', 6: '45.37%', 7: '84.80%', 8: '91.08%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 44/200, Train Loss: 0.062652, Train-Class-Acc: {0: '99.32%', 2: '97.92%', 3: '99.49%', 4: '98.73%', 5: '99.33%', 6: '95.16%', 7: '97.21%', 8: '98.57%', 9: '95.27%'}\n",
      "Val Loss: 1.762636, Val Acc: 86.90%, Val-Class-Acc: {0: '86.96%', 2: '94.44%', 3: '98.36%', 4: '87.50%', 5: '92.84%', 6: '44.44%', 7: '79.20%', 8: '88.54%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 45/200, Train Loss: 0.026506, Train-Class-Acc: {0: '99.32%', 2: '99.83%', 3: '99.90%', 4: '99.37%', 5: '99.93%', 6: '97.24%', 7: '98.20%', 8: '97.93%', 9: '97.30%'}\n",
      "Val Loss: 1.910164, Val Acc: 87.85%, Val-Class-Acc: {0: '94.57%', 2: '94.44%', 3: '98.36%', 4: '87.50%', 5: '93.43%', 6: '48.15%', 7: '81.60%', 8: '84.08%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 46/200, Train Loss: 0.051863, Train-Class-Acc: {0: '99.32%', 2: '99.31%', 3: '99.80%', 4: '99.37%', 5: '99.33%', 6: '96.77%', 7: '98.20%', 8: '98.09%', 9: '97.30%'}\n",
      "Val Loss: 1.568560, Val Acc: 87.63%, Val-Class-Acc: {0: '90.22%', 2: '91.67%', 3: '98.77%', 4: '87.50%', 5: '95.52%', 6: '46.30%', 7: '84.80%', 8: '85.99%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 47/200, Train Loss: 0.044606, Train-Class-Acc: {0: '99.32%', 2: '99.13%', 3: '99.49%', 4: '97.47%', 5: '99.55%', 6: '96.31%', 7: '97.80%', 8: '98.09%', 9: '95.27%'}\n",
      "Val Loss: 1.870591, Val Acc: 87.77%, Val-Class-Acc: {0: '86.96%', 2: '93.75%', 3: '97.54%', 4: '90.00%', 5: '96.12%', 6: '41.67%', 7: '85.60%', 8: '88.54%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 48/200, Train Loss: 0.029187, Train-Class-Acc: {0: '99.46%', 2: '99.83%', 3: '99.69%', 4: '99.37%', 5: '99.63%', 6: '97.24%', 7: '98.40%', 8: '98.41%', 9: '97.30%'}\n",
      "Val Loss: 1.667745, Val Acc: 88.43%, Val-Class-Acc: {0: '87.50%', 2: '93.75%', 3: '97.95%', 4: '87.50%', 5: '96.42%', 6: '50.00%', 7: '84.80%', 8: '88.54%', 9: '62.16%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_18.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_48.pth\n",
      "Epoch 49/200, Train Loss: 0.026797, Train-Class-Acc: {0: '99.32%', 2: '99.13%', 3: '99.90%', 4: '98.73%', 5: '99.70%', 6: '98.16%', 7: '99.00%', 8: '99.04%', 9: '97.97%'}\n",
      "Val Loss: 1.615506, Val Acc: 87.19%, Val-Class-Acc: {0: '89.13%', 2: '94.44%', 3: '97.54%', 4: '90.00%', 5: '95.52%', 6: '38.89%', 7: '80.80%', 8: '87.90%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 50/200, Train Loss: 0.053691, Train-Class-Acc: {0: '99.86%', 2: '99.48%', 3: '99.59%', 4: '98.10%', 5: '99.33%', 6: '98.62%', 7: '98.40%', 8: '98.73%', 9: '97.97%'}\n",
      "Val Loss: 1.838807, Val Acc: 87.26%, Val-Class-Acc: {0: '90.22%', 2: '95.14%', 3: '96.72%', 4: '87.50%', 5: '95.82%', 6: '41.67%', 7: '84.80%', 8: '85.99%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 51/200, Train Loss: 0.151266, Train-Class-Acc: {0: '98.77%', 2: '99.13%', 3: '98.36%', 4: '96.20%', 5: '98.88%', 6: '93.32%', 7: '95.81%', 8: '96.34%', 9: '90.54%'}\n",
      "Val Loss: 1.724995, Val Acc: 86.54%, Val-Class-Acc: {0: '86.96%', 2: '90.28%', 3: '97.13%', 4: '90.00%', 5: '96.12%', 6: '46.30%', 7: '81.60%', 8: '85.35%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 52/200, Train Loss: 0.044284, Train-Class-Acc: {0: '99.18%', 2: '99.31%', 3: '99.08%', 4: '99.37%', 5: '99.63%', 6: '94.93%', 7: '97.41%', 8: '98.25%', 9: '95.95%'}\n",
      "Val Loss: 2.099432, Val Acc: 87.26%, Val-Class-Acc: {0: '89.13%', 2: '94.44%', 3: '98.77%', 4: '87.50%', 5: '95.52%', 6: '41.67%', 7: '82.40%', 8: '85.35%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 53/200, Train Loss: 0.029283, Train-Class-Acc: {0: '99.32%', 2: '99.65%', 3: '99.69%', 4: '98.10%', 5: '99.70%', 6: '97.24%', 7: '98.40%', 8: '98.57%', 9: '98.65%'}\n",
      "Val Loss: 2.012166, Val Acc: 87.48%, Val-Class-Acc: {0: '88.59%', 2: '95.14%', 3: '96.31%', 4: '90.00%', 5: '96.12%', 6: '37.96%', 7: '83.20%', 8: '87.90%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 54/200, Train Loss: 0.021894, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '99.69%', 4: '99.37%', 5: '99.78%', 6: '97.93%', 7: '99.00%', 8: '98.89%', 9: '98.65%'}\n",
      "Val Loss: 1.899801, Val Acc: 87.05%, Val-Class-Acc: {0: '88.59%', 2: '95.14%', 3: '97.95%', 4: '87.50%', 5: '95.52%', 6: '44.44%', 7: '77.60%', 8: '84.71%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 55/200, Train Loss: 0.037353, Train-Class-Acc: {0: '99.59%', 2: '99.65%', 3: '99.90%', 4: '99.37%', 5: '99.85%', 6: '97.70%', 7: '98.00%', 8: '98.25%', 9: '98.65%'}\n",
      "Val Loss: 1.784189, Val Acc: 87.34%, Val-Class-Acc: {0: '88.04%', 2: '93.06%', 3: '96.72%', 4: '85.00%', 5: '94.63%', 6: '47.22%', 7: '83.20%', 8: '86.62%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 56/200, Train Loss: 0.091934, Train-Class-Acc: {0: '99.18%', 2: '99.31%', 3: '99.39%', 4: '98.10%', 5: '99.40%', 6: '94.47%', 7: '96.81%', 8: '97.13%', 9: '95.27%'}\n",
      "Val Loss: 2.455701, Val Acc: 86.90%, Val-Class-Acc: {0: '85.87%', 2: '93.75%', 3: '97.95%', 4: '87.50%', 5: '94.93%', 6: '39.81%', 7: '82.40%', 8: '89.17%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 57/200, Train Loss: 0.036117, Train-Class-Acc: {0: '99.18%', 2: '99.13%', 3: '99.59%', 4: '98.73%', 5: '99.55%', 6: '98.39%', 7: '97.80%', 8: '99.20%', 9: '95.27%'}\n",
      "Val Loss: 2.037812, Val Acc: 87.34%, Val-Class-Acc: {0: '87.50%', 2: '93.75%', 3: '97.13%', 4: '90.00%', 5: '96.42%', 6: '37.96%', 7: '84.80%', 8: '88.54%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 58/200, Train Loss: 0.147424, Train-Class-Acc: {0: '97.96%', 2: '97.57%', 3: '98.57%', 4: '98.10%', 5: '98.58%', 6: '92.63%', 7: '95.81%', 8: '96.50%', 9: '92.57%'}\n",
      "Val Loss: 2.246778, Val Acc: 87.92%, Val-Class-Acc: {0: '88.04%', 2: '93.06%', 3: '96.72%', 4: '90.00%', 5: '96.72%', 6: '47.22%', 7: '82.40%', 8: '88.54%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 59/200, Train Loss: 0.122568, Train-Class-Acc: {0: '98.37%', 2: '98.09%', 3: '98.77%', 5: '98.88%', 6: '93.55%', 7: '95.81%', 8: '96.97%', 9: '95.27%', 4: '97.47%'}\n",
      "Val Loss: 1.709375, Val Acc: 87.92%, Val-Class-Acc: {0: '87.50%', 2: '94.44%', 3: '97.95%', 4: '85.00%', 5: '95.82%', 6: '53.70%', 7: '80.80%', 8: '84.71%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 60/200, Train Loss: 0.063173, Train-Class-Acc: {0: '99.18%', 2: '99.83%', 3: '99.08%', 5: '99.48%', 6: '95.16%', 7: '96.01%', 8: '98.09%', 9: '95.95%', 4: '96.84%'}\n",
      "Val Loss: 1.961708, Val Acc: 87.48%, Val-Class-Acc: {0: '86.41%', 2: '88.89%', 3: '97.95%', 4: '82.50%', 5: '97.31%', 6: '46.30%', 7: '82.40%', 8: '89.17%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 61/200, Train Loss: 0.055476, Train-Class-Acc: {0: '99.05%', 2: '99.13%', 3: '99.49%', 4: '96.20%', 5: '99.63%', 6: '96.08%', 7: '97.01%', 8: '97.93%', 9: '93.24%'}\n",
      "Val Loss: 2.455925, Val Acc: 86.61%, Val-Class-Acc: {0: '81.52%', 2: '86.11%', 3: '97.95%', 4: '90.00%', 5: '97.31%', 6: '39.81%', 7: '83.20%', 8: '89.17%', 9: '75.68%'}, LR: 0.001000\n",
      "Epoch 62/200, Train Loss: 0.059732, Train-Class-Acc: {0: '99.18%', 2: '98.44%', 3: '99.80%', 4: '99.37%', 5: '99.25%', 6: '97.00%', 7: '98.20%', 8: '99.04%', 9: '97.30%'}\n",
      "Val Loss: 1.996924, Val Acc: 87.77%, Val-Class-Acc: {0: '90.22%', 2: '95.14%', 3: '97.54%', 4: '87.50%', 5: '97.31%', 6: '44.44%', 7: '87.20%', 8: '78.34%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 63/200, Train Loss: 0.026625, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '99.80%', 4: '99.37%', 5: '99.55%', 6: '97.24%', 7: '98.40%', 8: '98.57%', 9: '98.65%'}\n",
      "Val Loss: 1.932026, Val Acc: 87.55%, Val-Class-Acc: {0: '88.59%', 2: '95.14%', 3: '97.54%', 4: '87.50%', 5: '95.52%', 6: '41.67%', 7: '82.40%', 8: '88.54%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 64/200, Train Loss: 0.106572, Train-Class-Acc: {0: '98.77%', 2: '99.31%', 3: '99.28%', 4: '99.37%', 5: '99.03%', 6: '97.47%', 7: '97.80%', 8: '97.29%', 9: '97.30%'}\n",
      "Val Loss: 2.250815, Val Acc: 86.39%, Val-Class-Acc: {0: '84.78%', 2: '93.06%', 3: '97.54%', 4: '85.00%', 5: '95.22%', 6: '50.93%', 7: '80.80%', 8: '80.25%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 65/200, Train Loss: 0.125923, Train-Class-Acc: {0: '97.96%', 2: '99.13%', 3: '98.36%', 4: '96.84%', 5: '99.33%', 6: '93.55%', 7: '96.41%', 8: '96.66%', 9: '91.89%'}\n",
      "Val Loss: 2.241707, Val Acc: 87.19%, Val-Class-Acc: {0: '90.22%', 2: '94.44%', 3: '95.49%', 4: '85.00%', 5: '95.82%', 6: '37.96%', 7: '82.40%', 8: '88.54%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 66/200, Train Loss: 0.069517, Train-Class-Acc: {0: '98.37%', 2: '99.31%', 3: '99.28%', 5: '99.25%', 6: '95.62%', 7: '95.61%', 8: '98.41%', 4: '99.37%', 9: '97.30%'}\n",
      "Val Loss: 2.170494, Val Acc: 87.63%, Val-Class-Acc: {0: '86.41%', 2: '93.06%', 3: '97.95%', 4: '87.50%', 5: '96.42%', 6: '47.22%', 7: '85.60%', 8: '86.62%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 67/200, Train Loss: 0.077864, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '99.18%', 5: '99.40%', 6: '96.08%', 7: '98.00%', 8: '97.61%', 9: '93.24%', 4: '100.00%'}\n",
      "Val Loss: 1.930703, Val Acc: 86.68%, Val-Class-Acc: {0: '87.50%', 2: '95.83%', 3: '97.95%', 4: '85.00%', 5: '91.94%', 6: '41.67%', 7: '85.60%', 8: '87.26%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 68/200, Train Loss: 0.062127, Train-Class-Acc: {0: '98.09%', 2: '98.44%', 3: '99.80%', 4: '100.00%', 5: '99.25%', 6: '96.08%', 7: '97.21%', 8: '98.41%', 9: '93.24%'}\n",
      "Val Loss: 2.205666, Val Acc: 86.97%, Val-Class-Acc: {0: '84.78%', 2: '95.14%', 3: '97.54%', 4: '85.00%', 5: '95.22%', 6: '46.30%', 7: '84.80%', 8: '86.62%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 69/200, Train Loss: 0.041779, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '99.28%', 4: '99.37%', 5: '99.78%', 6: '97.24%', 7: '98.20%', 8: '98.09%', 9: '95.27%'}\n",
      "Val Loss: 2.440426, Val Acc: 87.63%, Val-Class-Acc: {0: '85.33%', 2: '93.06%', 3: '97.95%', 4: '87.50%', 5: '96.72%', 6: '42.59%', 7: '82.40%', 8: '89.17%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 70/200, Train Loss: 0.051359, Train-Class-Acc: {0: '99.73%', 2: '99.31%', 3: '99.59%', 4: '99.37%', 5: '99.55%', 6: '97.00%', 7: '98.80%', 8: '98.57%', 9: '97.30%'}\n",
      "Val Loss: 1.752889, Val Acc: 87.41%, Val-Class-Acc: {0: '90.22%', 2: '90.28%', 3: '97.13%', 4: '87.50%', 5: '95.22%', 6: '50.00%', 7: '79.20%', 8: '86.62%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 71/200, Train Loss: 0.087037, Train-Class-Acc: {0: '98.64%', 2: '98.44%', 3: '99.18%', 4: '100.00%', 5: '99.10%', 6: '93.55%', 7: '96.21%', 8: '98.09%', 9: '95.27%'}\n",
      "Val Loss: 1.963786, Val Acc: 86.75%, Val-Class-Acc: {0: '90.22%', 2: '85.42%', 3: '97.95%', 4: '87.50%', 5: '96.72%', 6: '47.22%', 7: '80.80%', 8: '82.80%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 72/200, Train Loss: 0.058128, Train-Class-Acc: {0: '98.64%', 2: '99.13%', 3: '99.69%', 4: '98.10%', 5: '99.10%', 6: '96.77%', 7: '98.40%', 8: '97.93%', 9: '97.30%'}\n",
      "Val Loss: 1.895581, Val Acc: 87.19%, Val-Class-Acc: {0: '85.87%', 2: '92.36%', 3: '98.77%', 4: '87.50%', 5: '93.73%', 6: '45.37%', 7: '83.20%', 8: '88.54%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 73/200, Train Loss: 0.078758, Train-Class-Acc: {0: '98.91%', 2: '98.44%', 3: '99.39%', 4: '100.00%', 5: '99.25%', 6: '96.77%', 7: '97.41%', 8: '98.57%', 9: '96.62%'}\n",
      "Val Loss: 1.769113, Val Acc: 87.63%, Val-Class-Acc: {0: '84.78%', 2: '95.14%', 3: '97.13%', 4: '87.50%', 5: '97.01%', 6: '42.59%', 7: '80.80%', 8: '88.54%', 9: '75.68%'}, LR: 0.001000\n",
      "Epoch 74/200, Train Loss: 0.212802, Train-Class-Acc: {0: '99.18%', 2: '98.79%', 3: '99.08%', 4: '98.73%', 5: '98.88%', 6: '96.54%', 7: '98.20%', 8: '98.73%', 9: '93.24%'}\n",
      "Val Loss: 2.942615, Val Acc: 86.75%, Val-Class-Acc: {0: '86.41%', 2: '88.89%', 3: '97.13%', 4: '82.50%', 5: '97.31%', 6: '40.74%', 7: '83.20%', 8: '88.54%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 75/200, Train Loss: 0.249860, Train-Class-Acc: {0: '96.46%', 2: '97.92%', 3: '97.75%', 5: '98.58%', 6: '89.63%', 7: '94.21%', 8: '94.43%', 9: '91.89%', 4: '92.41%'}\n",
      "Val Loss: 2.320893, Val Acc: 86.90%, Val-Class-Acc: {0: '87.50%', 2: '94.44%', 3: '92.21%', 4: '87.50%', 5: '97.01%', 6: '49.07%', 7: '80.00%', 8: '84.71%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 76/200, Train Loss: 0.043137, Train-Class-Acc: {0: '99.05%', 2: '98.96%', 3: '99.59%', 5: '99.48%', 6: '96.77%', 7: '98.20%', 8: '98.09%', 4: '98.10%', 9: '94.59%'}\n",
      "Val Loss: 2.142843, Val Acc: 88.06%, Val-Class-Acc: {0: '89.13%', 2: '94.44%', 3: '98.36%', 4: '85.00%', 5: '97.61%', 6: '46.30%', 7: '84.80%', 8: '83.44%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 77/200, Train Loss: 0.032412, Train-Class-Acc: {0: '99.32%', 2: '99.65%', 3: '100.00%', 4: '98.73%', 5: '99.55%', 6: '96.54%', 7: '98.40%', 8: '99.36%', 9: '97.97%'}\n",
      "Val Loss: 2.220605, Val Acc: 87.85%, Val-Class-Acc: {0: '82.07%', 2: '95.14%', 3: '97.95%', 4: '87.50%', 5: '97.61%', 6: '46.30%', 7: '84.00%', 8: '86.62%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 78/200, Train Loss: 0.039906, Train-Class-Acc: {0: '99.32%', 2: '99.48%', 3: '99.69%', 4: '99.37%', 5: '99.78%', 6: '97.70%', 7: '98.00%', 8: '98.73%', 9: '98.65%'}\n",
      "Val Loss: 2.387413, Val Acc: 87.70%, Val-Class-Acc: {0: '84.24%', 2: '94.44%', 3: '94.26%', 4: '87.50%', 5: '98.81%', 6: '46.30%', 7: '83.20%', 8: '87.26%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 79/200, Train Loss: 0.029545, Train-Class-Acc: {0: '98.91%', 2: '99.48%', 3: '99.69%', 4: '98.73%', 5: '99.78%', 6: '98.39%', 7: '99.00%', 8: '99.20%', 9: '95.27%'}\n",
      "Val Loss: 2.188305, Val Acc: 87.85%, Val-Class-Acc: {0: '91.30%', 2: '95.14%', 3: '96.31%', 4: '87.50%', 5: '96.42%', 6: '38.89%', 7: '84.00%', 8: '87.26%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 80/200, Train Loss: 0.017151, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '99.90%', 4: '99.37%', 5: '99.85%', 6: '97.70%', 7: '98.80%', 8: '99.36%', 9: '95.95%'}\n",
      "Val Loss: 2.236977, Val Acc: 87.99%, Val-Class-Acc: {0: '88.04%', 2: '90.97%', 3: '97.13%', 4: '87.50%', 5: '97.31%', 6: '51.85%', 7: '79.20%', 8: '87.26%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 81/200, Train Loss: 0.049014, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '99.90%', 4: '98.10%', 5: '99.48%', 6: '98.16%', 7: '97.60%', 8: '99.04%', 9: '97.97%'}\n",
      "Val Loss: 2.303094, Val Acc: 86.83%, Val-Class-Acc: {0: '81.52%', 2: '94.44%', 3: '97.95%', 4: '87.50%', 5: '95.82%', 6: '45.37%', 7: '84.80%', 8: '84.08%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 82/200, Train Loss: 0.041510, Train-Class-Acc: {0: '99.46%', 2: '99.65%', 3: '99.69%', 4: '98.73%', 5: '99.85%', 6: '98.16%', 7: '98.20%', 8: '99.04%', 9: '97.97%'}\n",
      "Val Loss: 1.973462, Val Acc: 86.32%, Val-Class-Acc: {0: '83.15%', 2: '95.83%', 3: '95.90%', 4: '92.50%', 5: '95.82%', 6: '43.52%', 7: '74.40%', 8: '88.54%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 83/200, Train Loss: 0.143583, Train-Class-Acc: {0: '98.23%', 2: '97.75%', 3: '99.08%', 5: '99.03%', 6: '94.93%', 7: '94.61%', 8: '98.25%', 9: '91.89%', 4: '99.37%'}\n",
      "Val Loss: 2.118382, Val Acc: 86.61%, Val-Class-Acc: {0: '88.04%', 2: '93.75%', 3: '97.95%', 4: '85.00%', 5: '93.43%', 6: '40.74%', 7: '80.00%', 8: '89.17%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 84/200, Train Loss: 0.059509, Train-Class-Acc: {0: '99.18%', 2: '99.31%', 3: '99.08%', 4: '96.84%', 5: '99.40%', 6: '97.00%', 7: '97.60%', 8: '98.25%', 9: '95.27%'}\n",
      "Val Loss: 2.433673, Val Acc: 87.05%, Val-Class-Acc: {0: '81.52%', 2: '91.67%', 3: '97.95%', 4: '87.50%', 5: '96.42%', 6: '45.37%', 7: '82.40%', 8: '88.54%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 85/200, Train Loss: 0.034185, Train-Class-Acc: {0: '98.64%', 2: '99.31%', 3: '99.90%', 4: '98.10%', 5: '99.55%', 6: '97.47%', 7: '99.60%', 8: '98.09%', 9: '97.30%'}\n",
      "Val Loss: 2.515349, Val Acc: 87.12%, Val-Class-Acc: {0: '86.41%', 2: '94.44%', 3: '95.49%', 4: '87.50%', 5: '97.31%', 6: '42.59%', 7: '82.40%', 8: '88.54%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 86/200, Train Loss: 0.028214, Train-Class-Acc: {0: '100.00%', 2: '99.48%', 3: '99.80%', 4: '99.37%', 5: '99.93%', 6: '97.93%', 7: '98.40%', 8: '98.89%', 9: '98.65%'}\n",
      "Val Loss: 2.409666, Val Acc: 86.90%, Val-Class-Acc: {0: '88.04%', 2: '93.06%', 3: '97.54%', 4: '82.50%', 5: '96.42%', 6: '40.74%', 7: '82.40%', 8: '87.90%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 87/200, Train Loss: 0.018714, Train-Class-Acc: {0: '99.73%', 2: '99.31%', 3: '99.90%', 4: '98.73%', 5: '99.78%', 6: '98.85%', 7: '99.40%', 8: '98.73%', 9: '100.00%'}\n",
      "Val Loss: 2.950035, Val Acc: 85.81%, Val-Class-Acc: {0: '83.70%', 2: '92.36%', 3: '94.26%', 4: '85.00%', 5: '96.12%', 6: '37.96%', 7: '84.80%', 8: '88.54%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 88/200, Train Loss: 0.086377, Train-Class-Acc: {0: '99.05%', 2: '99.65%', 3: '99.39%', 4: '98.10%', 5: '99.10%', 6: '93.78%', 7: '98.00%', 8: '97.61%', 9: '97.30%'}\n",
      "Val Loss: 2.653883, Val Acc: 88.06%, Val-Class-Acc: {0: '91.85%', 2: '90.97%', 3: '96.72%', 4: '87.50%', 5: '97.61%', 6: '53.70%', 7: '85.60%', 8: '80.89%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 89/200, Train Loss: 0.030868, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '99.49%', 4: '98.73%', 5: '99.85%', 6: '97.93%', 7: '98.20%', 8: '98.57%', 9: '97.30%'}\n",
      "Val Loss: 2.299593, Val Acc: 87.77%, Val-Class-Acc: {0: '90.22%', 2: '89.58%', 3: '97.54%', 4: '87.50%', 5: '97.01%', 6: '44.44%', 7: '84.00%', 8: '86.62%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 90/200, Train Loss: 0.061246, Train-Class-Acc: {0: '98.50%', 2: '99.48%', 3: '99.39%', 4: '98.73%', 5: '99.70%', 6: '97.70%', 7: '98.60%', 8: '97.93%', 9: '94.59%'}\n",
      "Val Loss: 2.073141, Val Acc: 86.61%, Val-Class-Acc: {0: '92.93%', 2: '90.97%', 3: '97.13%', 4: '85.00%', 5: '95.52%', 6: '44.44%', 7: '85.60%', 8: '77.71%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 91/200, Train Loss: 0.077885, Train-Class-Acc: {0: '99.32%', 2: '99.31%', 3: '99.49%', 4: '99.37%', 5: '99.03%', 6: '96.08%', 7: '98.20%', 8: '97.45%', 9: '95.95%'}\n",
      "Val Loss: 2.219808, Val Acc: 87.70%, Val-Class-Acc: {0: '85.33%', 2: '94.44%', 3: '96.31%', 4: '85.00%', 5: '96.42%', 6: '48.15%', 7: '83.20%', 8: '87.90%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 92/200, Train Loss: 0.046613, Train-Class-Acc: {0: '98.91%', 2: '99.31%', 3: '99.69%', 4: '97.47%', 5: '99.40%', 6: '97.47%', 7: '98.00%', 8: '99.04%', 9: '99.32%'}\n",
      "Val Loss: 2.371437, Val Acc: 87.34%, Val-Class-Acc: {0: '85.87%', 2: '92.36%', 3: '95.49%', 4: '90.00%', 5: '95.82%', 6: '49.07%', 7: '84.00%', 8: '87.90%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 93/200, Train Loss: 0.019715, Train-Class-Acc: {0: '99.59%', 2: '99.83%', 3: '99.80%', 4: '99.37%', 5: '99.78%', 6: '98.85%', 7: '99.00%', 8: '99.04%', 9: '98.65%'}\n",
      "Val Loss: 2.075233, Val Acc: 87.92%, Val-Class-Acc: {0: '87.50%', 2: '92.36%', 3: '97.54%', 4: '82.50%', 5: '95.82%', 6: '49.07%', 7: '84.80%', 8: '87.26%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 94/200, Train Loss: 0.034358, Train-Class-Acc: {0: '99.46%', 2: '99.31%', 3: '99.80%', 4: '98.73%', 5: '99.55%', 6: '96.54%', 7: '98.80%', 8: '98.57%', 9: '95.95%'}\n",
      "Val Loss: 2.258306, Val Acc: 86.97%, Val-Class-Acc: {0: '84.24%', 2: '91.67%', 3: '96.31%', 4: '87.50%', 5: '96.72%', 6: '42.59%', 7: '83.20%', 8: '90.45%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 95/200, Train Loss: 0.060839, Train-Class-Acc: {0: '99.05%', 2: '99.83%', 3: '99.49%', 4: '98.73%', 5: '99.63%', 6: '97.70%', 7: '97.80%', 8: '98.57%', 9: '97.30%'}\n",
      "Val Loss: 1.988750, Val Acc: 86.97%, Val-Class-Acc: {0: '88.04%', 2: '91.67%', 3: '96.72%', 4: '85.00%', 5: '94.93%', 6: '47.22%', 7: '82.40%', 8: '85.35%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 96/200, Train Loss: 0.040510, Train-Class-Acc: {0: '99.32%', 2: '99.31%', 3: '99.18%', 4: '99.37%', 5: '99.40%', 6: '95.39%', 7: '98.80%', 8: '98.41%', 9: '98.65%'}\n",
      "Val Loss: 2.548751, Val Acc: 87.26%, Val-Class-Acc: {0: '90.22%', 2: '93.06%', 3: '95.08%', 4: '87.50%', 5: '97.01%', 6: '41.67%', 7: '85.60%', 8: '83.44%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 97/200, Train Loss: 0.025241, Train-Class-Acc: {0: '99.32%', 2: '100.00%', 3: '99.80%', 4: '98.73%', 5: '99.85%', 6: '98.62%', 7: '98.60%', 8: '98.73%', 9: '99.32%'}\n",
      "Val Loss: 2.573706, Val Acc: 87.41%, Val-Class-Acc: {0: '85.33%', 2: '93.06%', 3: '95.90%', 4: '77.50%', 5: '96.12%', 6: '46.30%', 7: '86.40%', 8: '90.45%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 98/200, Train Loss: 0.013244, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '99.69%', 4: '99.37%', 5: '99.93%', 6: '98.62%', 7: '99.20%', 8: '99.52%', 9: '98.65%'}\n",
      "Val Loss: 2.168784, Val Acc: 87.63%, Val-Class-Acc: {0: '89.13%', 2: '93.06%', 3: '97.54%', 4: '77.50%', 5: '95.22%', 6: '50.00%', 7: '81.60%', 8: '87.26%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 99/200, Train Loss: 0.007814, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '100.00%', 4: '99.37%', 5: '100.00%', 6: '98.85%', 7: '99.40%', 8: '99.68%', 9: '100.00%'}\n",
      "Val Loss: 2.305466, Val Acc: 87.55%, Val-Class-Acc: {0: '86.96%', 2: '93.06%', 3: '96.31%', 4: '77.50%', 5: '96.12%', 6: '43.52%', 7: '87.20%', 8: '89.81%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 100/200, Train Loss: 0.020104, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '99.90%', 4: '98.73%', 5: '100.00%', 6: '98.85%', 7: '98.80%', 8: '99.04%', 9: '98.65%'}\n",
      "Val Loss: 2.231222, Val Acc: 87.48%, Val-Class-Acc: {0: '86.96%', 2: '94.44%', 3: '95.08%', 4: '85.00%', 5: '94.63%', 6: '43.52%', 7: '87.20%', 8: '89.81%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 101/200, Train Loss: 0.008610, Train-Class-Acc: {0: '100.00%', 2: '99.83%', 3: '99.90%', 4: '99.37%', 5: '99.93%', 6: '99.77%', 7: '99.60%', 8: '99.36%', 9: '97.30%'}\n",
      "Val Loss: 2.326081, Val Acc: 87.26%, Val-Class-Acc: {0: '82.61%', 2: '93.75%', 3: '95.08%', 4: '82.50%', 5: '96.42%', 6: '45.37%', 7: '86.40%', 8: '88.54%', 9: '75.68%'}, LR: 0.001000\n",
      "Epoch 102/200, Train Loss: 0.010236, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '99.85%', 6: '99.08%', 7: '99.20%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 2.279406, Val Acc: 87.70%, Val-Class-Acc: {0: '85.33%', 2: '94.44%', 3: '96.31%', 4: '87.50%', 5: '96.12%', 6: '44.44%', 7: '84.00%', 8: '89.81%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 103/200, Train Loss: 0.074703, Train-Class-Acc: {0: '99.59%', 2: '99.65%', 3: '99.49%', 4: '100.00%', 5: '99.78%', 6: '98.16%', 7: '98.40%', 8: '98.89%', 9: '98.65%'}\n",
      "Val Loss: 2.638657, Val Acc: 87.26%, Val-Class-Acc: {0: '91.85%', 2: '90.97%', 3: '94.67%', 4: '87.50%', 5: '94.63%', 6: '44.44%', 7: '83.20%', 8: '88.54%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 104/200, Train Loss: 0.174465, Train-Class-Acc: {0: '98.91%', 2: '98.09%', 3: '98.46%', 4: '98.73%', 5: '98.13%', 6: '95.16%', 7: '96.61%', 8: '96.97%', 9: '95.27%'}\n",
      "Val Loss: 2.035014, Val Acc: 85.23%, Val-Class-Acc: {0: '73.91%', 2: '93.75%', 3: '96.72%', 4: '77.50%', 5: '96.12%', 6: '46.30%', 7: '84.80%', 8: '84.71%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 105/200, Train Loss: 0.200269, Train-Class-Acc: {0: '97.41%', 2: '97.40%', 3: '97.75%', 4: '95.57%', 5: '98.43%', 6: '91.01%', 7: '94.01%', 8: '95.70%', 9: '94.59%'}\n",
      "Val Loss: 2.207337, Val Acc: 87.41%, Val-Class-Acc: {0: '88.59%', 2: '87.50%', 3: '97.54%', 4: '90.00%', 5: '95.82%', 6: '41.67%', 7: '86.40%', 8: '92.36%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 106/200, Train Loss: 0.061575, Train-Class-Acc: {0: '98.50%', 2: '98.96%', 3: '99.49%', 4: '97.47%', 5: '99.48%', 6: '96.31%', 7: '97.41%', 8: '98.41%', 9: '97.30%'}\n",
      "Val Loss: 1.974748, Val Acc: 86.61%, Val-Class-Acc: {0: '86.96%', 2: '94.44%', 3: '97.54%', 4: '87.50%', 5: '90.45%', 6: '46.30%', 7: '88.80%', 8: '87.90%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 107/200, Train Loss: 0.061411, Train-Class-Acc: {0: '98.91%', 2: '99.65%', 3: '99.49%', 4: '98.73%', 5: '99.63%', 6: '96.77%', 7: '97.41%', 8: '98.09%', 9: '96.62%'}\n",
      "Val Loss: 2.049286, Val Acc: 87.92%, Val-Class-Acc: {0: '86.41%', 2: '93.75%', 3: '97.13%', 4: '90.00%', 5: '95.82%', 6: '46.30%', 7: '86.40%', 8: '87.26%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 108/200, Train Loss: 0.055482, Train-Class-Acc: {0: '99.32%', 2: '99.48%', 3: '99.80%', 4: '97.47%', 5: '99.40%', 6: '97.00%', 7: '97.01%', 8: '98.25%', 9: '97.30%'}\n",
      "Val Loss: 1.762513, Val Acc: 87.63%, Val-Class-Acc: {0: '91.30%', 2: '93.75%', 3: '97.54%', 4: '90.00%', 5: '95.82%', 6: '47.22%', 7: '76.80%', 8: '85.99%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 109/200, Train Loss: 0.040640, Train-Class-Acc: {0: '99.86%', 2: '98.96%', 3: '99.80%', 4: '99.37%', 5: '99.55%', 6: '98.39%', 7: '99.40%', 8: '98.73%', 9: '97.97%'}\n",
      "Val Loss: 2.498713, Val Acc: 86.68%, Val-Class-Acc: {0: '83.70%', 2: '86.11%', 3: '97.13%', 4: '85.00%', 5: '98.21%', 6: '43.52%', 7: '85.60%', 8: '84.71%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 110/200, Train Loss: 0.043360, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '99.28%', 4: '98.73%', 5: '99.63%', 6: '98.62%', 7: '98.40%', 8: '99.36%', 9: '95.95%'}\n",
      "Val Loss: 1.957278, Val Acc: 87.19%, Val-Class-Acc: {0: '85.33%', 2: '93.06%', 3: '97.54%', 4: '90.00%', 5: '95.82%', 6: '41.67%', 7: '83.20%', 8: '86.62%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 111/200, Train Loss: 0.015074, Train-Class-Acc: {0: '99.73%', 2: '98.96%', 3: '99.69%', 4: '98.73%', 5: '99.78%', 6: '98.62%', 7: '98.80%', 8: '99.52%', 9: '98.65%'}\n",
      "Val Loss: 2.025814, Val Acc: 86.90%, Val-Class-Acc: {0: '88.04%', 2: '95.14%', 3: '97.54%', 4: '90.00%', 5: '93.73%', 6: '37.04%', 7: '83.20%', 8: '89.81%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 112/200, Train Loss: 0.009756, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '100.00%', 4: '99.37%', 5: '99.93%', 6: '98.85%', 7: '99.40%', 8: '99.36%', 9: '100.00%'}\n",
      "Val Loss: 2.060404, Val Acc: 87.77%, Val-Class-Acc: {0: '87.50%', 2: '91.67%', 3: '97.95%', 4: '90.00%', 5: '97.01%', 6: '40.74%', 7: '86.40%', 8: '86.62%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 113/200, Train Loss: 0.005818, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '99.93%', 6: '99.08%', 7: '99.80%', 8: '99.68%', 9: '100.00%'}\n",
      "Val Loss: 1.917070, Val Acc: 86.90%, Val-Class-Acc: {0: '87.50%', 2: '90.97%', 3: '96.31%', 4: '90.00%', 5: '95.52%', 6: '47.22%', 7: '85.60%', 8: '84.71%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 114/200, Train Loss: 0.015417, Train-Class-Acc: {0: '99.59%', 2: '99.83%', 3: '99.59%', 4: '99.37%', 5: '99.93%', 6: '99.31%', 7: '99.80%', 8: '99.68%', 9: '100.00%'}\n",
      "Val Loss: 2.534954, Val Acc: 86.68%, Val-Class-Acc: {0: '92.93%', 2: '93.75%', 3: '98.36%', 4: '90.00%', 5: '97.01%', 6: '34.26%', 7: '73.60%', 8: '84.08%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 115/200, Train Loss: 0.079038, Train-Class-Acc: {0: '99.46%', 2: '99.31%', 3: '99.28%', 5: '99.55%', 6: '98.85%', 7: '97.60%', 8: '98.41%', 4: '98.10%', 9: '97.97%'}\n",
      "Val Loss: 2.717912, Val Acc: 87.12%, Val-Class-Acc: {0: '84.24%', 2: '89.58%', 3: '97.95%', 4: '80.00%', 5: '97.01%', 6: '41.67%', 7: '88.80%', 8: '87.90%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 116/200, Train Loss: 0.033481, Train-Class-Acc: {0: '99.46%', 2: '100.00%', 3: '99.80%', 4: '99.37%', 5: '99.78%', 6: '97.00%', 7: '98.20%', 8: '98.57%', 9: '95.27%'}\n",
      "Val Loss: 2.429755, Val Acc: 87.41%, Val-Class-Acc: {0: '89.13%', 2: '90.97%', 3: '97.95%', 4: '87.50%', 5: '96.42%', 6: '44.44%', 7: '78.40%', 8: '87.26%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 117/200, Train Loss: 0.015705, Train-Class-Acc: {0: '99.59%', 2: '100.00%', 3: '100.00%', 4: '98.10%', 5: '99.63%', 6: '98.85%', 7: '99.20%', 8: '99.52%', 9: '99.32%'}\n",
      "Val Loss: 2.251857, Val Acc: 87.77%, Val-Class-Acc: {0: '85.87%', 2: '91.67%', 3: '96.72%', 4: '90.00%', 5: '95.52%', 6: '50.93%', 7: '83.20%', 8: '89.17%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 118/200, Train Loss: 0.060573, Train-Class-Acc: {0: '99.73%', 2: '99.13%', 3: '99.69%', 4: '99.37%', 5: '99.55%', 6: '99.08%', 7: '98.20%', 8: '99.20%', 9: '97.97%'}\n",
      "Val Loss: 1.909897, Val Acc: 86.75%, Val-Class-Acc: {0: '86.96%', 2: '92.36%', 3: '95.49%', 4: '87.50%', 5: '96.42%', 6: '47.22%', 7: '82.40%', 8: '84.71%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 119/200, Train Loss: 0.261345, Train-Class-Acc: {0: '98.77%', 2: '98.96%', 3: '98.36%', 4: '97.47%', 5: '98.73%', 6: '92.17%', 7: '96.41%', 8: '96.97%', 9: '91.89%'}\n",
      "Val Loss: 3.214616, Val Acc: 87.26%, Val-Class-Acc: {0: '91.30%', 2: '89.58%', 3: '96.31%', 4: '82.50%', 5: '97.01%', 6: '42.59%', 7: '86.40%', 8: '85.35%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 120/200, Train Loss: 0.068143, Train-Class-Acc: {0: '98.64%', 2: '99.13%', 3: '99.18%', 4: '98.73%', 5: '99.40%', 6: '95.39%', 7: '97.80%', 8: '97.77%', 9: '97.30%'}\n",
      "Val Loss: 2.681600, Val Acc: 86.90%, Val-Class-Acc: {0: '84.78%', 2: '93.06%', 3: '97.54%', 4: '90.00%', 5: '94.63%', 6: '55.56%', 7: '80.00%', 8: '82.80%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 121/200, Train Loss: 0.265105, Train-Class-Acc: {0: '98.64%', 2: '97.92%', 3: '98.87%', 4: '98.10%', 5: '99.33%', 6: '96.08%', 7: '96.21%', 8: '97.93%', 9: '93.92%'}\n",
      "Val Loss: 2.864236, Val Acc: 83.70%, Val-Class-Acc: {0: '71.74%', 2: '92.36%', 3: '94.67%', 4: '75.00%', 5: '96.12%', 6: '43.52%', 7: '72.80%', 8: '91.08%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 122/200, Train Loss: 0.293955, Train-Class-Acc: {0: '97.14%', 2: '96.19%', 3: '98.26%', 4: '95.57%', 5: '98.35%', 6: '90.78%', 7: '93.21%', 8: '95.70%', 9: '90.54%'}\n",
      "Val Loss: 2.712169, Val Acc: 85.37%, Val-Class-Acc: {0: '85.87%', 2: '88.89%', 3: '96.72%', 4: '90.00%', 5: '93.43%', 6: '40.74%', 7: '80.80%', 8: '89.81%', 9: '43.24%'}, LR: 0.001000\n",
      "Epoch 123/200, Train Loss: 0.050563, Train-Class-Acc: {0: '98.64%', 2: '99.31%', 3: '99.39%', 4: '99.37%', 5: '99.48%', 6: '95.39%', 7: '97.21%', 8: '98.57%', 9: '97.97%'}\n",
      "Val Loss: 2.262807, Val Acc: 86.54%, Val-Class-Acc: {0: '87.50%', 2: '92.36%', 3: '97.13%', 4: '87.50%', 5: '95.52%', 6: '46.30%', 7: '76.80%', 8: '84.71%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 124/200, Train Loss: 0.025815, Train-Class-Acc: {0: '99.46%', 2: '99.65%', 3: '99.49%', 4: '98.73%', 5: '100.00%', 6: '97.47%', 7: '98.40%', 8: '98.57%', 9: '96.62%'}\n",
      "Val Loss: 2.543516, Val Acc: 86.68%, Val-Class-Acc: {0: '85.87%', 2: '90.97%', 3: '97.13%', 4: '87.50%', 5: '96.12%', 6: '40.74%', 7: '78.40%', 8: '91.08%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 125/200, Train Loss: 0.042229, Train-Class-Acc: {0: '99.46%', 2: '98.96%', 3: '99.80%', 4: '100.00%', 5: '99.55%', 6: '96.77%', 7: '98.00%', 8: '98.41%', 9: '95.95%'}\n",
      "Val Loss: 2.558730, Val Acc: 86.61%, Val-Class-Acc: {0: '78.26%', 2: '90.97%', 3: '96.31%', 4: '82.50%', 5: '97.01%', 6: '41.67%', 7: '87.20%', 8: '89.81%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 126/200, Train Loss: 0.022333, Train-Class-Acc: {0: '99.32%', 2: '99.65%', 3: '99.39%', 4: '98.73%', 5: '99.70%', 6: '98.39%', 7: '99.20%', 8: '99.20%', 9: '97.97%'}\n",
      "Val Loss: 2.291661, Val Acc: 86.97%, Val-Class-Acc: {0: '85.87%', 2: '93.06%', 3: '95.49%', 4: '87.50%', 5: '95.52%', 6: '44.44%', 7: '84.80%', 8: '87.26%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 127/200, Train Loss: 0.013761, Train-Class-Acc: {0: '99.59%', 2: '99.65%', 3: '99.69%', 4: '99.37%', 5: '100.00%', 6: '98.85%', 7: '99.40%', 8: '99.04%', 9: '99.32%'}\n",
      "Val Loss: 2.455302, Val Acc: 87.26%, Val-Class-Acc: {0: '89.67%', 2: '93.06%', 3: '96.72%', 4: '87.50%', 5: '95.82%', 6: '38.89%', 7: '82.40%', 8: '89.81%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 128/200, Train Loss: 0.036307, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '99.80%', 4: '100.00%', 5: '99.78%', 6: '99.31%', 7: '98.40%', 8: '99.20%', 9: '98.65%'}\n",
      "Val Loss: 2.579421, Val Acc: 87.77%, Val-Class-Acc: {0: '91.30%', 2: '91.67%', 3: '97.13%', 4: '80.00%', 5: '96.42%', 6: '43.52%', 7: '86.40%', 8: '85.35%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 129/200, Train Loss: 0.015878, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '99.80%', 4: '98.10%', 5: '99.93%', 6: '98.62%', 7: '99.00%', 8: '98.89%', 9: '98.65%'}\n",
      "Val Loss: 2.349563, Val Acc: 87.34%, Val-Class-Acc: {0: '86.41%', 2: '93.75%', 3: '96.72%', 4: '87.50%', 5: '95.82%', 6: '38.89%', 7: '84.80%', 8: '89.81%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 130/200, Train Loss: 0.011695, Train-Class-Acc: {0: '99.32%', 2: '99.83%', 3: '100.00%', 4: '100.00%', 5: '99.85%', 6: '99.31%', 7: '99.20%', 8: '99.52%', 9: '97.97%'}\n",
      "Val Loss: 2.193311, Val Acc: 87.19%, Val-Class-Acc: {0: '89.13%', 2: '93.06%', 3: '96.31%', 4: '87.50%', 5: '94.93%', 6: '45.37%', 7: '86.40%', 8: '82.17%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 131/200, Train Loss: 0.013541, Train-Class-Acc: {0: '99.59%', 2: '99.83%', 3: '99.80%', 5: '99.93%', 6: '98.62%', 7: '99.20%', 8: '99.52%', 9: '99.32%', 4: '100.00%'}\n",
      "Val Loss: 2.567712, Val Acc: 87.48%, Val-Class-Acc: {0: '90.22%', 2: '90.28%', 3: '97.13%', 4: '87.50%', 5: '96.42%', 6: '41.67%', 7: '85.60%', 8: '86.62%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 132/200, Train Loss: 0.033097, Train-Class-Acc: {0: '99.32%', 2: '99.13%', 3: '99.39%', 4: '99.37%', 5: '99.70%', 6: '98.39%', 7: '99.20%', 8: '99.52%', 9: '97.30%'}\n",
      "Val Loss: 2.466683, Val Acc: 86.97%, Val-Class-Acc: {0: '88.04%', 2: '89.58%', 3: '96.31%', 4: '87.50%', 5: '95.82%', 6: '45.37%', 7: '84.00%', 8: '84.71%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 133/200, Train Loss: 0.011135, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '99.93%', 6: '99.08%', 7: '99.20%', 8: '99.20%', 9: '98.65%'}\n",
      "Val Loss: 2.570647, Val Acc: 87.05%, Val-Class-Acc: {0: '88.04%', 2: '90.28%', 3: '96.72%', 4: '85.00%', 5: '95.22%', 6: '41.67%', 7: '87.20%', 8: '85.99%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 134/200, Train Loss: 0.007170, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '100.00%', 4: '100.00%', 5: '99.85%', 6: '99.54%', 7: '99.40%', 8: '99.52%', 9: '99.32%'}\n",
      "Val Loss: 2.348973, Val Acc: 87.19%, Val-Class-Acc: {0: '91.30%', 2: '91.67%', 3: '97.13%', 4: '82.50%', 5: '95.52%', 6: '42.59%', 7: '88.00%', 8: '84.08%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 135/200, Train Loss: 0.008707, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '100.00%', 4: '99.37%', 5: '99.85%', 6: '99.54%', 7: '99.60%', 8: '100.00%', 9: '98.65%'}\n",
      "Val Loss: 2.257692, Val Acc: 86.46%, Val-Class-Acc: {0: '83.70%', 2: '92.36%', 3: '97.95%', 4: '87.50%', 5: '94.03%', 6: '46.30%', 7: '82.40%', 8: '85.35%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 136/200, Train Loss: 0.022589, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '99.59%', 4: '100.00%', 5: '99.78%', 6: '98.85%', 7: '99.20%', 8: '99.20%', 9: '99.32%'}\n",
      "Val Loss: 2.312832, Val Acc: 86.61%, Val-Class-Acc: {0: '89.67%', 2: '93.06%', 3: '96.31%', 4: '87.50%', 5: '95.82%', 6: '44.44%', 7: '80.00%', 8: '80.89%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 137/200, Train Loss: 0.016752, Train-Class-Acc: {0: '99.59%', 2: '100.00%', 3: '99.80%', 4: '100.00%', 5: '99.85%', 6: '98.85%', 7: '99.00%', 8: '99.36%', 9: '100.00%'}\n",
      "Val Loss: 2.302370, Val Acc: 86.83%, Val-Class-Acc: {0: '91.30%', 2: '90.97%', 3: '97.54%', 4: '85.00%', 5: '97.31%', 6: '45.37%', 7: '79.20%', 8: '78.98%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 138/200, Train Loss: 0.016724, Train-Class-Acc: {0: '99.73%', 2: '99.31%', 3: '100.00%', 4: '100.00%', 5: '99.70%', 6: '98.62%', 7: '98.60%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 2.142139, Val Acc: 86.68%, Val-Class-Acc: {0: '90.76%', 2: '93.06%', 3: '96.31%', 4: '85.00%', 5: '95.82%', 6: '45.37%', 7: '77.60%', 8: '82.80%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 139/200, Train Loss: 0.006547, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '99.85%', 6: '99.08%', 7: '99.60%', 8: '99.52%', 9: '100.00%'}\n",
      "Val Loss: 2.250049, Val Acc: 87.63%, Val-Class-Acc: {0: '89.67%', 2: '91.67%', 3: '97.54%', 4: '87.50%', 5: '96.12%', 6: '42.59%', 7: '84.00%', 8: '88.54%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 140/200, Train Loss: 0.004955, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '99.90%', 4: '99.37%', 5: '100.00%', 6: '99.31%', 7: '99.60%', 8: '100.00%', 9: '99.32%'}\n",
      "Val Loss: 2.242117, Val Acc: 86.90%, Val-Class-Acc: {0: '92.39%', 2: '92.36%', 3: '97.54%', 4: '87.50%', 5: '95.22%', 6: '45.37%', 7: '79.20%', 8: '81.53%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 141/200, Train Loss: 0.003102, Train-Class-Acc: {0: '100.00%', 3: '100.00%', 4: '100.00%', 5: '99.93%', 6: '99.54%', 7: '99.80%', 8: '99.84%', 9: '100.00%', 2: '100.00%'}\n",
      "Val Loss: 2.213441, Val Acc: 87.12%, Val-Class-Acc: {0: '89.13%', 2: '93.75%', 3: '96.31%', 4: '87.50%', 5: '95.52%', 6: '39.81%', 7: '84.00%', 8: '87.90%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 142/200, Train Loss: 0.003336, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '100.00%', 4: '99.37%', 5: '99.93%', 6: '100.00%', 7: '99.80%', 8: '99.68%', 9: '100.00%'}\n",
      "Val Loss: 2.386176, Val Acc: 87.85%, Val-Class-Acc: {0: '90.76%', 2: '95.14%', 3: '97.95%', 4: '90.00%', 5: '95.52%', 6: '37.04%', 7: '83.20%', 8: '89.17%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 143/200, Train Loss: 0.002530, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '100.00%', 6: '99.31%', 7: '99.80%', 8: '99.68%', 9: '100.00%'}\n",
      "Val Loss: 2.426287, Val Acc: 87.26%, Val-Class-Acc: {0: '91.30%', 2: '91.67%', 3: '97.95%', 4: '87.50%', 5: '94.63%', 6: '45.37%', 7: '84.00%', 8: '83.44%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 144/200, Train Loss: 0.012582, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '99.78%', 6: '99.31%', 7: '98.80%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 2.641598, Val Acc: 87.55%, Val-Class-Acc: {0: '87.50%', 2: '92.36%', 3: '97.54%', 4: '87.50%', 5: '97.61%', 6: '43.52%', 7: '81.60%', 8: '86.62%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 145/200, Train Loss: 0.076864, Train-Class-Acc: {0: '98.50%', 2: '98.96%', 3: '99.90%', 4: '98.10%', 5: '99.40%', 6: '97.93%', 7: '98.40%', 8: '98.25%', 9: '97.30%'}\n",
      "Val Loss: 2.832787, Val Acc: 85.74%, Val-Class-Acc: {0: '84.24%', 2: '94.44%', 3: '94.67%', 4: '87.50%', 5: '96.12%', 6: '37.96%', 7: '82.40%', 8: '82.80%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 146/200, Train Loss: 0.150311, Train-Class-Acc: {0: '98.37%', 2: '98.09%', 3: '98.26%', 4: '96.84%', 5: '98.58%', 6: '93.32%', 7: '94.81%', 8: '95.22%', 9: '92.57%'}\n",
      "Val Loss: 3.378633, Val Acc: 85.52%, Val-Class-Acc: {0: '91.85%', 2: '92.36%', 3: '97.54%', 4: '80.00%', 5: '93.13%', 6: '47.22%', 7: '80.00%', 8: '77.07%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 147/200, Train Loss: 0.076529, Train-Class-Acc: {0: '98.77%', 2: '98.09%', 3: '99.39%', 4: '99.37%', 5: '99.48%', 6: '96.54%', 7: '96.81%', 8: '98.25%', 9: '95.95%'}\n",
      "Val Loss: 3.389212, Val Acc: 86.17%, Val-Class-Acc: {0: '83.15%', 2: '93.06%', 3: '95.90%', 4: '85.00%', 5: '97.01%', 6: '44.44%', 7: '81.60%', 8: '82.80%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 148/200, Train Loss: 0.365713, Train-Class-Acc: {0: '97.28%', 2: '98.61%', 3: '97.64%', 5: '97.76%', 6: '89.17%', 7: '93.01%', 8: '94.59%', 9: '88.51%', 4: '98.10%'}\n",
      "Val Loss: 2.445052, Val Acc: 84.28%, Val-Class-Acc: {0: '91.85%', 2: '87.50%', 3: '96.72%', 4: '82.50%', 5: '88.06%', 6: '40.74%', 7: '86.40%', 8: '78.34%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 149/200, Train Loss: 0.106204, Train-Class-Acc: {0: '98.23%', 2: '98.96%', 3: '98.98%', 4: '98.73%', 5: '99.40%', 6: '94.93%', 7: '96.21%', 8: '96.82%', 9: '95.95%'}\n",
      "Val Loss: 1.700711, Val Acc: 87.19%, Val-Class-Acc: {0: '89.67%', 2: '93.06%', 3: '97.13%', 4: '85.00%', 5: '94.33%', 6: '44.44%', 7: '86.40%', 8: '84.71%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 150/200, Train Loss: 0.057427, Train-Class-Acc: {0: '99.32%', 2: '99.83%', 3: '99.28%', 4: '98.73%', 5: '99.48%', 6: '95.62%', 7: '97.80%', 8: '98.25%', 9: '96.62%'}\n",
      "Val Loss: 2.010451, Val Acc: 87.12%, Val-Class-Acc: {0: '86.41%', 2: '91.67%', 3: '97.13%', 4: '87.50%', 5: '96.12%', 6: '50.93%', 7: '80.80%', 8: '83.44%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 151/200, Train Loss: 0.022169, Train-Class-Acc: {0: '99.46%', 2: '99.83%', 3: '99.49%', 4: '99.37%', 5: '99.85%', 7: '99.00%', 8: '99.68%', 9: '97.30%', 6: '98.62%'}\n",
      "Val Loss: 1.928161, Val Acc: 86.90%, Val-Class-Acc: {0: '90.22%', 2: '93.75%', 3: '95.49%', 4: '87.50%', 5: '94.63%', 6: '48.15%', 7: '82.40%', 8: '80.89%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 152/200, Train Loss: 0.033516, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '99.49%', 4: '99.37%', 5: '99.70%', 6: '98.16%', 7: '98.40%', 8: '99.04%', 9: '97.97%'}\n",
      "Val Loss: 2.313950, Val Acc: 86.54%, Val-Class-Acc: {0: '88.59%', 2: '91.67%', 3: '94.67%', 4: '85.00%', 5: '94.93%', 6: '43.52%', 7: '87.20%', 8: '84.08%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 153/200, Train Loss: 0.025522, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '99.49%', 5: '99.55%', 6: '98.85%', 7: '99.60%', 8: '99.36%', 9: '97.97%', 4: '100.00%'}\n",
      "Val Loss: 1.820754, Val Acc: 86.24%, Val-Class-Acc: {0: '84.24%', 2: '84.03%', 3: '97.13%', 4: '85.00%', 5: '95.82%', 6: '46.30%', 7: '81.60%', 8: '87.90%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 154/200, Train Loss: 0.036795, Train-Class-Acc: {0: '99.18%', 2: '99.48%', 3: '99.90%', 4: '100.00%', 5: '99.55%', 6: '99.08%', 7: '98.80%', 8: '98.89%', 9: '96.62%'}\n",
      "Val Loss: 2.258267, Val Acc: 86.03%, Val-Class-Acc: {0: '83.70%', 2: '84.72%', 3: '95.49%', 4: '80.00%', 5: '96.42%', 6: '37.04%', 7: '84.80%', 8: '93.63%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 155/200, Train Loss: 0.340482, Train-Class-Acc: {0: '99.05%', 2: '98.79%', 3: '99.49%', 4: '98.10%', 5: '98.73%', 6: '94.93%', 7: '97.60%', 8: '97.61%', 9: '97.30%'}\n",
      "Val Loss: 1.986651, Val Acc: 86.46%, Val-Class-Acc: {0: '87.50%', 2: '90.97%', 3: '95.90%', 4: '87.50%', 5: '95.52%', 6: '42.59%', 7: '84.80%', 8: '83.44%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 156/200, Train Loss: 0.024377, Train-Class-Acc: {0: '99.18%', 2: '99.65%', 3: '99.59%', 4: '99.37%', 5: '99.78%', 6: '97.47%', 7: '98.40%', 8: '99.20%', 9: '98.65%'}\n",
      "Val Loss: 2.158124, Val Acc: 85.74%, Val-Class-Acc: {0: '88.04%', 2: '84.72%', 3: '97.54%', 4: '80.00%', 5: '96.72%', 6: '46.30%', 7: '79.20%', 8: '83.44%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 157/200, Train Loss: 0.011443, Train-Class-Acc: {0: '99.86%', 2: '99.65%', 3: '99.90%', 4: '100.00%', 5: '99.78%', 6: '98.62%', 7: '99.40%', 8: '99.36%', 9: '99.32%'}\n",
      "Val Loss: 2.191641, Val Acc: 87.92%, Val-Class-Acc: {0: '91.85%', 2: '95.14%', 3: '94.67%', 4: '85.00%', 5: '96.42%', 6: '43.52%', 7: '84.80%', 8: '87.90%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 158/200, Train Loss: 0.008006, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '99.80%', 4: '99.37%', 5: '99.93%', 6: '99.31%', 7: '99.00%', 8: '99.84%', 9: '100.00%'}\n",
      "Val Loss: 2.103750, Val Acc: 87.55%, Val-Class-Acc: {0: '91.30%', 2: '92.36%', 3: '95.90%', 4: '85.00%', 5: '96.72%', 6: '40.74%', 7: '86.40%', 8: '85.99%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 159/200, Train Loss: 0.031369, Train-Class-Acc: {0: '99.32%', 2: '99.83%', 3: '99.80%', 4: '98.10%', 5: '99.55%', 6: '98.16%', 7: '99.20%', 8: '98.89%', 9: '98.65%'}\n",
      "Val Loss: 2.190294, Val Acc: 87.05%, Val-Class-Acc: {0: '83.15%', 2: '94.44%', 3: '97.13%', 4: '87.50%', 5: '95.22%', 6: '42.59%', 7: '81.60%', 8: '91.08%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 160/200, Train Loss: 0.024225, Train-Class-Acc: {2: '99.65%', 3: '100.00%', 4: '100.00%', 5: '99.85%', 6: '99.08%', 7: '99.20%', 8: '99.20%', 9: '98.65%', 0: '99.18%'}\n",
      "Val Loss: 2.214320, Val Acc: 87.26%, Val-Class-Acc: {0: '87.50%', 2: '93.06%', 3: '95.49%', 4: '87.50%', 5: '95.82%', 6: '47.22%', 7: '86.40%', 8: '85.35%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 161/200, Train Loss: 0.088917, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '99.18%', 4: '100.00%', 5: '99.10%', 6: '98.39%', 7: '99.60%', 8: '99.04%', 9: '97.30%'}\n",
      "Val Loss: 2.426217, Val Acc: 85.88%, Val-Class-Acc: {0: '82.61%', 2: '93.06%', 3: '94.67%', 4: '90.00%', 5: '95.82%', 6: '37.96%', 7: '78.40%', 8: '90.45%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 162/200, Train Loss: 0.029533, Train-Class-Acc: {0: '99.05%', 2: '99.13%', 3: '99.69%', 4: '98.73%', 5: '99.78%', 6: '98.16%', 7: '99.00%', 8: '98.57%', 9: '97.97%'}\n",
      "Val Loss: 2.481835, Val Acc: 87.12%, Val-Class-Acc: {0: '85.33%', 2: '93.75%', 3: '95.90%', 4: '87.50%', 5: '96.42%', 6: '42.59%', 7: '86.40%', 8: '86.62%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 163/200, Train Loss: 0.016564, Train-Class-Acc: {0: '99.46%', 2: '99.83%', 3: '100.00%', 4: '99.37%', 5: '99.85%', 6: '98.16%', 7: '98.60%', 8: '99.52%', 9: '98.65%'}\n",
      "Val Loss: 2.438629, Val Acc: 86.61%, Val-Class-Acc: {0: '86.41%', 2: '94.44%', 3: '95.49%', 4: '87.50%', 5: '95.82%', 6: '38.89%', 7: '84.80%', 8: '86.62%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 164/200, Train Loss: 0.009718, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '99.90%', 4: '99.37%', 5: '99.93%', 6: '98.85%', 7: '99.60%', 8: '99.52%', 9: '97.97%'}\n",
      "Val Loss: 2.397203, Val Acc: 87.48%, Val-Class-Acc: {0: '87.50%', 2: '92.36%', 3: '96.72%', 4: '87.50%', 5: '97.01%', 6: '46.30%', 7: '83.20%', 8: '87.26%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 165/200, Train Loss: 0.007831, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '99.80%', 4: '99.37%', 5: '100.00%', 6: '99.54%', 7: '99.80%', 8: '99.84%', 9: '98.65%'}\n",
      "Val Loss: 2.434588, Val Acc: 86.83%, Val-Class-Acc: {0: '84.24%', 2: '91.67%', 3: '96.72%', 4: '85.00%', 5: '96.42%', 6: '38.89%', 7: '82.40%', 8: '89.81%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 166/200, Train Loss: 0.019428, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '99.80%', 4: '99.37%', 5: '99.85%', 6: '98.16%', 7: '99.80%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 2.168901, Val Acc: 86.32%, Val-Class-Acc: {0: '91.85%', 2: '85.42%', 3: '96.31%', 4: '85.00%', 5: '94.63%', 6: '40.74%', 7: '81.60%', 8: '89.17%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 167/200, Train Loss: 0.016084, Train-Class-Acc: {0: '99.59%', 2: '99.65%', 3: '99.90%', 4: '99.37%', 5: '99.85%', 6: '99.08%', 7: '99.00%', 8: '99.52%', 9: '97.97%'}\n",
      "Val Loss: 2.307422, Val Acc: 86.61%, Val-Class-Acc: {0: '91.30%', 2: '92.36%', 3: '95.49%', 4: '85.00%', 5: '94.03%', 6: '39.81%', 7: '84.80%', 8: '85.35%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 168/200, Train Loss: 0.015904, Train-Class-Acc: {0: '99.05%', 2: '99.65%', 3: '100.00%', 4: '98.73%', 5: '99.93%', 6: '99.08%', 7: '99.60%', 8: '99.20%', 9: '98.65%'}\n",
      "Val Loss: 2.370257, Val Acc: 85.74%, Val-Class-Acc: {0: '86.96%', 2: '88.89%', 3: '96.31%', 4: '87.50%', 5: '94.03%', 6: '42.59%', 7: '84.00%', 8: '85.35%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 169/200, Train Loss: 0.008808, Train-Class-Acc: {0: '99.59%', 2: '99.83%', 3: '100.00%', 4: '100.00%', 5: '100.00%', 6: '99.31%', 7: '99.20%', 8: '99.68%', 9: '97.97%'}\n",
      "Val Loss: 2.266549, Val Acc: 86.68%, Val-Class-Acc: {0: '87.50%', 2: '93.75%', 3: '95.08%', 4: '85.00%', 5: '95.52%', 6: '42.59%', 7: '84.80%', 8: '84.71%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 170/200, Train Loss: 0.006552, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '99.90%', 4: '100.00%', 5: '99.78%', 6: '99.54%', 7: '99.60%', 8: '99.84%', 9: '99.32%'}\n",
      "Val Loss: 2.346665, Val Acc: 86.68%, Val-Class-Acc: {0: '86.41%', 2: '94.44%', 3: '95.90%', 4: '82.50%', 5: '95.52%', 6: '45.37%', 7: '79.20%', 8: '89.17%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 171/200, Train Loss: 0.006216, Train-Class-Acc: {0: '100.00%', 2: '99.65%', 3: '100.00%', 5: '99.93%', 6: '99.31%', 7: '99.00%', 8: '100.00%', 9: '99.32%', 4: '100.00%'}\n",
      "Val Loss: 2.386678, Val Acc: 86.54%, Val-Class-Acc: {0: '86.41%', 2: '89.58%', 3: '97.13%', 4: '82.50%', 5: '96.42%', 6: '46.30%', 7: '82.40%', 8: '85.35%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 172/200, Train Loss: 0.061204, Train-Class-Acc: {0: '99.86%', 2: '99.31%', 3: '99.80%', 4: '98.10%', 5: '99.70%', 6: '98.85%', 7: '98.80%', 8: '99.36%', 9: '97.97%'}\n",
      "Val Loss: 3.434758, Val Acc: 86.46%, Val-Class-Acc: {0: '85.87%', 2: '93.75%', 3: '96.72%', 4: '82.50%', 5: '97.61%', 6: '36.11%', 7: '81.60%', 8: '86.62%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 173/200, Train Loss: 0.099119, Train-Class-Acc: {0: '98.64%', 2: '98.09%', 3: '98.77%', 4: '97.47%', 5: '99.10%', 6: '95.85%', 7: '96.61%', 8: '97.61%', 9: '95.95%'}\n",
      "Val Loss: 2.679171, Val Acc: 86.46%, Val-Class-Acc: {0: '85.33%', 2: '90.28%', 3: '95.08%', 4: '82.50%', 5: '95.82%', 6: '44.44%', 7: '80.80%', 8: '89.17%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 174/200, Train Loss: 0.091062, Train-Class-Acc: {0: '98.77%', 2: '99.13%', 3: '99.18%', 4: '99.37%', 5: '99.25%', 6: '95.16%', 7: '98.40%', 8: '97.93%', 9: '94.59%'}\n",
      "Val Loss: 2.756888, Val Acc: 86.46%, Val-Class-Acc: {0: '83.70%', 2: '94.44%', 3: '93.85%', 4: '87.50%', 5: '98.51%', 6: '46.30%', 7: '78.40%', 8: '86.62%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 175/200, Train Loss: 0.072653, Train-Class-Acc: {0: '99.32%', 2: '98.96%', 3: '99.18%', 4: '100.00%', 5: '99.40%', 6: '96.77%', 7: '98.20%', 8: '96.97%', 9: '96.62%'}\n",
      "Val Loss: 2.737700, Val Acc: 86.97%, Val-Class-Acc: {0: '84.78%', 2: '89.58%', 3: '96.31%', 4: '85.00%', 5: '97.31%', 6: '49.07%', 7: '79.20%', 8: '88.54%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 176/200, Train Loss: 0.025466, Train-Class-Acc: {0: '99.18%', 2: '98.96%', 3: '100.00%', 4: '99.37%', 5: '99.85%', 6: '98.16%', 7: '98.40%', 8: '99.20%', 9: '97.97%'}\n",
      "Val Loss: 2.002464, Val Acc: 86.83%, Val-Class-Acc: {0: '88.04%', 2: '93.75%', 3: '93.03%', 4: '87.50%', 5: '93.13%', 6: '54.63%', 7: '80.80%', 8: '88.54%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 177/200, Train Loss: 0.024071, Train-Class-Acc: {0: '99.46%', 2: '99.83%', 3: '99.39%', 4: '98.73%', 5: '99.55%', 6: '97.93%', 7: '99.60%', 8: '98.73%', 9: '97.97%'}\n",
      "Val Loss: 2.212737, Val Acc: 87.26%, Val-Class-Acc: {0: '90.22%', 2: '93.06%', 3: '96.31%', 4: '85.00%', 5: '97.01%', 6: '37.04%', 7: '82.40%', 8: '89.81%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 178/200, Train Loss: 0.024763, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '100.00%', 4: '100.00%', 5: '100.00%', 6: '99.54%', 7: '99.20%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 1.828871, Val Acc: 86.75%, Val-Class-Acc: {0: '87.50%', 2: '91.67%', 3: '95.90%', 4: '85.00%', 5: '94.33%', 6: '45.37%', 7: '84.00%', 8: '87.26%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 179/200, Train Loss: 0.013438, Train-Class-Acc: {0: '99.46%', 2: '99.65%', 3: '99.69%', 5: '99.78%', 6: '99.08%', 7: '98.80%', 8: '99.52%', 9: '99.32%', 4: '100.00%'}\n",
      "Val Loss: 2.347042, Val Acc: 85.88%, Val-Class-Acc: {0: '90.22%', 2: '95.14%', 3: '95.90%', 4: '85.00%', 5: '93.73%', 6: '45.37%', 7: '81.60%', 8: '78.34%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 180/200, Train Loss: 0.024201, Train-Class-Acc: {0: '99.46%', 2: '99.65%', 3: '100.00%', 4: '98.73%', 5: '99.85%', 6: '99.08%', 7: '99.20%', 8: '98.73%', 9: '97.30%'}\n",
      "Val Loss: 2.267007, Val Acc: 86.90%, Val-Class-Acc: {0: '86.41%', 2: '94.44%', 3: '95.49%', 4: '85.00%', 5: '95.52%', 6: '44.44%', 7: '84.00%', 8: '85.99%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 181/200, Train Loss: 0.008576, Train-Class-Acc: {0: '99.86%', 2: '99.65%', 3: '99.90%', 4: '100.00%', 5: '99.70%', 6: '99.08%', 7: '100.00%', 8: '99.52%', 9: '100.00%'}\n",
      "Val Loss: 2.114911, Val Acc: 86.46%, Val-Class-Acc: {0: '86.41%', 2: '93.06%', 3: '96.72%', 4: '87.50%', 5: '95.82%', 6: '37.96%', 7: '82.40%', 8: '86.62%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 182/200, Train Loss: 0.010026, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '99.90%', 4: '99.37%', 5: '99.93%', 6: '99.77%', 7: '99.60%', 8: '99.52%', 9: '98.65%'}\n",
      "Val Loss: 2.276633, Val Acc: 86.83%, Val-Class-Acc: {0: '91.30%', 2: '93.06%', 3: '95.90%', 4: '87.50%', 5: '94.63%', 6: '37.96%', 7: '80.00%', 8: '89.17%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 183/200, Train Loss: 0.046695, Train-Class-Acc: {0: '99.86%', 2: '99.65%', 3: '99.69%', 4: '100.00%', 5: '99.48%', 6: '97.93%', 7: '99.20%', 8: '99.36%', 9: '100.00%'}\n",
      "Val Loss: 2.831486, Val Acc: 85.44%, Val-Class-Acc: {0: '88.59%', 2: '90.28%', 3: '84.43%', 4: '82.50%', 5: '97.31%', 6: '55.56%', 7: '80.00%', 8: '85.99%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 184/200, Train Loss: 0.136995, Train-Class-Acc: {0: '98.09%', 2: '98.44%', 3: '98.26%', 4: '96.84%', 5: '98.80%', 6: '94.24%', 7: '97.01%', 8: '97.77%', 9: '95.95%'}\n",
      "Val Loss: 2.045403, Val Acc: 86.24%, Val-Class-Acc: {0: '86.41%', 2: '90.28%', 3: '95.08%', 4: '90.00%', 5: '94.33%', 6: '49.07%', 7: '80.00%', 8: '85.99%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 185/200, Train Loss: 0.030051, Train-Class-Acc: {0: '99.05%', 2: '99.31%', 3: '99.80%', 4: '98.73%', 5: '99.70%', 6: '98.39%', 7: '98.20%', 8: '97.77%', 9: '98.65%'}\n",
      "Val Loss: 1.982663, Val Acc: 86.83%, Val-Class-Acc: {0: '83.15%', 2: '93.06%', 3: '95.08%', 4: '87.50%', 5: '95.52%', 6: '45.37%', 7: '82.40%', 8: '90.45%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 186/200, Train Loss: 0.031248, Train-Class-Acc: {0: '99.59%', 2: '100.00%', 3: '100.00%', 4: '99.37%', 5: '99.85%', 6: '99.08%', 7: '99.20%', 8: '99.52%', 9: '96.62%'}\n",
      "Val Loss: 1.982415, Val Acc: 86.90%, Val-Class-Acc: {0: '89.67%', 2: '92.36%', 3: '96.72%', 4: '90.00%', 5: '94.63%', 6: '43.52%', 7: '84.00%', 8: '84.71%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 187/200, Train Loss: 0.020725, Train-Class-Acc: {0: '99.46%', 2: '99.65%', 3: '99.69%', 4: '99.37%', 5: '99.85%', 6: '98.85%', 7: '97.80%', 8: '99.20%', 9: '97.97%'}\n",
      "Val Loss: 2.336444, Val Acc: 85.95%, Val-Class-Acc: {0: '84.24%', 2: '93.75%', 3: '97.95%', 4: '87.50%', 5: '93.13%', 6: '37.04%', 7: '81.60%', 8: '89.81%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 188/200, Train Loss: 0.091385, Train-Class-Acc: {0: '98.50%', 2: '99.83%', 3: '98.98%', 4: '99.37%', 5: '99.33%', 6: '97.47%', 7: '98.40%', 8: '98.57%', 9: '95.95%'}\n",
      "Val Loss: 3.520618, Val Acc: 84.86%, Val-Class-Acc: {0: '86.41%', 2: '90.97%', 3: '97.54%', 4: '87.50%', 5: '89.55%', 6: '38.89%', 7: '82.40%', 8: '85.35%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 189/200, Train Loss: 0.291302, Train-Class-Acc: {0: '96.46%', 2: '97.92%', 3: '96.62%', 5: '98.65%', 6: '92.17%', 7: '96.21%', 8: '96.02%', 9: '91.89%', 4: '94.94%'}\n",
      "Val Loss: 2.524975, Val Acc: 85.52%, Val-Class-Acc: {0: '85.87%', 2: '89.58%', 3: '96.31%', 4: '87.50%', 5: '93.73%', 6: '44.44%', 7: '82.40%', 8: '82.80%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 190/200, Train Loss: 0.051286, Train-Class-Acc: {0: '99.18%', 2: '99.13%', 3: '99.39%', 4: '99.37%', 5: '99.63%', 7: '98.00%', 8: '98.41%', 9: '97.30%', 6: '96.54%'}\n",
      "Val Loss: 2.685363, Val Acc: 85.74%, Val-Class-Acc: {0: '83.15%', 2: '91.67%', 3: '97.95%', 4: '82.50%', 5: '91.64%', 6: '43.52%', 7: '82.40%', 8: '89.17%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 191/200, Train Loss: 0.052429, Train-Class-Acc: {0: '99.05%', 2: '99.31%', 3: '99.80%', 4: '98.73%', 5: '99.40%', 6: '97.47%', 7: '98.20%', 8: '98.57%', 9: '97.97%'}\n",
      "Val Loss: 2.679574, Val Acc: 84.93%, Val-Class-Acc: {0: '88.04%', 2: '95.83%', 3: '95.49%', 4: '87.50%', 5: '90.45%', 6: '37.96%', 7: '81.60%', 8: '83.44%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 192/200, Train Loss: 0.035375, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '99.90%', 5: '99.70%', 6: '97.70%', 7: '98.40%', 8: '99.52%', 9: '98.65%', 4: '98.10%'}\n",
      "Val Loss: 2.924629, Val Acc: 85.30%, Val-Class-Acc: {0: '85.33%', 2: '86.81%', 3: '95.49%', 4: '82.50%', 5: '94.93%', 6: '44.44%', 7: '83.20%', 8: '83.44%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 193/200, Train Loss: 0.022520, Train-Class-Acc: {0: '99.18%', 2: '99.31%', 3: '99.80%', 4: '99.37%', 5: '99.85%', 6: '98.85%', 7: '99.00%', 8: '98.89%', 9: '99.32%'}\n",
      "Val Loss: 2.730636, Val Acc: 85.59%, Val-Class-Acc: {0: '88.04%', 2: '92.36%', 3: '97.54%', 4: '87.50%', 5: '94.03%', 6: '39.81%', 7: '77.60%', 8: '84.71%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 194/200, Train Loss: 0.032172, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '99.90%', 4: '99.37%', 5: '99.78%', 6: '99.08%', 7: '98.40%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 2.626641, Val Acc: 85.88%, Val-Class-Acc: {0: '86.41%', 2: '88.89%', 3: '96.31%', 4: '85.00%', 5: '95.22%', 6: '50.00%', 7: '80.80%', 8: '82.17%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 195/200, Train Loss: 0.009722, Train-Class-Acc: {0: '99.59%', 2: '99.31%', 3: '99.69%', 4: '99.37%', 5: '99.93%', 6: '99.77%', 7: '99.40%', 8: '99.68%', 9: '100.00%'}\n",
      "Val Loss: 2.582116, Val Acc: 85.95%, Val-Class-Acc: {0: '81.52%', 2: '93.06%', 3: '95.90%', 4: '87.50%', 5: '95.22%', 6: '46.30%', 7: '80.00%', 8: '87.90%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 196/200, Train Loss: 0.012665, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '99.90%', 4: '99.37%', 5: '99.85%', 6: '98.85%', 7: '99.20%', 8: '99.20%', 9: '98.65%'}\n",
      "Val Loss: 2.973532, Val Acc: 85.81%, Val-Class-Acc: {0: '83.70%', 2: '89.58%', 3: '97.13%', 4: '77.50%', 5: '93.43%', 6: '45.37%', 7: '84.00%', 8: '87.90%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 197/200, Train Loss: 0.008746, Train-Class-Acc: {0: '99.46%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '99.85%', 6: '99.77%', 7: '99.60%', 8: '99.20%', 9: '97.97%'}\n",
      "Val Loss: 2.720599, Val Acc: 85.95%, Val-Class-Acc: {0: '85.87%', 2: '90.28%', 3: '94.67%', 4: '80.00%', 5: '94.63%', 6: '44.44%', 7: '81.60%', 8: '90.45%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 198/200, Train Loss: 0.008518, Train-Class-Acc: {0: '100.00%', 2: '99.83%', 3: '99.80%', 4: '98.73%', 5: '99.93%', 6: '100.00%', 7: '99.40%', 8: '99.84%', 9: '97.97%'}\n",
      "Val Loss: 3.040558, Val Acc: 86.54%, Val-Class-Acc: {0: '85.87%', 2: '92.36%', 3: '97.13%', 4: '87.50%', 5: '95.52%', 6: '42.59%', 7: '78.40%', 8: '89.81%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 199/200, Train Loss: 0.003741, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '99.90%', 4: '100.00%', 5: '100.00%', 6: '99.31%', 7: '99.80%', 8: '99.84%', 9: '100.00%'}\n",
      "Val Loss: 2.962479, Val Acc: 86.68%, Val-Class-Acc: {0: '87.50%', 2: '94.44%', 3: '97.95%', 4: '85.00%', 5: '93.73%', 6: '42.59%', 7: '79.20%', 8: '89.17%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 200/200, Train Loss: 0.011161, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '99.69%', 4: '98.73%', 5: '99.85%', 6: '99.77%', 7: '99.60%', 8: '99.84%', 9: '99.32%'}\n",
      "Val Loss: 2.595464, Val Acc: 86.03%, Val-Class-Acc: {0: '82.61%', 2: '91.67%', 3: '97.95%', 4: '87.50%', 5: '94.63%', 6: '45.37%', 7: '80.00%', 8: '87.26%', 9: '56.76%'}, LR: 0.001000\n",
      "\n",
      "üèÜ Best model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_best.pth (Val Accuracy: 88.79%)\n",
      "\n",
      "üìå Final model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_final.pth\n",
      "\n",
      "üéØ Top 5 Best Models:\n",
      "Epoch 15, Train Loss: 0.232491, Train-Acc: {0: '97.82%', 2: '97.75%', 3: '98.26%', 4: '94.30%', 5: '97.68%', 6: '84.56%', 7: '87.82%', 8: '93.31%', 9: '87.16%'},\n",
      "Val Loss: 1.472040, Val Acc: 88.79%, Val-Acc: {0: '91.85%', 2: '95.14%', 3: '98.77%', 4: '87.50%', 5: '97.01%', 6: '42.59%', 7: '83.20%', 8: '86.62%', 9: '72.97%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_15.pth\n",
      "Epoch 48, Train Loss: 0.029187, Train-Acc: {0: '99.46%', 2: '99.83%', 3: '99.69%', 4: '99.37%', 5: '99.63%', 6: '97.24%', 7: '98.40%', 8: '98.41%', 9: '97.30%'},\n",
      "Val Loss: 1.667745, Val Acc: 88.43%, Val-Acc: {0: '87.50%', 2: '93.75%', 3: '97.95%', 4: '87.50%', 5: '96.42%', 6: '50.00%', 7: '84.80%', 8: '88.54%', 9: '62.16%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_48.pth\n",
      "Epoch 19, Train Loss: 0.078465, Train-Acc: {0: '98.77%', 2: '98.61%', 3: '99.49%', 4: '97.47%', 5: '99.18%', 6: '91.94%', 7: '95.01%', 8: '96.02%', 9: '92.57%'},\n",
      "Val Loss: 1.347853, Val Acc: 88.43%, Val-Acc: {0: '88.59%', 2: '93.75%', 3: '98.36%', 4: '87.50%', 5: '97.01%', 6: '44.44%', 7: '86.40%', 8: '87.90%', 9: '62.16%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_19.pth\n",
      "Epoch 27, Train Loss: 0.106859, Train-Acc: {0: '99.05%', 2: '98.44%', 3: '99.28%', 4: '97.47%', 5: '99.18%', 6: '91.01%', 7: '94.61%', 8: '95.38%', 9: '91.89%'},\n",
      "Val Loss: 1.635546, Val Acc: 88.28%, Val-Acc: {0: '91.30%', 2: '95.14%', 3: '98.36%', 4: '87.50%', 5: '97.01%', 6: '45.37%', 7: '80.80%', 8: '86.62%', 9: '59.46%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_27.pth\n",
      "Epoch 21, Train Loss: 0.059342, Train-Acc: {0: '98.91%', 2: '99.48%', 3: '99.69%', 4: '98.73%', 5: '99.25%', 6: '93.55%', 7: '96.21%', 8: '96.82%', 9: '94.59%'},\n",
      "Val Loss: 1.423097, Val Acc: 88.28%, Val-Acc: {0: '92.93%', 2: '94.44%', 3: '98.36%', 4: '85.00%', 5: '96.72%', 6: '44.44%', 7: '85.60%', 8: '82.17%', 9: '64.86%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/ResNet18_1D_LoRA_epoch_21.pth\n",
      "\n",
      "üß† Model Summary:\n",
      "Total Parameters: 3,895,946\n",
      "Model Size (float32): 14.86 MB\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "Number of LoRA adapters: 8\n",
      "Number of LoRA groups: 1\n",
      "Total Training Time: 886.21 seconds\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "---\n",
      "### Period 4 (alpha = 0.0, similarity_threshold = 0.7)\n",
      "+ ##### Total training time: 886.21 seconds\n",
      "+ ##### Model: ResNet18_1D_LoRA\n",
      "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4'*\n",
      "+ ##### Best Epoch: 15\n",
      "#### __Val Accuracy: 88.79%__\n",
      "#### __Val-Class-Acc: {0: '91.85%', 2: '95.14%', 3: '98.77%', 4: '87.50%', 5: '97.01%', 6: '42.59%', 7: '83.20%', 8: '86.62%', 9: '72.97%'}__\n",
      "#### __Total Parameters: 3,895,946__\n",
      "#### __Model Size (float32): 14.86 MB__\n",
      "#### __Number of LoRA adapters: 8__\n",
      "#### __Number of LoRA groups: 1__\n",
      "\n",
      "Saved class features to: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v4/Period_4/class_features.pkl\n",
      "üßπ Cleaned up all training memory.\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå Period 4: DynEx-CLoRA Training (ECG)\n",
    "# ================================\n",
    "period = 4\n",
    "\n",
    "# ==== Paths ====\n",
    "stop_signal_file = os.path.join(BASE_DIR, \"stop_training.txt\")\n",
    "model_saving_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v4\", f\"Period_{period}\")\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# ==== Load Period 4 Data ====\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{period}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{period}.npy\"))\n",
    "X_val   = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val   = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "# ==== Device ====\n",
    "device = auto_select_cuda_device()\n",
    "\n",
    "# ==== Load Period 3 Features ====\n",
    "prev_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v3\", \"Period_3\")\n",
    "class_features_path = os.path.join(prev_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, \"rb\") as f:\n",
    "    class_features_dict = pickle.load(f)\n",
    "print(f\"‚úÖ Loaded class features from: {class_features_path}\")\n",
    "\n",
    "# ==== Load Period 3 Checkpoint ====\n",
    "prev_model_path = os.path.join(prev_folder, \"ResNet18_1D_LoRA_best.pth\")\n",
    "checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "\n",
    "input_channels = X_train.shape[2]\n",
    "output_size = int(np.max(y_train)) + 1  # e.g., max=9 ‚Üí output_size=10\n",
    "\n",
    "# ==== Build Teacher Model ====\n",
    "teacher_output_size = len(np.unique(np.load(os.path.join(save_dir, f\"y_train_p{period-1}.npy\"))))\n",
    "teacher_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=teacher_output_size).to(device)\n",
    "teacher_model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "teacher_model.eval()\n",
    "\n",
    "# ==== Build Student Model ====\n",
    "model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size).to(device)\n",
    "\n",
    "# ==== Sync Adapter Structure ====\n",
    "num_lora_groups = checkpoint.get(\"num_lora_groups\", 0)\n",
    "print(f\"üîÑ Number of LoRA groups: {num_lora_groups}\")\n",
    "related_labels = checkpoint.get(\"related_labels\", {\"base\": [0, 1]})\n",
    "for _ in range(num_lora_groups):\n",
    "    model.add_lora_adapter()\n",
    "\n",
    "# ==== Load Shared Weights (excluding FC) ====\n",
    "model_dict = model.state_dict()\n",
    "prev_state_dict = checkpoint[\"model_state_dict\"]\n",
    "filtered_dict = {\n",
    "    k: v for k, v in prev_state_dict.items()\n",
    "    if k in model_dict and model_dict[k].shape == v.shape and k not in [\"fc.weight\", \"fc.bias\"]\n",
    "}\n",
    "model.load_state_dict(filtered_dict, strict=False)\n",
    "for k in model_dict:\n",
    "    if k not in filtered_dict:\n",
    "        print(f\"üîç Not loaded: {k}, shape={model_dict[k].shape}\")\n",
    "print(\"‚úÖ Loaded shared weights from Period 3 (excluding FC only)\")\n",
    "\n",
    "# ==== Training Configuration ====\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "alpha = 0.0 # No distillation\n",
    "similarity_threshold = 0.70\n",
    "stable_classes = [0, 2, 3, 4, 5]  # ‚Üê Ë´ãÊ†πÊìö‰Ω†ÂâçÊúüÈ°ûÂà•Ë™øÊï¥ÔºÅ\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# ==== Start Training ====\n",
    "train_with_dynex_clora_ecg(\n",
    "    model=model,\n",
    "    teacher_model=None,\n",
    "    output_size=output_size,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    alpha=alpha,\n",
    "    model_saving_folder=model_saving_folder,\n",
    "    model_name=\"ResNet18_1D_LoRA\",\n",
    "    stop_signal_file=stop_signal_file,\n",
    "    scheduler=scheduler,\n",
    "    period=period,\n",
    "    stable_classes=stable_classes,\n",
    "    similarity_threshold=similarity_threshold,\n",
    "    class_features_dict=class_features_dict,\n",
    "    related_labels=related_labels,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ==== Cleanup ====\n",
    "del X_train, y_train, X_val, y_val\n",
    "del model, teacher_model, checkpoint, model_dict, prev_state_dict, filtered_dict\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657bb049",
   "metadata": {},
   "source": [
    "#### v5 no distillation, all freeze, th = 0.75\n",
    "##### Period 4 (alpha = 0.0, similarity_threshold = 0.75)\n",
    "+ ##### Total training time: 748.19 seconds\n",
    "+ ##### Model: ResNet18_1D_LoRA\n",
    "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4'*\n",
    "+ ##### Best Epoch: 111\n",
    "##### __Val Accuracy: 89.08%__\n",
    "##### __Val-Class-Acc: {0: '91.85%', 2: '93.75%', 3: '97.95%', 4: '92.50%', 5: '97.01%', 6: '54.63%', 7: '80.80%', 8: '87.26%', 9: '59.46%'}__\n",
    "##### __Total Parameters: 3,895,946__\n",
    "##### __Model Size (float32): 14.86 MB__\n",
    "##### __Number of LoRA adapters: 8__\n",
    "##### __Number of LoRA groups: 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56f6cf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 1\n",
      "    - Memory Used    : 283 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ Loaded class features from: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/class_features.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1775968/1758234116.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Number of LoRA groups: 1\n",
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n",
      "üîç Not loaded: fc.weight, shape=torch.Size([10, 1024])\n",
      "üîç Not loaded: fc.bias, shape=torch.Size([10])\n",
      "‚úÖ Loaded shared weights from Period 3 (excluding FC only)\n",
      "\n",
      "üöÄ 'train_with_dynex_clora_ecg' started for Period 4\n",
      "\n",
      "‚úÖ Removed existing folder: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4\n",
      "\n",
      "‚úÖ Data Overview:\n",
      "X_train: torch.Size([5493, 5000, 12]), y_train: torch.Size([5493])\n",
      "X_val: torch.Size([1374, 5000, 12]), y_val: torch.Size([1374])\n",
      "\n",
      "üîé Similarity Analysis:\n",
      "  Similarity threshold: 0.7500\n",
      "  Existing classes: [0, 1, 2, 3, 4, 5]\n",
      "  Current classes: [0, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  New classes: [6, 7, 8, 9]\n",
      "\n",
      "üìä Similarity Scores:\n",
      "  New Class 6:\n",
      "    - Existing Class 1: 0.9078\n",
      "    - Existing Class 4: 0.9039\n",
      "    - Existing Class 0: 0.8786\n",
      "    - Existing Class 2: 0.8730\n",
      "    - Existing Class 3: 0.8699\n",
      "    - Existing Class 5: 0.8550\n",
      "  New Class 7:\n",
      "    - Existing Class 1: 0.9061\n",
      "    - Existing Class 4: 0.9041\n",
      "    - Existing Class 0: 0.8745\n",
      "    - Existing Class 2: 0.8719\n",
      "    - Existing Class 3: 0.8712\n",
      "    - Existing Class 5: 0.8631\n",
      "  New Class 8:\n",
      "    - Existing Class 1: 0.8973\n",
      "    - Existing Class 4: 0.8882\n",
      "    - Existing Class 0: 0.8746\n",
      "    - Existing Class 2: 0.8499\n",
      "    - Existing Class 3: 0.8435\n",
      "    - Existing Class 5: 0.8316\n",
      "  New Class 9:\n",
      "    - Existing Class 1: 0.8995\n",
      "    - Existing Class 4: 0.8924\n",
      "    - Existing Class 0: 0.8799\n",
      "    - Existing Class 2: 0.8592\n",
      "    - Existing Class 3: 0.8532\n",
      "    - Existing Class 5: 0.8383\n",
      "\n",
      "  Average similarity: 0.8682, Std: 0.0262\n",
      "\n",
      "üß© Managing LoRA adapters for 4 new classes...\n",
      "üîÑ New Class 8 is similar to Class 1 (sim=0.8973) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 8 is similar to Class 2 (sim=0.8499) ‚Üí Added to adapter '0'\n",
      "üîÑ New Class 9 is similar to Class 1 (sim=0.8995) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 9 is similar to Class 2 (sim=0.8592) ‚Üí Added to adapter '0'\n",
      "üîÑ New Class 6 is similar to Class 1 (sim=0.9078) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 6 is similar to Class 2 (sim=0.8730) ‚Üí Added to adapter '0'\n",
      "üîÑ New Class 7 is similar to Class 1 (sim=0.9061) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 7 is similar to Class 2 (sim=0.8719) ‚Üí Added to adapter '0'\n",
      "\n",
      "üîç Checking stability of existing classes...\n",
      "  Class 0 similarity with itself: 0.8814\n",
      "  Class 2 similarity with itself: 0.8816\n",
      "  Class 3 similarity with itself: 0.8985\n",
      "  Class 4 similarity with itself: 0.8817\n",
      "  Class 5 similarity with itself: 0.9125\n",
      "\n",
      "üîí Default: Freezing ALL model parameters\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå fc.weight                                          | shape=[10, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[10]\n",
      "\n",
      "üîì Unfreezing selected adapters (by group):\n",
      "  - Adapter Group #0 (all blocks) (classes: [2, 4, 5, 8, 9, 6, 7])\n",
      "  - Base layers (classes: [0, 1, 3, 4, 5, 8, 9, 6, 7])\n",
      "\n",
      "üîç Frozen Parameters: (After Unfreeze specific adapter groups)\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå fc.weight                                          | shape=[10, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[10]\n",
      "\n",
      "üîç Frozen Parameters (After Unfreeze newly added adapter groups):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå fc.weight                                          | shape=[10, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[10]\n",
      "\n",
      "üîç Frozen Parameters (After unfreeze the final layer):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "\n",
      "üìã Related Labels Summary:\n",
      "  - Base network: Classes [0, 1, 3, 4, 5, 8, 9, 6, 7]\n",
      "  - Adapter #0: Classes [2, 4, 5, 8, 9, 6, 7]\n",
      "\n",
      "üîß Trainable Parameter Status:\n",
      "  ‚úÖ layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ fc.weight                                          | shape=[10, 1024]\n",
      "  ‚úÖ fc.bias                                            | shape=[10]\n",
      "trainable_count: 26\n",
      "\n",
      "üìä Parameter Statistics:\n",
      "  - Total parameters: 3,895,946\n",
      "  - Trainable parameters: 2,129,930 (54.67%)\n",
      "  - Frozen parameters: 1,766,016 (45.33%)\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "\n",
      "üß† Parameters currently controlled by the optimizer: (78)\n",
      "  ‚õî conv1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer1.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.0.conv2.weight\n",
      "  ‚õî layer1.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer1.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.conv2.weight\n",
      "  ‚õî layer1.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer2.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.conv2.weight\n",
      "  ‚õî layer2.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer2.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.conv2.weight\n",
      "  ‚õî layer2.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer3.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.conv2.weight\n",
      "  ‚õî layer3.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer3.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.conv2.weight\n",
      "  ‚õî layer3.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer4.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.conv2.weight\n",
      "  ‚õî layer4.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer4.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.conv2.weight\n",
      "  ‚õî layer4.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ fc.weight\n",
      "  ‚úÖ fc.bias\n",
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/200, Train Loss: 5.456641, Train-Class-Acc: {0: '56.27%', 2: '58.58%', 3: '71.11%', 4: '41.14%', 5: '77.56%', 6: '16.13%', 7: '34.13%', 8: '36.15%', 9: '6.08%'}\n",
      "Val Loss: 1.440075, Val Acc: 82.46%, Val-Class-Acc: {0: '87.50%', 2: '93.75%', 3: '96.72%', 4: '82.50%', 5: '94.03%', 6: '28.70%', 7: '75.20%', 8: '77.71%', 9: '16.22%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_1.pth\n",
      "Epoch 2/200, Train Loss: 1.680116, Train-Class-Acc: {0: '79.70%', 2: '83.36%', 3: '89.04%', 4: '71.52%', 5: '91.02%', 6: '31.34%', 7: '58.68%', 8: '63.22%', 9: '28.38%'}\n",
      "Val Loss: 1.053426, Val Acc: 85.66%, Val-Class-Acc: {0: '88.04%', 2: '93.75%', 3: '97.13%', 4: '87.50%', 5: '94.03%', 6: '37.04%', 7: '87.20%', 8: '81.53%', 9: '43.24%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_2.pth\n",
      "Epoch 3/200, Train Loss: 1.260997, Train-Class-Acc: {0: '86.24%', 2: '90.64%', 3: '91.29%', 5: '93.49%', 6: '46.31%', 7: '67.07%', 8: '69.75%', 4: '77.22%', 9: '36.49%'}\n",
      "Val Loss: 1.166707, Val Acc: 86.32%, Val-Class-Acc: {0: '90.76%', 2: '92.36%', 3: '98.36%', 4: '87.50%', 5: '95.22%', 6: '40.74%', 7: '80.80%', 8: '82.80%', 9: '45.95%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_3.pth\n",
      "Epoch 4/200, Train Loss: 1.030442, Train-Class-Acc: {0: '87.06%', 2: '91.16%', 3: '94.57%', 4: '84.81%', 5: '92.97%', 6: '53.92%', 8: '75.64%', 9: '47.97%', 7: '69.26%'}\n",
      "Val Loss: 1.097120, Val Acc: 87.77%, Val-Class-Acc: {0: '91.30%', 2: '95.14%', 3: '97.54%', 4: '80.00%', 5: '96.12%', 6: '43.52%', 7: '86.40%', 8: '84.71%', 9: '56.76%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_4.pth\n",
      "Epoch 5/200, Train Loss: 0.784076, Train-Class-Acc: {0: '92.23%', 2: '92.72%', 3: '94.67%', 4: '83.54%', 5: '95.36%', 6: '57.60%', 7: '72.46%', 8: '76.27%', 9: '52.70%'}\n",
      "Val Loss: 1.060386, Val Acc: 87.77%, Val-Class-Acc: {0: '89.13%', 2: '93.06%', 3: '98.77%', 4: '87.50%', 5: '97.01%', 6: '46.30%', 7: '83.20%', 8: '84.08%', 9: '56.76%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_5.pth\n",
      "Epoch 6/200, Train Loss: 0.591224, Train-Class-Acc: {0: '93.19%', 2: '95.15%', 3: '96.93%', 4: '86.71%', 5: '95.21%', 6: '62.44%', 7: '74.05%', 8: '80.25%', 9: '56.76%'}\n",
      "Val Loss: 1.196440, Val Acc: 88.50%, Val-Class-Acc: {0: '91.85%', 2: '93.75%', 3: '97.95%', 4: '87.50%', 5: '95.82%', 6: '50.93%', 7: '86.40%', 8: '84.08%', 9: '59.46%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_1.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_6.pth\n",
      "Epoch 7/200, Train Loss: 0.510192, Train-Class-Acc: {0: '94.69%', 2: '94.28%', 3: '95.90%', 4: '84.81%', 5: '96.86%', 6: '71.66%', 7: '78.44%', 8: '84.87%', 9: '62.84%'}\n",
      "Val Loss: 1.330575, Val Acc: 88.36%, Val-Class-Acc: {0: '89.67%', 2: '93.06%', 3: '98.36%', 4: '87.50%', 5: '97.31%', 6: '50.00%', 7: '82.40%', 8: '86.62%', 9: '56.76%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_2.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_7.pth\n",
      "Epoch 8/200, Train Loss: 0.412324, Train-Class-Acc: {0: '95.10%', 2: '95.32%', 3: '96.72%', 5: '97.16%', 6: '72.12%', 7: '78.84%', 8: '85.19%', 9: '66.89%', 4: '90.51%'}\n",
      "Val Loss: 1.276210, Val Acc: 87.92%, Val-Class-Acc: {0: '88.04%', 2: '93.06%', 3: '98.77%', 4: '80.00%', 5: '96.42%', 6: '46.30%', 7: '82.40%', 8: '89.17%', 9: '62.16%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_3.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_8.pth\n",
      "Epoch 9/200, Train Loss: 0.384973, Train-Class-Acc: {0: '95.64%', 2: '97.05%', 3: '97.54%', 5: '96.63%', 6: '75.81%', 7: '81.44%', 8: '85.35%', 9: '71.62%', 4: '92.41%'}\n",
      "Val Loss: 1.368522, Val Acc: 88.06%, Val-Class-Acc: {0: '90.76%', 2: '93.75%', 3: '97.13%', 4: '87.50%', 5: '96.72%', 6: '49.07%', 7: '80.00%', 8: '88.54%', 9: '54.05%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_4.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_9.pth\n",
      "Epoch 10/200, Train Loss: 0.371176, Train-Class-Acc: {0: '95.78%', 2: '96.53%', 3: '98.26%', 4: '90.51%', 5: '97.38%', 6: '77.42%', 7: '84.03%', 8: '89.01%', 9: '76.35%'}\n",
      "Val Loss: 1.226610, Val Acc: 87.99%, Val-Class-Acc: {0: '91.30%', 2: '93.06%', 3: '98.36%', 4: '90.00%', 5: '95.82%', 6: '48.15%', 7: '80.00%', 8: '84.71%', 9: '67.57%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_5.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_10.pth\n",
      "Epoch 11/200, Train Loss: 0.254509, Train-Class-Acc: {0: '96.32%', 2: '97.40%', 3: '98.05%', 4: '90.51%', 5: '98.13%', 6: '81.80%', 7: '87.62%', 8: '89.97%', 9: '75.00%'}\n",
      "Val Loss: 1.382130, Val Acc: 87.99%, Val-Class-Acc: {0: '90.22%', 2: '93.75%', 3: '98.36%', 4: '87.50%', 5: '97.01%', 6: '52.78%', 7: '75.20%', 8: '85.35%', 9: '62.16%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_8.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_11.pth\n",
      "Epoch 12/200, Train Loss: 0.269215, Train-Class-Acc: {0: '97.68%', 2: '96.71%', 3: '97.95%', 4: '93.04%', 5: '97.61%', 6: '79.26%', 7: '87.62%', 8: '89.01%', 9: '79.05%'}\n",
      "Val Loss: 1.340070, Val Acc: 87.99%, Val-Class-Acc: {0: '92.39%', 2: '93.06%', 3: '97.54%', 4: '87.50%', 5: '97.61%', 6: '47.22%', 7: '79.20%', 8: '84.71%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 13/200, Train Loss: 0.221674, Train-Class-Acc: {0: '97.41%', 2: '97.40%', 3: '97.75%', 4: '92.41%', 5: '98.06%', 6: '82.26%', 7: '86.23%', 8: '90.45%', 9: '81.76%'}\n",
      "Val Loss: 1.318137, Val Acc: 88.28%, Val-Class-Acc: {0: '91.85%', 2: '95.14%', 3: '96.72%', 4: '90.00%', 5: '96.12%', 6: '51.85%', 7: '81.60%', 8: '82.80%', 9: '67.57%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_10.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_13.pth\n",
      "Epoch 14/200, Train Loss: 0.121268, Train-Class-Acc: {0: '98.23%', 2: '98.27%', 3: '98.98%', 4: '96.20%', 5: '98.95%', 6: '86.18%', 7: '93.21%', 8: '93.63%', 9: '87.16%'}\n",
      "Val Loss: 1.241357, Val Acc: 88.36%, Val-Class-Acc: {0: '90.22%', 2: '94.44%', 3: '98.77%', 4: '87.50%', 5: '96.42%', 6: '48.15%', 7: '82.40%', 8: '84.08%', 9: '70.27%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_11.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_14.pth\n",
      "Epoch 15/200, Train Loss: 0.337463, Train-Class-Acc: {0: '98.37%', 2: '97.75%', 3: '97.85%', 4: '94.94%', 5: '98.06%', 6: '85.02%', 7: '88.62%', 8: '92.99%', 9: '81.76%'}\n",
      "Val Loss: 1.653070, Val Acc: 87.70%, Val-Class-Acc: {0: '88.04%', 2: '94.44%', 3: '95.08%', 4: '87.50%', 5: '96.42%', 6: '46.30%', 7: '84.00%', 8: '85.99%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 16/200, Train Loss: 0.317903, Train-Class-Acc: {0: '96.59%', 2: '96.53%', 3: '98.16%', 4: '91.77%', 5: '97.46%', 6: '83.87%', 7: '86.43%', 8: '92.04%', 9: '87.16%'}\n",
      "Val Loss: 1.847035, Val Acc: 86.75%, Val-Class-Acc: {0: '93.48%', 2: '94.44%', 3: '93.85%', 4: '80.00%', 5: '97.01%', 6: '46.30%', 7: '80.80%', 8: '81.53%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 17/200, Train Loss: 0.309927, Train-Class-Acc: {0: '97.96%', 2: '97.05%', 3: '98.36%', 4: '93.04%', 5: '98.13%', 6: '85.94%', 7: '91.02%', 8: '91.72%', 9: '85.14%'}\n",
      "Val Loss: 1.297437, Val Acc: 87.92%, Val-Class-Acc: {0: '90.22%', 2: '93.75%', 3: '97.13%', 4: '90.00%', 5: '96.42%', 6: '40.74%', 7: '82.40%', 8: '89.17%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 18/200, Train Loss: 0.138370, Train-Class-Acc: {0: '97.82%', 2: '98.61%', 3: '98.67%', 4: '93.67%', 5: '98.50%', 6: '90.78%', 7: '91.82%', 8: '94.11%', 9: '80.41%'}\n",
      "Val Loss: 1.438689, Val Acc: 88.86%, Val-Class-Acc: {0: '89.67%', 2: '94.44%', 3: '97.54%', 4: '85.00%', 5: '97.01%', 6: '46.30%', 7: '84.00%', 8: '90.45%', 9: '70.27%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_9.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_18.pth\n",
      "Epoch 19/200, Train Loss: 0.097188, Train-Class-Acc: {0: '98.77%', 2: '98.96%', 3: '98.57%', 5: '99.18%', 6: '91.24%', 7: '94.81%', 8: '94.75%', 9: '91.89%', 4: '96.84%'}\n",
      "Val Loss: 1.297302, Val Acc: 88.79%, Val-Class-Acc: {0: '90.22%', 2: '93.06%', 3: '97.54%', 4: '87.50%', 5: '97.61%', 6: '44.44%', 7: '84.80%', 8: '88.54%', 9: '72.97%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_13.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_19.pth\n",
      "Epoch 20/200, Train Loss: 0.066612, Train-Class-Acc: {0: '98.77%', 2: '98.79%', 3: '99.59%', 4: '98.10%', 5: '98.95%', 6: '93.78%', 7: '95.61%', 8: '97.13%', 9: '92.57%'}\n",
      "Val Loss: 1.266269, Val Acc: 88.57%, Val-Class-Acc: {0: '90.76%', 2: '93.06%', 3: '97.13%', 4: '90.00%', 5: '97.61%', 6: '42.59%', 7: '84.00%', 8: '89.81%', 9: '64.86%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_7.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_20.pth\n",
      "Epoch 21/200, Train Loss: 0.069263, Train-Class-Acc: {0: '98.77%', 2: '99.31%', 3: '99.49%', 4: '97.47%', 5: '99.40%', 6: '93.78%', 7: '95.81%', 8: '96.82%', 9: '95.95%'}\n",
      "Val Loss: 1.265767, Val Acc: 88.57%, Val-Class-Acc: {0: '91.30%', 2: '93.75%', 3: '96.31%', 4: '80.00%', 5: '97.61%', 6: '50.93%', 7: '79.20%', 8: '90.45%', 9: '64.86%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_14.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_21.pth\n",
      "Epoch 22/200, Train Loss: 0.064779, Train-Class-Acc: {0: '99.18%', 2: '99.13%', 3: '99.08%', 4: '98.73%', 5: '99.55%', 6: '94.70%', 7: '95.81%', 8: '97.13%', 9: '96.62%'}\n",
      "Val Loss: 1.049854, Val Acc: 88.21%, Val-Class-Acc: {0: '91.30%', 2: '95.14%', 3: '99.59%', 4: '90.00%', 5: '94.33%', 6: '50.00%', 7: '82.40%', 8: '84.08%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 23/200, Train Loss: 0.086657, Train-Class-Acc: {0: '99.05%', 2: '99.31%', 3: '99.28%', 4: '96.84%', 5: '99.18%', 6: '93.09%', 7: '95.81%', 8: '96.50%', 9: '95.27%'}\n",
      "Val Loss: 1.457087, Val Acc: 88.21%, Val-Class-Acc: {0: '85.33%', 2: '93.75%', 3: '97.95%', 4: '82.50%', 5: '97.31%', 6: '49.07%', 7: '84.80%', 8: '89.17%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 24/200, Train Loss: 0.065877, Train-Class-Acc: {0: '98.64%', 2: '99.48%', 3: '99.28%', 4: '98.73%', 5: '99.33%', 6: '95.39%', 7: '95.81%', 8: '97.13%', 9: '95.27%'}\n",
      "Val Loss: 1.518759, Val Acc: 88.50%, Val-Class-Acc: {0: '89.67%', 2: '90.97%', 3: '96.31%', 4: '82.50%', 5: '97.61%', 6: '54.63%', 7: '81.60%', 8: '88.54%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 25/200, Train Loss: 0.117305, Train-Class-Acc: {0: '97.68%', 2: '98.61%', 3: '98.98%', 4: '98.10%', 5: '99.10%', 6: '93.78%', 7: '94.41%', 8: '96.34%', 9: '93.24%'}\n",
      "Val Loss: 1.383564, Val Acc: 87.85%, Val-Class-Acc: {0: '90.22%', 2: '92.36%', 3: '96.31%', 4: '82.50%', 5: '96.72%', 6: '50.00%', 7: '82.40%', 8: '86.62%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 26/200, Train Loss: 0.087264, Train-Class-Acc: {0: '98.77%', 2: '99.31%', 3: '99.59%', 4: '98.10%', 5: '99.18%', 6: '94.70%', 7: '94.81%', 8: '96.66%', 9: '91.22%'}\n",
      "Val Loss: 1.571864, Val Acc: 88.72%, Val-Class-Acc: {0: '89.67%', 2: '93.06%', 3: '97.13%', 4: '85.00%', 5: '96.72%', 6: '57.41%', 7: '84.00%', 8: '85.35%', 9: '64.86%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_6.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_26.pth\n",
      "Epoch 27/200, Train Loss: 0.045441, Train-Class-Acc: {0: '99.05%', 2: '98.96%', 3: '99.49%', 4: '98.10%', 5: '99.33%', 6: '95.16%', 7: '97.41%', 8: '98.41%', 9: '93.24%'}\n",
      "Val Loss: 1.405677, Val Acc: 88.79%, Val-Class-Acc: {0: '89.67%', 2: '93.06%', 3: '97.13%', 4: '87.50%', 5: '97.31%', 6: '48.15%', 7: '84.00%', 8: '88.54%', 9: '72.97%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_20.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_27.pth\n",
      "Epoch 28/200, Train Loss: 0.092615, Train-Class-Acc: {0: '99.18%', 2: '99.83%', 3: '99.59%', 4: '98.73%', 5: '99.33%', 6: '96.54%', 7: '95.61%', 8: '97.61%', 9: '94.59%'}\n",
      "Val Loss: 1.204393, Val Acc: 88.14%, Val-Class-Acc: {0: '85.87%', 2: '92.36%', 3: '97.95%', 4: '85.00%', 5: '97.31%', 6: '50.93%', 7: '78.40%', 8: '89.81%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 29/200, Train Loss: 0.044269, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '99.49%', 4: '98.10%', 5: '99.33%', 6: '95.39%', 7: '97.21%', 8: '99.04%', 9: '95.95%'}\n",
      "Val Loss: 1.541163, Val Acc: 88.43%, Val-Class-Acc: {0: '92.39%', 2: '92.36%', 3: '97.95%', 4: '90.00%', 5: '97.61%', 6: '50.00%', 7: '81.60%', 8: '80.89%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 30/200, Train Loss: 0.041006, Train-Class-Acc: {0: '99.46%', 2: '98.96%', 3: '99.80%', 4: '99.37%', 5: '99.63%', 6: '97.24%', 7: '97.21%', 8: '98.09%', 9: '95.27%'}\n",
      "Val Loss: 1.681682, Val Acc: 88.57%, Val-Class-Acc: {0: '92.39%', 2: '90.28%', 3: '96.72%', 4: '85.00%', 5: '96.72%', 6: '58.33%', 7: '84.00%', 8: '85.35%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 31/200, Train Loss: 0.383315, Train-Class-Acc: {0: '97.00%', 2: '97.75%', 3: '96.93%', 4: '96.20%', 5: '98.20%', 6: '90.78%', 7: '92.61%', 8: '95.70%', 9: '89.86%'}\n",
      "Val Loss: 1.670014, Val Acc: 86.97%, Val-Class-Acc: {0: '94.57%', 2: '91.67%', 3: '92.21%', 4: '85.00%', 5: '97.61%', 6: '56.48%', 7: '77.60%', 8: '79.62%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 32/200, Train Loss: 0.578839, Train-Class-Acc: {0: '95.37%', 2: '95.67%', 3: '95.80%', 4: '92.41%', 5: '97.01%', 6: '83.18%', 7: '87.62%', 8: '89.97%', 9: '83.11%'}\n",
      "Val Loss: 1.690155, Val Acc: 85.95%, Val-Class-Acc: {0: '82.07%', 2: '92.36%', 3: '97.95%', 4: '90.00%', 5: '92.24%', 6: '48.15%', 7: '78.40%', 8: '89.17%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 33/200, Train Loss: 0.276968, Train-Class-Acc: {0: '95.78%', 2: '97.57%', 3: '97.34%', 4: '91.14%', 5: '97.98%', 6: '84.56%', 7: '88.62%', 8: '90.29%', 9: '81.08%'}\n",
      "Val Loss: 1.590152, Val Acc: 87.26%, Val-Class-Acc: {0: '89.67%', 2: '92.36%', 3: '97.54%', 4: '87.50%', 5: '95.22%', 6: '50.93%', 7: '76.80%', 8: '87.26%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 34/200, Train Loss: 0.100649, Train-Class-Acc: {0: '98.77%', 2: '98.61%', 3: '99.08%', 4: '96.84%', 5: '99.40%', 6: '92.40%', 7: '94.41%', 8: '96.18%', 9: '89.86%'}\n",
      "Val Loss: 1.513691, Val Acc: 87.92%, Val-Class-Acc: {0: '94.02%', 2: '93.06%', 3: '97.13%', 4: '85.00%', 5: '97.01%', 6: '48.15%', 7: '81.60%', 8: '82.17%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 35/200, Train Loss: 0.059204, Train-Class-Acc: {0: '99.18%', 2: '99.31%', 3: '99.28%', 4: '98.73%', 5: '99.55%', 6: '94.70%', 7: '96.01%', 8: '96.50%', 9: '92.57%'}\n",
      "Val Loss: 1.623342, Val Acc: 88.28%, Val-Class-Acc: {0: '92.39%', 2: '93.75%', 3: '98.77%', 4: '90.00%', 5: '96.12%', 6: '47.22%', 7: '79.20%', 8: '85.35%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 36/200, Train Loss: 0.046229, Train-Class-Acc: {0: '99.86%', 2: '99.48%', 3: '99.59%', 4: '98.73%', 5: '99.40%', 6: '95.85%', 7: '97.21%', 8: '98.09%', 9: '95.27%'}\n",
      "Val Loss: 1.591899, Val Acc: 87.92%, Val-Class-Acc: {0: '87.50%', 2: '93.75%', 3: '99.18%', 4: '85.00%', 5: '96.42%', 6: '52.78%', 7: '78.40%', 8: '82.80%', 9: '75.68%'}, LR: 0.001000\n",
      "Epoch 37/200, Train Loss: 0.037488, Train-Class-Acc: {0: '99.46%', 2: '98.96%', 3: '99.39%', 4: '97.47%', 5: '99.63%', 6: '97.24%', 7: '97.80%', 8: '98.57%', 9: '96.62%'}\n",
      "Val Loss: 1.592318, Val Acc: 88.36%, Val-Class-Acc: {0: '89.67%', 2: '93.06%', 3: '97.54%', 4: '87.50%', 5: '95.82%', 6: '49.07%', 7: '83.20%', 8: '88.54%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 38/200, Train Loss: 0.027757, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '99.69%', 4: '99.37%', 5: '99.78%', 6: '97.70%', 8: '98.57%', 9: '97.30%', 7: '98.80%'}\n",
      "Val Loss: 1.524761, Val Acc: 88.57%, Val-Class-Acc: {0: '91.30%', 2: '93.06%', 3: '98.77%', 4: '87.50%', 5: '96.72%', 6: '46.30%', 7: '81.60%', 8: '85.99%', 9: '75.68%'}, LR: 0.001000\n",
      "Epoch 39/200, Train Loss: 0.027672, Train-Class-Acc: {0: '99.59%', 2: '99.31%', 3: '99.49%', 4: '98.73%', 5: '99.63%', 6: '96.77%', 7: '98.80%', 8: '99.36%', 9: '97.30%'}\n",
      "Val Loss: 1.591182, Val Acc: 88.43%, Val-Class-Acc: {0: '91.85%', 2: '91.67%', 3: '97.95%', 4: '82.50%', 5: '97.01%', 6: '54.63%', 7: '80.00%', 8: '85.35%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 40/200, Train Loss: 0.025677, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '100.00%', 4: '97.47%', 5: '99.93%', 6: '97.00%', 7: '98.60%', 8: '98.25%', 9: '97.30%'}\n",
      "Val Loss: 1.520041, Val Acc: 88.21%, Val-Class-Acc: {0: '85.87%', 2: '94.44%', 3: '96.72%', 4: '87.50%', 5: '95.82%', 6: '50.00%', 7: '84.00%', 8: '90.45%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 41/200, Train Loss: 0.051590, Train-Class-Acc: {0: '99.18%', 2: '99.65%', 3: '99.59%', 4: '98.73%', 5: '99.85%', 6: '98.62%', 7: '99.20%', 8: '98.41%', 9: '97.97%'}\n",
      "Val Loss: 1.627588, Val Acc: 88.14%, Val-Class-Acc: {0: '92.39%', 2: '89.58%', 3: '98.77%', 4: '90.00%', 5: '95.82%', 6: '41.67%', 7: '82.40%', 8: '90.45%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 42/200, Train Loss: 0.029491, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '99.80%', 4: '98.73%', 5: '99.48%', 6: '97.70%', 7: '97.80%', 8: '99.20%', 9: '98.65%'}\n",
      "Val Loss: 1.832838, Val Acc: 88.36%, Val-Class-Acc: {0: '86.41%', 2: '90.97%', 3: '98.77%', 4: '90.00%', 5: '97.31%', 6: '51.85%', 7: '81.60%', 8: '87.26%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 43/200, Train Loss: 0.108240, Train-Class-Acc: {0: '99.32%', 2: '99.13%', 3: '99.39%', 4: '98.73%', 5: '99.18%', 6: '94.70%', 7: '97.01%', 8: '97.29%', 9: '94.59%'}\n",
      "Val Loss: 1.718720, Val Acc: 87.26%, Val-Class-Acc: {0: '89.67%', 2: '92.36%', 3: '97.13%', 4: '87.50%', 5: '95.82%', 6: '50.93%', 7: '78.40%', 8: '87.90%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 44/200, Train Loss: 0.042166, Train-Class-Acc: {0: '99.18%', 2: '99.13%', 3: '99.59%', 4: '98.73%', 5: '99.48%', 6: '97.70%', 7: '97.21%', 8: '98.89%', 9: '95.27%'}\n",
      "Val Loss: 1.901528, Val Acc: 87.92%, Val-Class-Acc: {0: '90.76%', 2: '92.36%', 3: '98.36%', 4: '87.50%', 5: '97.61%', 6: '36.11%', 7: '84.80%', 8: '86.62%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 45/200, Train Loss: 0.208395, Train-Class-Acc: {0: '98.23%', 2: '98.09%', 3: '98.67%', 4: '98.73%', 5: '98.43%', 6: '92.63%', 7: '96.41%', 8: '95.22%', 9: '93.92%'}\n",
      "Val Loss: 2.043652, Val Acc: 86.90%, Val-Class-Acc: {0: '86.41%', 2: '91.67%', 3: '95.90%', 4: '80.00%', 5: '97.91%', 6: '43.52%', 7: '84.00%', 8: '87.26%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 46/200, Train Loss: 0.113605, Train-Class-Acc: {0: '98.09%', 2: '97.40%', 3: '98.46%', 4: '96.20%', 5: '99.48%', 6: '93.09%', 7: '96.21%', 8: '96.82%', 9: '93.92%'}\n",
      "Val Loss: 2.092153, Val Acc: 86.54%, Val-Class-Acc: {0: '86.96%', 2: '89.58%', 3: '93.44%', 4: '90.00%', 5: '97.31%', 6: '54.63%', 7: '80.00%', 8: '82.17%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 47/200, Train Loss: 0.114476, Train-Class-Acc: {0: '98.37%', 2: '97.40%', 3: '98.98%', 5: '99.40%', 6: '90.09%', 7: '96.01%', 8: '96.18%', 9: '91.89%', 4: '98.10%'}\n",
      "Val Loss: 1.849832, Val Acc: 87.85%, Val-Class-Acc: {0: '90.22%', 2: '90.97%', 3: '97.95%', 4: '87.50%', 5: '94.93%', 6: '56.48%', 7: '80.80%', 8: '85.35%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 48/200, Train Loss: 0.071821, Train-Class-Acc: {0: '98.50%', 2: '99.65%', 3: '99.08%', 4: '98.73%', 5: '99.10%', 6: '95.39%', 7: '96.41%', 8: '97.45%', 9: '93.92%'}\n",
      "Val Loss: 1.858030, Val Acc: 87.63%, Val-Class-Acc: {0: '92.39%', 2: '91.67%', 3: '95.49%', 4: '87.50%', 5: '97.01%', 6: '54.63%', 7: '80.80%', 8: '84.08%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 49/200, Train Loss: 0.050351, Train-Class-Acc: {0: '98.91%', 2: '99.48%', 3: '99.59%', 4: '98.73%', 5: '99.70%', 6: '95.62%', 7: '96.41%', 8: '97.77%', 9: '96.62%'}\n",
      "Val Loss: 1.678214, Val Acc: 87.99%, Val-Class-Acc: {0: '93.48%', 2: '90.97%', 3: '97.13%', 4: '90.00%', 5: '96.42%', 6: '46.30%', 7: '79.20%', 8: '85.35%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 50/200, Train Loss: 0.027690, Train-Class-Acc: {0: '100.00%', 2: '99.48%', 3: '99.69%', 4: '99.37%', 5: '99.55%', 6: '97.24%', 7: '98.20%', 8: '99.20%', 9: '97.97%'}\n",
      "Val Loss: 1.440605, Val Acc: 87.48%, Val-Class-Acc: {0: '86.41%', 2: '95.83%', 3: '96.31%', 4: '90.00%', 5: '93.73%', 6: '47.22%', 7: '81.60%', 8: '91.08%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 51/200, Train Loss: 0.504244, Train-Class-Acc: {0: '97.96%', 2: '97.40%', 3: '97.34%', 4: '94.94%', 5: '97.76%', 6: '89.86%', 7: '93.21%', 8: '93.79%', 9: '87.84%'}\n",
      "Val Loss: 3.410720, Val Acc: 86.39%, Val-Class-Acc: {0: '78.80%', 2: '94.44%', 3: '95.90%', 4: '85.00%', 5: '96.42%', 6: '62.04%', 7: '80.80%', 8: '78.34%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 52/200, Train Loss: 0.306294, Train-Class-Acc: {0: '96.59%', 2: '97.23%', 3: '97.64%', 4: '95.57%', 5: '98.73%', 6: '86.18%', 7: '89.22%', 8: '93.31%', 9: '81.76%'}\n",
      "Val Loss: 1.459999, Val Acc: 87.41%, Val-Class-Acc: {0: '89.13%', 2: '92.36%', 3: '96.72%', 4: '90.00%', 5: '96.72%', 6: '50.93%', 7: '82.40%', 8: '78.98%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 53/200, Train Loss: 0.091168, Train-Class-Acc: {0: '98.77%', 2: '99.13%', 3: '98.87%', 4: '98.73%', 5: '99.18%', 6: '94.01%', 7: '97.21%', 8: '96.34%', 9: '88.51%'}\n",
      "Val Loss: 1.544340, Val Acc: 87.34%, Val-Class-Acc: {0: '90.76%', 2: '90.97%', 3: '98.77%', 4: '82.50%', 5: '94.63%', 6: '51.85%', 7: '81.60%', 8: '83.44%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 54/200, Train Loss: 0.042434, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '99.69%', 5: '99.70%', 6: '95.62%', 7: '97.01%', 8: '98.25%', 9: '97.30%', 4: '98.10%'}\n",
      "Val Loss: 1.660022, Val Acc: 87.70%, Val-Class-Acc: {0: '91.30%', 2: '90.97%', 3: '96.72%', 4: '92.50%', 5: '94.33%', 6: '39.81%', 7: '86.40%', 8: '91.08%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 55/200, Train Loss: 0.043566, Train-Class-Acc: {0: '99.73%', 2: '99.31%', 3: '99.69%', 5: '99.85%', 6: '96.31%', 7: '98.00%', 8: '97.61%', 9: '95.95%', 4: '96.84%'}\n",
      "Val Loss: 1.736037, Val Acc: 87.55%, Val-Class-Acc: {0: '88.59%', 2: '93.06%', 3: '97.13%', 4: '90.00%', 5: '95.52%', 6: '50.93%', 7: '80.80%', 8: '84.08%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 56/200, Train Loss: 0.034467, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '99.59%', 4: '98.73%', 5: '99.63%', 6: '97.70%', 7: '98.00%', 8: '98.89%', 9: '97.97%'}\n",
      "Val Loss: 1.669189, Val Acc: 87.55%, Val-Class-Acc: {0: '92.93%', 2: '93.06%', 3: '96.72%', 4: '90.00%', 5: '94.93%', 6: '37.96%', 7: '82.40%', 8: '87.90%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 57/200, Train Loss: 0.045096, Train-Class-Acc: {0: '99.73%', 2: '99.31%', 3: '99.69%', 4: '98.73%', 5: '99.55%', 6: '98.16%', 7: '98.60%', 8: '98.41%', 9: '96.62%'}\n",
      "Val Loss: 1.763952, Val Acc: 87.63%, Val-Class-Acc: {0: '85.33%', 2: '90.97%', 3: '96.72%', 4: '90.00%', 5: '97.01%', 6: '50.93%', 7: '81.60%', 8: '85.99%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 58/200, Train Loss: 0.030401, Train-Class-Acc: {0: '99.59%', 2: '99.31%', 3: '99.59%', 4: '99.37%', 5: '99.78%', 6: '98.39%', 7: '98.00%', 8: '99.04%', 9: '99.32%'}\n",
      "Val Loss: 1.703934, Val Acc: 87.63%, Val-Class-Acc: {0: '90.76%', 2: '92.36%', 3: '96.31%', 4: '87.50%', 5: '95.22%', 6: '48.15%', 7: '82.40%', 8: '86.62%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 59/200, Train Loss: 0.019815, Train-Class-Acc: {0: '99.59%', 2: '99.65%', 3: '100.00%', 4: '100.00%', 5: '99.85%', 6: '98.62%', 7: '99.00%', 8: '99.04%', 9: '98.65%'}\n",
      "Val Loss: 1.988498, Val Acc: 87.77%, Val-Class-Acc: {0: '88.59%', 2: '93.75%', 3: '96.72%', 4: '87.50%', 5: '96.12%', 6: '40.74%', 7: '85.60%', 8: '89.17%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 60/200, Train Loss: 0.022116, Train-Class-Acc: {0: '99.59%', 2: '100.00%', 3: '99.90%', 5: '99.70%', 6: '97.70%', 7: '98.20%', 8: '98.73%', 9: '97.97%', 4: '99.37%'}\n",
      "Val Loss: 1.979909, Val Acc: 87.92%, Val-Class-Acc: {0: '89.13%', 2: '92.36%', 3: '97.54%', 4: '87.50%', 5: '96.72%', 6: '47.22%', 7: '80.00%', 8: '87.26%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 61/200, Train Loss: 0.033184, Train-Class-Acc: {0: '99.59%', 2: '98.61%', 3: '99.39%', 4: '98.73%', 5: '99.70%', 6: '97.93%', 7: '98.60%', 8: '98.73%', 9: '97.97%'}\n",
      "Val Loss: 2.022865, Val Acc: 87.99%, Val-Class-Acc: {0: '92.39%', 2: '88.19%', 3: '95.90%', 4: '87.50%', 5: '97.61%', 6: '43.52%', 7: '85.60%', 8: '87.26%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 62/200, Train Loss: 0.034731, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '99.69%', 4: '99.37%', 5: '99.48%', 6: '97.70%', 7: '97.60%', 8: '98.73%', 9: '97.30%'}\n",
      "Val Loss: 1.769571, Val Acc: 88.28%, Val-Class-Acc: {0: '92.93%', 2: '91.67%', 3: '97.54%', 4: '87.50%', 5: '97.31%', 6: '53.70%', 7: '83.20%', 8: '80.25%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 63/200, Train Loss: 0.023250, Train-Class-Acc: {0: '99.05%', 2: '99.65%', 3: '100.00%', 4: '100.00%', 5: '99.78%', 6: '98.16%', 7: '98.40%', 8: '99.20%', 9: '97.97%'}\n",
      "Val Loss: 2.110630, Val Acc: 87.77%, Val-Class-Acc: {0: '92.93%', 2: '91.67%', 3: '96.31%', 4: '90.00%', 5: '96.72%', 6: '55.56%', 7: '82.40%', 8: '80.89%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 64/200, Train Loss: 0.024333, Train-Class-Acc: {0: '99.46%', 2: '99.65%', 3: '99.69%', 4: '98.73%', 5: '99.85%', 6: '98.62%', 7: '98.60%', 8: '98.73%', 9: '97.97%'}\n",
      "Val Loss: 1.700858, Val Acc: 88.50%, Val-Class-Acc: {0: '90.76%', 2: '91.67%', 3: '96.72%', 4: '85.00%', 5: '97.01%', 6: '55.56%', 7: '83.20%', 8: '87.26%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 65/200, Train Loss: 0.022322, Train-Class-Acc: {0: '99.18%', 2: '99.83%', 3: '99.90%', 4: '98.10%', 5: '99.78%', 6: '98.62%', 7: '99.00%', 8: '98.57%', 9: '98.65%'}\n",
      "Val Loss: 1.794791, Val Acc: 88.43%, Val-Class-Acc: {0: '90.22%', 2: '92.36%', 3: '97.13%', 4: '92.50%', 5: '98.51%', 6: '48.15%', 7: '84.00%', 8: '87.90%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 66/200, Train Loss: 0.047017, Train-Class-Acc: {0: '99.05%', 2: '99.48%', 3: '99.59%', 4: '98.10%', 5: '99.70%', 6: '97.70%', 7: '98.40%', 8: '99.20%', 9: '93.24%'}\n",
      "Val Loss: 1.863699, Val Acc: 88.36%, Val-Class-Acc: {0: '89.67%', 2: '93.75%', 3: '96.72%', 4: '90.00%', 5: '96.12%', 6: '51.85%', 7: '80.00%', 8: '90.45%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 67/200, Train Loss: 0.281627, Train-Class-Acc: {0: '97.14%', 2: '97.40%', 3: '96.72%', 4: '97.47%', 5: '98.06%', 6: '89.17%', 7: '92.22%', 8: '95.22%', 9: '87.16%'}\n",
      "Val Loss: 2.028237, Val Acc: 86.83%, Val-Class-Acc: {0: '87.50%', 2: '90.97%', 3: '95.49%', 4: '80.00%', 5: '97.91%', 6: '48.15%', 7: '80.80%', 8: '87.26%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 68/200, Train Loss: 0.162771, Train-Class-Acc: {0: '98.23%', 2: '98.09%', 3: '98.87%', 4: '96.20%', 5: '98.80%', 6: '90.32%', 7: '92.61%', 8: '95.38%', 9: '92.57%'}\n",
      "Val Loss: 2.931891, Val Acc: 85.74%, Val-Class-Acc: {0: '83.15%', 2: '84.72%', 3: '94.67%', 4: '82.50%', 5: '98.81%', 6: '49.07%', 7: '79.20%', 8: '85.35%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 69/200, Train Loss: 0.175780, Train-Class-Acc: {0: '98.23%', 2: '99.31%', 3: '98.87%', 4: '97.47%', 5: '98.88%', 6: '92.40%', 7: '95.21%', 8: '96.97%', 9: '91.89%'}\n",
      "Val Loss: 2.114159, Val Acc: 87.77%, Val-Class-Acc: {0: '92.39%', 2: '91.67%', 3: '97.95%', 4: '92.50%', 5: '96.72%', 6: '37.96%', 7: '81.60%', 8: '87.90%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 70/200, Train Loss: 0.087032, Train-Class-Acc: {0: '97.82%', 2: '98.79%', 3: '99.59%', 4: '98.10%', 5: '99.33%', 6: '93.78%', 7: '96.81%', 8: '96.97%', 9: '95.27%'}\n",
      "Val Loss: 1.709920, Val Acc: 87.55%, Val-Class-Acc: {0: '88.59%', 2: '92.36%', 3: '97.54%', 4: '85.00%', 5: '94.93%', 6: '54.63%', 7: '81.60%', 8: '86.62%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 71/200, Train Loss: 0.026958, Train-Class-Acc: {0: '99.32%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '99.85%', 6: '98.85%', 7: '98.00%', 8: '98.09%', 9: '97.30%'}\n",
      "Val Loss: 1.858928, Val Acc: 87.70%, Val-Class-Acc: {0: '89.67%', 2: '91.67%', 3: '97.13%', 4: '85.00%', 5: '95.82%', 6: '45.37%', 7: '85.60%', 8: '89.17%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 72/200, Train Loss: 0.025646, Train-Class-Acc: {0: '99.59%', 2: '99.31%', 3: '100.00%', 5: '99.70%', 6: '97.93%', 7: '98.20%', 8: '98.89%', 9: '95.27%', 4: '98.10%'}\n",
      "Val Loss: 1.975748, Val Acc: 87.99%, Val-Class-Acc: {0: '91.85%', 2: '93.06%', 3: '97.54%', 4: '87.50%', 5: '96.42%', 6: '50.93%', 7: '79.20%', 8: '85.35%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 73/200, Train Loss: 0.030427, Train-Class-Acc: {0: '99.18%', 2: '100.00%', 3: '99.90%', 5: '99.85%', 6: '97.93%', 7: '97.80%', 8: '98.73%', 4: '100.00%', 9: '98.65%'}\n",
      "Val Loss: 1.835310, Val Acc: 87.92%, Val-Class-Acc: {0: '91.85%', 2: '93.06%', 3: '97.95%', 4: '87.50%', 5: '96.42%', 6: '55.56%', 7: '76.80%', 8: '82.17%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 74/200, Train Loss: 0.018160, Train-Class-Acc: {0: '99.59%', 2: '99.65%', 3: '99.90%', 4: '99.37%', 5: '99.78%', 6: '98.85%', 7: '98.80%', 8: '99.20%', 9: '95.27%'}\n",
      "Val Loss: 2.010247, Val Acc: 88.21%, Val-Class-Acc: {0: '85.87%', 2: '92.36%', 3: '97.95%', 4: '92.50%', 5: '97.61%', 6: '46.30%', 7: '82.40%', 8: '89.17%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 75/200, Train Loss: 0.026628, Train-Class-Acc: {0: '99.59%', 2: '99.65%', 3: '99.90%', 4: '98.73%', 5: '99.78%', 6: '98.39%', 7: '99.00%', 8: '99.36%', 9: '97.97%'}\n",
      "Val Loss: 1.726331, Val Acc: 86.10%, Val-Class-Acc: {0: '88.04%', 2: '75.69%', 3: '97.54%', 4: '87.50%', 5: '95.52%', 6: '49.07%', 7: '82.40%', 8: '88.54%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 76/200, Train Loss: 0.071099, Train-Class-Acc: {0: '99.46%', 2: '98.09%', 3: '99.80%', 4: '97.47%', 5: '99.55%', 6: '97.00%', 7: '98.00%', 8: '97.45%', 9: '95.95%'}\n",
      "Val Loss: 1.743444, Val Acc: 86.97%, Val-Class-Acc: {0: '85.33%', 2: '89.58%', 3: '97.54%', 4: '87.50%', 5: '97.01%', 6: '40.74%', 7: '84.80%', 8: '87.26%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 77/200, Train Loss: 0.099872, Train-Class-Acc: {0: '99.32%', 2: '98.79%', 3: '99.39%', 4: '96.20%', 5: '99.03%', 6: '94.70%', 7: '97.80%', 8: '98.09%', 9: '95.27%'}\n",
      "Val Loss: 2.322479, Val Acc: 87.63%, Val-Class-Acc: {0: '89.13%', 2: '91.67%', 3: '97.13%', 4: '90.00%', 5: '95.82%', 6: '38.89%', 7: '85.60%', 8: '89.81%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 78/200, Train Loss: 0.043490, Train-Class-Acc: {0: '99.46%', 2: '98.61%', 3: '99.49%', 4: '100.00%', 5: '99.40%', 6: '97.70%', 7: '97.60%', 8: '98.89%', 9: '94.59%'}\n",
      "Val Loss: 1.828787, Val Acc: 87.85%, Val-Class-Acc: {0: '90.22%', 2: '90.97%', 3: '96.72%', 4: '82.50%', 5: '95.82%', 6: '45.37%', 7: '86.40%', 8: '87.26%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 79/200, Train Loss: 0.038448, Train-Class-Acc: {0: '99.05%', 2: '98.44%', 3: '99.80%', 4: '99.37%', 5: '99.70%', 6: '97.70%', 7: '99.00%', 8: '99.36%', 9: '97.97%'}\n",
      "Val Loss: 1.995297, Val Acc: 87.55%, Val-Class-Acc: {0: '91.85%', 2: '88.19%', 3: '97.13%', 4: '90.00%', 5: '96.12%', 6: '50.00%', 7: '81.60%', 8: '85.35%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 80/200, Train Loss: 0.026885, Train-Class-Acc: {0: '99.18%', 2: '99.65%', 3: '99.39%', 4: '98.73%', 5: '99.78%', 6: '97.24%', 7: '99.00%', 8: '99.20%', 9: '96.62%'}\n",
      "Val Loss: 2.032924, Val Acc: 88.21%, Val-Class-Acc: {0: '84.24%', 2: '90.97%', 3: '97.54%', 4: '87.50%', 5: '97.91%', 6: '54.63%', 7: '80.00%', 8: '87.90%', 9: '75.68%'}, LR: 0.001000\n",
      "Epoch 81/200, Train Loss: 0.010841, Train-Class-Acc: {0: '99.46%', 2: '99.83%', 3: '100.00%', 4: '100.00%', 5: '99.93%', 6: '98.39%', 7: '99.40%', 8: '99.52%', 9: '97.97%'}\n",
      "Val Loss: 1.958721, Val Acc: 87.85%, Val-Class-Acc: {0: '89.13%', 2: '90.97%', 3: '96.31%', 4: '85.00%', 5: '96.72%', 6: '48.15%', 7: '81.60%', 8: '87.90%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 82/200, Train Loss: 0.011515, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '99.80%', 5: '99.70%', 6: '99.54%', 7: '99.20%', 8: '99.52%', 4: '99.37%', 9: '99.32%'}\n",
      "Val Loss: 2.061473, Val Acc: 88.06%, Val-Class-Acc: {0: '87.50%', 2: '93.06%', 3: '97.13%', 4: '90.00%', 5: '97.61%', 6: '46.30%', 7: '80.80%', 8: '87.26%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 83/200, Train Loss: 0.010644, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '99.90%', 4: '100.00%', 5: '99.93%', 6: '98.62%', 8: '99.52%', 7: '99.00%', 9: '99.32%'}\n",
      "Val Loss: 1.961328, Val Acc: 87.77%, Val-Class-Acc: {0: '90.22%', 2: '90.28%', 3: '97.54%', 4: '80.00%', 5: '97.61%', 6: '43.52%', 7: '80.80%', 8: '87.90%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 84/200, Train Loss: 0.005609, Train-Class-Acc: {0: '100.00%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '100.00%', 6: '99.08%', 7: '99.40%', 8: '100.00%', 9: '100.00%'}\n",
      "Val Loss: 1.926208, Val Acc: 87.92%, Val-Class-Acc: {0: '88.59%', 2: '92.36%', 3: '97.54%', 4: '82.50%', 5: '95.82%', 6: '49.07%', 7: '84.80%', 8: '87.26%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 85/200, Train Loss: 0.011143, Train-Class-Acc: {0: '100.00%', 2: '99.83%', 3: '99.80%', 4: '100.00%', 5: '99.93%', 6: '99.08%', 7: '99.80%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 1.850555, Val Acc: 88.14%, Val-Class-Acc: {0: '89.13%', 2: '93.75%', 3: '96.72%', 4: '80.00%', 5: '96.12%', 6: '53.70%', 7: '82.40%', 8: '85.99%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 86/200, Train Loss: 0.031473, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '99.18%', 4: '98.73%', 5: '99.63%', 6: '98.85%', 7: '98.20%', 8: '98.73%', 9: '97.97%'}\n",
      "Val Loss: 2.086997, Val Acc: 87.55%, Val-Class-Acc: {0: '92.39%', 2: '93.06%', 3: '97.54%', 4: '87.50%', 5: '96.72%', 6: '37.96%', 7: '74.40%', 8: '92.36%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 87/200, Train Loss: 0.023213, Train-Class-Acc: {0: '99.18%', 2: '99.65%', 3: '99.90%', 4: '99.37%', 5: '99.85%', 6: '97.93%', 7: '98.80%', 8: '98.89%', 9: '98.65%'}\n",
      "Val Loss: 2.063211, Val Acc: 88.50%, Val-Class-Acc: {0: '92.93%', 2: '93.75%', 3: '96.72%', 4: '87.50%', 5: '97.01%', 6: '40.74%', 7: '84.00%', 8: '89.17%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 88/200, Train Loss: 0.012078, Train-Class-Acc: {0: '99.86%', 2: '99.65%', 3: '99.90%', 4: '99.37%', 5: '100.00%', 6: '98.16%', 7: '99.40%', 8: '99.20%', 9: '99.32%'}\n",
      "Val Loss: 2.031159, Val Acc: 88.14%, Val-Class-Acc: {0: '87.50%', 2: '89.58%', 3: '97.95%', 4: '90.00%', 5: '97.61%', 6: '50.93%', 7: '80.00%', 8: '87.90%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 89/200, Train Loss: 0.019391, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '100.00%', 4: '100.00%', 5: '99.93%', 7: '99.20%', 8: '99.36%', 9: '97.97%', 6: '98.85%'}\n",
      "Val Loss: 2.058420, Val Acc: 88.14%, Val-Class-Acc: {0: '91.85%', 2: '93.06%', 3: '97.95%', 4: '82.50%', 5: '97.91%', 6: '43.52%', 7: '78.40%', 8: '85.99%', 9: '75.68%'}, LR: 0.001000\n",
      "Epoch 90/200, Train Loss: 0.011907, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '99.90%', 4: '99.37%', 5: '99.85%', 6: '99.31%', 7: '99.40%', 8: '99.52%', 9: '99.32%'}\n",
      "Val Loss: 1.712340, Val Acc: 87.55%, Val-Class-Acc: {0: '86.41%', 2: '93.75%', 3: '97.95%', 4: '87.50%', 5: '95.22%', 6: '49.07%', 7: '78.40%', 8: '89.17%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 91/200, Train Loss: 0.008153, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '99.90%', 4: '100.00%', 5: '99.85%', 6: '99.08%', 7: '99.40%', 8: '99.68%', 9: '100.00%'}\n",
      "Val Loss: 1.840797, Val Acc: 88.14%, Val-Class-Acc: {0: '88.59%', 2: '89.58%', 3: '96.31%', 4: '85.00%', 5: '98.51%', 6: '50.93%', 7: '80.00%', 8: '89.17%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 92/200, Train Loss: 0.172224, Train-Class-Acc: {0: '99.46%', 2: '99.13%', 3: '99.49%', 4: '98.10%', 5: '99.10%', 6: '97.47%', 7: '96.81%', 8: '98.41%', 9: '97.30%'}\n",
      "Val Loss: 3.148519, Val Acc: 86.75%, Val-Class-Acc: {0: '88.59%', 2: '88.89%', 3: '98.36%', 4: '92.50%', 5: '94.33%', 6: '47.22%', 7: '78.40%', 8: '87.90%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 93/200, Train Loss: 0.308411, Train-Class-Acc: {0: '95.78%', 2: '96.53%', 3: '96.00%', 5: '98.06%', 6: '86.18%', 7: '93.81%', 8: '93.63%', 9: '85.14%', 4: '91.77%'}\n",
      "Val Loss: 1.341405, Val Acc: 85.59%, Val-Class-Acc: {0: '92.39%', 2: '95.14%', 3: '95.08%', 4: '90.00%', 5: '89.25%', 6: '40.74%', 7: '80.80%', 8: '84.71%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 94/200, Train Loss: 0.289221, Train-Class-Acc: {0: '96.46%', 2: '97.40%', 3: '97.95%', 4: '97.47%', 5: '97.98%', 6: '87.79%', 7: '93.81%', 8: '93.79%', 9: '89.19%'}\n",
      "Val Loss: 2.313187, Val Acc: 87.34%, Val-Class-Acc: {0: '89.13%', 2: '86.11%', 3: '96.72%', 4: '90.00%', 5: '97.61%', 6: '46.30%', 7: '83.20%', 8: '88.54%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 95/200, Train Loss: 0.077284, Train-Class-Acc: {0: '98.37%', 2: '99.13%', 3: '99.18%', 4: '96.84%', 5: '99.33%', 6: '95.16%', 7: '95.01%', 8: '96.02%', 9: '95.27%'}\n",
      "Val Loss: 1.759494, Val Acc: 87.34%, Val-Class-Acc: {0: '90.76%', 2: '90.28%', 3: '95.08%', 4: '95.00%', 5: '94.93%', 6: '39.81%', 7: '81.60%', 8: '91.08%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 96/200, Train Loss: 0.065607, Train-Class-Acc: {0: '99.46%', 2: '99.65%', 3: '99.18%', 4: '98.10%', 5: '99.48%', 6: '96.77%', 7: '96.81%', 8: '98.57%', 9: '95.95%'}\n",
      "Val Loss: 2.338565, Val Acc: 86.83%, Val-Class-Acc: {0: '88.59%', 2: '86.11%', 3: '97.54%', 4: '75.00%', 5: '94.93%', 6: '43.52%', 7: '86.40%', 8: '87.26%', 9: '75.68%'}, LR: 0.001000\n",
      "Epoch 97/200, Train Loss: 0.044545, Train-Class-Acc: {0: '98.77%', 2: '99.13%', 3: '99.49%', 4: '100.00%', 5: '99.48%', 6: '95.62%', 7: '97.01%', 8: '96.97%', 9: '96.62%'}\n",
      "Val Loss: 2.069980, Val Acc: 88.36%, Val-Class-Acc: {0: '89.67%', 2: '93.75%', 3: '96.72%', 4: '87.50%', 5: '96.12%', 6: '50.93%', 7: '81.60%', 8: '88.54%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 98/200, Train Loss: 0.040294, Train-Class-Acc: {0: '99.73%', 2: '99.13%', 3: '99.90%', 4: '97.47%', 5: '99.70%', 6: '98.16%', 7: '99.20%', 8: '98.73%', 9: '97.30%'}\n",
      "Val Loss: 1.858460, Val Acc: 87.92%, Val-Class-Acc: {0: '91.85%', 2: '90.28%', 3: '96.72%', 4: '82.50%', 5: '95.82%', 6: '49.07%', 7: '83.20%', 8: '86.62%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 99/200, Train Loss: 0.028080, Train-Class-Acc: {0: '100.00%', 2: '99.48%', 3: '99.69%', 4: '98.10%', 5: '99.70%', 6: '97.24%', 7: '97.60%', 8: '98.89%', 9: '99.32%'}\n",
      "Val Loss: 1.973302, Val Acc: 86.46%, Val-Class-Acc: {0: '82.07%', 2: '92.36%', 3: '96.31%', 4: '90.00%', 5: '94.03%', 6: '50.00%', 7: '84.00%', 8: '87.26%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 100/200, Train Loss: 0.088468, Train-Class-Acc: {0: '99.32%', 2: '98.79%', 3: '98.98%', 4: '98.10%', 5: '98.95%', 6: '96.31%', 7: '97.80%', 8: '97.93%', 9: '96.62%'}\n",
      "Val Loss: 2.934747, Val Acc: 87.19%, Val-Class-Acc: {0: '89.13%', 2: '90.28%', 3: '97.13%', 4: '87.50%', 5: '97.31%', 6: '31.48%', 7: '88.00%', 8: '87.90%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 101/200, Train Loss: 0.065349, Train-Class-Acc: {0: '98.64%', 2: '98.96%', 3: '99.39%', 5: '99.33%', 6: '95.62%', 7: '96.81%', 8: '96.97%', 9: '95.95%', 4: '99.37%'}\n",
      "Val Loss: 2.241932, Val Acc: 88.43%, Val-Class-Acc: {0: '92.93%', 2: '93.06%', 3: '96.31%', 4: '82.50%', 5: '96.72%', 6: '48.15%', 7: '82.40%', 8: '88.54%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 102/200, Train Loss: 0.022843, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '99.69%', 4: '100.00%', 5: '99.85%', 6: '98.39%', 7: '99.20%', 8: '98.89%', 9: '97.30%'}\n",
      "Val Loss: 1.923470, Val Acc: 88.36%, Val-Class-Acc: {0: '91.30%', 2: '91.67%', 3: '95.08%', 4: '80.00%', 5: '97.31%', 6: '51.85%', 7: '85.60%', 8: '86.62%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 103/200, Train Loss: 0.023803, Train-Class-Acc: {0: '99.32%', 2: '99.65%', 3: '99.59%', 5: '99.70%', 6: '97.93%', 7: '98.60%', 8: '99.36%', 9: '99.32%', 4: '99.37%'}\n",
      "Val Loss: 2.035754, Val Acc: 88.14%, Val-Class-Acc: {0: '92.93%', 2: '88.89%', 3: '96.31%', 4: '80.00%', 5: '96.72%', 6: '48.15%', 7: '87.20%', 8: '86.62%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 104/200, Train Loss: 0.010508, Train-Class-Acc: {0: '99.86%', 2: '99.65%', 3: '99.90%', 5: '100.00%', 6: '98.85%', 7: '99.00%', 8: '99.36%', 9: '99.32%', 4: '100.00%'}\n",
      "Val Loss: 2.097630, Val Acc: 88.43%, Val-Class-Acc: {0: '91.30%', 2: '92.36%', 3: '97.95%', 4: '80.00%', 5: '95.22%', 6: '50.93%', 7: '84.80%', 8: '87.90%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 105/200, Train Loss: 0.093375, Train-Class-Acc: {0: '99.46%', 2: '99.31%', 3: '99.49%', 5: '99.55%', 6: '97.70%', 7: '97.80%', 8: '98.73%', 9: '97.97%', 4: '98.73%'}\n",
      "Val Loss: 2.629892, Val Acc: 86.75%, Val-Class-Acc: {0: '89.67%', 2: '95.14%', 3: '92.62%', 4: '77.50%', 5: '97.01%', 6: '41.67%', 7: '84.00%', 8: '89.81%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 106/200, Train Loss: 0.066021, Train-Class-Acc: {0: '99.05%', 2: '99.31%', 3: '99.39%', 4: '98.73%', 5: '99.18%', 6: '96.54%', 7: '96.21%', 8: '98.41%', 9: '97.30%'}\n",
      "Val Loss: 2.187552, Val Acc: 87.55%, Val-Class-Acc: {0: '91.30%', 2: '87.50%', 3: '97.54%', 4: '90.00%', 5: '97.91%', 6: '55.56%', 7: '74.40%', 8: '85.35%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 107/200, Train Loss: 0.052225, Train-Class-Acc: {0: '99.73%', 2: '99.48%', 3: '99.80%', 4: '99.37%', 5: '99.63%', 6: '98.39%', 7: '98.00%', 8: '98.57%', 9: '95.95%'}\n",
      "Val Loss: 2.007180, Val Acc: 87.26%, Val-Class-Acc: {0: '78.80%', 2: '90.97%', 3: '96.72%', 4: '77.50%', 5: '96.72%', 6: '57.41%', 7: '87.20%', 8: '84.71%', 9: '75.68%'}, LR: 0.001000\n",
      "Epoch 108/200, Train Loss: 0.029059, Train-Class-Acc: {0: '99.46%', 2: '99.13%', 3: '99.80%', 5: '99.70%', 6: '98.62%', 7: '99.00%', 8: '98.57%', 9: '97.30%', 4: '99.37%'}\n",
      "Val Loss: 1.938818, Val Acc: 87.63%, Val-Class-Acc: {0: '86.41%', 2: '90.97%', 3: '96.31%', 4: '87.50%', 5: '96.72%', 6: '48.15%', 7: '81.60%', 8: '89.17%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 109/200, Train Loss: 0.017871, Train-Class-Acc: {0: '99.32%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '99.93%', 6: '98.85%', 7: '99.60%', 8: '98.41%', 9: '98.65%'}\n",
      "Val Loss: 2.087872, Val Acc: 88.36%, Val-Class-Acc: {0: '87.50%', 2: '93.75%', 3: '98.36%', 4: '90.00%', 5: '96.12%', 6: '44.44%', 7: '85.60%', 8: '89.81%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 110/200, Train Loss: 0.017485, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '100.00%', 4: '99.37%', 5: '99.85%', 6: '98.62%', 7: '99.20%', 8: '99.68%', 9: '97.97%'}\n",
      "Val Loss: 2.010381, Val Acc: 88.94%, Val-Class-Acc: {0: '90.76%', 2: '93.06%', 3: '98.36%', 4: '85.00%', 5: '97.31%', 6: '54.63%', 7: '79.20%', 8: '87.90%', 9: '67.57%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_21.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_110.pth\n",
      "Epoch 111/200, Train Loss: 0.016696, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '99.90%', 4: '99.37%', 5: '99.85%', 6: '98.85%', 7: '97.80%', 8: '98.89%', 9: '98.65%'}\n",
      "Val Loss: 1.979686, Val Acc: 89.08%, Val-Class-Acc: {0: '91.85%', 2: '93.75%', 3: '97.95%', 4: '92.50%', 5: '97.01%', 6: '54.63%', 7: '80.80%', 8: '87.26%', 9: '59.46%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_26.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_111.pth\n",
      "Epoch 112/200, Train Loss: 0.037487, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '99.90%', 4: '99.37%', 5: '99.93%', 6: '98.62%', 7: '98.80%', 8: '99.20%', 9: '97.97%'}\n",
      "Val Loss: 2.555329, Val Acc: 86.10%, Val-Class-Acc: {0: '88.04%', 2: '78.47%', 3: '98.36%', 4: '90.00%', 5: '96.12%', 6: '37.96%', 7: '82.40%', 8: '90.45%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 113/200, Train Loss: 0.020654, Train-Class-Acc: {0: '99.73%', 2: '99.13%', 3: '99.80%', 4: '99.37%', 5: '99.63%', 6: '99.31%', 7: '99.40%', 8: '99.84%', 9: '99.32%'}\n",
      "Val Loss: 2.349458, Val Acc: 87.70%, Val-Class-Acc: {0: '91.30%', 2: '89.58%', 3: '95.08%', 4: '87.50%', 5: '96.12%', 6: '43.52%', 7: '87.20%', 8: '87.26%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 114/200, Train Loss: 0.154085, Train-Class-Acc: {0: '99.32%', 2: '98.61%', 3: '98.16%', 4: '98.73%', 5: '98.88%', 6: '97.00%', 7: '96.61%', 8: '98.25%', 9: '97.30%'}\n",
      "Val Loss: 2.283251, Val Acc: 86.10%, Val-Class-Acc: {0: '85.33%', 2: '87.50%', 3: '93.44%', 4: '90.00%', 5: '96.12%', 6: '44.44%', 7: '87.20%', 8: '89.17%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 115/200, Train Loss: 0.145319, Train-Class-Acc: {0: '97.41%', 2: '98.09%', 3: '98.67%', 4: '97.47%', 5: '98.80%', 6: '92.86%', 7: '96.01%', 8: '96.97%', 9: '93.92%'}\n",
      "Val Loss: 2.121055, Val Acc: 85.59%, Val-Class-Acc: {0: '83.70%', 2: '86.11%', 3: '98.36%', 4: '80.00%', 5: '92.84%', 6: '59.26%', 7: '76.80%', 8: '84.71%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 116/200, Train Loss: 0.059602, Train-Class-Acc: {0: '98.77%', 2: '99.48%', 3: '99.08%', 4: '96.84%', 5: '99.70%', 6: '95.16%', 7: '96.61%', 8: '97.93%', 9: '95.27%'}\n",
      "Val Loss: 2.064712, Val Acc: 87.12%, Val-Class-Acc: {0: '90.22%', 2: '89.58%', 3: '96.31%', 4: '82.50%', 5: '93.73%', 6: '46.30%', 7: '86.40%', 8: '87.26%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 117/200, Train Loss: 0.060464, Train-Class-Acc: {0: '99.46%', 2: '99.31%', 3: '99.39%', 4: '99.37%', 5: '99.33%', 6: '97.93%', 7: '98.80%', 8: '99.20%', 9: '97.97%'}\n",
      "Val Loss: 1.712875, Val Acc: 87.26%, Val-Class-Acc: {0: '80.98%', 2: '91.67%', 3: '97.95%', 4: '90.00%', 5: '94.93%', 6: '54.63%', 7: '84.80%', 8: '86.62%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 118/200, Train Loss: 0.032963, Train-Class-Acc: {0: '99.46%', 2: '99.83%', 3: '99.69%', 5: '99.55%', 6: '96.08%', 7: '98.40%', 8: '98.57%', 9: '97.30%', 4: '99.37%'}\n",
      "Val Loss: 1.998756, Val Acc: 87.85%, Val-Class-Acc: {0: '85.87%', 2: '93.06%', 3: '96.72%', 4: '87.50%', 5: '96.12%', 6: '50.93%', 7: '80.80%', 8: '90.45%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 119/200, Train Loss: 0.014715, Train-Class-Acc: {0: '99.32%', 2: '100.00%', 3: '99.80%', 4: '99.37%', 5: '99.93%', 6: '98.39%', 7: '99.20%', 8: '99.36%', 9: '100.00%'}\n",
      "Val Loss: 2.070615, Val Acc: 87.99%, Val-Class-Acc: {0: '91.85%', 2: '92.36%', 3: '95.08%', 4: '90.00%', 5: '95.82%', 6: '50.93%', 7: '84.80%', 8: '88.54%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 120/200, Train Loss: 0.014783, Train-Class-Acc: {0: '99.59%', 2: '100.00%', 3: '99.90%', 4: '100.00%', 5: '99.78%', 6: '98.85%', 7: '99.40%', 8: '99.20%', 9: '97.97%'}\n",
      "Val Loss: 2.148142, Val Acc: 87.12%, Val-Class-Acc: {0: '84.24%', 2: '92.36%', 3: '96.31%', 4: '92.50%', 5: '94.63%', 6: '49.07%', 7: '82.40%', 8: '90.45%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 121/200, Train Loss: 0.008316, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '99.85%', 6: '99.77%', 7: '99.40%', 8: '99.52%', 9: '98.65%'}\n",
      "Val Loss: 2.356687, Val Acc: 86.68%, Val-Class-Acc: {0: '83.15%', 2: '90.28%', 3: '97.13%', 4: '85.00%', 5: '96.42%', 6: '38.89%', 7: '81.60%', 8: '91.72%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 122/200, Train Loss: 0.011590, Train-Class-Acc: {0: '100.00%', 2: '99.83%', 3: '99.80%', 4: '98.73%', 5: '99.85%', 6: '98.62%', 7: '99.60%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 2.311422, Val Acc: 86.90%, Val-Class-Acc: {0: '83.70%', 2: '92.36%', 3: '97.54%', 4: '82.50%', 5: '94.63%', 6: '46.30%', 7: '84.00%', 8: '87.26%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 123/200, Train Loss: 0.026854, Train-Class-Acc: {0: '99.59%', 2: '99.31%', 3: '100.00%', 4: '99.37%', 5: '99.85%', 6: '99.31%', 7: '99.60%', 8: '99.36%', 9: '99.32%'}\n",
      "Val Loss: 2.092955, Val Acc: 86.54%, Val-Class-Acc: {0: '87.50%', 2: '93.06%', 3: '96.72%', 4: '87.50%', 5: '92.24%', 6: '38.89%', 7: '85.60%', 8: '92.36%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 124/200, Train Loss: 0.048726, Train-Class-Acc: {0: '99.18%', 2: '98.79%', 3: '99.69%', 4: '96.84%', 5: '99.55%', 6: '97.70%', 7: '98.00%', 8: '98.41%', 9: '96.62%'}\n",
      "Val Loss: 3.013312, Val Acc: 86.83%, Val-Class-Acc: {0: '86.96%', 2: '90.28%', 3: '95.49%', 4: '72.50%', 5: '97.91%', 6: '50.93%', 7: '80.80%', 8: '85.35%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 125/200, Train Loss: 0.044673, Train-Class-Acc: {0: '98.64%', 2: '99.31%', 3: '99.69%', 4: '99.37%', 5: '99.70%', 6: '97.70%', 7: '97.80%', 8: '98.73%', 9: '95.95%'}\n",
      "Val Loss: 2.382058, Val Acc: 86.90%, Val-Class-Acc: {0: '85.87%', 2: '89.58%', 3: '97.13%', 4: '77.50%', 5: '97.01%', 6: '48.15%', 7: '84.00%', 8: '82.80%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 126/200, Train Loss: 0.022561, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '100.00%', 4: '98.73%', 5: '99.55%', 6: '98.39%', 7: '99.60%', 8: '99.20%', 9: '98.65%'}\n",
      "Val Loss: 2.294763, Val Acc: 87.48%, Val-Class-Acc: {0: '83.70%', 2: '90.97%', 3: '97.13%', 4: '85.00%', 5: '98.21%', 6: '40.74%', 7: '81.60%', 8: '92.36%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 127/200, Train Loss: 0.023003, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '99.80%', 4: '100.00%', 5: '99.93%', 6: '98.39%', 7: '98.60%', 8: '99.52%', 9: '99.32%'}\n",
      "Val Loss: 2.245874, Val Acc: 86.10%, Val-Class-Acc: {0: '90.76%', 2: '92.36%', 3: '98.36%', 4: '77.50%', 5: '91.04%', 6: '49.07%', 7: '78.40%', 8: '85.35%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 128/200, Train Loss: 0.189471, Train-Class-Acc: {0: '98.09%', 2: '96.88%', 3: '98.67%', 4: '96.84%', 5: '98.80%', 6: '94.24%', 7: '94.01%', 8: '97.13%', 9: '93.92%'}\n",
      "Val Loss: 2.507756, Val Acc: 86.24%, Val-Class-Acc: {0: '89.67%', 2: '93.75%', 3: '95.90%', 4: '87.50%', 5: '97.01%', 6: '44.44%', 7: '73.60%', 8: '81.53%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 129/200, Train Loss: 0.054646, Train-Class-Acc: {0: '98.77%', 2: '99.48%', 3: '98.98%', 5: '99.40%', 6: '95.85%', 7: '96.81%', 8: '98.57%', 9: '94.59%', 4: '98.73%'}\n",
      "Val Loss: 2.325194, Val Acc: 85.88%, Val-Class-Acc: {0: '85.87%', 2: '90.97%', 3: '97.95%', 4: '90.00%', 5: '93.43%', 6: '38.89%', 7: '84.80%', 8: '83.44%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 130/200, Train Loss: 0.207148, Train-Class-Acc: {0: '97.28%', 2: '98.96%', 3: '98.05%', 4: '97.47%', 5: '98.65%', 6: '93.55%', 7: '96.01%', 8: '96.18%', 9: '93.92%'}\n",
      "Val Loss: 2.836006, Val Acc: 84.06%, Val-Class-Acc: {0: '86.96%', 2: '84.72%', 3: '95.49%', 4: '80.00%', 5: '93.73%', 6: '32.41%', 7: '80.80%', 8: '87.26%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 131/200, Train Loss: 0.066573, Train-Class-Acc: {0: '98.91%', 2: '98.79%', 3: '99.08%', 5: '99.25%', 6: '96.08%', 7: '98.20%', 8: '98.25%', 9: '95.27%', 4: '98.73%'}\n",
      "Val Loss: 2.430906, Val Acc: 85.95%, Val-Class-Acc: {0: '83.15%', 2: '90.97%', 3: '97.13%', 4: '82.50%', 5: '94.33%', 6: '50.00%', 7: '80.00%', 8: '87.26%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 132/200, Train Loss: 0.025386, Train-Class-Acc: {0: '99.46%', 2: '99.83%', 3: '99.59%', 4: '98.73%', 5: '99.78%', 6: '97.24%', 7: '98.40%', 8: '98.41%', 9: '97.30%'}\n",
      "Val Loss: 2.302837, Val Acc: 86.75%, Val-Class-Acc: {0: '89.13%', 2: '90.97%', 3: '96.31%', 4: '77.50%', 5: '94.93%', 6: '57.41%', 7: '83.20%', 8: '80.25%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 133/200, Train Loss: 0.028886, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '99.80%', 4: '98.73%', 5: '99.48%', 6: '97.70%', 7: '99.00%', 8: '98.57%', 9: '98.65%'}\n",
      "Val Loss: 2.397177, Val Acc: 86.75%, Val-Class-Acc: {0: '85.33%', 2: '91.67%', 3: '96.72%', 4: '92.50%', 5: '96.12%', 6: '44.44%', 7: '81.60%', 8: '88.54%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 134/200, Train Loss: 0.061913, Train-Class-Acc: {0: '99.46%', 2: '99.31%', 3: '98.98%', 5: '99.55%', 6: '97.93%', 7: '97.41%', 8: '98.57%', 4: '99.37%', 9: '95.27%'}\n",
      "Val Loss: 2.229891, Val Acc: 86.97%, Val-Class-Acc: {0: '91.30%', 2: '93.75%', 3: '97.13%', 4: '90.00%', 5: '94.63%', 6: '50.00%', 7: '78.40%', 8: '86.62%', 9: '37.84%'}, LR: 0.001000\n",
      "Epoch 135/200, Train Loss: 0.040067, Train-Class-Acc: {0: '99.05%', 2: '99.48%', 3: '99.59%', 4: '97.47%', 5: '99.33%', 6: '97.47%', 7: '97.80%', 8: '98.73%', 9: '99.32%'}\n",
      "Val Loss: 2.610048, Val Acc: 86.68%, Val-Class-Acc: {0: '84.24%', 2: '87.50%', 3: '95.08%', 4: '85.00%', 5: '97.91%', 6: '52.78%', 7: '84.00%', 8: '87.90%', 9: '43.24%'}, LR: 0.001000\n",
      "Epoch 136/200, Train Loss: 0.036001, Train-Class-Acc: {0: '99.32%', 2: '99.48%', 3: '99.69%', 5: '99.78%', 6: '97.24%', 7: '99.20%', 8: '98.89%', 9: '98.65%', 4: '98.73%'}\n",
      "Val Loss: 1.697557, Val Acc: 87.92%, Val-Class-Acc: {0: '91.85%', 2: '92.36%', 3: '95.08%', 4: '85.00%', 5: '96.42%', 6: '54.63%', 7: '84.80%', 8: '84.71%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 137/200, Train Loss: 0.184860, Train-Class-Acc: {0: '98.77%', 2: '99.13%', 3: '98.05%', 4: '98.73%', 5: '98.80%', 6: '95.62%', 7: '97.60%', 8: '97.61%', 9: '95.95%'}\n",
      "Val Loss: 2.898766, Val Acc: 85.30%, Val-Class-Acc: {0: '79.35%', 2: '90.28%', 3: '97.13%', 4: '82.50%', 5: '90.75%', 6: '65.74%', 7: '80.00%', 8: '84.71%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 138/200, Train Loss: 0.147736, Train-Class-Acc: {0: '97.82%', 2: '97.75%', 3: '98.26%', 4: '98.10%', 5: '98.88%', 6: '94.47%', 7: '96.21%', 8: '97.45%', 9: '91.89%'}\n",
      "Val Loss: 1.926991, Val Acc: 86.46%, Val-Class-Acc: {0: '85.87%', 2: '89.58%', 3: '96.31%', 4: '85.00%', 5: '94.63%', 6: '51.85%', 7: '85.60%', 8: '84.71%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 139/200, Train Loss: 0.053680, Train-Class-Acc: {0: '98.09%', 2: '99.31%', 3: '99.49%', 4: '98.10%', 5: '99.63%', 6: '96.54%', 7: '96.41%', 8: '97.93%', 9: '94.59%'}\n",
      "Val Loss: 2.010865, Val Acc: 86.32%, Val-Class-Acc: {0: '91.85%', 2: '90.97%', 3: '95.49%', 4: '87.50%', 5: '93.43%', 6: '49.07%', 7: '83.20%', 8: '81.53%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 140/200, Train Loss: 0.034721, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '99.80%', 4: '100.00%', 5: '99.63%', 6: '97.93%', 7: '98.20%', 8: '99.36%', 9: '97.30%'}\n",
      "Val Loss: 1.877865, Val Acc: 87.34%, Val-Class-Acc: {0: '89.13%', 2: '88.89%', 3: '95.90%', 4: '90.00%', 5: '97.01%', 6: '48.15%', 7: '80.00%', 8: '87.90%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 141/200, Train Loss: 0.053238, Train-Class-Acc: {0: '99.46%', 2: '99.65%', 3: '99.69%', 5: '99.63%', 6: '98.39%', 7: '98.40%', 8: '99.04%', 9: '99.32%', 4: '99.37%'}\n",
      "Val Loss: 2.355720, Val Acc: 86.54%, Val-Class-Acc: {0: '87.50%', 2: '89.58%', 3: '94.67%', 4: '90.00%', 5: '99.10%', 6: '47.22%', 7: '83.20%', 8: '81.53%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 142/200, Train Loss: 0.082998, Train-Class-Acc: {0: '98.91%', 2: '99.48%', 3: '99.39%', 5: '99.33%', 6: '97.93%', 7: '98.60%', 8: '98.73%', 9: '98.65%', 4: '100.00%'}\n",
      "Val Loss: 2.049313, Val Acc: 87.12%, Val-Class-Acc: {0: '89.13%', 2: '91.67%', 3: '95.90%', 4: '92.50%', 5: '95.82%', 6: '45.37%', 7: '81.60%', 8: '86.62%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 143/200, Train Loss: 0.037030, Train-Class-Acc: {0: '99.32%', 2: '99.13%', 3: '99.59%', 4: '98.73%', 5: '99.93%', 6: '97.70%', 7: '98.80%', 8: '98.25%', 9: '95.95%'}\n",
      "Val Loss: 1.830094, Val Acc: 86.46%, Val-Class-Acc: {0: '90.22%', 2: '89.58%', 3: '95.90%', 4: '90.00%', 5: '94.33%', 6: '38.89%', 7: '83.20%', 8: '88.54%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 144/200, Train Loss: 0.029597, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '99.80%', 4: '98.10%', 5: '99.85%', 6: '98.16%', 7: '98.20%', 8: '98.25%', 9: '97.97%'}\n",
      "Val Loss: 1.958683, Val Acc: 86.83%, Val-Class-Acc: {0: '85.87%', 2: '92.36%', 3: '96.31%', 4: '87.50%', 5: '94.03%', 6: '55.56%', 7: '79.20%', 8: '85.99%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 145/200, Train Loss: 0.015940, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '99.90%', 4: '99.37%', 5: '99.85%', 6: '99.08%', 7: '99.20%', 8: '98.89%', 9: '97.97%'}\n",
      "Val Loss: 1.913354, Val Acc: 87.19%, Val-Class-Acc: {0: '88.59%', 2: '91.67%', 3: '95.90%', 4: '90.00%', 5: '97.01%', 6: '39.81%', 7: '84.80%', 8: '85.35%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 146/200, Train Loss: 0.013340, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '99.80%', 4: '100.00%', 5: '99.85%', 6: '98.85%', 7: '99.40%', 8: '100.00%', 9: '98.65%'}\n",
      "Val Loss: 2.017919, Val Acc: 87.55%, Val-Class-Acc: {0: '86.41%', 2: '90.97%', 3: '97.54%', 4: '90.00%', 5: '95.22%', 6: '49.07%', 7: '84.80%', 8: '85.99%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 147/200, Train Loss: 0.003777, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '100.00%', 5: '99.93%', 6: '100.00%', 7: '99.60%', 8: '99.68%', 9: '99.32%', 4: '99.37%'}\n",
      "Val Loss: 2.147408, Val Acc: 87.34%, Val-Class-Acc: {0: '88.59%', 2: '90.28%', 3: '96.31%', 4: '90.00%', 5: '95.22%', 6: '44.44%', 7: '82.40%', 8: '91.08%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 148/200, Train Loss: 0.004862, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '99.90%', 4: '100.00%', 5: '99.93%', 6: '99.31%', 7: '100.00%', 8: '100.00%', 9: '99.32%'}\n",
      "Val Loss: 2.077951, Val Acc: 87.92%, Val-Class-Acc: {0: '88.59%', 2: '92.36%', 3: '96.72%', 4: '85.00%', 5: '96.72%', 6: '47.22%', 7: '81.60%', 8: '88.54%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 149/200, Train Loss: 0.006072, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '99.85%', 6: '99.54%', 7: '99.20%', 8: '99.52%', 9: '100.00%'}\n",
      "Val Loss: 2.069064, Val Acc: 87.34%, Val-Class-Acc: {0: '88.04%', 2: '91.67%', 3: '96.31%', 4: '90.00%', 5: '95.52%', 6: '48.15%', 7: '80.80%', 8: '87.90%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 150/200, Train Loss: 0.189611, Train-Class-Acc: {0: '98.64%', 2: '99.31%', 3: '98.98%', 4: '98.10%', 5: '98.58%', 6: '94.70%', 7: '96.41%', 8: '97.61%', 9: '96.62%'}\n",
      "Val Loss: 2.050552, Val Acc: 85.37%, Val-Class-Acc: {0: '82.61%', 2: '84.72%', 3: '95.90%', 4: '92.50%', 5: '97.91%', 6: '53.70%', 7: '80.00%', 8: '78.98%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 151/200, Train Loss: 0.090235, Train-Class-Acc: {0: '99.18%', 2: '97.92%', 3: '99.18%', 5: '99.40%', 6: '94.70%', 7: '97.21%', 8: '96.82%', 9: '95.95%', 4: '98.73%'}\n",
      "Val Loss: 1.998118, Val Acc: 86.68%, Val-Class-Acc: {0: '86.96%', 2: '90.28%', 3: '95.90%', 4: '90.00%', 5: '94.63%', 6: '44.44%', 7: '84.00%', 8: '88.54%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 152/200, Train Loss: 0.062883, Train-Class-Acc: {0: '99.18%', 2: '99.31%', 3: '99.39%', 4: '98.73%', 5: '99.48%', 6: '97.93%', 7: '97.41%', 8: '98.41%', 9: '97.30%'}\n",
      "Val Loss: 2.185907, Val Acc: 84.72%, Val-Class-Acc: {0: '83.70%', 2: '90.28%', 3: '97.54%', 4: '70.00%', 5: '92.84%', 6: '45.37%', 7: '76.00%', 8: '86.62%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 153/200, Train Loss: 0.045449, Train-Class-Acc: {0: '99.05%', 2: '99.65%', 3: '99.59%', 4: '100.00%', 5: '99.85%', 6: '97.93%', 7: '98.60%', 8: '98.57%', 9: '97.30%'}\n",
      "Val Loss: 2.053574, Val Acc: 85.95%, Val-Class-Acc: {0: '83.70%', 2: '93.75%', 3: '94.26%', 4: '90.00%', 5: '94.03%', 6: '44.44%', 7: '81.60%', 8: '87.26%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 154/200, Train Loss: 0.016286, Train-Class-Acc: {0: '99.59%', 2: '99.65%', 3: '99.59%', 4: '100.00%', 5: '99.85%', 6: '98.16%', 7: '98.60%', 8: '99.36%', 9: '98.65%'}\n",
      "Val Loss: 2.245510, Val Acc: 86.32%, Val-Class-Acc: {0: '81.52%', 2: '94.44%', 3: '95.90%', 4: '82.50%', 5: '95.52%', 6: '48.15%', 7: '85.60%', 8: '85.99%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 155/200, Train Loss: 0.022159, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '99.49%', 4: '99.37%', 5: '99.85%', 6: '98.62%', 7: '98.40%', 8: '98.89%', 9: '99.32%'}\n",
      "Val Loss: 2.234328, Val Acc: 87.12%, Val-Class-Acc: {0: '86.96%', 2: '94.44%', 3: '97.54%', 4: '87.50%', 5: '95.52%', 6: '43.52%', 7: '87.20%', 8: '82.80%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 156/200, Train Loss: 0.013928, Train-Class-Acc: {0: '99.73%', 2: '99.48%', 3: '99.90%', 5: '99.93%', 6: '98.62%', 7: '99.40%', 8: '99.20%', 9: '97.97%', 4: '98.10%'}\n",
      "Val Loss: 2.222015, Val Acc: 86.83%, Val-Class-Acc: {0: '88.59%', 2: '94.44%', 3: '97.13%', 4: '80.00%', 5: '95.52%', 6: '42.59%', 7: '80.80%', 8: '84.71%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 157/200, Train Loss: 0.007700, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '99.80%', 4: '99.37%', 5: '99.85%', 6: '99.54%', 7: '99.20%', 8: '99.84%', 9: '100.00%'}\n",
      "Val Loss: 2.231890, Val Acc: 86.83%, Val-Class-Acc: {0: '87.50%', 2: '93.75%', 3: '96.31%', 4: '90.00%', 5: '93.43%', 6: '38.89%', 7: '86.40%', 8: '89.81%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 158/200, Train Loss: 0.012417, Train-Class-Acc: {0: '99.46%', 2: '99.83%', 3: '100.00%', 4: '99.37%', 5: '100.00%', 6: '99.31%', 7: '99.60%', 8: '99.20%', 9: '100.00%'}\n",
      "Val Loss: 2.200958, Val Acc: 87.48%, Val-Class-Acc: {0: '84.24%', 2: '94.44%', 3: '96.31%', 4: '92.50%', 5: '96.72%', 6: '49.07%', 7: '81.60%', 8: '88.54%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 159/200, Train Loss: 0.075370, Train-Class-Acc: {0: '98.77%', 2: '98.61%', 3: '99.28%', 4: '98.73%', 5: '99.48%', 6: '97.00%', 7: '98.00%', 8: '98.41%', 9: '96.62%'}\n",
      "Val Loss: 2.504257, Val Acc: 86.17%, Val-Class-Acc: {0: '83.70%', 2: '94.44%', 3: '97.13%', 4: '82.50%', 5: '95.82%', 6: '47.22%', 7: '79.20%', 8: '86.62%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 160/200, Train Loss: 0.024604, Train-Class-Acc: {0: '99.18%', 2: '99.31%', 3: '99.90%', 4: '100.00%', 5: '99.85%', 6: '99.54%', 7: '98.20%', 8: '98.73%', 9: '96.62%'}\n",
      "Val Loss: 2.248666, Val Acc: 86.54%, Val-Class-Acc: {0: '87.50%', 2: '93.75%', 3: '95.90%', 4: '80.00%', 5: '93.73%', 6: '46.30%', 7: '83.20%', 8: '87.26%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 161/200, Train Loss: 0.011299, Train-Class-Acc: {0: '99.86%', 2: '99.65%', 3: '100.00%', 5: '99.85%', 6: '100.00%', 7: '99.80%', 8: '99.36%', 9: '99.32%', 4: '98.73%'}\n",
      "Val Loss: 2.144619, Val Acc: 87.41%, Val-Class-Acc: {0: '91.30%', 2: '90.97%', 3: '96.31%', 4: '82.50%', 5: '97.31%', 6: '49.07%', 7: '80.80%', 8: '85.35%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 162/200, Train Loss: 0.090968, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '99.49%', 5: '99.48%', 6: '97.47%', 7: '98.40%', 8: '98.09%', 9: '98.65%', 4: '98.10%'}\n",
      "Val Loss: 2.443733, Val Acc: 86.61%, Val-Class-Acc: {0: '84.78%', 2: '94.44%', 3: '96.31%', 4: '85.00%', 5: '95.82%', 6: '47.22%', 7: '77.60%', 8: '85.99%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 163/200, Train Loss: 0.023845, Train-Class-Acc: {0: '99.86%', 2: '99.65%', 3: '99.59%', 4: '97.47%', 5: '99.70%', 6: '97.70%', 7: '98.80%', 8: '99.20%', 9: '97.30%'}\n",
      "Val Loss: 2.429472, Val Acc: 86.46%, Val-Class-Acc: {0: '88.59%', 2: '92.36%', 3: '97.95%', 4: '92.50%', 5: '94.63%', 6: '43.52%', 7: '80.00%', 8: '81.53%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 164/200, Train Loss: 0.048589, Train-Class-Acc: {0: '99.59%', 2: '99.65%', 3: '99.90%', 4: '99.37%', 5: '99.63%', 6: '99.08%', 7: '98.80%', 8: '99.04%', 9: '98.65%'}\n",
      "Val Loss: 2.382709, Val Acc: 86.10%, Val-Class-Acc: {0: '86.41%', 2: '87.50%', 3: '95.08%', 4: '90.00%', 5: '94.33%', 6: '48.15%', 7: '83.20%', 8: '85.99%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 165/200, Train Loss: 0.037324, Train-Class-Acc: {0: '99.46%', 2: '98.79%', 3: '99.59%', 4: '99.37%', 5: '99.70%', 6: '97.47%', 7: '98.20%', 8: '98.41%', 9: '97.97%'}\n",
      "Val Loss: 2.375386, Val Acc: 86.10%, Val-Class-Acc: {0: '86.96%', 2: '93.75%', 3: '92.21%', 4: '87.50%', 5: '94.33%', 6: '54.63%', 7: '80.00%', 8: '83.44%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 166/200, Train Loss: 0.017915, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '99.80%', 5: '99.93%', 6: '98.62%', 7: '99.40%', 8: '99.36%', 9: '97.97%', 4: '99.37%'}\n",
      "Val Loss: 2.427788, Val Acc: 85.01%, Val-Class-Acc: {0: '87.50%', 2: '94.44%', 3: '92.62%', 4: '75.00%', 5: '92.84%', 6: '43.52%', 7: '84.80%', 8: '83.44%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 167/200, Train Loss: 0.012582, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '99.80%', 4: '98.73%', 5: '99.85%', 6: '100.00%', 7: '99.00%', 8: '99.84%', 9: '100.00%'}\n",
      "Val Loss: 2.457835, Val Acc: 86.61%, Val-Class-Acc: {0: '88.59%', 2: '93.06%', 3: '96.72%', 4: '90.00%', 5: '93.43%', 6: '37.04%', 7: '85.60%', 8: '87.90%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 168/200, Train Loss: 0.016389, Train-Class-Acc: {0: '99.18%', 2: '99.83%', 3: '100.00%', 4: '98.73%', 5: '99.85%', 6: '98.85%', 7: '98.80%', 8: '99.68%', 9: '97.97%'}\n",
      "Val Loss: 2.697214, Val Acc: 86.90%, Val-Class-Acc: {0: '85.33%', 2: '94.44%', 3: '96.31%', 4: '95.00%', 5: '95.52%', 6: '45.37%', 7: '83.20%', 8: '86.62%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 169/200, Train Loss: 0.011220, Train-Class-Acc: {0: '99.32%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '99.85%', 6: '99.08%', 7: '99.60%', 8: '99.36%', 9: '97.97%'}\n",
      "Val Loss: 2.733718, Val Acc: 86.90%, Val-Class-Acc: {0: '91.30%', 2: '93.75%', 3: '96.72%', 4: '82.50%', 5: '94.93%', 6: '46.30%', 7: '80.80%', 8: '85.35%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 170/200, Train Loss: 0.038618, Train-Class-Acc: {0: '99.59%', 2: '99.65%', 3: '99.69%', 4: '98.73%', 5: '99.63%', 6: '98.85%', 7: '99.00%', 8: '99.20%', 9: '96.62%'}\n",
      "Val Loss: 2.762622, Val Acc: 84.50%, Val-Class-Acc: {0: '83.15%', 2: '81.25%', 3: '95.90%', 4: '80.00%', 5: '92.54%', 6: '45.37%', 7: '85.60%', 8: '84.71%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 171/200, Train Loss: 0.013116, Train-Class-Acc: {0: '99.32%', 2: '99.48%', 3: '99.90%', 4: '100.00%', 5: '99.85%', 6: '98.62%', 7: '99.20%', 8: '99.52%', 9: '98.65%'}\n",
      "Val Loss: 2.633083, Val Acc: 87.41%, Val-Class-Acc: {0: '88.59%', 2: '93.75%', 3: '97.13%', 4: '90.00%', 5: '95.82%', 6: '46.30%', 7: '80.80%', 8: '87.26%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 172/200, Train Loss: 0.017543, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '99.90%', 4: '100.00%', 5: '99.70%', 6: '98.85%', 7: '99.20%', 8: '99.20%', 9: '99.32%'}\n",
      "Val Loss: 2.400213, Val Acc: 86.68%, Val-Class-Acc: {0: '86.41%', 2: '92.36%', 3: '94.26%', 4: '90.00%', 5: '93.73%', 6: '51.85%', 7: '84.00%', 8: '87.90%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 173/200, Train Loss: 0.020909, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '99.69%', 4: '98.73%', 5: '99.78%', 6: '99.31%', 7: '99.20%', 8: '98.89%', 9: '99.32%'}\n",
      "Val Loss: 2.232307, Val Acc: 86.03%, Val-Class-Acc: {0: '88.59%', 2: '92.36%', 3: '96.72%', 4: '92.50%', 5: '90.15%', 6: '53.70%', 7: '84.00%', 8: '82.80%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 174/200, Train Loss: 0.030529, Train-Class-Acc: {0: '99.05%', 2: '98.61%', 3: '99.49%', 4: '99.37%', 5: '99.85%', 6: '99.31%', 7: '99.20%', 8: '99.04%', 9: '97.30%'}\n",
      "Val Loss: 2.549329, Val Acc: 87.26%, Val-Class-Acc: {0: '90.22%', 2: '88.89%', 3: '98.36%', 4: '87.50%', 5: '97.91%', 6: '50.00%', 7: '78.40%', 8: '83.44%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 175/200, Train Loss: 0.062499, Train-Class-Acc: {0: '98.91%', 2: '98.79%', 3: '98.87%', 4: '99.37%', 5: '99.25%', 6: '96.77%', 7: '98.20%', 8: '98.89%', 9: '97.30%'}\n",
      "Val Loss: 2.252521, Val Acc: 84.93%, Val-Class-Acc: {0: '87.50%', 2: '88.89%', 3: '93.85%', 4: '90.00%', 5: '95.82%', 6: '37.04%', 7: '84.00%', 8: '83.44%', 9: '43.24%'}, LR: 0.001000\n",
      "Epoch 176/200, Train Loss: 0.090376, Train-Class-Acc: {0: '98.23%', 2: '98.27%', 3: '99.18%', 4: '99.37%', 5: '98.95%', 6: '95.39%', 7: '98.00%', 8: '97.61%', 9: '97.97%'}\n",
      "Val Loss: 2.821036, Val Acc: 83.04%, Val-Class-Acc: {0: '78.26%', 2: '82.64%', 3: '96.31%', 4: '82.50%', 5: '88.96%', 6: '47.22%', 7: '81.60%', 8: '87.26%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 177/200, Train Loss: 0.105295, Train-Class-Acc: {0: '98.64%', 2: '98.61%', 3: '99.08%', 4: '100.00%', 5: '99.25%', 6: '96.77%', 7: '97.80%', 8: '96.18%', 9: '93.92%'}\n",
      "Val Loss: 2.398398, Val Acc: 86.17%, Val-Class-Acc: {0: '89.13%', 2: '92.36%', 3: '95.49%', 4: '90.00%', 5: '95.22%', 6: '34.26%', 7: '79.20%', 8: '89.81%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 178/200, Train Loss: 0.063124, Train-Class-Acc: {0: '99.32%', 2: '99.65%', 3: '99.69%', 4: '100.00%', 5: '99.70%', 6: '97.24%', 7: '97.60%', 8: '98.41%', 9: '95.95%'}\n",
      "Val Loss: 2.468832, Val Acc: 86.39%, Val-Class-Acc: {0: '86.96%', 2: '88.89%', 3: '97.13%', 4: '90.00%', 5: '94.33%', 6: '41.67%', 7: '81.60%', 8: '89.81%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 179/200, Train Loss: 0.027097, Train-Class-Acc: {0: '99.59%', 2: '99.83%', 3: '99.69%', 4: '99.37%', 5: '99.78%', 6: '97.24%', 7: '99.20%', 8: '98.41%', 9: '98.65%'}\n",
      "Val Loss: 2.751311, Val Acc: 86.46%, Val-Class-Acc: {0: '86.96%', 2: '88.19%', 3: '95.90%', 4: '82.50%', 5: '97.61%', 6: '46.30%', 7: '72.80%', 8: '91.08%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 180/200, Train Loss: 0.022665, Train-Class-Acc: {0: '99.32%', 2: '99.83%', 3: '99.80%', 5: '99.93%', 6: '99.08%', 7: '98.20%', 8: '99.04%', 9: '96.62%', 4: '98.73%'}\n",
      "Val Loss: 2.411312, Val Acc: 87.26%, Val-Class-Acc: {0: '94.02%', 2: '92.36%', 3: '96.72%', 4: '95.00%', 5: '94.63%', 6: '39.81%', 7: '80.00%', 8: '87.90%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 181/200, Train Loss: 0.015289, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '99.90%', 4: '99.37%', 5: '99.93%', 6: '98.16%', 7: '98.80%', 8: '100.00%', 9: '97.30%'}\n",
      "Val Loss: 2.044545, Val Acc: 88.36%, Val-Class-Acc: {0: '91.85%', 2: '95.14%', 3: '95.49%', 4: '92.50%', 5: '94.93%', 6: '50.93%', 7: '81.60%', 8: '87.26%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 182/200, Train Loss: 0.031090, Train-Class-Acc: {0: '98.91%', 2: '99.83%', 3: '99.80%', 4: '98.10%', 5: '99.48%', 6: '98.39%', 7: '98.80%', 8: '99.20%', 9: '100.00%'}\n",
      "Val Loss: 2.559515, Val Acc: 87.55%, Val-Class-Acc: {0: '90.22%', 2: '95.14%', 3: '94.26%', 4: '85.00%', 5: '96.72%', 6: '42.59%', 7: '86.40%', 8: '85.99%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 183/200, Train Loss: 0.017551, Train-Class-Acc: {0: '99.59%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '99.78%', 6: '98.85%', 7: '98.80%', 8: '99.52%', 9: '99.32%'}\n",
      "Val Loss: 2.500083, Val Acc: 88.21%, Val-Class-Acc: {0: '90.22%', 2: '90.97%', 3: '95.08%', 4: '87.50%', 5: '96.42%', 6: '61.11%', 7: '84.80%', 8: '83.44%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 184/200, Train Loss: 0.014353, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '99.59%', 4: '100.00%', 5: '99.85%', 6: '98.62%', 7: '99.00%', 8: '98.89%', 9: '99.32%'}\n",
      "Val Loss: 2.256164, Val Acc: 87.85%, Val-Class-Acc: {0: '90.22%', 2: '91.67%', 3: '96.72%', 4: '92.50%', 5: '94.33%', 6: '50.93%', 7: '82.40%', 8: '88.54%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 185/200, Train Loss: 0.003404, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '100.00%', 4: '100.00%', 5: '100.00%', 6: '99.77%', 7: '100.00%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 2.507076, Val Acc: 87.77%, Val-Class-Acc: {0: '86.96%', 2: '94.44%', 3: '97.13%', 4: '92.50%', 5: '95.22%', 6: '45.37%', 7: '80.80%', 8: '90.45%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 186/200, Train Loss: 0.009496, Train-Class-Acc: {0: '99.46%', 2: '100.00%', 3: '100.00%', 4: '99.37%', 5: '100.00%', 6: '99.77%', 7: '99.60%', 8: '99.52%', 9: '99.32%'}\n",
      "Val Loss: 2.464591, Val Acc: 87.92%, Val-Class-Acc: {0: '90.76%', 2: '94.44%', 3: '96.72%', 4: '92.50%', 5: '94.93%', 6: '52.78%', 7: '80.80%', 8: '85.99%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 187/200, Train Loss: 0.005553, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '99.93%', 6: '99.54%', 7: '99.60%', 8: '99.52%', 9: '98.65%'}\n",
      "Val Loss: 2.359117, Val Acc: 87.99%, Val-Class-Acc: {0: '91.30%', 2: '94.44%', 3: '95.90%', 4: '90.00%', 5: '95.82%', 6: '51.85%', 7: '83.20%', 8: '85.35%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 188/200, Train Loss: 0.001987, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '100.00%', 5: '100.00%', 6: '99.77%', 7: '100.00%', 8: '99.84%', 4: '100.00%', 9: '99.32%'}\n",
      "Val Loss: 2.447461, Val Acc: 87.92%, Val-Class-Acc: {0: '90.22%', 2: '94.44%', 3: '96.31%', 4: '85.00%', 5: '95.82%', 6: '50.00%', 7: '84.80%', 8: '86.62%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 189/200, Train Loss: 0.001561, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '100.00%', 6: '99.77%', 7: '100.00%', 8: '99.84%', 9: '100.00%'}\n",
      "Val Loss: 2.446062, Val Acc: 87.70%, Val-Class-Acc: {0: '88.59%', 2: '95.14%', 3: '96.72%', 4: '90.00%', 5: '95.82%', 6: '46.30%', 7: '81.60%', 8: '87.26%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 190/200, Train Loss: 0.003906, Train-Class-Acc: {0: '100.00%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '99.93%', 6: '99.77%', 7: '100.00%', 8: '100.00%', 9: '99.32%'}\n",
      "Val Loss: 2.611555, Val Acc: 88.06%, Val-Class-Acc: {0: '91.85%', 2: '92.36%', 3: '96.72%', 4: '87.50%', 5: '96.42%', 6: '50.00%', 7: '82.40%', 8: '87.90%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 191/200, Train Loss: 0.001326, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '99.80%', 4: '99.37%', 5: '99.93%', 6: '100.00%', 7: '100.00%', 8: '100.00%', 9: '100.00%'}\n",
      "Val Loss: 2.327406, Val Acc: 88.43%, Val-Class-Acc: {0: '89.67%', 2: '95.14%', 3: '96.31%', 4: '90.00%', 5: '97.01%', 6: '52.78%', 7: '81.60%', 8: '84.71%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 192/200, Train Loss: 0.001866, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '100.00%', 6: '100.00%', 7: '99.80%', 8: '100.00%', 9: '99.32%'}\n",
      "Val Loss: 2.374138, Val Acc: 88.79%, Val-Class-Acc: {0: '90.22%', 2: '95.14%', 3: '96.31%', 4: '90.00%', 5: '97.01%', 6: '52.78%', 7: '79.20%', 8: '87.90%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 193/200, Train Loss: 0.032777, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '99.49%', 4: '100.00%', 5: '99.93%', 6: '99.31%', 7: '99.80%', 8: '99.84%', 9: '98.65%'}\n",
      "Val Loss: 2.170663, Val Acc: 84.50%, Val-Class-Acc: {0: '82.07%', 2: '91.67%', 3: '90.98%', 4: '85.00%', 5: '89.55%', 6: '47.22%', 7: '84.80%', 8: '88.54%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 194/200, Train Loss: 0.076879, Train-Class-Acc: {0: '98.64%', 2: '98.96%', 3: '99.59%', 4: '99.37%', 5: '99.33%', 6: '96.31%', 7: '96.81%', 8: '97.61%', 9: '96.62%'}\n",
      "Val Loss: 2.426994, Val Acc: 85.88%, Val-Class-Acc: {0: '78.80%', 2: '93.75%', 3: '94.67%', 4: '72.50%', 5: '94.33%', 6: '57.41%', 7: '82.40%', 8: '86.62%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 195/200, Train Loss: 0.090256, Train-Class-Acc: {0: '98.77%', 2: '99.48%', 3: '99.08%', 5: '99.25%', 6: '94.70%', 7: '97.41%', 8: '97.93%', 9: '97.97%', 4: '96.84%'}\n",
      "Val Loss: 2.327579, Val Acc: 87.41%, Val-Class-Acc: {0: '88.04%', 2: '94.44%', 3: '95.49%', 4: '87.50%', 5: '94.93%', 6: '55.56%', 7: '83.20%', 8: '86.62%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 196/200, Train Loss: 0.029357, Train-Class-Acc: {0: '99.46%', 2: '99.83%', 3: '99.39%', 4: '98.73%', 5: '99.70%', 6: '96.77%', 7: '98.40%', 8: '98.73%', 9: '97.97%'}\n",
      "Val Loss: 2.051816, Val Acc: 86.54%, Val-Class-Acc: {0: '83.15%', 2: '95.14%', 3: '95.90%', 4: '85.00%', 5: '94.63%', 6: '52.78%', 7: '82.40%', 8: '85.99%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 197/200, Train Loss: 0.020073, Train-Class-Acc: {0: '99.32%', 2: '99.83%', 3: '99.59%', 4: '100.00%', 5: '99.85%', 6: '98.39%', 7: '99.00%', 8: '98.89%', 9: '98.65%'}\n",
      "Val Loss: 2.343650, Val Acc: 86.83%, Val-Class-Acc: {0: '91.30%', 2: '93.75%', 3: '95.08%', 4: '87.50%', 5: '94.93%', 6: '40.74%', 7: '82.40%', 8: '88.54%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 198/200, Train Loss: 0.007573, Train-Class-Acc: {0: '99.86%', 2: '99.65%', 3: '100.00%', 4: '100.00%', 5: '100.00%', 6: '99.08%', 7: '99.20%', 8: '99.84%', 9: '98.65%'}\n",
      "Val Loss: 2.185498, Val Acc: 87.63%, Val-Class-Acc: {0: '85.87%', 2: '93.75%', 3: '95.90%', 4: '87.50%', 5: '94.93%', 6: '55.56%', 7: '82.40%', 8: '86.62%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 199/200, Train Loss: 0.084784, Train-Class-Acc: {0: '100.00%', 2: '98.79%', 3: '99.80%', 4: '100.00%', 5: '99.55%', 6: '98.16%', 7: '98.20%', 8: '99.20%', 9: '98.65%'}\n",
      "Val Loss: 2.757190, Val Acc: 83.99%, Val-Class-Acc: {0: '76.63%', 2: '88.19%', 3: '96.31%', 4: '82.50%', 5: '94.63%', 6: '50.93%', 7: '69.60%', 8: '87.26%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 200/200, Train Loss: 0.162604, Train-Class-Acc: {0: '98.23%', 2: '98.09%', 3: '98.16%', 4: '96.84%', 5: '98.58%', 6: '94.24%', 7: '95.61%', 8: '96.34%', 9: '93.24%'}\n",
      "Val Loss: 2.763292, Val Acc: 85.37%, Val-Class-Acc: {0: '86.41%', 2: '91.67%', 3: '95.90%', 4: '85.00%', 5: '91.94%', 6: '45.37%', 7: '88.80%', 8: '77.71%', 9: '64.86%'}, LR: 0.001000\n",
      "\n",
      "üèÜ Best model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_best.pth (Val Accuracy: 89.08%)\n",
      "\n",
      "üìå Final model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_final.pth\n",
      "\n",
      "üéØ Top 5 Best Models:\n",
      "Epoch 111, Train Loss: 0.016696, Train-Acc: {0: '99.73%', 2: '99.83%', 3: '99.90%', 4: '99.37%', 5: '99.85%', 6: '98.85%', 7: '97.80%', 8: '98.89%', 9: '98.65%'},\n",
      "Val Loss: 1.979686, Val Acc: 89.08%, Val-Acc: {0: '91.85%', 2: '93.75%', 3: '97.95%', 4: '92.50%', 5: '97.01%', 6: '54.63%', 7: '80.80%', 8: '87.26%', 9: '59.46%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_111.pth\n",
      "Epoch 110, Train Loss: 0.017485, Train-Acc: {0: '99.86%', 2: '100.00%', 3: '100.00%', 4: '99.37%', 5: '99.85%', 6: '98.62%', 7: '99.20%', 8: '99.68%', 9: '97.97%'},\n",
      "Val Loss: 2.010381, Val Acc: 88.94%, Val-Acc: {0: '90.76%', 2: '93.06%', 3: '98.36%', 4: '85.00%', 5: '97.31%', 6: '54.63%', 7: '79.20%', 8: '87.90%', 9: '67.57%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_110.pth\n",
      "Epoch 18, Train Loss: 0.138370, Train-Acc: {0: '97.82%', 2: '98.61%', 3: '98.67%', 4: '93.67%', 5: '98.50%', 6: '90.78%', 7: '91.82%', 8: '94.11%', 9: '80.41%'},\n",
      "Val Loss: 1.438689, Val Acc: 88.86%, Val-Acc: {0: '89.67%', 2: '94.44%', 3: '97.54%', 4: '85.00%', 5: '97.01%', 6: '46.30%', 7: '84.00%', 8: '90.45%', 9: '70.27%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_18.pth\n",
      "Epoch 27, Train Loss: 0.045441, Train-Acc: {0: '99.05%', 2: '98.96%', 3: '99.49%', 4: '98.10%', 5: '99.33%', 6: '95.16%', 7: '97.41%', 8: '98.41%', 9: '93.24%'},\n",
      "Val Loss: 1.405677, Val Acc: 88.79%, Val-Acc: {0: '89.67%', 2: '93.06%', 3: '97.13%', 4: '87.50%', 5: '97.31%', 6: '48.15%', 7: '84.00%', 8: '88.54%', 9: '72.97%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_27.pth\n",
      "Epoch 19, Train Loss: 0.097188, Train-Acc: {0: '98.77%', 2: '98.96%', 3: '98.57%', 5: '99.18%', 6: '91.24%', 7: '94.81%', 8: '94.75%', 9: '91.89%', 4: '96.84%'},\n",
      "Val Loss: 1.297302, Val Acc: 88.79%, Val-Acc: {0: '90.22%', 2: '93.06%', 3: '97.54%', 4: '87.50%', 5: '97.61%', 6: '44.44%', 7: '84.80%', 8: '88.54%', 9: '72.97%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/ResNet18_1D_LoRA_epoch_19.pth\n",
      "\n",
      "üß† Model Summary:\n",
      "Total Parameters: 3,895,946\n",
      "Model Size (float32): 14.86 MB\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "Number of LoRA adapters: 8\n",
      "Number of LoRA groups: 1\n",
      "Total Training Time: 748.19 seconds\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "---\n",
      "### Period 4 (alpha = 0.0, similarity_threshold = 0.75)\n",
      "+ ##### Total training time: 748.19 seconds\n",
      "+ ##### Model: ResNet18_1D_LoRA\n",
      "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4'*\n",
      "+ ##### Best Epoch: 111\n",
      "#### __Val Accuracy: 89.08%__\n",
      "#### __Val-Class-Acc: {0: '91.85%', 2: '93.75%', 3: '97.95%', 4: '92.50%', 5: '97.01%', 6: '54.63%', 7: '80.80%', 8: '87.26%', 9: '59.46%'}__\n",
      "#### __Total Parameters: 3,895,946__\n",
      "#### __Model Size (float32): 14.86 MB__\n",
      "#### __Number of LoRA adapters: 8__\n",
      "#### __Number of LoRA groups: 1__\n",
      "\n",
      "Saved class features to: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v5/Period_4/class_features.pkl\n",
      "üßπ Cleaned up all training memory.\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå Period 4: DynEx-CLoRA Training (ECG)\n",
    "# ================================\n",
    "period = 4\n",
    "\n",
    "# ==== Paths ====\n",
    "stop_signal_file = os.path.join(BASE_DIR, \"stop_training.txt\")\n",
    "model_saving_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v5\", f\"Period_{period}\")\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# ==== Load Period 4 Data ====\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{period}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{period}.npy\"))\n",
    "X_val   = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val   = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "# ==== Device ====\n",
    "device = auto_select_cuda_device()\n",
    "\n",
    "# ==== Load Period 3 Features ====\n",
    "prev_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v3\", \"Period_3\")\n",
    "class_features_path = os.path.join(prev_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, \"rb\") as f:\n",
    "    class_features_dict = pickle.load(f)\n",
    "print(f\"‚úÖ Loaded class features from: {class_features_path}\")\n",
    "\n",
    "# ==== Load Period 3 Checkpoint ====\n",
    "prev_model_path = os.path.join(prev_folder, \"ResNet18_1D_LoRA_best.pth\")\n",
    "checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "\n",
    "input_channels = X_train.shape[2]\n",
    "output_size = int(np.max(y_train)) + 1  # e.g., max=9 ‚Üí output_size=10\n",
    "\n",
    "# ==== Build Teacher Model ====\n",
    "teacher_output_size = len(np.unique(np.load(os.path.join(save_dir, f\"y_train_p{period-1}.npy\"))))\n",
    "teacher_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=teacher_output_size).to(device)\n",
    "teacher_model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "teacher_model.eval()\n",
    "\n",
    "# ==== Build Student Model ====\n",
    "model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size).to(device)\n",
    "\n",
    "# ==== Sync Adapter Structure ====\n",
    "num_lora_groups = checkpoint.get(\"num_lora_groups\", 0)\n",
    "print(f\"üîÑ Number of LoRA groups: {num_lora_groups}\")\n",
    "related_labels = checkpoint.get(\"related_labels\", {\"base\": [0, 1]})\n",
    "for _ in range(num_lora_groups):\n",
    "    model.add_lora_adapter()\n",
    "\n",
    "# ==== Load Shared Weights (excluding FC) ====\n",
    "model_dict = model.state_dict()\n",
    "prev_state_dict = checkpoint[\"model_state_dict\"]\n",
    "filtered_dict = {\n",
    "    k: v for k, v in prev_state_dict.items()\n",
    "    if k in model_dict and model_dict[k].shape == v.shape and k not in [\"fc.weight\", \"fc.bias\"]\n",
    "}\n",
    "model.load_state_dict(filtered_dict, strict=False)\n",
    "for k in model_dict:\n",
    "    if k not in filtered_dict:\n",
    "        print(f\"üîç Not loaded: {k}, shape={model_dict[k].shape}\")\n",
    "print(\"‚úÖ Loaded shared weights from Period 3 (excluding FC only)\")\n",
    "\n",
    "# ==== Training Configuration ====\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "alpha = 0.0 # No distillation\n",
    "similarity_threshold = 0.75\n",
    "stable_classes = [0, 2, 3, 4, 5]  # ‚Üê Ë´ãÊ†πÊìö‰Ω†ÂâçÊúüÈ°ûÂà•Ë™øÊï¥ÔºÅ\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# ==== Start Training ====\n",
    "train_with_dynex_clora_ecg(\n",
    "    model=model,\n",
    "    teacher_model=None,\n",
    "    output_size=output_size,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    alpha=alpha,\n",
    "    model_saving_folder=model_saving_folder,\n",
    "    model_name=\"ResNet18_1D_LoRA\",\n",
    "    stop_signal_file=stop_signal_file,\n",
    "    scheduler=scheduler,\n",
    "    period=period,\n",
    "    stable_classes=stable_classes,\n",
    "    similarity_threshold=similarity_threshold,\n",
    "    class_features_dict=class_features_dict,\n",
    "    related_labels=related_labels,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ==== Cleanup ====\n",
    "del X_train, y_train, X_val, y_val\n",
    "del model, teacher_model, checkpoint, model_dict, prev_state_dict, filtered_dict\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea461a4",
   "metadata": {},
   "source": [
    "#### v6 no distillation, all freeze, th = 0.79\n",
    "##### Period 4 (alpha = 0.0, similarity_threshold = 0.79)\n",
    "+ ##### Total training time: 556.05 seconds\n",
    "+ ##### Model: ResNet18_1D_LoRA\n",
    "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4'*\n",
    "+ ##### Best Epoch: 19\n",
    "##### __Val Accuracy: 89.23%__\n",
    "##### __Val-Class-Acc: {0: '89.67%', 2: '94.44%', 3: '97.95%', 4: '82.50%', 5: '97.91%', 6: '50.00%', 7: '87.20%', 8: '90.45%', 9: '54.05%'}__\n",
    "##### __Total Parameters: 3,895,946__\n",
    "##### __Model Size (float32): 14.86 MB__\n",
    "##### __Number of LoRA adapters: 8__\n",
    "##### __Number of LoRA groups: 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04dd9ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 0\n",
      "    - Memory Used    : 389 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ Loaded class features from: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v3/Period_3/class_features.pkl\n",
      "üîÑ Number of LoRA groups: 1\n",
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1775968/3810861107.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Not loaded: fc.weight, shape=torch.Size([10, 1024])\n",
      "üîç Not loaded: fc.bias, shape=torch.Size([10])\n",
      "‚úÖ Loaded shared weights from Period 3 (excluding FC only)\n",
      "\n",
      "üöÄ 'train_with_dynex_clora_ecg' started for Period 4\n",
      "\n",
      "‚úÖ Removed existing folder: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4\n",
      "\n",
      "‚úÖ Data Overview:\n",
      "X_train: torch.Size([5493, 5000, 12]), y_train: torch.Size([5493])\n",
      "X_val: torch.Size([1374, 5000, 12]), y_val: torch.Size([1374])\n",
      "\n",
      "üîé Similarity Analysis:\n",
      "  Similarity threshold: 0.7900\n",
      "  Existing classes: [0, 1, 2, 3, 4, 5]\n",
      "  Current classes: [0, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  New classes: [6, 7, 8, 9]\n",
      "\n",
      "üìä Similarity Scores:\n",
      "  New Class 6:\n",
      "    - Existing Class 1: 0.9078\n",
      "    - Existing Class 4: 0.9039\n",
      "    - Existing Class 0: 0.8786\n",
      "    - Existing Class 2: 0.8730\n",
      "    - Existing Class 3: 0.8699\n",
      "    - Existing Class 5: 0.8550\n",
      "  New Class 7:\n",
      "    - Existing Class 1: 0.9061\n",
      "    - Existing Class 4: 0.9041\n",
      "    - Existing Class 0: 0.8745\n",
      "    - Existing Class 2: 0.8719\n",
      "    - Existing Class 3: 0.8712\n",
      "    - Existing Class 5: 0.8631\n",
      "  New Class 8:\n",
      "    - Existing Class 1: 0.8973\n",
      "    - Existing Class 4: 0.8882\n",
      "    - Existing Class 0: 0.8746\n",
      "    - Existing Class 2: 0.8499\n",
      "    - Existing Class 3: 0.8435\n",
      "    - Existing Class 5: 0.8316\n",
      "  New Class 9:\n",
      "    - Existing Class 1: 0.8995\n",
      "    - Existing Class 4: 0.8924\n",
      "    - Existing Class 0: 0.8799\n",
      "    - Existing Class 2: 0.8592\n",
      "    - Existing Class 3: 0.8532\n",
      "    - Existing Class 5: 0.8383\n",
      "\n",
      "  Average similarity: 0.8682, Std: 0.0262\n",
      "\n",
      "üß© Managing LoRA adapters for 4 new classes...\n",
      "üîÑ New Class 8 is similar to Class 1 (sim=0.8973) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 8 is similar to Class 2 (sim=0.8499) ‚Üí Added to adapter '0'\n",
      "üîÑ New Class 9 is similar to Class 1 (sim=0.8995) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 9 is similar to Class 2 (sim=0.8592) ‚Üí Added to adapter '0'\n",
      "üîÑ New Class 6 is similar to Class 1 (sim=0.9078) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 6 is similar to Class 2 (sim=0.8730) ‚Üí Added to adapter '0'\n",
      "üîÑ New Class 7 is similar to Class 1 (sim=0.9061) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 7 is similar to Class 2 (sim=0.8719) ‚Üí Added to adapter '0'\n",
      "\n",
      "üîç Checking stability of existing classes...\n",
      "  Class 0 similarity with itself: 0.8814\n",
      "  Class 2 similarity with itself: 0.8816\n",
      "  Class 3 similarity with itself: 0.8985\n",
      "  Class 4 similarity with itself: 0.8817\n",
      "  Class 5 similarity with itself: 0.9125\n",
      "\n",
      "üîí Default: Freezing ALL model parameters\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå fc.weight                                          | shape=[10, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[10]\n",
      "\n",
      "üîì Unfreezing selected adapters (by group):\n",
      "  - Adapter Group #0 (all blocks) (classes: [2, 4, 5, 8, 9, 6, 7])\n",
      "  - Base layers (classes: [0, 1, 3, 4, 5, 8, 9, 6, 7])\n",
      "\n",
      "üîç Frozen Parameters: (After Unfreeze specific adapter groups)\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå fc.weight                                          | shape=[10, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[10]\n",
      "\n",
      "üîç Frozen Parameters (After Unfreeze newly added adapter groups):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå fc.weight                                          | shape=[10, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[10]\n",
      "\n",
      "üîç Frozen Parameters (After unfreeze the final layer):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "\n",
      "üìã Related Labels Summary:\n",
      "  - Base network: Classes [0, 1, 3, 4, 5, 8, 9, 6, 7]\n",
      "  - Adapter #0: Classes [2, 4, 5, 8, 9, 6, 7]\n",
      "\n",
      "üîß Trainable Parameter Status:\n",
      "  ‚úÖ layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ fc.weight                                          | shape=[10, 1024]\n",
      "  ‚úÖ fc.bias                                            | shape=[10]\n",
      "trainable_count: 26\n",
      "\n",
      "üìä Parameter Statistics:\n",
      "  - Total parameters: 3,895,946\n",
      "  - Trainable parameters: 2,129,930 (54.67%)\n",
      "  - Frozen parameters: 1,766,016 (45.33%)\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "\n",
      "üß† Parameters currently controlled by the optimizer: (78)\n",
      "  ‚õî conv1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer1.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.0.conv2.weight\n",
      "  ‚õî layer1.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer1.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.conv2.weight\n",
      "  ‚õî layer1.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer2.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.conv2.weight\n",
      "  ‚õî layer2.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer2.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.conv2.weight\n",
      "  ‚õî layer2.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer3.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.conv2.weight\n",
      "  ‚õî layer3.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer3.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.conv2.weight\n",
      "  ‚õî layer3.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer4.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.conv2.weight\n",
      "  ‚õî layer4.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer4.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.conv2.weight\n",
      "  ‚õî layer4.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ fc.weight\n",
      "  ‚úÖ fc.bias\n",
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/200, Train Loss: 6.232568, Train-Class-Acc: {0: '56.68%', 2: '50.26%', 3: '67.01%', 4: '31.65%', 5: '77.86%', 6: '13.36%', 7: '35.93%', 8: '40.29%', 9: '10.81%'}\n",
      "Val Loss: 1.327000, Val Acc: 81.66%, Val-Class-Acc: {0: '80.98%', 2: '91.67%', 3: '96.72%', 4: '62.50%', 5: '95.22%', 6: '32.41%', 7: '73.60%', 8: '81.53%', 9: '16.22%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_1.pth\n",
      "Epoch 2/200, Train Loss: 1.757406, Train-Class-Acc: {0: '79.02%', 2: '82.84%', 3: '89.55%', 4: '67.72%', 5: '89.60%', 6: '31.34%', 7: '57.49%', 8: '63.54%', 9: '31.08%'}\n",
      "Val Loss: 1.548627, Val Acc: 85.15%, Val-Class-Acc: {0: '89.13%', 2: '93.75%', 3: '98.77%', 4: '77.50%', 5: '94.93%', 6: '30.56%', 7: '76.00%', 8: '87.90%', 9: '40.54%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_2.pth\n",
      "Epoch 3/200, Train Loss: 1.294685, Train-Class-Acc: {0: '85.69%', 2: '87.35%', 3: '91.91%', 4: '77.85%', 5: '92.30%', 6: '43.09%', 7: '63.67%', 8: '71.02%', 9: '37.84%'}\n",
      "Val Loss: 1.384688, Val Acc: 85.37%, Val-Class-Acc: {0: '89.67%', 2: '90.28%', 3: '98.36%', 4: '80.00%', 5: '96.12%', 6: '41.67%', 7: '80.00%', 8: '77.71%', 9: '45.95%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_3.pth\n",
      "Epoch 4/200, Train Loss: 1.019353, Train-Class-Acc: {0: '87.06%', 2: '91.33%', 3: '93.44%', 5: '94.02%', 6: '49.08%', 7: '68.46%', 8: '73.73%', 9: '52.03%', 4: '82.28%'}\n",
      "Val Loss: 1.229999, Val Acc: 87.48%, Val-Class-Acc: {0: '92.39%', 2: '92.36%', 3: '98.77%', 4: '85.00%', 5: '97.01%', 6: '49.07%', 7: '81.60%', 8: '78.98%', 9: '54.05%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_4.pth\n",
      "Epoch 5/200, Train Loss: 0.736957, Train-Class-Acc: {0: '91.14%', 2: '94.11%', 3: '96.00%', 5: '95.14%', 6: '58.06%', 7: '75.05%', 8: '77.87%', 9: '54.05%', 4: '84.18%'}\n",
      "Val Loss: 1.219947, Val Acc: 87.34%, Val-Class-Acc: {0: '84.78%', 2: '95.14%', 3: '97.95%', 4: '85.00%', 5: '97.31%', 6: '47.22%', 7: '80.00%', 8: '87.26%', 9: '54.05%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_5.pth\n",
      "Epoch 6/200, Train Loss: 0.570645, Train-Class-Acc: {0: '93.05%', 2: '93.07%', 3: '97.13%', 4: '89.24%', 5: '96.78%', 6: '64.75%', 7: '76.85%', 8: '81.69%', 9: '64.86%'}\n",
      "Val Loss: 1.085198, Val Acc: 88.06%, Val-Class-Acc: {0: '90.76%', 2: '93.75%', 3: '97.13%', 4: '87.50%', 5: '95.82%', 6: '51.85%', 7: '80.80%', 8: '87.90%', 9: '54.05%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_1.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_6.pth\n",
      "Epoch 7/200, Train Loss: 0.471184, Train-Class-Acc: {0: '93.60%', 2: '96.19%', 3: '97.13%', 4: '91.14%', 5: '96.11%', 6: '66.59%', 7: '78.44%', 8: '84.24%', 9: '70.95%'}\n",
      "Val Loss: 1.096408, Val Acc: 87.63%, Val-Class-Acc: {0: '91.30%', 2: '94.44%', 3: '97.54%', 4: '85.00%', 5: '97.31%', 6: '44.44%', 7: '88.00%', 8: '78.34%', 9: '56.76%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_2.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_7.pth\n",
      "Epoch 8/200, Train Loss: 0.347046, Train-Class-Acc: {0: '94.28%', 2: '95.49%', 3: '96.72%', 4: '89.87%', 5: '97.31%', 6: '69.59%', 7: '81.24%', 8: '86.78%', 9: '70.95%'}\n",
      "Val Loss: 1.116794, Val Acc: 89.01%, Val-Class-Acc: {0: '92.39%', 2: '95.14%', 3: '97.54%', 4: '87.50%', 5: '97.31%', 6: '52.78%', 7: '84.80%', 8: '85.35%', 9: '54.05%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_3.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_8.pth\n",
      "Epoch 9/200, Train Loss: 0.335397, Train-Class-Acc: {0: '95.91%', 2: '96.71%', 3: '97.03%', 5: '97.46%', 6: '73.96%', 7: '82.04%', 8: '85.99%', 9: '76.35%', 4: '91.14%'}\n",
      "Val Loss: 1.317959, Val Acc: 88.43%, Val-Class-Acc: {0: '90.76%', 2: '95.14%', 3: '98.36%', 4: '87.50%', 5: '97.31%', 6: '49.07%', 7: '81.60%', 8: '85.99%', 9: '54.05%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_5.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_9.pth\n",
      "Epoch 10/200, Train Loss: 0.339963, Train-Class-Acc: {0: '94.96%', 2: '97.23%', 3: '98.67%', 4: '92.41%', 5: '97.46%', 6: '77.19%', 7: '84.23%', 8: '88.54%', 9: '75.00%'}\n",
      "Val Loss: 1.440209, Val Acc: 88.28%, Val-Class-Acc: {0: '94.02%', 2: '93.06%', 3: '98.36%', 4: '85.00%', 5: '97.31%', 6: '50.93%', 7: '86.40%', 8: '77.71%', 9: '56.76%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_4.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_10.pth\n",
      "Epoch 11/200, Train Loss: 0.352918, Train-Class-Acc: {0: '96.05%', 2: '97.05%', 3: '98.26%', 4: '93.04%', 5: '97.83%', 6: '78.80%', 7: '84.03%', 8: '87.26%', 9: '83.78%'}\n",
      "Val Loss: 1.392777, Val Acc: 89.23%, Val-Class-Acc: {0: '93.48%', 2: '95.14%', 3: '96.72%', 4: '82.50%', 5: '98.81%', 6: '54.63%', 7: '80.00%', 8: '87.26%', 9: '56.76%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_7.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_11.pth\n",
      "Epoch 12/200, Train Loss: 0.344430, Train-Class-Acc: {0: '96.32%', 2: '97.05%', 3: '97.44%', 4: '90.51%', 5: '97.91%', 6: '78.80%', 7: '86.03%', 8: '89.97%', 9: '77.70%'}\n",
      "Val Loss: 1.161812, Val Acc: 88.79%, Val-Class-Acc: {0: '89.13%', 2: '93.75%', 3: '96.72%', 4: '90.00%', 5: '97.31%', 6: '55.56%', 7: '83.20%', 8: '85.99%', 9: '64.86%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_6.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_12.pth\n",
      "Epoch 13/200, Train Loss: 0.251760, Train-Class-Acc: {0: '97.28%', 2: '97.57%', 3: '97.95%', 4: '93.04%', 5: '98.13%', 6: '83.87%', 7: '87.43%', 8: '91.24%', 9: '86.49%'}\n",
      "Val Loss: 0.881353, Val Acc: 88.28%, Val-Class-Acc: {0: '89.13%', 2: '96.53%', 3: '97.54%', 4: '90.00%', 5: '94.63%', 6: '50.93%', 7: '86.40%', 8: '84.08%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 14/200, Train Loss: 0.275570, Train-Class-Acc: {0: '97.14%', 2: '97.05%', 3: '97.44%', 4: '91.77%', 5: '97.68%', 6: '80.18%', 7: '89.42%', 8: '90.13%', 9: '83.11%'}\n",
      "Val Loss: 1.323004, Val Acc: 88.57%, Val-Class-Acc: {0: '89.67%', 2: '92.36%', 3: '98.36%', 4: '90.00%', 5: '98.21%', 6: '46.30%', 7: '84.00%', 8: '88.54%', 9: '54.05%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_10.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_14.pth\n",
      "Epoch 15/200, Train Loss: 0.164465, Train-Class-Acc: {0: '98.50%', 2: '97.75%', 3: '98.77%', 4: '99.37%', 5: '98.35%', 6: '88.25%', 7: '91.82%', 8: '93.31%', 9: '87.16%'}\n",
      "Val Loss: 1.563445, Val Acc: 88.21%, Val-Class-Acc: {0: '92.93%', 2: '91.67%', 3: '98.36%', 4: '85.00%', 5: '99.40%', 6: '44.44%', 7: '83.20%', 8: '82.17%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 16/200, Train Loss: 0.116396, Train-Class-Acc: {0: '98.37%', 2: '97.23%', 3: '98.57%', 4: '95.57%', 5: '98.88%', 6: '86.18%', 7: '92.42%', 8: '94.43%', 9: '89.19%'}\n",
      "Val Loss: 1.376984, Val Acc: 88.94%, Val-Class-Acc: {0: '89.67%', 2: '95.14%', 3: '98.36%', 4: '85.00%', 5: '98.51%', 6: '51.85%', 7: '81.60%', 8: '87.90%', 9: '54.05%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_9.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_16.pth\n",
      "Epoch 17/200, Train Loss: 0.106213, Train-Class-Acc: {0: '98.09%', 2: '98.79%', 3: '99.39%', 4: '96.84%', 5: '99.25%', 6: '91.01%', 7: '93.01%', 8: '94.43%', 9: '91.22%'}\n",
      "Val Loss: 1.521074, Val Acc: 88.72%, Val-Class-Acc: {0: '92.93%', 2: '94.44%', 3: '98.36%', 4: '90.00%', 5: '97.61%', 6: '53.70%', 7: '77.60%', 8: '83.44%', 9: '62.16%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_14.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_17.pth\n",
      "Epoch 18/200, Train Loss: 0.157745, Train-Class-Acc: {0: '97.96%', 2: '98.09%', 3: '99.18%', 4: '94.30%', 5: '98.73%', 6: '87.79%', 7: '92.42%', 8: '93.63%', 9: '86.49%'}\n",
      "Val Loss: 1.576513, Val Acc: 88.06%, Val-Class-Acc: {0: '92.39%', 2: '92.36%', 3: '97.13%', 4: '90.00%', 5: '96.12%', 6: '52.78%', 7: '82.40%', 8: '85.35%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 19/200, Train Loss: 0.229003, Train-Class-Acc: {0: '96.73%', 2: '98.44%', 3: '97.44%', 4: '95.57%', 5: '98.43%', 6: '86.41%', 7: '92.02%', 8: '93.47%', 9: '86.49%'}\n",
      "Val Loss: 1.519432, Val Acc: 89.23%, Val-Class-Acc: {0: '89.67%', 2: '94.44%', 3: '97.95%', 4: '82.50%', 5: '97.91%', 6: '50.00%', 7: '87.20%', 8: '90.45%', 9: '54.05%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_17.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_19.pth\n",
      "Epoch 20/200, Train Loss: 0.112972, Train-Class-Acc: {0: '97.14%', 2: '98.61%', 3: '98.87%', 4: '95.57%', 5: '99.03%', 6: '91.47%', 7: '92.61%', 8: '93.79%', 9: '89.86%'}\n",
      "Val Loss: 1.559689, Val Acc: 88.72%, Val-Class-Acc: {0: '89.13%', 2: '93.75%', 3: '98.77%', 4: '82.50%', 5: '97.91%', 6: '52.78%', 7: '83.20%', 8: '88.54%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 21/200, Train Loss: 0.098597, Train-Class-Acc: {0: '98.91%', 2: '98.61%', 3: '99.18%', 4: '98.10%', 5: '99.25%', 6: '91.47%', 7: '94.01%', 8: '96.82%', 9: '88.51%'}\n",
      "Val Loss: 1.159460, Val Acc: 88.57%, Val-Class-Acc: {0: '91.30%', 2: '95.14%', 3: '96.72%', 4: '87.50%', 5: '96.72%', 6: '52.78%', 7: '81.60%', 8: '87.26%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 22/200, Train Loss: 0.088843, Train-Class-Acc: {0: '98.37%', 2: '98.79%', 3: '99.28%', 4: '98.10%', 5: '99.03%', 6: '91.47%', 7: '95.61%', 8: '96.34%', 9: '93.24%'}\n",
      "Val Loss: 1.324273, Val Acc: 88.50%, Val-Class-Acc: {0: '89.67%', 2: '95.83%', 3: '95.90%', 4: '87.50%', 5: '97.01%', 6: '50.93%', 7: '82.40%', 8: '87.26%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 23/200, Train Loss: 0.104239, Train-Class-Acc: {0: '99.05%', 2: '99.13%', 3: '99.18%', 4: '97.47%', 5: '99.40%', 6: '92.17%', 7: '93.21%', 8: '96.66%', 9: '91.89%'}\n",
      "Val Loss: 1.244387, Val Acc: 88.79%, Val-Class-Acc: {0: '91.85%', 2: '93.75%', 3: '97.13%', 4: '90.00%', 5: '97.91%', 6: '55.56%', 7: '87.20%', 8: '79.62%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 24/200, Train Loss: 0.086484, Train-Class-Acc: {0: '98.91%', 2: '98.79%', 3: '99.49%', 4: '96.84%', 5: '98.95%', 6: '95.16%', 7: '94.81%', 8: '95.70%', 9: '93.92%'}\n",
      "Val Loss: 1.474003, Val Acc: 88.57%, Val-Class-Acc: {0: '94.02%', 2: '93.06%', 3: '98.77%', 4: '85.00%', 5: '97.61%', 6: '48.15%', 7: '82.40%', 8: '80.89%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 25/200, Train Loss: 0.041318, Train-Class-Acc: {0: '99.73%', 2: '99.31%', 3: '99.59%', 4: '97.47%', 5: '99.70%', 6: '95.39%', 7: '96.21%', 8: '97.13%', 9: '97.30%'}\n",
      "Val Loss: 1.608286, Val Acc: 89.16%, Val-Class-Acc: {0: '88.59%', 2: '93.06%', 3: '97.54%', 4: '90.00%', 5: '97.61%', 6: '50.00%', 7: '87.20%', 8: '90.45%', 9: '59.46%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_12.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_25.pth\n",
      "Epoch 26/200, Train Loss: 0.055336, Train-Class-Acc: {0: '99.73%', 2: '98.79%', 3: '99.39%', 4: '98.73%', 5: '99.33%', 6: '93.09%', 7: '95.81%', 8: '97.77%', 9: '95.27%'}\n",
      "Val Loss: 1.589037, Val Acc: 88.28%, Val-Class-Acc: {0: '83.70%', 2: '91.67%', 3: '98.36%', 4: '77.50%', 5: '98.21%', 6: '54.63%', 7: '84.80%', 8: '87.90%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 27/200, Train Loss: 0.166492, Train-Class-Acc: {0: '97.96%', 2: '98.61%', 3: '98.87%', 4: '95.57%', 5: '98.65%', 6: '90.32%', 7: '93.01%', 8: '94.27%', 9: '93.24%'}\n",
      "Val Loss: 1.138493, Val Acc: 87.19%, Val-Class-Acc: {0: '89.13%', 2: '88.19%', 3: '98.36%', 4: '87.50%', 5: '95.22%', 6: '49.07%', 7: '86.40%', 8: '82.80%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 28/200, Train Loss: 0.150319, Train-Class-Acc: {0: '98.23%', 2: '98.27%', 3: '99.08%', 4: '94.94%', 5: '98.20%', 6: '90.78%', 7: '93.61%', 8: '95.38%', 9: '89.19%'}\n",
      "Val Loss: 1.096856, Val Acc: 88.14%, Val-Class-Acc: {0: '89.67%', 2: '93.75%', 3: '97.95%', 4: '87.50%', 5: '96.72%', 6: '47.22%', 7: '84.80%', 8: '86.62%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 29/200, Train Loss: 0.122177, Train-Class-Acc: {0: '98.64%', 2: '98.44%', 3: '98.77%', 4: '98.10%', 5: '99.03%', 6: '91.47%', 7: '93.81%', 8: '97.13%', 9: '94.59%'}\n",
      "Val Loss: 1.482198, Val Acc: 88.57%, Val-Class-Acc: {0: '92.93%', 2: '93.75%', 3: '99.18%', 4: '90.00%', 5: '97.91%', 6: '50.93%', 7: '80.80%', 8: '81.53%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 30/200, Train Loss: 0.111264, Train-Class-Acc: {0: '98.64%', 2: '98.61%', 3: '99.49%', 4: '98.10%', 5: '99.03%', 6: '93.55%', 7: '96.61%', 8: '95.70%', 9: '95.95%'}\n",
      "Val Loss: 1.797620, Val Acc: 88.86%, Val-Class-Acc: {0: '90.76%', 2: '95.14%', 3: '97.13%', 4: '90.00%', 5: '97.91%', 6: '49.07%', 7: '83.20%', 8: '85.99%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 31/200, Train Loss: 0.158400, Train-Class-Acc: {0: '98.09%', 2: '97.23%', 3: '98.98%', 4: '98.10%', 5: '98.65%', 6: '91.94%', 7: '95.61%', 8: '96.18%', 9: '92.57%'}\n",
      "Val Loss: 1.223706, Val Acc: 87.99%, Val-Class-Acc: {0: '91.85%', 2: '95.14%', 3: '98.36%', 4: '85.00%', 5: '96.12%', 6: '44.44%', 7: '88.00%', 8: '82.17%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 32/200, Train Loss: 0.287473, Train-Class-Acc: {0: '97.14%', 2: '99.48%', 3: '99.08%', 4: '94.94%', 5: '98.50%', 6: '93.32%', 7: '92.02%', 8: '96.18%', 9: '89.19%'}\n",
      "Val Loss: 1.746719, Val Acc: 87.12%, Val-Class-Acc: {0: '85.87%', 2: '94.44%', 3: '98.36%', 4: '80.00%', 5: '93.73%', 6: '46.30%', 7: '88.80%', 8: '85.99%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 33/200, Train Loss: 0.149764, Train-Class-Acc: {0: '98.50%', 2: '97.57%', 3: '99.08%', 4: '97.47%', 5: '98.35%', 6: '89.86%', 7: '93.61%', 8: '95.38%', 9: '91.89%'}\n",
      "Val Loss: 1.437819, Val Acc: 87.70%, Val-Class-Acc: {0: '89.67%', 2: '93.75%', 3: '96.72%', 4: '82.50%', 5: '96.42%', 6: '48.15%', 7: '80.80%', 8: '87.26%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 34/200, Train Loss: 0.076532, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '98.87%', 4: '96.84%', 5: '99.33%', 6: '93.09%', 7: '95.61%', 8: '96.18%', 9: '96.62%'}\n",
      "Val Loss: 1.493503, Val Acc: 87.70%, Val-Class-Acc: {0: '85.87%', 2: '94.44%', 3: '97.13%', 4: '87.50%', 5: '97.01%', 6: '45.37%', 7: '84.00%', 8: '85.35%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 35/200, Train Loss: 0.061557, Train-Class-Acc: {0: '99.46%', 2: '98.79%', 3: '99.59%', 4: '97.47%', 5: '99.40%', 6: '95.85%', 7: '97.01%', 8: '98.25%', 9: '94.59%'}\n",
      "Val Loss: 1.636806, Val Acc: 87.05%, Val-Class-Acc: {0: '84.78%', 2: '92.36%', 3: '97.54%', 4: '85.00%', 5: '95.52%', 6: '50.00%', 7: '83.20%', 8: '84.71%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 36/200, Train Loss: 0.073044, Train-Class-Acc: {0: '98.50%', 2: '98.96%', 3: '99.39%', 4: '98.73%', 5: '99.33%', 6: '95.62%', 7: '96.01%', 8: '97.61%', 9: '94.59%'}\n",
      "Val Loss: 2.250338, Val Acc: 87.77%, Val-Class-Acc: {0: '90.76%', 2: '95.14%', 3: '99.18%', 4: '82.50%', 5: '97.01%', 6: '33.33%', 7: '87.20%', 8: '84.08%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 37/200, Train Loss: 0.202497, Train-Class-Acc: {0: '99.05%', 2: '98.61%', 3: '98.77%', 4: '96.84%', 5: '99.03%', 6: '93.55%', 7: '95.21%', 8: '96.50%', 9: '93.92%'}\n",
      "Val Loss: 1.512111, Val Acc: 88.50%, Val-Class-Acc: {0: '91.30%', 2: '95.14%', 3: '97.54%', 4: '85.00%', 5: '97.31%', 6: '50.93%', 7: '85.60%', 8: '82.17%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 38/200, Train Loss: 0.114059, Train-Class-Acc: {0: '98.64%', 2: '98.79%', 3: '99.08%', 4: '96.84%', 5: '98.73%', 6: '94.24%', 7: '95.21%', 8: '96.34%', 9: '94.59%'}\n",
      "Val Loss: 1.563600, Val Acc: 88.57%, Val-Class-Acc: {0: '89.13%', 2: '93.06%', 3: '97.54%', 4: '85.00%', 5: '97.61%', 6: '51.85%', 7: '84.80%', 8: '87.26%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 39/200, Train Loss: 0.075659, Train-Class-Acc: {0: '99.05%', 2: '98.79%', 3: '99.18%', 4: '98.10%', 5: '99.48%', 6: '93.55%', 7: '96.41%', 8: '96.66%', 9: '95.27%'}\n",
      "Val Loss: 1.370765, Val Acc: 88.57%, Val-Class-Acc: {0: '88.04%', 2: '94.44%', 3: '97.54%', 4: '85.00%', 5: '97.01%', 6: '60.19%', 7: '81.60%', 8: '83.44%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 40/200, Train Loss: 0.044703, Train-Class-Acc: {0: '99.05%', 2: '99.48%', 3: '99.59%', 4: '99.37%', 5: '99.55%', 6: '96.77%', 7: '96.81%', 8: '97.77%', 9: '95.95%'}\n",
      "Val Loss: 1.717600, Val Acc: 88.50%, Val-Class-Acc: {0: '89.67%', 2: '94.44%', 3: '97.95%', 4: '85.00%', 5: '97.01%', 6: '49.07%', 7: '84.00%', 8: '87.26%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 41/200, Train Loss: 0.134460, Train-Class-Acc: {0: '98.91%', 2: '99.83%', 3: '99.08%', 4: '100.00%', 5: '99.40%', 6: '96.08%', 7: '98.20%', 8: '98.57%', 9: '98.65%'}\n",
      "Val Loss: 2.895579, Val Acc: 86.03%, Val-Class-Acc: {0: '92.39%', 2: '94.44%', 3: '87.30%', 4: '87.50%', 5: '98.51%', 6: '41.67%', 7: '82.40%', 8: '82.80%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 42/200, Train Loss: 0.253419, Train-Class-Acc: {0: '97.55%', 2: '98.61%', 3: '98.26%', 4: '94.30%', 5: '98.06%', 6: '90.78%', 7: '92.81%', 8: '94.11%', 9: '87.84%'}\n",
      "Val Loss: 1.642409, Val Acc: 87.26%, Val-Class-Acc: {0: '90.76%', 2: '92.36%', 3: '94.67%', 4: '77.50%', 5: '95.82%', 6: '44.44%', 7: '89.60%', 8: '82.80%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 43/200, Train Loss: 0.110897, Train-Class-Acc: {0: '99.05%', 2: '98.61%', 3: '99.28%', 4: '98.73%', 5: '99.18%', 6: '93.32%', 7: '95.81%', 8: '97.29%', 9: '91.89%'}\n",
      "Val Loss: 2.380785, Val Acc: 87.70%, Val-Class-Acc: {0: '89.67%', 2: '92.36%', 3: '96.31%', 4: '90.00%', 5: '98.51%', 6: '43.52%', 7: '79.20%', 8: '85.99%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 44/200, Train Loss: 0.132634, Train-Class-Acc: {0: '98.23%', 2: '98.61%', 3: '98.05%', 4: '98.73%', 5: '99.25%', 6: '93.09%', 7: '95.01%', 8: '95.70%', 9: '91.22%'}\n",
      "Val Loss: 2.347328, Val Acc: 86.39%, Val-Class-Acc: {0: '83.70%', 2: '95.83%', 3: '98.36%', 4: '85.00%', 5: '95.52%', 6: '34.26%', 7: '82.40%', 8: '89.81%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 45/200, Train Loss: 0.123985, Train-Class-Acc: {0: '97.96%', 2: '97.92%', 3: '99.18%', 4: '96.84%', 5: '98.65%', 6: '91.71%', 7: '95.61%', 8: '96.18%', 9: '89.86%'}\n",
      "Val Loss: 2.093594, Val Acc: 86.83%, Val-Class-Acc: {0: '90.76%', 2: '86.11%', 3: '97.95%', 4: '75.00%', 5: '95.22%', 6: '42.59%', 7: '84.80%', 8: '89.81%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 46/200, Train Loss: 0.098406, Train-Class-Acc: {0: '98.50%', 2: '98.09%', 3: '99.08%', 4: '98.73%', 5: '99.55%', 6: '94.93%', 7: '95.81%', 8: '97.45%', 9: '95.27%'}\n",
      "Val Loss: 1.773765, Val Acc: 86.97%, Val-Class-Acc: {0: '94.57%', 2: '95.14%', 3: '96.72%', 4: '82.50%', 5: '91.34%', 6: '47.22%', 7: '80.80%', 8: '84.08%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 47/200, Train Loss: 0.076689, Train-Class-Acc: {0: '99.46%', 2: '99.13%', 3: '99.28%', 4: '98.73%', 5: '99.33%', 6: '94.93%', 7: '95.81%', 8: '96.66%', 9: '95.27%'}\n",
      "Val Loss: 2.395328, Val Acc: 87.99%, Val-Class-Acc: {0: '91.85%', 2: '93.06%', 3: '95.90%', 4: '82.50%', 5: '97.31%', 6: '42.59%', 7: '86.40%', 8: '84.71%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 48/200, Train Loss: 0.166693, Train-Class-Acc: {0: '98.09%', 2: '98.79%', 3: '98.87%', 5: '98.95%', 6: '94.24%', 7: '96.01%', 8: '96.50%', 9: '93.24%', 4: '98.73%'}\n",
      "Val Loss: 1.911719, Val Acc: 85.81%, Val-Class-Acc: {0: '92.93%', 2: '92.36%', 3: '99.18%', 4: '77.50%', 5: '89.55%', 6: '53.70%', 7: '84.00%', 8: '76.43%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 49/200, Train Loss: 0.370097, Train-Class-Acc: {0: '96.87%', 2: '96.53%', 3: '96.31%', 4: '91.77%', 5: '96.41%', 6: '84.56%', 7: '90.02%', 8: '91.08%', 9: '85.14%'}\n",
      "Val Loss: 1.700362, Val Acc: 87.63%, Val-Class-Acc: {0: '85.87%', 2: '93.06%', 3: '96.72%', 4: '85.00%', 5: '96.12%', 6: '54.63%', 7: '84.80%', 8: '86.62%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 50/200, Train Loss: 0.067568, Train-Class-Acc: {0: '98.64%', 2: '99.13%', 3: '99.18%', 4: '96.84%', 5: '99.40%', 6: '93.55%', 7: '95.81%', 8: '97.45%', 9: '93.24%'}\n",
      "Val Loss: 1.748859, Val Acc: 87.99%, Val-Class-Acc: {0: '90.76%', 2: '93.75%', 3: '97.95%', 4: '85.00%', 5: '94.63%', 6: '49.07%', 7: '85.60%', 8: '82.80%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 51/200, Train Loss: 0.051420, Train-Class-Acc: {0: '99.05%', 2: '99.31%', 3: '99.59%', 4: '99.37%', 5: '99.48%', 6: '97.00%', 7: '97.41%', 8: '98.25%', 9: '97.30%'}\n",
      "Val Loss: 2.059435, Val Acc: 87.48%, Val-Class-Acc: {0: '88.04%', 2: '95.14%', 3: '93.85%', 4: '87.50%', 5: '97.91%', 6: '42.59%', 7: '86.40%', 8: '84.71%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 52/200, Train Loss: 0.200429, Train-Class-Acc: {0: '98.77%', 2: '99.48%', 3: '98.87%', 4: '98.73%', 5: '99.18%', 6: '92.86%', 7: '96.01%', 8: '97.13%', 9: '93.92%'}\n",
      "Val Loss: 2.525966, Val Acc: 86.46%, Val-Class-Acc: {0: '91.85%', 2: '91.67%', 3: '95.49%', 4: '85.00%', 5: '91.34%', 6: '52.78%', 7: '83.20%', 8: '84.71%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 53/200, Train Loss: 0.229392, Train-Class-Acc: {0: '98.09%', 2: '97.05%', 3: '98.05%', 4: '95.57%', 5: '98.06%', 6: '91.94%', 7: '92.81%', 8: '96.18%', 9: '90.54%'}\n",
      "Val Loss: 1.494973, Val Acc: 87.63%, Val-Class-Acc: {0: '88.04%', 2: '92.36%', 3: '95.49%', 4: '87.50%', 5: '96.12%', 6: '47.22%', 7: '87.20%', 8: '85.35%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 54/200, Train Loss: 0.130568, Train-Class-Acc: {0: '99.46%', 2: '99.65%', 3: '99.18%', 4: '96.84%', 5: '98.65%', 6: '94.93%', 7: '94.01%', 8: '96.34%', 9: '95.95%'}\n",
      "Val Loss: 1.399734, Val Acc: 88.36%, Val-Class-Acc: {0: '89.13%', 2: '94.44%', 3: '97.13%', 4: '85.00%', 5: '97.31%', 6: '55.56%', 7: '84.80%', 8: '81.53%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 55/200, Train Loss: 0.048808, Train-Class-Acc: {0: '99.05%', 2: '99.31%', 3: '99.49%', 4: '97.47%', 5: '99.55%', 6: '94.93%', 7: '97.60%', 8: '98.09%', 9: '95.95%'}\n",
      "Val Loss: 1.682032, Val Acc: 88.06%, Val-Class-Acc: {0: '90.76%', 2: '94.44%', 3: '96.31%', 4: '85.00%', 5: '96.72%', 6: '53.70%', 7: '81.60%', 8: '85.99%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 56/200, Train Loss: 0.034693, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '99.80%', 4: '99.37%', 5: '99.70%', 6: '98.39%', 7: '98.20%', 8: '99.04%', 9: '97.97%'}\n",
      "Val Loss: 1.961981, Val Acc: 88.28%, Val-Class-Acc: {0: '90.22%', 2: '95.14%', 3: '96.31%', 4: '90.00%', 5: '96.12%', 6: '50.00%', 7: '83.20%', 8: '85.99%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 57/200, Train Loss: 0.054531, Train-Class-Acc: {0: '99.18%', 2: '99.65%', 3: '99.59%', 4: '98.73%', 5: '99.55%', 6: '96.54%', 7: '97.01%', 8: '97.77%', 9: '95.27%'}\n",
      "Val Loss: 1.893091, Val Acc: 87.99%, Val-Class-Acc: {0: '83.15%', 2: '93.75%', 3: '98.36%', 4: '87.50%', 5: '97.91%', 6: '49.07%', 7: '82.40%', 8: '86.62%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 58/200, Train Loss: 0.031067, Train-Class-Acc: {0: '99.46%', 2: '99.31%', 3: '99.69%', 4: '99.37%', 5: '99.70%', 6: '97.00%', 7: '97.80%', 8: '98.89%', 9: '97.30%'}\n",
      "Val Loss: 1.794961, Val Acc: 88.06%, Val-Class-Acc: {0: '88.04%', 2: '95.14%', 3: '97.95%', 4: '87.50%', 5: '96.12%', 6: '55.56%', 7: '84.80%', 8: '82.80%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 59/200, Train Loss: 0.033296, Train-Class-Acc: {0: '98.91%', 2: '98.96%', 3: '99.59%', 4: '99.37%', 5: '99.93%', 6: '98.16%', 7: '97.80%', 8: '98.57%', 9: '98.65%'}\n",
      "Val Loss: 1.761172, Val Acc: 87.92%, Val-Class-Acc: {0: '86.96%', 2: '93.75%', 3: '97.13%', 4: '87.50%', 5: '96.12%', 6: '50.93%', 7: '85.60%', 8: '86.62%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 60/200, Train Loss: 0.029058, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '100.00%', 5: '99.63%', 6: '97.70%', 7: '98.20%', 8: '98.57%', 4: '99.37%', 9: '97.30%'}\n",
      "Val Loss: 1.797470, Val Acc: 87.77%, Val-Class-Acc: {0: '86.41%', 2: '93.75%', 3: '97.54%', 4: '87.50%', 5: '95.22%', 6: '57.41%', 7: '79.20%', 8: '89.17%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 61/200, Train Loss: 0.020358, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '99.90%', 5: '99.85%', 6: '97.93%', 7: '98.20%', 8: '98.73%', 9: '98.65%', 4: '98.73%'}\n",
      "Val Loss: 1.823720, Val Acc: 87.63%, Val-Class-Acc: {0: '89.13%', 2: '94.44%', 3: '96.31%', 4: '90.00%', 5: '97.91%', 6: '44.44%', 7: '80.00%', 8: '85.35%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 62/200, Train Loss: 0.012829, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '99.80%', 4: '100.00%', 5: '99.78%', 6: '99.54%', 7: '98.40%', 8: '99.36%', 9: '98.65%'}\n",
      "Val Loss: 1.819282, Val Acc: 87.92%, Val-Class-Acc: {0: '86.41%', 2: '94.44%', 3: '97.54%', 4: '90.00%', 5: '95.22%', 6: '49.07%', 7: '84.80%', 8: '86.62%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 63/200, Train Loss: 0.008284, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '100.00%', 5: '99.93%', 6: '99.31%', 7: '99.40%', 8: '99.52%', 9: '96.62%', 4: '100.00%'}\n",
      "Val Loss: 1.780203, Val Acc: 88.28%, Val-Class-Acc: {0: '89.13%', 2: '95.14%', 3: '97.54%', 4: '87.50%', 5: '95.82%', 6: '50.00%', 7: '87.20%', 8: '84.71%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 64/200, Train Loss: 0.029481, Train-Class-Acc: {0: '99.73%', 2: '99.48%', 3: '100.00%', 5: '99.70%', 6: '99.08%', 7: '98.80%', 8: '98.73%', 9: '100.00%', 4: '98.73%'}\n",
      "Val Loss: 2.232259, Val Acc: 88.79%, Val-Class-Acc: {0: '86.96%', 2: '95.14%', 3: '97.95%', 4: '90.00%', 5: '97.61%', 6: '50.93%', 7: '84.80%', 8: '88.54%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 65/200, Train Loss: 0.104062, Train-Class-Acc: {0: '98.50%', 2: '99.48%', 3: '99.39%', 4: '98.10%', 5: '98.88%', 6: '95.39%', 7: '96.01%', 8: '98.41%', 9: '95.95%'}\n",
      "Val Loss: 1.564305, Val Acc: 87.26%, Val-Class-Acc: {0: '86.96%', 2: '90.97%', 3: '95.90%', 4: '90.00%', 5: '94.63%', 6: '47.22%', 7: '87.20%', 8: '87.26%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 66/200, Train Loss: 0.068629, Train-Class-Acc: {0: '99.18%', 2: '98.61%', 3: '99.49%', 4: '98.73%', 5: '99.63%', 6: '95.16%', 7: '96.61%', 8: '98.25%', 9: '99.32%'}\n",
      "Val Loss: 1.917208, Val Acc: 88.06%, Val-Class-Acc: {0: '88.59%', 2: '93.75%', 3: '94.67%', 4: '87.50%', 5: '97.61%', 6: '42.59%', 7: '86.40%', 8: '89.81%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 67/200, Train Loss: 0.033262, Train-Class-Acc: {0: '99.59%', 2: '99.31%', 3: '99.28%', 4: '98.10%', 5: '99.85%', 6: '97.70%', 7: '98.00%', 8: '99.52%', 9: '99.32%'}\n",
      "Val Loss: 1.912257, Val Acc: 88.06%, Val-Class-Acc: {0: '88.59%', 2: '93.06%', 3: '97.13%', 4: '90.00%', 5: '96.42%', 6: '46.30%', 7: '87.20%', 8: '88.54%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 68/200, Train Loss: 0.015626, Train-Class-Acc: {0: '99.46%', 2: '99.65%', 3: '99.69%', 4: '99.37%', 5: '99.93%', 6: '99.08%', 7: '98.20%', 8: '99.36%', 9: '97.30%'}\n",
      "Val Loss: 1.763297, Val Acc: 88.21%, Val-Class-Acc: {0: '90.76%', 2: '93.75%', 3: '96.72%', 4: '87.50%', 5: '96.72%', 6: '43.52%', 7: '88.00%', 8: '85.99%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 69/200, Train Loss: 0.030340, Train-Class-Acc: {0: '99.46%', 2: '99.65%', 3: '99.80%', 5: '99.40%', 6: '99.08%', 7: '99.20%', 8: '99.04%', 9: '99.32%', 4: '98.73%'}\n",
      "Val Loss: 1.888663, Val Acc: 87.99%, Val-Class-Acc: {0: '92.93%', 2: '92.36%', 3: '97.13%', 4: '77.50%', 5: '98.81%', 6: '44.44%', 7: '82.40%', 8: '84.71%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 70/200, Train Loss: 0.193047, Train-Class-Acc: {0: '97.55%', 2: '97.05%', 3: '98.36%', 4: '97.47%', 5: '99.03%', 6: '92.86%', 7: '95.61%', 8: '95.54%', 9: '93.92%'}\n",
      "Val Loss: 2.353160, Val Acc: 86.24%, Val-Class-Acc: {0: '92.93%', 2: '89.58%', 3: '94.67%', 4: '90.00%', 5: '96.42%', 6: '39.81%', 7: '84.80%', 8: '79.62%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 71/200, Train Loss: 0.160235, Train-Class-Acc: {0: '97.41%', 2: '97.92%', 3: '98.26%', 5: '98.58%', 6: '89.86%', 7: '94.81%', 8: '96.18%', 9: '89.86%', 4: '96.20%'}\n",
      "Val Loss: 1.658938, Val Acc: 86.75%, Val-Class-Acc: {0: '94.02%', 2: '93.06%', 3: '94.67%', 4: '87.50%', 5: '97.01%', 6: '49.07%', 7: '76.80%', 8: '82.17%', 9: '43.24%'}, LR: 0.001000\n",
      "Epoch 72/200, Train Loss: 0.098866, Train-Class-Acc: {0: '98.37%', 2: '98.44%', 3: '99.39%', 4: '98.10%', 5: '99.03%', 6: '94.24%', 7: '96.61%', 8: '97.93%', 9: '92.57%'}\n",
      "Val Loss: 1.998330, Val Acc: 88.14%, Val-Class-Acc: {0: '90.22%', 2: '93.75%', 3: '96.72%', 4: '85.00%', 5: '96.12%', 6: '49.07%', 7: '86.40%', 8: '85.35%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 73/200, Train Loss: 0.285162, Train-Class-Acc: {0: '97.96%', 2: '97.75%', 3: '97.95%', 4: '98.73%', 5: '97.91%', 6: '90.32%', 7: '95.61%', 8: '96.18%', 9: '89.86%'}\n",
      "Val Loss: 2.751266, Val Acc: 86.90%, Val-Class-Acc: {0: '89.13%', 2: '91.67%', 3: '94.67%', 4: '80.00%', 5: '98.81%', 6: '51.85%', 7: '84.80%', 8: '77.07%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 74/200, Train Loss: 0.120297, Train-Class-Acc: {0: '98.91%', 2: '99.65%', 3: '99.49%', 4: '96.20%', 5: '99.33%', 6: '94.24%', 7: '96.01%', 8: '96.97%', 9: '93.92%'}\n",
      "Val Loss: 2.048135, Val Acc: 87.63%, Val-Class-Acc: {0: '89.67%', 2: '93.75%', 3: '97.13%', 4: '77.50%', 5: '97.31%', 6: '47.22%', 7: '83.20%', 8: '80.89%', 9: '75.68%'}, LR: 0.001000\n",
      "Epoch 75/200, Train Loss: 0.054658, Train-Class-Acc: {0: '99.32%', 2: '99.48%', 3: '99.59%', 4: '98.73%', 5: '99.48%', 6: '96.54%', 7: '97.60%', 8: '97.77%', 9: '97.30%'}\n",
      "Val Loss: 1.694659, Val Acc: 87.92%, Val-Class-Acc: {0: '87.50%', 2: '93.75%', 3: '97.54%', 4: '85.00%', 5: '97.01%', 6: '53.70%', 7: '82.40%', 8: '82.17%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 76/200, Train Loss: 0.024593, Train-Class-Acc: {0: '99.32%', 2: '99.65%', 3: '99.69%', 5: '99.70%', 6: '97.70%', 7: '98.40%', 8: '99.04%', 9: '97.97%', 4: '100.00%'}\n",
      "Val Loss: 1.865989, Val Acc: 87.19%, Val-Class-Acc: {0: '88.59%', 2: '94.44%', 3: '97.54%', 4: '85.00%', 5: '94.63%', 6: '40.74%', 7: '87.20%', 8: '83.44%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 77/200, Train Loss: 0.022829, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '99.69%', 4: '99.37%', 5: '99.85%', 6: '99.31%', 7: '98.20%', 8: '98.73%', 9: '98.65%'}\n",
      "Val Loss: 1.759166, Val Acc: 88.28%, Val-Class-Acc: {0: '86.96%', 2: '94.44%', 3: '98.36%', 4: '87.50%', 5: '97.31%', 6: '47.22%', 7: '87.20%', 8: '85.35%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 78/200, Train Loss: 0.016364, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '99.80%', 4: '98.73%', 5: '99.93%', 6: '99.08%', 7: '98.80%', 8: '99.20%', 9: '98.65%'}\n",
      "Val Loss: 1.995751, Val Acc: 88.06%, Val-Class-Acc: {0: '87.50%', 2: '94.44%', 3: '95.90%', 4: '90.00%', 5: '98.51%', 6: '43.52%', 7: '84.80%', 8: '85.99%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 79/200, Train Loss: 0.012368, Train-Class-Acc: {0: '100.00%', 2: '99.83%', 3: '99.80%', 4: '100.00%', 5: '99.78%', 6: '99.31%', 7: '99.00%', 8: '99.36%', 9: '100.00%'}\n",
      "Val Loss: 1.773656, Val Acc: 88.21%, Val-Class-Acc: {0: '87.50%', 2: '93.75%', 3: '97.13%', 4: '90.00%', 5: '97.01%', 6: '50.93%', 7: '84.00%', 8: '85.35%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 80/200, Train Loss: 0.024213, Train-Class-Acc: {0: '99.59%', 2: '100.00%', 3: '99.90%', 4: '98.10%', 5: '99.78%', 6: '98.39%', 7: '98.00%', 8: '99.36%', 9: '96.62%'}\n",
      "Val Loss: 2.089537, Val Acc: 87.99%, Val-Class-Acc: {0: '90.76%', 2: '88.89%', 3: '97.95%', 4: '87.50%', 5: '97.31%', 6: '51.85%', 7: '80.00%', 8: '85.99%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 81/200, Train Loss: 0.017303, Train-Class-Acc: {0: '99.86%', 2: '99.65%', 3: '99.90%', 4: '100.00%', 5: '99.78%', 6: '97.93%', 7: '98.40%', 8: '99.36%', 9: '97.97%'}\n",
      "Val Loss: 2.276970, Val Acc: 88.21%, Val-Class-Acc: {0: '87.50%', 2: '93.06%', 3: '96.72%', 4: '87.50%', 5: '98.21%', 6: '55.56%', 7: '83.20%', 8: '85.35%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 82/200, Train Loss: 0.013163, Train-Class-Acc: {0: '99.46%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '99.85%', 6: '98.62%', 7: '99.40%', 8: '99.68%', 9: '97.97%'}\n",
      "Val Loss: 2.475298, Val Acc: 88.86%, Val-Class-Acc: {0: '91.85%', 2: '93.75%', 3: '98.36%', 4: '90.00%', 5: '98.51%', 6: '42.59%', 7: '85.60%', 8: '85.99%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 83/200, Train Loss: 0.020210, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '99.90%', 4: '99.37%', 5: '99.93%', 6: '97.93%', 7: '99.00%', 8: '97.77%', 9: '100.00%'}\n",
      "Val Loss: 2.265836, Val Acc: 87.99%, Val-Class-Acc: {0: '90.22%', 2: '92.36%', 3: '97.54%', 4: '82.50%', 5: '98.21%', 6: '46.30%', 7: '79.20%', 8: '86.62%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 84/200, Train Loss: 0.008917, Train-Class-Acc: {0: '99.86%', 2: '99.65%', 3: '99.90%', 4: '100.00%', 5: '99.93%', 6: '99.31%', 7: '99.20%', 8: '99.68%', 9: '98.65%'}\n",
      "Val Loss: 2.267328, Val Acc: 88.65%, Val-Class-Acc: {0: '92.93%', 2: '91.67%', 3: '95.49%', 4: '90.00%', 5: '98.51%', 6: '49.07%', 7: '85.60%', 8: '83.44%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 85/200, Train Loss: 0.002781, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '99.90%', 4: '100.00%', 5: '99.93%', 6: '99.77%', 7: '99.80%', 8: '99.84%', 9: '100.00%'}\n",
      "Val Loss: 2.294748, Val Acc: 88.65%, Val-Class-Acc: {0: '90.76%', 2: '92.36%', 3: '97.54%', 4: '90.00%', 5: '97.91%', 6: '48.15%', 7: '84.00%', 8: '84.71%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 86/200, Train Loss: 0.012491, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '100.00%', 4: '100.00%', 5: '99.93%', 6: '99.08%', 7: '99.60%', 8: '99.68%', 9: '98.65%'}\n",
      "Val Loss: 2.165227, Val Acc: 88.94%, Val-Class-Acc: {0: '87.50%', 2: '93.75%', 3: '97.54%', 4: '85.00%', 5: '98.21%', 6: '50.93%', 7: '83.20%', 8: '88.54%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 87/200, Train Loss: 0.011936, Train-Class-Acc: {0: '99.32%', 2: '99.83%', 3: '99.80%', 4: '100.00%', 5: '99.85%', 6: '99.08%', 7: '99.80%', 8: '99.68%', 9: '98.65%'}\n",
      "Val Loss: 2.097097, Val Acc: 89.08%, Val-Class-Acc: {0: '89.13%', 2: '93.75%', 3: '97.95%', 4: '85.00%', 5: '97.31%', 6: '55.56%', 7: '83.20%', 8: '86.62%', 9: '70.27%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_16.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_87.pth\n",
      "Epoch 88/200, Train Loss: 0.009557, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '100.00%', 4: '100.00%', 5: '99.93%', 6: '99.54%', 7: '99.20%', 8: '99.36%', 9: '98.65%'}\n",
      "Val Loss: 2.063895, Val Acc: 88.06%, Val-Class-Acc: {0: '89.67%', 2: '92.36%', 3: '96.72%', 4: '87.50%', 5: '96.12%', 6: '44.44%', 7: '84.80%', 8: '88.54%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 89/200, Train Loss: 0.025696, Train-Class-Acc: {0: '99.05%', 2: '99.31%', 3: '99.90%', 5: '99.85%', 6: '98.85%', 7: '98.20%', 8: '99.20%', 9: '97.30%', 4: '100.00%'}\n",
      "Val Loss: 2.236258, Val Acc: 88.36%, Val-Class-Acc: {0: '91.30%', 2: '92.36%', 3: '96.72%', 4: '87.50%', 5: '97.01%', 6: '46.30%', 7: '82.40%', 8: '89.81%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 90/200, Train Loss: 0.014972, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '100.00%', 4: '100.00%', 5: '99.93%', 6: '98.85%', 7: '98.20%', 8: '99.20%', 9: '98.65%'}\n",
      "Val Loss: 2.145583, Val Acc: 88.14%, Val-Class-Acc: {0: '88.04%', 2: '94.44%', 3: '97.13%', 4: '85.00%', 5: '96.42%', 6: '51.85%', 7: '86.40%', 8: '87.26%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 91/200, Train Loss: 0.009179, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '100.00%', 4: '100.00%', 5: '99.85%', 6: '99.08%', 7: '99.20%', 8: '99.68%', 9: '97.97%'}\n",
      "Val Loss: 1.989398, Val Acc: 88.50%, Val-Class-Acc: {0: '86.41%', 2: '94.44%', 3: '96.72%', 4: '87.50%', 5: '97.61%', 6: '47.22%', 7: '84.80%', 8: '89.81%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 92/200, Train Loss: 0.007412, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '100.00%', 4: '98.73%', 5: '99.85%', 6: '100.00%', 7: '99.00%', 8: '99.52%', 9: '99.32%'}\n",
      "Val Loss: 1.961834, Val Acc: 88.28%, Val-Class-Acc: {0: '90.22%', 2: '93.06%', 3: '97.54%', 4: '90.00%', 5: '96.42%', 6: '46.30%', 7: '87.20%', 8: '84.08%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 93/200, Train Loss: 0.010107, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '100.00%', 4: '100.00%', 5: '99.85%', 6: '98.85%', 7: '99.20%', 8: '99.20%', 9: '98.65%'}\n",
      "Val Loss: 2.117393, Val Acc: 88.06%, Val-Class-Acc: {0: '86.96%', 2: '95.14%', 3: '95.49%', 4: '85.00%', 5: '98.21%', 6: '42.59%', 7: '86.40%', 8: '88.54%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 94/200, Train Loss: 0.007459, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '100.00%', 4: '99.37%', 5: '99.85%', 6: '99.54%', 7: '99.80%', 8: '99.20%', 9: '99.32%'}\n",
      "Val Loss: 2.132889, Val Acc: 88.86%, Val-Class-Acc: {0: '90.22%', 2: '93.75%', 3: '97.13%', 4: '90.00%', 5: '96.42%', 6: '47.22%', 7: '85.60%', 8: '89.81%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 95/200, Train Loss: 0.012716, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '99.69%', 4: '99.37%', 5: '99.85%', 6: '98.16%', 7: '99.20%', 8: '99.84%', 9: '100.00%'}\n",
      "Val Loss: 1.954928, Val Acc: 88.14%, Val-Class-Acc: {0: '86.41%', 2: '92.36%', 3: '96.31%', 4: '90.00%', 5: '97.01%', 6: '48.15%', 7: '84.80%', 8: '90.45%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 96/200, Train Loss: 0.018890, Train-Class-Acc: {0: '99.46%', 2: '99.31%', 3: '99.59%', 4: '100.00%', 5: '99.93%', 6: '98.39%', 7: '99.00%', 8: '98.89%', 9: '100.00%'}\n",
      "Val Loss: 2.007355, Val Acc: 87.70%, Val-Class-Acc: {0: '86.96%', 2: '93.06%', 3: '95.90%', 4: '82.50%', 5: '97.01%', 6: '52.78%', 7: '84.80%', 8: '85.99%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 97/200, Train Loss: 0.037919, Train-Class-Acc: {0: '99.18%', 2: '98.44%', 3: '99.28%', 4: '99.37%', 5: '99.85%', 6: '97.24%', 7: '98.80%', 8: '99.36%', 9: '95.95%'}\n",
      "Val Loss: 1.730235, Val Acc: 86.97%, Val-Class-Acc: {0: '89.13%', 2: '92.36%', 3: '96.31%', 4: '87.50%', 5: '97.01%', 6: '46.30%', 7: '82.40%', 8: '78.98%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 98/200, Train Loss: 0.037583, Train-Class-Acc: {0: '99.46%', 2: '99.13%', 3: '99.59%', 4: '98.73%', 5: '99.48%', 6: '97.00%', 7: '98.20%', 8: '98.09%', 9: '97.30%'}\n",
      "Val Loss: 2.439924, Val Acc: 87.41%, Val-Class-Acc: {0: '90.76%', 2: '93.06%', 3: '98.36%', 4: '85.00%', 5: '96.12%', 6: '48.15%', 7: '82.40%', 8: '81.53%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 99/200, Train Loss: 0.128924, Train-Class-Acc: {0: '97.55%', 2: '96.01%', 3: '98.98%', 4: '97.47%', 5: '99.25%', 6: '94.70%', 7: '96.01%', 8: '97.93%', 9: '94.59%'}\n",
      "Val Loss: 2.026875, Val Acc: 86.97%, Val-Class-Acc: {0: '82.07%', 2: '92.36%', 3: '96.31%', 4: '82.50%', 5: '95.22%', 6: '58.33%', 7: '80.80%', 8: '88.54%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 100/200, Train Loss: 0.130891, Train-Class-Acc: {0: '98.64%', 2: '99.31%', 3: '98.98%', 4: '96.20%', 5: '99.03%', 6: '95.62%', 7: '95.01%', 8: '97.13%', 9: '93.92%'}\n",
      "Val Loss: 2.044584, Val Acc: 86.32%, Val-Class-Acc: {0: '85.33%', 2: '87.50%', 3: '95.90%', 4: '82.50%', 5: '96.72%', 6: '42.59%', 7: '83.20%', 8: '88.54%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 101/200, Train Loss: 0.103921, Train-Class-Acc: {0: '98.77%', 2: '97.57%', 3: '98.87%', 4: '97.47%', 5: '99.33%', 6: '95.16%', 7: '97.01%', 8: '96.82%', 9: '97.30%'}\n",
      "Val Loss: 1.986007, Val Acc: 86.24%, Val-Class-Acc: {0: '86.96%', 2: '95.14%', 3: '93.85%', 4: '87.50%', 5: '95.52%', 6: '39.81%', 7: '87.20%', 8: '86.62%', 9: '43.24%'}, LR: 0.001000\n",
      "Epoch 102/200, Train Loss: 0.285483, Train-Class-Acc: {0: '98.37%', 2: '97.40%', 3: '98.36%', 4: '98.10%', 5: '99.18%', 6: '93.32%', 7: '95.81%', 8: '96.50%', 9: '93.24%'}\n",
      "Val Loss: 2.926602, Val Acc: 83.99%, Val-Class-Acc: {0: '86.41%', 2: '83.33%', 3: '94.26%', 4: '85.00%', 5: '90.45%', 6: '40.74%', 7: '88.00%', 8: '82.17%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 103/200, Train Loss: 0.383459, Train-Class-Acc: {0: '96.19%', 2: '96.19%', 3: '97.85%', 5: '96.86%', 6: '83.64%', 7: '91.62%', 8: '91.40%', 9: '87.16%', 4: '94.94%'}\n",
      "Val Loss: 1.710242, Val Acc: 85.66%, Val-Class-Acc: {0: '85.33%', 2: '90.28%', 3: '95.90%', 4: '85.00%', 5: '92.54%', 6: '44.44%', 7: '83.20%', 8: '85.35%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 104/200, Train Loss: 0.105196, Train-Class-Acc: {0: '98.23%', 2: '98.27%', 3: '98.77%', 4: '99.37%', 5: '98.95%', 6: '95.62%', 7: '95.61%', 8: '96.97%', 9: '96.62%'}\n",
      "Val Loss: 2.485985, Val Acc: 86.10%, Val-Class-Acc: {0: '90.22%', 2: '91.67%', 3: '93.85%', 4: '85.00%', 5: '95.52%', 6: '36.11%', 7: '85.60%', 8: '86.62%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 105/200, Train Loss: 0.071789, Train-Class-Acc: {0: '98.91%', 2: '99.31%', 3: '99.08%', 4: '98.10%', 5: '99.55%', 6: '94.93%', 7: '96.21%', 8: '97.29%', 9: '93.92%'}\n",
      "Val Loss: 2.153724, Val Acc: 86.10%, Val-Class-Acc: {0: '81.52%', 2: '93.75%', 3: '95.90%', 4: '87.50%', 5: '95.22%', 6: '38.89%', 7: '82.40%', 8: '90.45%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 106/200, Train Loss: 0.040052, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '99.59%', 5: '99.48%', 6: '96.77%', 7: '98.00%', 8: '98.57%', 9: '97.30%', 4: '98.10%'}\n",
      "Val Loss: 2.057715, Val Acc: 86.17%, Val-Class-Acc: {0: '85.33%', 2: '92.36%', 3: '96.72%', 4: '82.50%', 5: '94.33%', 6: '40.74%', 7: '84.80%', 8: '85.99%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 107/200, Train Loss: 0.018874, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '99.49%', 4: '98.73%', 5: '99.78%', 6: '98.39%', 7: '98.20%', 8: '98.89%', 9: '97.97%'}\n",
      "Val Loss: 2.259847, Val Acc: 86.97%, Val-Class-Acc: {0: '88.59%', 2: '93.06%', 3: '96.72%', 4: '87.50%', 5: '96.12%', 6: '40.74%', 7: '81.60%', 8: '87.26%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 108/200, Train Loss: 0.014386, Train-Class-Acc: {0: '99.86%', 2: '99.48%', 3: '99.80%', 4: '100.00%', 5: '99.78%', 6: '98.85%', 7: '99.00%', 8: '99.68%', 9: '97.30%'}\n",
      "Val Loss: 2.124741, Val Acc: 86.97%, Val-Class-Acc: {0: '92.39%', 2: '91.67%', 3: '95.90%', 4: '85.00%', 5: '95.82%', 6: '42.59%', 7: '80.80%', 8: '84.08%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 109/200, Train Loss: 0.019727, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '99.90%', 4: '99.37%', 5: '100.00%', 6: '98.16%', 7: '99.00%', 8: '99.36%', 9: '99.32%'}\n",
      "Val Loss: 2.029863, Val Acc: 86.83%, Val-Class-Acc: {0: '92.39%', 2: '92.36%', 3: '95.90%', 4: '82.50%', 5: '94.33%', 6: '38.89%', 7: '84.80%', 8: '86.62%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 110/200, Train Loss: 0.016212, Train-Class-Acc: {0: '99.59%', 2: '99.83%', 3: '99.90%', 4: '98.73%', 5: '99.70%', 6: '99.08%', 7: '99.20%', 8: '99.04%', 9: '97.97%'}\n",
      "Val Loss: 2.143242, Val Acc: 87.34%, Val-Class-Acc: {0: '91.85%', 2: '90.97%', 3: '95.49%', 4: '87.50%', 5: '95.22%', 6: '46.30%', 7: '84.00%', 8: '84.08%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 111/200, Train Loss: 0.039519, Train-Class-Acc: {0: '99.59%', 2: '99.83%', 3: '99.49%', 5: '99.70%', 6: '98.62%', 7: '98.80%', 8: '99.36%', 9: '97.97%', 4: '99.37%'}\n",
      "Val Loss: 2.191106, Val Acc: 87.63%, Val-Class-Acc: {0: '90.22%', 2: '93.75%', 3: '97.95%', 4: '87.50%', 5: '93.73%', 6: '49.07%', 7: '87.20%', 8: '82.80%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 112/200, Train Loss: 0.079752, Train-Class-Acc: {0: '99.59%', 2: '99.13%', 3: '99.28%', 5: '99.25%', 6: '96.54%', 7: '98.00%', 8: '98.09%', 9: '98.65%', 4: '97.47%'}\n",
      "Val Loss: 2.479782, Val Acc: 86.90%, Val-Class-Acc: {0: '85.87%', 2: '91.67%', 3: '93.44%', 4: '87.50%', 5: '96.12%', 6: '50.00%', 7: '87.20%', 8: '84.71%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 113/200, Train Loss: 0.205009, Train-Class-Acc: {0: '97.68%', 2: '97.40%', 3: '98.36%', 4: '97.47%', 5: '98.95%', 6: '94.24%', 7: '95.81%', 8: '96.66%', 9: '95.95%'}\n",
      "Val Loss: 1.723168, Val Acc: 86.83%, Val-Class-Acc: {0: '87.50%', 2: '86.81%', 3: '97.54%', 4: '90.00%', 5: '93.13%', 6: '46.30%', 7: '84.80%', 8: '88.54%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 114/200, Train Loss: 0.065076, Train-Class-Acc: {0: '98.37%', 2: '99.31%', 3: '99.59%', 4: '100.00%', 5: '99.48%', 6: '95.62%', 7: '96.81%', 8: '96.82%', 9: '97.30%'}\n",
      "Val Loss: 1.772550, Val Acc: 87.05%, Val-Class-Acc: {0: '90.22%', 2: '93.75%', 3: '97.54%', 4: '87.50%', 5: '94.93%', 6: '31.48%', 7: '85.60%', 8: '89.17%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 115/200, Train Loss: 0.022464, Train-Class-Acc: {0: '99.59%', 2: '100.00%', 3: '99.59%', 4: '99.37%', 5: '99.70%', 6: '97.47%', 7: '98.80%', 8: '98.73%', 9: '97.97%'}\n",
      "Val Loss: 1.762860, Val Acc: 87.34%, Val-Class-Acc: {0: '89.13%', 2: '92.36%', 3: '97.13%', 4: '85.00%', 5: '92.54%', 6: '46.30%', 7: '84.00%', 8: '89.17%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 116/200, Train Loss: 0.063672, Train-Class-Acc: {0: '99.59%', 2: '99.65%', 3: '99.59%', 4: '98.73%', 5: '99.70%', 6: '97.24%', 7: '98.80%', 8: '99.36%', 9: '98.65%'}\n",
      "Val Loss: 2.092233, Val Acc: 86.54%, Val-Class-Acc: {0: '90.76%', 2: '93.06%', 3: '94.67%', 4: '87.50%', 5: '94.33%', 6: '48.15%', 7: '83.20%', 8: '79.62%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 117/200, Train Loss: 0.060683, Train-Class-Acc: {0: '99.05%', 2: '98.27%', 3: '99.39%', 5: '99.55%', 6: '96.08%', 7: '97.21%', 8: '97.45%', 9: '97.30%', 4: '99.37%'}\n",
      "Val Loss: 2.246382, Val Acc: 86.75%, Val-Class-Acc: {0: '87.50%', 2: '93.06%', 3: '95.49%', 4: '87.50%', 5: '94.93%', 6: '42.59%', 7: '79.20%', 8: '88.54%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 118/200, Train Loss: 0.019083, Train-Class-Acc: {0: '99.32%', 2: '99.31%', 3: '99.49%', 4: '98.73%', 5: '99.85%', 6: '98.85%', 7: '99.40%', 8: '99.20%', 9: '96.62%'}\n",
      "Val Loss: 2.372562, Val Acc: 87.19%, Val-Class-Acc: {0: '88.59%', 2: '93.75%', 3: '95.90%', 4: '87.50%', 5: '94.03%', 6: '44.44%', 7: '83.20%', 8: '87.90%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 119/200, Train Loss: 0.043437, Train-Class-Acc: {0: '99.59%', 2: '98.96%', 3: '99.80%', 5: '99.48%', 6: '99.08%', 7: '97.21%', 8: '99.36%', 4: '98.10%', 9: '97.97%'}\n",
      "Val Loss: 2.216081, Val Acc: 86.90%, Val-Class-Acc: {0: '86.96%', 2: '93.75%', 3: '97.54%', 4: '87.50%', 5: '95.22%', 6: '35.19%', 7: '82.40%', 8: '88.54%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 120/200, Train Loss: 0.038814, Train-Class-Acc: {0: '99.05%', 2: '99.48%', 3: '99.80%', 4: '97.47%', 5: '99.55%', 6: '97.93%', 7: '98.40%', 8: '98.57%', 9: '96.62%'}\n",
      "Val Loss: 2.270234, Val Acc: 87.34%, Val-Class-Acc: {0: '85.87%', 2: '93.75%', 3: '97.54%', 4: '85.00%', 5: '95.22%', 6: '43.52%', 7: '80.00%', 8: '91.08%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 121/200, Train Loss: 0.020132, Train-Class-Acc: {0: '99.05%', 2: '99.65%', 3: '100.00%', 4: '100.00%', 5: '99.85%', 6: '99.08%', 7: '99.00%', 8: '99.36%', 9: '99.32%'}\n",
      "Val Loss: 2.053065, Val Acc: 87.70%, Val-Class-Acc: {0: '88.04%', 2: '92.36%', 3: '96.31%', 4: '85.00%', 5: '96.12%', 6: '46.30%', 7: '84.00%', 8: '89.81%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 122/200, Train Loss: 0.011489, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '99.90%', 4: '100.00%', 5: '100.00%', 6: '98.62%', 7: '99.00%', 8: '99.20%', 9: '99.32%'}\n",
      "Val Loss: 2.133479, Val Acc: 87.92%, Val-Class-Acc: {0: '89.13%', 2: '91.67%', 3: '96.72%', 4: '87.50%', 5: '94.93%', 6: '44.44%', 7: '88.80%', 8: '89.81%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 123/200, Train Loss: 0.008213, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '99.80%', 4: '100.00%', 5: '99.93%', 6: '99.31%', 7: '99.40%', 8: '100.00%', 9: '99.32%'}\n",
      "Val Loss: 2.256711, Val Acc: 88.36%, Val-Class-Acc: {0: '90.76%', 2: '91.67%', 3: '96.72%', 4: '82.50%', 5: '97.31%', 6: '50.00%', 7: '82.40%', 8: '89.17%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 124/200, Train Loss: 0.007913, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '99.90%', 4: '100.00%', 5: '99.85%', 6: '99.77%', 7: '99.20%', 8: '99.52%', 9: '99.32%'}\n",
      "Val Loss: 2.053972, Val Acc: 87.77%, Val-Class-Acc: {0: '92.39%', 2: '94.44%', 3: '95.49%', 4: '87.50%', 5: '95.52%', 6: '45.37%', 7: '86.40%', 8: '87.90%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 125/200, Train Loss: 0.005300, Train-Class-Acc: {0: '100.00%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '100.00%', 6: '99.77%', 7: '99.60%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 2.291082, Val Acc: 88.86%, Val-Class-Acc: {0: '91.85%', 2: '94.44%', 3: '97.13%', 4: '85.00%', 5: '96.72%', 6: '50.93%', 7: '84.80%', 8: '87.26%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 126/200, Train Loss: 0.004798, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '99.90%', 5: '99.85%', 6: '99.77%', 7: '99.40%', 8: '99.84%', 9: '100.00%', 4: '100.00%'}\n",
      "Val Loss: 2.050180, Val Acc: 88.28%, Val-Class-Acc: {0: '94.02%', 2: '93.75%', 3: '97.95%', 4: '85.00%', 5: '95.82%', 6: '42.59%', 7: '84.00%', 8: '85.99%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 127/200, Train Loss: 0.006132, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '100.00%', 4: '100.00%', 5: '99.85%', 6: '99.08%', 7: '100.00%', 8: '99.52%', 9: '99.32%'}\n",
      "Val Loss: 2.223643, Val Acc: 88.14%, Val-Class-Acc: {0: '94.02%', 2: '94.44%', 3: '95.08%', 4: '85.00%', 5: '93.73%', 6: '50.00%', 7: '83.20%', 8: '87.90%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 128/200, Train Loss: 0.028868, Train-Class-Acc: {0: '99.86%', 2: '99.65%', 3: '99.80%', 4: '100.00%', 5: '99.55%', 6: '99.08%', 7: '98.00%', 8: '99.52%', 9: '99.32%'}\n",
      "Val Loss: 2.379385, Val Acc: 87.41%, Val-Class-Acc: {0: '88.59%', 2: '94.44%', 3: '97.13%', 4: '85.00%', 5: '93.73%', 6: '38.89%', 7: '90.40%', 8: '89.81%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 129/200, Train Loss: 0.038108, Train-Class-Acc: {0: '99.73%', 2: '99.48%', 3: '99.90%', 4: '98.10%', 5: '99.78%', 6: '97.00%', 7: '98.20%', 8: '98.89%', 9: '97.30%'}\n",
      "Val Loss: 2.162315, Val Acc: 88.28%, Val-Class-Acc: {0: '85.87%', 2: '93.06%', 3: '97.54%', 4: '87.50%', 5: '96.72%', 6: '51.85%', 7: '80.80%', 8: '90.45%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 130/200, Train Loss: 0.044650, Train-Class-Acc: {0: '99.46%', 2: '98.96%', 3: '99.49%', 4: '98.10%', 5: '99.63%', 6: '97.70%', 7: '97.80%', 8: '98.89%', 9: '97.97%'}\n",
      "Val Loss: 2.262254, Val Acc: 87.19%, Val-Class-Acc: {0: '94.02%', 2: '90.97%', 3: '95.08%', 4: '85.00%', 5: '95.52%', 6: '47.22%', 7: '81.60%', 8: '84.08%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 131/200, Train Loss: 0.033711, Train-Class-Acc: {0: '99.59%', 2: '99.65%', 3: '99.69%', 4: '99.37%', 5: '99.70%', 6: '97.47%', 7: '98.60%', 8: '98.89%', 9: '97.30%'}\n",
      "Val Loss: 2.081300, Val Acc: 87.70%, Val-Class-Acc: {0: '89.13%', 2: '95.14%', 3: '96.72%', 4: '87.50%', 5: '95.82%', 6: '47.22%', 7: '78.40%', 8: '91.08%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 132/200, Train Loss: 0.021741, Train-Class-Acc: {0: '99.32%', 2: '99.65%', 3: '99.59%', 4: '100.00%', 5: '99.78%', 6: '98.39%', 7: '99.00%', 8: '98.25%', 9: '97.30%'}\n",
      "Val Loss: 2.185964, Val Acc: 87.63%, Val-Class-Acc: {0: '90.76%', 2: '92.36%', 3: '97.13%', 4: '82.50%', 5: '94.33%', 6: '54.63%', 7: '80.80%', 8: '86.62%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 133/200, Train Loss: 0.025288, Train-Class-Acc: {0: '99.32%', 2: '99.65%', 3: '99.90%', 4: '98.73%', 5: '99.63%', 6: '98.39%', 7: '98.60%', 8: '99.36%', 9: '97.30%'}\n",
      "Val Loss: 2.107712, Val Acc: 87.63%, Val-Class-Acc: {0: '91.85%', 2: '90.97%', 3: '97.54%', 4: '85.00%', 5: '95.52%', 6: '50.93%', 7: '83.20%', 8: '82.80%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 134/200, Train Loss: 0.119720, Train-Class-Acc: {0: '98.91%', 2: '99.65%', 3: '98.57%', 4: '99.37%', 5: '99.55%', 6: '97.47%', 7: '98.00%', 8: '98.41%', 9: '97.30%'}\n",
      "Val Loss: 2.819124, Val Acc: 85.44%, Val-Class-Acc: {0: '76.63%', 2: '92.36%', 3: '95.08%', 4: '87.50%', 5: '99.10%', 6: '37.96%', 7: '75.20%', 8: '90.45%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 135/200, Train Loss: 0.076506, Train-Class-Acc: {0: '99.32%', 2: '99.13%', 3: '98.98%', 4: '97.47%', 5: '99.33%', 6: '96.31%', 7: '97.41%', 8: '97.93%', 9: '89.86%'}\n",
      "Val Loss: 2.141124, Val Acc: 87.48%, Val-Class-Acc: {0: '83.15%', 2: '95.14%', 3: '97.13%', 4: '80.00%', 5: '96.42%', 6: '48.15%', 7: '84.00%', 8: '89.17%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 136/200, Train Loss: 0.041984, Train-Class-Acc: {0: '98.77%', 2: '98.96%', 3: '99.69%', 4: '98.73%', 5: '99.70%', 6: '96.08%', 7: '98.00%', 8: '97.29%', 9: '95.27%'}\n",
      "Val Loss: 2.087866, Val Acc: 86.17%, Val-Class-Acc: {0: '92.93%', 2: '93.75%', 3: '96.31%', 4: '77.50%', 5: '92.84%', 6: '37.04%', 7: '87.20%', 8: '84.71%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 137/200, Train Loss: 0.053107, Train-Class-Acc: {0: '99.18%', 2: '99.48%', 3: '99.28%', 4: '98.73%', 5: '99.55%', 6: '96.54%', 7: '99.00%', 8: '98.89%', 9: '96.62%'}\n",
      "Val Loss: 1.791290, Val Acc: 86.10%, Val-Class-Acc: {0: '90.22%', 2: '90.97%', 3: '93.44%', 4: '82.50%', 5: '97.61%', 6: '43.52%', 7: '84.00%', 8: '82.17%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 138/200, Train Loss: 0.047980, Train-Class-Acc: {0: '98.77%', 2: '98.79%', 3: '99.39%', 4: '98.10%', 5: '99.55%', 6: '97.70%', 7: '98.60%', 8: '97.93%', 9: '97.30%'}\n",
      "Val Loss: 2.346609, Val Acc: 86.83%, Val-Class-Acc: {0: '90.22%', 2: '89.58%', 3: '97.95%', 4: '90.00%', 5: '96.42%', 6: '39.81%', 7: '82.40%', 8: '84.08%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 139/200, Train Loss: 0.194245, Train-Class-Acc: {0: '98.91%', 2: '99.13%', 3: '98.57%', 4: '96.84%', 5: '99.25%', 6: '96.08%', 7: '97.41%', 8: '96.82%', 9: '92.57%'}\n",
      "Val Loss: 2.541615, Val Acc: 84.93%, Val-Class-Acc: {0: '79.35%', 2: '89.58%', 3: '96.31%', 4: '87.50%', 5: '91.04%', 6: '41.67%', 7: '85.60%', 8: '90.45%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 140/200, Train Loss: 0.238576, Train-Class-Acc: {0: '96.59%', 2: '98.09%', 3: '97.03%', 5: '98.28%', 6: '89.40%', 7: '95.01%', 8: '95.22%', 9: '90.54%', 4: '95.57%'}\n",
      "Val Loss: 2.225323, Val Acc: 86.24%, Val-Class-Acc: {0: '87.50%', 2: '84.72%', 3: '97.13%', 4: '87.50%', 5: '94.93%', 6: '42.59%', 7: '80.00%', 8: '88.54%', 9: '72.97%'}, LR: 0.001000\n",
      "Epoch 141/200, Train Loss: 0.114306, Train-Class-Acc: {0: '98.50%', 2: '98.96%', 3: '98.77%', 4: '98.73%', 5: '98.95%', 6: '96.08%', 7: '96.41%', 8: '97.61%', 9: '97.97%'}\n",
      "Val Loss: 2.188080, Val Acc: 85.30%, Val-Class-Acc: {0: '82.61%', 2: '90.97%', 3: '97.13%', 4: '85.00%', 5: '94.63%', 6: '48.15%', 7: '80.80%', 8: '82.17%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 142/200, Train Loss: 0.111093, Train-Class-Acc: {0: '98.91%', 2: '98.44%', 3: '98.77%', 4: '98.73%', 5: '99.40%', 6: '95.39%', 7: '95.61%', 8: '97.45%', 9: '94.59%'}\n",
      "Val Loss: 2.111561, Val Acc: 87.05%, Val-Class-Acc: {0: '92.39%', 2: '93.06%', 3: '97.54%', 4: '80.00%', 5: '96.42%', 6: '47.22%', 7: '80.00%', 8: '82.17%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 143/200, Train Loss: 0.111260, Train-Class-Acc: {0: '98.77%', 2: '99.31%', 3: '99.69%', 4: '98.10%', 5: '99.33%', 6: '96.77%', 7: '97.80%', 8: '98.25%', 9: '95.27%'}\n",
      "Val Loss: 2.379933, Val Acc: 86.24%, Val-Class-Acc: {0: '88.04%', 2: '93.06%', 3: '94.26%', 4: '75.00%', 5: '95.22%', 6: '37.04%', 7: '87.20%', 8: '85.99%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 144/200, Train Loss: 0.063651, Train-Class-Acc: {0: '99.18%', 2: '98.61%', 3: '99.49%', 4: '99.37%', 5: '99.48%', 6: '97.00%', 7: '97.60%', 8: '97.77%', 9: '96.62%'}\n",
      "Val Loss: 2.332719, Val Acc: 86.83%, Val-Class-Acc: {0: '84.24%', 2: '90.28%', 3: '96.72%', 4: '85.00%', 5: '96.42%', 6: '44.44%', 7: '84.00%', 8: '87.26%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 145/200, Train Loss: 0.034678, Train-Class-Acc: {0: '98.91%', 2: '99.83%', 3: '99.80%', 4: '99.37%', 5: '99.70%', 6: '98.39%', 7: '98.60%', 8: '99.04%', 9: '97.30%'}\n",
      "Val Loss: 2.404300, Val Acc: 87.63%, Val-Class-Acc: {0: '91.30%', 2: '89.58%', 3: '97.95%', 4: '85.00%', 5: '97.61%', 6: '44.44%', 7: '84.80%', 8: '82.17%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 146/200, Train Loss: 0.026598, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '100.00%', 4: '99.37%', 5: '99.63%', 6: '98.85%', 7: '97.60%', 8: '98.89%', 9: '97.97%'}\n",
      "Val Loss: 2.539468, Val Acc: 88.06%, Val-Class-Acc: {0: '90.76%', 2: '90.28%', 3: '96.72%', 4: '85.00%', 5: '97.91%', 6: '45.37%', 7: '81.60%', 8: '87.90%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 147/200, Train Loss: 0.016846, Train-Class-Acc: {0: '99.73%', 2: '99.48%', 3: '99.90%', 4: '99.37%', 5: '99.93%', 6: '98.85%', 7: '98.80%', 8: '99.36%', 9: '97.97%'}\n",
      "Val Loss: 2.310266, Val Acc: 87.55%, Val-Class-Acc: {0: '86.41%', 2: '94.44%', 3: '95.49%', 4: '85.00%', 5: '94.63%', 6: '54.63%', 7: '84.80%', 8: '89.17%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 148/200, Train Loss: 0.011155, Train-Class-Acc: {0: '100.00%', 2: '99.83%', 3: '100.00%', 4: '100.00%', 5: '99.85%', 6: '98.85%', 7: '99.20%', 8: '99.20%', 9: '99.32%'}\n",
      "Val Loss: 2.402887, Val Acc: 87.77%, Val-Class-Acc: {0: '86.96%', 2: '95.14%', 3: '96.31%', 4: '85.00%', 5: '97.01%', 6: '50.93%', 7: '83.20%', 8: '87.90%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 149/200, Train Loss: 0.208713, Train-Class-Acc: {0: '98.91%', 2: '98.79%', 3: '98.77%', 5: '98.35%', 6: '95.62%', 7: '96.21%', 8: '97.29%', 9: '93.92%', 4: '96.20%'}\n",
      "Val Loss: 2.502575, Val Acc: 86.03%, Val-Class-Acc: {0: '86.96%', 2: '90.28%', 3: '96.31%', 4: '87.50%', 5: '96.72%', 6: '44.44%', 7: '82.40%', 8: '83.44%', 9: '43.24%'}, LR: 0.001000\n",
      "Epoch 150/200, Train Loss: 0.069761, Train-Class-Acc: {0: '98.37%', 2: '98.79%', 3: '99.80%', 4: '100.00%', 5: '99.25%', 6: '95.39%', 7: '98.40%', 8: '97.93%', 9: '94.59%'}\n",
      "Val Loss: 2.466418, Val Acc: 85.95%, Val-Class-Acc: {0: '90.22%', 2: '88.89%', 3: '95.90%', 4: '90.00%', 5: '95.52%', 6: '50.93%', 7: '84.00%', 8: '76.43%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 151/200, Train Loss: 0.029962, Train-Class-Acc: {0: '99.73%', 2: '99.13%', 3: '99.49%', 4: '99.37%', 5: '99.63%', 6: '98.39%', 7: '98.40%', 8: '98.41%', 9: '99.32%'}\n",
      "Val Loss: 2.497474, Val Acc: 87.19%, Val-Class-Acc: {0: '90.22%', 2: '93.06%', 3: '96.72%', 4: '90.00%', 5: '94.03%', 6: '49.07%', 7: '81.60%', 8: '87.26%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 152/200, Train Loss: 0.014127, Train-Class-Acc: {0: '99.73%', 2: '99.83%', 3: '100.00%', 4: '100.00%', 5: '99.85%', 6: '98.62%', 7: '98.20%', 8: '99.84%', 9: '97.97%'}\n",
      "Val Loss: 2.349790, Val Acc: 87.48%, Val-Class-Acc: {0: '90.76%', 2: '90.97%', 3: '96.31%', 4: '87.50%', 5: '96.42%', 6: '38.89%', 7: '85.60%', 8: '89.17%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 153/200, Train Loss: 0.022831, Train-Class-Acc: {0: '99.59%', 2: '99.13%', 3: '99.59%', 4: '99.37%', 5: '99.93%', 6: '99.08%', 7: '99.60%', 8: '99.20%', 9: '97.30%'}\n",
      "Val Loss: 2.600483, Val Acc: 87.41%, Val-Class-Acc: {0: '90.76%', 2: '92.36%', 3: '96.31%', 4: '90.00%', 5: '96.72%', 6: '37.96%', 7: '88.00%', 8: '85.99%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 154/200, Train Loss: 0.017459, Train-Class-Acc: {0: '100.00%', 2: '99.83%', 3: '99.80%', 4: '100.00%', 5: '99.70%', 6: '99.77%', 7: '99.00%', 8: '99.20%', 9: '98.65%'}\n",
      "Val Loss: 2.710460, Val Acc: 87.19%, Val-Class-Acc: {0: '86.96%', 2: '93.75%', 3: '95.90%', 4: '85.00%', 5: '97.01%', 6: '36.11%', 7: '85.60%', 8: '91.72%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 155/200, Train Loss: 0.011972, Train-Class-Acc: {0: '99.86%', 2: '99.65%', 3: '99.80%', 4: '98.73%', 5: '99.93%', 6: '99.08%', 7: '99.60%', 8: '99.20%', 9: '99.32%'}\n",
      "Val Loss: 2.565758, Val Acc: 86.83%, Val-Class-Acc: {0: '87.50%', 2: '92.36%', 3: '97.13%', 4: '90.00%', 5: '94.63%', 6: '42.59%', 7: '83.20%', 8: '89.81%', 9: '48.65%'}, LR: 0.001000\n",
      "Epoch 156/200, Train Loss: 0.013414, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '99.90%', 4: '99.37%', 5: '99.78%', 6: '99.54%', 7: '99.20%', 8: '100.00%', 9: '99.32%'}\n",
      "Val Loss: 2.411476, Val Acc: 87.48%, Val-Class-Acc: {0: '90.22%', 2: '90.97%', 3: '96.31%', 4: '87.50%', 5: '96.72%', 6: '44.44%', 7: '85.60%', 8: '89.17%', 9: '43.24%'}, LR: 0.001000\n",
      "Epoch 157/200, Train Loss: 0.008094, Train-Class-Acc: {0: '100.00%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '99.85%', 6: '99.31%', 7: '98.80%', 8: '99.84%', 9: '98.65%'}\n",
      "Val Loss: 2.434035, Val Acc: 87.41%, Val-Class-Acc: {0: '85.87%', 2: '91.67%', 3: '96.31%', 4: '87.50%', 5: '97.31%', 6: '47.22%', 7: '84.80%', 8: '88.54%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 158/200, Train Loss: 0.011685, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '99.90%', 4: '100.00%', 5: '99.78%', 6: '99.31%', 7: '99.20%', 8: '99.20%', 9: '99.32%'}\n",
      "Val Loss: 2.683072, Val Acc: 86.61%, Val-Class-Acc: {0: '88.04%', 2: '89.58%', 3: '95.08%', 4: '85.00%', 5: '97.61%', 6: '37.96%', 7: '86.40%', 8: '87.90%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 159/200, Train Loss: 0.063514, Train-Class-Acc: {0: '99.18%', 2: '99.13%', 3: '99.18%', 4: '98.73%', 5: '99.55%', 6: '97.93%', 7: '98.00%', 8: '98.73%', 9: '97.30%'}\n",
      "Val Loss: 1.906754, Val Acc: 85.74%, Val-Class-Acc: {0: '85.33%', 2: '90.28%', 3: '95.49%', 4: '85.00%', 5: '93.73%', 6: '39.81%', 7: '84.80%', 8: '88.54%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 160/200, Train Loss: 0.017526, Train-Class-Acc: {0: '99.59%', 2: '99.83%', 3: '99.69%', 4: '100.00%', 5: '100.00%', 6: '98.16%', 7: '99.20%', 8: '99.20%', 9: '98.65%'}\n",
      "Val Loss: 2.021016, Val Acc: 87.19%, Val-Class-Acc: {0: '93.48%', 2: '93.75%', 3: '97.13%', 4: '85.00%', 5: '94.33%', 6: '41.67%', 7: '84.80%', 8: '86.62%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 161/200, Train Loss: 0.012094, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '100.00%', 5: '99.85%', 6: '98.39%', 7: '99.40%', 8: '99.68%', 4: '99.37%', 9: '99.32%'}\n",
      "Val Loss: 2.089927, Val Acc: 86.32%, Val-Class-Acc: {0: '85.33%', 2: '88.89%', 3: '95.90%', 4: '85.00%', 5: '95.82%', 6: '45.37%', 7: '81.60%', 8: '89.17%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 162/200, Train Loss: 0.007727, Train-Class-Acc: {0: '99.59%', 2: '99.65%', 3: '100.00%', 4: '100.00%', 5: '99.93%', 6: '99.54%', 7: '99.20%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 2.132877, Val Acc: 86.54%, Val-Class-Acc: {0: '85.87%', 2: '93.06%', 3: '96.72%', 4: '87.50%', 5: '94.33%', 6: '43.52%', 7: '85.60%', 8: '86.62%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 163/200, Train Loss: 0.014972, Train-Class-Acc: {0: '99.46%', 2: '100.00%', 3: '99.90%', 4: '99.37%', 5: '99.78%', 6: '98.39%', 7: '99.20%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 2.232752, Val Acc: 87.05%, Val-Class-Acc: {0: '90.22%', 2: '90.97%', 3: '95.08%', 4: '90.00%', 5: '95.52%', 6: '39.81%', 7: '84.80%', 8: '89.17%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 164/200, Train Loss: 0.014186, Train-Class-Acc: {0: '99.59%', 2: '99.83%', 3: '99.80%', 5: '99.93%', 6: '99.31%', 7: '98.80%', 8: '99.36%', 9: '98.65%', 4: '100.00%'}\n",
      "Val Loss: 2.088801, Val Acc: 87.34%, Val-Class-Acc: {0: '88.04%', 2: '89.58%', 3: '97.54%', 4: '85.00%', 5: '95.52%', 6: '37.96%', 7: '88.80%', 8: '88.54%', 9: '70.27%'}, LR: 0.001000\n",
      "Epoch 165/200, Train Loss: 0.008183, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '99.93%', 6: '99.08%', 7: '99.20%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 2.178595, Val Acc: 87.05%, Val-Class-Acc: {0: '82.07%', 2: '93.75%', 3: '97.13%', 4: '85.00%', 5: '95.82%', 6: '42.59%', 7: '84.80%', 8: '91.72%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 166/200, Train Loss: 0.007763, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '99.85%', 6: '98.85%', 7: '99.20%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 2.085066, Val Acc: 87.12%, Val-Class-Acc: {0: '87.50%', 2: '93.75%', 3: '97.54%', 4: '87.50%', 5: '95.82%', 6: '40.74%', 7: '85.60%', 8: '85.99%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 167/200, Train Loss: 0.003117, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '100.00%', 5: '100.00%', 6: '99.77%', 7: '100.00%', 8: '99.84%', 9: '100.00%', 4: '99.37%'}\n",
      "Val Loss: 2.270058, Val Acc: 87.41%, Val-Class-Acc: {0: '90.22%', 2: '93.75%', 3: '96.72%', 4: '87.50%', 5: '96.12%', 6: '38.89%', 7: '87.20%', 8: '85.35%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 168/200, Train Loss: 0.006593, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '99.93%', 6: '99.54%', 7: '99.80%', 8: '99.84%', 9: '98.65%'}\n",
      "Val Loss: 2.120840, Val Acc: 87.48%, Val-Class-Acc: {0: '89.13%', 2: '94.44%', 3: '95.90%', 4: '85.00%', 5: '94.03%', 6: '46.30%', 7: '84.80%', 8: '88.54%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 169/200, Train Loss: 0.003788, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '99.90%', 5: '99.85%', 6: '100.00%', 7: '99.80%', 8: '99.84%', 9: '99.32%', 4: '100.00%'}\n",
      "Val Loss: 2.306578, Val Acc: 87.34%, Val-Class-Acc: {0: '88.04%', 2: '92.36%', 3: '95.90%', 4: '87.50%', 5: '97.01%', 6: '40.74%', 7: '85.60%', 8: '85.99%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 170/200, Train Loss: 0.005847, Train-Class-Acc: {0: '100.00%', 2: '99.83%', 3: '100.00%', 4: '100.00%', 5: '100.00%', 6: '100.00%', 7: '99.40%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 2.156282, Val Acc: 86.61%, Val-Class-Acc: {0: '86.96%', 2: '93.75%', 3: '94.26%', 4: '87.50%', 5: '94.03%', 6: '37.96%', 7: '87.20%', 8: '91.72%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 171/200, Train Loss: 0.019015, Train-Class-Acc: {0: '99.86%', 2: '99.65%', 3: '99.69%', 4: '100.00%', 5: '99.93%', 6: '99.31%', 7: '99.20%', 8: '99.84%', 9: '100.00%'}\n",
      "Val Loss: 2.074336, Val Acc: 86.90%, Val-Class-Acc: {0: '90.76%', 2: '85.42%', 3: '96.31%', 4: '87.50%', 5: '95.82%', 6: '41.67%', 7: '84.80%', 8: '89.81%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 172/200, Train Loss: 0.040159, Train-Class-Acc: {0: '99.59%', 2: '99.83%', 3: '99.18%', 4: '99.37%', 5: '100.00%', 6: '98.85%', 7: '97.60%', 8: '98.73%', 9: '97.97%'}\n",
      "Val Loss: 2.441972, Val Acc: 86.17%, Val-Class-Acc: {0: '84.78%', 2: '92.36%', 3: '97.54%', 4: '90.00%', 5: '95.22%', 6: '39.81%', 7: '85.60%', 8: '86.62%', 9: '43.24%'}, LR: 0.001000\n",
      "Epoch 173/200, Train Loss: 0.039545, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '99.80%', 4: '98.10%', 5: '99.63%', 6: '97.70%', 7: '99.20%', 8: '98.41%', 9: '97.97%'}\n",
      "Val Loss: 3.041708, Val Acc: 86.39%, Val-Class-Acc: {0: '85.33%', 2: '85.42%', 3: '97.95%', 4: '85.00%', 5: '97.31%', 6: '48.15%', 7: '81.60%', 8: '85.99%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 174/200, Train Loss: 0.292314, Train-Class-Acc: {0: '97.41%', 2: '97.40%', 3: '98.98%', 4: '95.57%', 5: '98.88%', 6: '93.32%', 7: '94.01%', 8: '96.50%', 9: '91.89%'}\n",
      "Val Loss: 3.409877, Val Acc: 81.80%, Val-Class-Acc: {0: '83.15%', 2: '85.42%', 3: '97.13%', 4: '75.00%', 5: '82.99%', 6: '47.22%', 7: '80.80%', 8: '84.08%', 9: '51.35%'}, LR: 0.001000\n",
      "Epoch 175/200, Train Loss: 0.241405, Train-Class-Acc: {0: '96.87%', 2: '97.57%', 3: '98.67%', 4: '97.47%', 5: '96.86%', 6: '91.94%', 7: '94.21%', 8: '95.38%', 9: '93.24%'}\n",
      "Val Loss: 2.339350, Val Acc: 86.17%, Val-Class-Acc: {0: '90.76%', 2: '89.58%', 3: '97.13%', 4: '80.00%', 5: '96.42%', 6: '43.52%', 7: '78.40%', 8: '82.80%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 176/200, Train Loss: 0.064696, Train-Class-Acc: {0: '98.64%', 2: '99.65%', 3: '99.49%', 5: '99.40%', 6: '96.31%', 7: '97.21%', 8: '97.29%', 9: '95.27%', 4: '97.47%'}\n",
      "Val Loss: 2.200210, Val Acc: 87.19%, Val-Class-Acc: {0: '86.96%', 2: '91.67%', 3: '96.31%', 4: '85.00%', 5: '97.01%', 6: '41.67%', 7: '80.80%', 8: '91.72%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 177/200, Train Loss: 0.020777, Train-Class-Acc: {0: '99.18%', 2: '99.65%', 3: '99.80%', 4: '100.00%', 5: '99.93%', 6: '98.62%', 7: '98.00%', 8: '98.25%', 9: '99.32%'}\n",
      "Val Loss: 2.022540, Val Acc: 87.48%, Val-Class-Acc: {0: '90.22%', 2: '93.75%', 3: '96.72%', 4: '85.00%', 5: '94.33%', 6: '50.00%', 7: '82.40%', 8: '87.90%', 9: '54.05%'}, LR: 0.001000\n",
      "Epoch 178/200, Train Loss: 0.013945, Train-Class-Acc: {0: '99.46%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '99.93%', 6: '98.16%', 7: '99.40%', 8: '99.52%', 9: '97.30%'}\n",
      "Val Loss: 2.109375, Val Acc: 86.97%, Val-Class-Acc: {0: '88.59%', 2: '90.97%', 3: '96.72%', 4: '85.00%', 5: '94.03%', 6: '50.93%', 7: '84.00%', 8: '84.71%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 179/200, Train Loss: 0.030684, Train-Class-Acc: {0: '99.18%', 2: '99.65%', 3: '99.49%', 5: '99.70%', 6: '99.08%', 7: '99.00%', 8: '99.52%', 9: '100.00%', 4: '100.00%'}\n",
      "Val Loss: 1.962046, Val Acc: 86.17%, Val-Class-Acc: {0: '82.61%', 2: '92.36%', 3: '97.95%', 4: '87.50%', 5: '96.42%', 6: '46.30%', 7: '75.20%', 8: '87.26%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 180/200, Train Loss: 0.045101, Train-Class-Acc: {0: '99.59%', 2: '99.48%', 3: '99.59%', 4: '98.10%', 5: '99.70%', 6: '97.47%', 7: '98.60%', 8: '98.57%', 9: '96.62%'}\n",
      "Val Loss: 2.021493, Val Acc: 86.32%, Val-Class-Acc: {0: '79.35%', 2: '91.67%', 3: '96.31%', 4: '87.50%', 5: '95.52%', 6: '41.67%', 7: '81.60%', 8: '92.99%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 181/200, Train Loss: 0.023878, Train-Class-Acc: {0: '99.46%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '99.40%', 6: '99.54%', 7: '99.00%', 8: '98.73%', 9: '97.97%'}\n",
      "Val Loss: 2.225201, Val Acc: 87.05%, Val-Class-Acc: {0: '88.04%', 2: '93.06%', 3: '96.31%', 4: '85.00%', 5: '97.31%', 6: '37.96%', 7: '80.80%', 8: '90.45%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 182/200, Train Loss: 0.009324, Train-Class-Acc: {0: '99.46%', 2: '99.83%', 3: '100.00%', 4: '100.00%', 5: '99.85%', 6: '98.62%', 7: '99.20%', 8: '99.84%', 9: '100.00%'}\n",
      "Val Loss: 2.381627, Val Acc: 86.75%, Val-Class-Acc: {0: '88.59%', 2: '93.75%', 3: '96.31%', 4: '77.50%', 5: '95.82%', 6: '52.78%', 7: '81.60%', 8: '80.25%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 183/200, Train Loss: 0.016892, Train-Class-Acc: {0: '99.73%', 2: '99.65%', 3: '99.90%', 4: '98.73%', 5: '99.85%', 6: '99.54%', 7: '99.40%', 8: '99.52%', 9: '99.32%'}\n",
      "Val Loss: 2.179076, Val Acc: 87.12%, Val-Class-Acc: {0: '90.22%', 2: '90.97%', 3: '97.13%', 4: '87.50%', 5: '98.21%', 6: '37.96%', 7: '84.80%', 8: '81.53%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 184/200, Train Loss: 0.168442, Train-Class-Acc: {0: '98.91%', 3: '98.16%', 5: '98.73%', 6: '96.54%', 7: '97.21%', 8: '97.61%', 9: '96.62%', 2: '99.13%', 4: '99.37%'}\n",
      "Val Loss: 2.207316, Val Acc: 84.86%, Val-Class-Acc: {0: '76.09%', 2: '88.89%', 3: '94.67%', 4: '82.50%', 5: '95.82%', 6: '47.22%', 7: '83.20%', 8: '87.26%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 185/200, Train Loss: 0.155451, Train-Class-Acc: {0: '98.09%', 2: '98.27%', 3: '98.87%', 5: '98.95%', 6: '95.39%', 7: '96.61%', 8: '96.18%', 9: '93.92%', 4: '95.57%'}\n",
      "Val Loss: 2.158166, Val Acc: 85.74%, Val-Class-Acc: {0: '91.30%', 2: '88.19%', 3: '97.54%', 4: '80.00%', 5: '94.63%', 6: '37.96%', 7: '80.00%', 8: '82.80%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 186/200, Train Loss: 0.079109, Train-Class-Acc: {0: '98.23%', 2: '98.96%', 3: '99.49%', 4: '98.10%', 5: '99.63%', 6: '97.47%', 7: '95.21%', 8: '98.89%', 9: '95.95%'}\n",
      "Val Loss: 2.108734, Val Acc: 84.93%, Val-Class-Acc: {0: '86.96%', 2: '92.36%', 3: '93.44%', 4: '87.50%', 5: '94.33%', 6: '39.81%', 7: '82.40%', 8: '81.53%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 187/200, Train Loss: 0.039228, Train-Class-Acc: {0: '98.77%', 2: '99.48%', 3: '99.49%', 5: '99.63%', 7: '97.60%', 8: '98.09%', 9: '98.65%', 4: '99.37%', 6: '97.47%'}\n",
      "Val Loss: 2.121356, Val Acc: 87.63%, Val-Class-Acc: {0: '90.76%', 2: '91.67%', 3: '95.90%', 4: '85.00%', 5: '97.01%', 6: '50.00%', 7: '81.60%', 8: '88.54%', 9: '45.95%'}, LR: 0.001000\n",
      "Epoch 188/200, Train Loss: 0.019463, Train-Class-Acc: {0: '99.46%', 2: '99.48%', 3: '99.80%', 4: '99.37%', 5: '99.70%', 6: '98.62%', 7: '98.80%', 8: '99.04%', 9: '97.30%'}\n",
      "Val Loss: 2.035050, Val Acc: 86.90%, Val-Class-Acc: {0: '88.04%', 2: '88.89%', 3: '95.08%', 4: '80.00%', 5: '97.01%', 6: '48.15%', 7: '85.60%', 8: '85.35%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 189/200, Train Loss: 0.012483, Train-Class-Acc: {0: '99.46%', 2: '99.83%', 3: '99.69%', 4: '99.37%', 5: '99.85%', 6: '99.31%', 7: '99.60%', 8: '99.68%', 9: '98.65%'}\n",
      "Val Loss: 2.295397, Val Acc: 87.77%, Val-Class-Acc: {0: '91.30%', 2: '89.58%', 3: '96.31%', 4: '82.50%', 5: '97.31%', 6: '41.67%', 7: '85.60%', 8: '89.17%', 9: '62.16%'}, LR: 0.001000\n",
      "Epoch 190/200, Train Loss: 0.009898, Train-Class-Acc: {0: '99.86%', 2: '99.65%', 3: '99.90%', 4: '100.00%', 5: '99.93%', 6: '99.08%', 7: '99.00%', 8: '99.36%', 9: '100.00%'}\n",
      "Val Loss: 2.124658, Val Acc: 87.26%, Val-Class-Acc: {0: '86.41%', 2: '91.67%', 3: '97.13%', 4: '82.50%', 5: '95.22%', 6: '42.59%', 7: '85.60%', 8: '89.81%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 191/200, Train Loss: 0.009697, Train-Class-Acc: {0: '99.86%', 2: '99.83%', 3: '100.00%', 4: '100.00%', 5: '100.00%', 6: '99.31%', 7: '99.20%', 8: '99.52%', 9: '99.32%'}\n",
      "Val Loss: 2.278392, Val Acc: 87.19%, Val-Class-Acc: {0: '90.22%', 2: '88.19%', 3: '95.49%', 4: '82.50%', 5: '97.01%', 6: '40.74%', 7: '88.00%', 8: '86.62%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 192/200, Train Loss: 0.008690, Train-Class-Acc: {0: '99.73%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '100.00%', 6: '98.39%', 7: '99.60%', 8: '99.52%', 9: '100.00%'}\n",
      "Val Loss: 1.960237, Val Acc: 87.63%, Val-Class-Acc: {0: '87.50%', 2: '90.28%', 3: '95.90%', 4: '80.00%', 5: '96.12%', 6: '51.85%', 7: '84.80%', 8: '90.45%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 193/200, Train Loss: 0.011729, Train-Class-Acc: {0: '99.46%', 2: '99.83%', 3: '99.90%', 4: '100.00%', 5: '99.93%', 6: '99.77%', 7: '99.60%', 8: '99.52%', 9: '99.32%'}\n",
      "Val Loss: 2.102688, Val Acc: 87.26%, Val-Class-Acc: {0: '91.30%', 2: '93.06%', 3: '94.67%', 4: '85.00%', 5: '96.42%', 6: '40.74%', 7: '85.60%', 8: '86.62%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 194/200, Train Loss: 0.007380, Train-Class-Acc: {0: '100.00%', 2: '99.83%', 3: '99.90%', 5: '99.85%', 6: '99.54%', 7: '99.60%', 8: '99.84%', 9: '98.65%', 4: '100.00%'}\n",
      "Val Loss: 2.040882, Val Acc: 87.48%, Val-Class-Acc: {0: '89.67%', 2: '92.36%', 3: '96.72%', 4: '82.50%', 5: '96.72%', 6: '41.67%', 7: '81.60%', 8: '91.08%', 9: '56.76%'}, LR: 0.001000\n",
      "Epoch 195/200, Train Loss: 0.007637, Train-Class-Acc: {0: '99.86%', 2: '100.00%', 3: '100.00%', 4: '99.37%', 5: '99.93%', 6: '99.54%', 7: '99.60%', 8: '99.68%', 9: '98.65%'}\n",
      "Val Loss: 2.057828, Val Acc: 88.28%, Val-Class-Acc: {0: '93.48%', 2: '93.06%', 3: '97.54%', 4: '80.00%', 5: '95.52%', 6: '50.93%', 7: '80.80%', 8: '86.62%', 9: '67.57%'}, LR: 0.001000\n",
      "Epoch 196/200, Train Loss: 0.003016, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '100.00%', 5: '100.00%', 6: '99.54%', 7: '99.60%', 8: '99.84%', 9: '99.32%', 4: '100.00%'}\n",
      "Val Loss: 2.214068, Val Acc: 87.77%, Val-Class-Acc: {0: '89.67%', 2: '93.06%', 3: '97.54%', 4: '87.50%', 5: '95.22%', 6: '51.85%', 7: '83.20%', 8: '84.71%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 197/200, Train Loss: 0.002805, Train-Class-Acc: {0: '99.59%', 2: '100.00%', 3: '100.00%', 4: '100.00%', 5: '100.00%', 6: '99.77%', 7: '99.60%', 8: '99.84%', 9: '100.00%'}\n",
      "Val Loss: 2.227496, Val Acc: 87.55%, Val-Class-Acc: {0: '91.85%', 2: '93.06%', 3: '97.13%', 4: '85.00%', 5: '94.63%', 6: '41.67%', 7: '84.00%', 8: '89.17%', 9: '59.46%'}, LR: 0.001000\n",
      "Epoch 198/200, Train Loss: 0.003496, Train-Class-Acc: {0: '100.00%', 2: '100.00%', 3: '99.90%', 4: '99.37%', 5: '100.00%', 6: '99.77%', 7: '99.60%', 8: '99.68%', 9: '99.32%'}\n",
      "Val Loss: 2.101662, Val Acc: 87.41%, Val-Class-Acc: {0: '87.50%', 2: '93.06%', 3: '95.90%', 4: '82.50%', 5: '95.22%', 6: '45.37%', 7: '84.80%', 8: '87.26%', 9: '75.68%'}, LR: 0.001000\n",
      "Epoch 199/200, Train Loss: 0.010924, Train-Class-Acc: {0: '99.59%', 2: '99.83%', 3: '99.59%', 5: '100.00%', 6: '98.85%', 7: '99.80%', 8: '99.68%', 9: '99.32%', 4: '100.00%'}\n",
      "Val Loss: 2.212022, Val Acc: 87.63%, Val-Class-Acc: {0: '91.30%', 2: '93.06%', 3: '97.13%', 4: '82.50%', 5: '96.72%', 6: '42.59%', 7: '82.40%', 8: '85.99%', 9: '64.86%'}, LR: 0.001000\n",
      "Epoch 200/200, Train Loss: 0.022690, Train-Class-Acc: {0: '99.59%', 2: '99.31%', 3: '99.90%', 5: '99.93%', 6: '99.54%', 7: '99.00%', 8: '99.52%', 9: '97.97%', 4: '99.37%'}\n",
      "Val Loss: 2.605068, Val Acc: 87.05%, Val-Class-Acc: {0: '92.93%', 2: '92.36%', 3: '95.08%', 4: '85.00%', 5: '95.22%', 6: '52.78%', 7: '84.80%', 8: '80.89%', 9: '45.95%'}, LR: 0.001000\n",
      "\n",
      "üèÜ Best model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_best.pth (Val Accuracy: 89.23%)\n",
      "\n",
      "üìå Final model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_final.pth\n",
      "\n",
      "üéØ Top 5 Best Models:\n",
      "Epoch 19, Train Loss: 0.229003, Train-Acc: {0: '96.73%', 2: '98.44%', 3: '97.44%', 4: '95.57%', 5: '98.43%', 6: '86.41%', 7: '92.02%', 8: '93.47%', 9: '86.49%'},\n",
      "Val Loss: 1.519432, Val Acc: 89.23%, Val-Acc: {0: '89.67%', 2: '94.44%', 3: '97.95%', 4: '82.50%', 5: '97.91%', 6: '50.00%', 7: '87.20%', 8: '90.45%', 9: '54.05%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_19.pth\n",
      "Epoch 11, Train Loss: 0.352918, Train-Acc: {0: '96.05%', 2: '97.05%', 3: '98.26%', 4: '93.04%', 5: '97.83%', 6: '78.80%', 7: '84.03%', 8: '87.26%', 9: '83.78%'},\n",
      "Val Loss: 1.392777, Val Acc: 89.23%, Val-Acc: {0: '93.48%', 2: '95.14%', 3: '96.72%', 4: '82.50%', 5: '98.81%', 6: '54.63%', 7: '80.00%', 8: '87.26%', 9: '56.76%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_11.pth\n",
      "Epoch 25, Train Loss: 0.041318, Train-Acc: {0: '99.73%', 2: '99.31%', 3: '99.59%', 4: '97.47%', 5: '99.70%', 6: '95.39%', 7: '96.21%', 8: '97.13%', 9: '97.30%'},\n",
      "Val Loss: 1.608286, Val Acc: 89.16%, Val-Acc: {0: '88.59%', 2: '93.06%', 3: '97.54%', 4: '90.00%', 5: '97.61%', 6: '50.00%', 7: '87.20%', 8: '90.45%', 9: '59.46%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_25.pth\n",
      "Epoch 87, Train Loss: 0.011936, Train-Acc: {0: '99.32%', 2: '99.83%', 3: '99.80%', 4: '100.00%', 5: '99.85%', 6: '99.08%', 7: '99.80%', 8: '99.68%', 9: '98.65%'},\n",
      "Val Loss: 2.097097, Val Acc: 89.08%, Val-Acc: {0: '89.13%', 2: '93.75%', 3: '97.95%', 4: '85.00%', 5: '97.31%', 6: '55.56%', 7: '83.20%', 8: '86.62%', 9: '70.27%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_87.pth\n",
      "Epoch 8, Train Loss: 0.347046, Train-Acc: {0: '94.28%', 2: '95.49%', 3: '96.72%', 4: '89.87%', 5: '97.31%', 6: '69.59%', 7: '81.24%', 8: '86.78%', 9: '70.95%'},\n",
      "Val Loss: 1.116794, Val Acc: 89.01%, Val-Acc: {0: '92.39%', 2: '95.14%', 3: '97.54%', 4: '87.50%', 5: '97.31%', 6: '52.78%', 7: '84.80%', 8: '85.35%', 9: '54.05%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/ResNet18_1D_LoRA_epoch_8.pth\n",
      "\n",
      "üß† Model Summary:\n",
      "Total Parameters: 3,895,946\n",
      "Model Size (float32): 14.86 MB\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "Number of LoRA adapters: 8\n",
      "Number of LoRA groups: 1\n",
      "Total Training Time: 556.05 seconds\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "---\n",
      "### Period 4 (alpha = 0.0, similarity_threshold = 0.79)\n",
      "+ ##### Total training time: 556.05 seconds\n",
      "+ ##### Model: ResNet18_1D_LoRA\n",
      "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4'*\n",
      "+ ##### Best Epoch: 19\n",
      "#### __Val Accuracy: 89.23%__\n",
      "#### __Val-Class-Acc: {0: '89.67%', 2: '94.44%', 3: '97.95%', 4: '82.50%', 5: '97.91%', 6: '50.00%', 7: '87.20%', 8: '90.45%', 9: '54.05%'}__\n",
      "#### __Total Parameters: 3,895,946__\n",
      "#### __Model Size (float32): 14.86 MB__\n",
      "#### __Number of LoRA adapters: 8__\n",
      "#### __Number of LoRA groups: 1__\n",
      "\n",
      "Saved class features to: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v6/Period_4/class_features.pkl\n",
      "üßπ Cleaned up all training memory.\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå Period 4: DynEx-CLoRA Training (ECG)\n",
    "# ================================\n",
    "period = 4\n",
    "\n",
    "# ==== Paths ====\n",
    "stop_signal_file = os.path.join(BASE_DIR, \"stop_training.txt\")\n",
    "model_saving_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v6\", f\"Period_{period}\")\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# ==== Load Period 4 Data ====\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{period}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{period}.npy\"))\n",
    "X_val   = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val   = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "# ==== Device ====\n",
    "device = auto_select_cuda_device()\n",
    "\n",
    "# ==== Load Period 3 Features ====\n",
    "prev_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v3\", \"Period_3\")\n",
    "class_features_path = os.path.join(prev_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, \"rb\") as f:\n",
    "    class_features_dict = pickle.load(f)\n",
    "print(f\"‚úÖ Loaded class features from: {class_features_path}\")\n",
    "\n",
    "# ==== Load Period 3 Checkpoint ====\n",
    "prev_model_path = os.path.join(prev_folder, \"ResNet18_1D_LoRA_best.pth\")\n",
    "checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "\n",
    "input_channels = X_train.shape[2]\n",
    "output_size = int(np.max(y_train)) + 1  # e.g., max=9 ‚Üí output_size=10\n",
    "\n",
    "# ==== Build Teacher Model ====\n",
    "teacher_output_size = len(np.unique(np.load(os.path.join(save_dir, f\"y_train_p{period-1}.npy\"))))\n",
    "teacher_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=teacher_output_size).to(device)\n",
    "teacher_model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "teacher_model.eval()\n",
    "\n",
    "# ==== Build Student Model ====\n",
    "model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size).to(device)\n",
    "\n",
    "# ==== Sync Adapter Structure ====\n",
    "num_lora_groups = checkpoint.get(\"num_lora_groups\", 0)\n",
    "print(f\"üîÑ Number of LoRA groups: {num_lora_groups}\")\n",
    "related_labels = checkpoint.get(\"related_labels\", {\"base\": [0, 1]})\n",
    "for _ in range(num_lora_groups):\n",
    "    model.add_lora_adapter()\n",
    "\n",
    "# ==== Load Shared Weights (excluding FC) ====\n",
    "model_dict = model.state_dict()\n",
    "prev_state_dict = checkpoint[\"model_state_dict\"]\n",
    "filtered_dict = {\n",
    "    k: v for k, v in prev_state_dict.items()\n",
    "    if k in model_dict and model_dict[k].shape == v.shape and k not in [\"fc.weight\", \"fc.bias\"]\n",
    "}\n",
    "model.load_state_dict(filtered_dict, strict=False)\n",
    "for k in model_dict:\n",
    "    if k not in filtered_dict:\n",
    "        print(f\"üîç Not loaded: {k}, shape={model_dict[k].shape}\")\n",
    "print(\"‚úÖ Loaded shared weights from Period 3 (excluding FC only)\")\n",
    "\n",
    "# ==== Training Configuration ====\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "alpha = 0.0 # No distillation\n",
    "similarity_threshold = 0.79\n",
    "stable_classes = [0, 2, 3, 4, 5]  # ‚Üê Ë´ãÊ†πÊìö‰Ω†ÂâçÊúüÈ°ûÂà•Ë™øÊï¥ÔºÅ\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# ==== Start Training ====\n",
    "train_with_dynex_clora_ecg(\n",
    "    model=model,\n",
    "    teacher_model=None,\n",
    "    output_size=output_size,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    alpha=alpha,\n",
    "    model_saving_folder=model_saving_folder,\n",
    "    model_name=\"ResNet18_1D_LoRA\",\n",
    "    stop_signal_file=stop_signal_file,\n",
    "    scheduler=scheduler,\n",
    "    period=period,\n",
    "    stable_classes=stable_classes,\n",
    "    similarity_threshold=similarity_threshold,\n",
    "    class_features_dict=class_features_dict,\n",
    "    related_labels=related_labels,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ==== Cleanup ====\n",
    "del X_train, y_train, X_val, y_val\n",
    "del model, teacher_model, checkpoint, model_dict, prev_state_dict, filtered_dict\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856491bf",
   "metadata": {},
   "source": [
    "##  Compute FWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba030d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fwt_ecg(previous_model, init_model, X_val, y_val, known_classes, batch_size=64):\n",
    "    \"\"\"\n",
    "    FWT computation for ECG-style inputs with 1D CNN (e.g., ResNet18_1D).\n",
    "    X_val: shape [B, T, C]  (e.g., [N, 5000, 12])\n",
    "    y_val: shape [B]        (e.g., [N])\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    previous_model.to(device).eval()\n",
    "    init_model.to(device).eval()\n",
    "\n",
    "    # Âè™ÈÅ∏Âèñ known classes\n",
    "    mask = np.isin(y_val, known_classes)\n",
    "    X_known = X_val[mask]\n",
    "    y_known = y_val[mask]\n",
    "\n",
    "    if len(y_known) == 0:\n",
    "        print(f\"‚ö†Ô∏è No validation samples for known classes {known_classes}.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(f\"üìã Total samples for known classes {known_classes}: {len(y_known)}\")\n",
    "\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(X_known, dtype=torch.float32),\n",
    "        torch.tensor(y_known, dtype=torch.long)\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    correct_prev, correct_init, total = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            out_prev = previous_model(xb)  # [B, C]\n",
    "            out_init = init_model(xb)\n",
    "\n",
    "            preds_prev = torch.argmax(out_prev, dim=-1)\n",
    "            preds_init = torch.argmax(out_init, dim=-1)\n",
    "\n",
    "            correct_prev += (preds_prev == yb).sum().item()\n",
    "            correct_init += (preds_init == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "    acc_prev = 100 * correct_prev / total\n",
    "    acc_init = 100 * correct_init / total\n",
    "    fwt_value = acc_prev - acc_init\n",
    "\n",
    "    print(f\"\\n### üîç FWT Debug Info:\")\n",
    "    print(f\"- Total evaluated samples: {total}\")\n",
    "    print(f\"- Accuracy by previous model: {acc_prev:.2f}%\")\n",
    "    print(f\"- Accuracy by init model:     {acc_init:.2f}%\")\n",
    "    print(f\"- FWT = Acc_prev - Acc_init = {fwt_value:.2f}%\")\n",
    "\n",
    "    return fwt_value, acc_prev, acc_init\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0679b6",
   "metadata": {},
   "source": [
    "### Period 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e21f5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 0\n",
      "    - Memory Used    : 21086 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ Loaded frozen LoRA model with 0 adapter group(s)\n",
      "üìã Total samples for known classes [0, 1]: 428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3955042/1579125195.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_model_path, map_location=device)\n",
      "/tmp/ipykernel_3955042/1383618855.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_known, dtype=torch.float32),\n",
      "/tmp/ipykernel_3955042/1383618855.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_known, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### üîç FWT Debug Info:\n",
      "- Total evaluated samples: 428\n",
      "- Accuracy by previous model: 89.25%\n",
      "- Accuracy by init model:     42.99%\n",
      "- FWT = Acc_prev - Acc_init = 46.26%\n",
      "\n",
      "### Period 2:\n",
      "- FWT (DynEx-CLoRA ECG Period 2, old classes [0, 1]): 46.26%\n",
      "- Accuracy by previous model: 89.25%\n",
      "- Accuracy by init model:     42.99%\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå FWT - Period 2 (DynEx-CLoRA - ECG)\n",
    "# ================================\n",
    "period = 2\n",
    "\n",
    "# === Load Validation Data ===\n",
    "X_val = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "device = auto_select_cuda_device()\n",
    "input_channels = X_val.shape[2]\n",
    "output_size_prev = 2\n",
    "known_classes = [0, 1]\n",
    "\n",
    "# === Load Period 1 best model with LoRA group info ===\n",
    "prev_model_path = os.path.join(BASE_DIR, \"ResNet18_Selection\", \"ResNet18_big_inplane_v1\", \"ResNet18_big_inplane_1D_best.pth\")\n",
    "checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "prev_state_dict = checkpoint[\"model_state_dict\"]\n",
    "num_lora_groups = checkpoint.get(\"num_lora_groups\", 0)\n",
    "\n",
    "# === Build frozen_model (previous) ===\n",
    "frozen_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size_prev).to(device)\n",
    "for _ in range(num_lora_groups):\n",
    "    frozen_model.add_lora_adapter()\n",
    "frozen_model.load_state_dict(prev_state_dict, strict=False)\n",
    "frozen_model.eval()\n",
    "frozen_model.output_size = output_size_prev\n",
    "print(f\"‚úÖ Loaded frozen LoRA model with {num_lora_groups} adapter group(s)\")\n",
    "\n",
    "# === Build init_model (untrained) ===\n",
    "init_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size_prev).to(device)\n",
    "for _ in range(num_lora_groups):\n",
    "    init_model.add_lora_adapter()\n",
    "init_model.output_size = output_size_prev\n",
    "\n",
    "# === Tensor Conversion ===\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# === Run FWT Evaluation ===\n",
    "fwt, acc_prev, acc_init = compute_fwt_ecg(frozen_model, init_model, X_val_tensor, y_val_tensor, known_classes)\n",
    "\n",
    "print(f\"\\n### Period 2:\")\n",
    "print(f\"- FWT (DynEx-CLoRA ECG Period 2, old classes {known_classes}): {fwt:.2f}%\")\n",
    "print(f\"- Accuracy by previous model: {acc_prev:.2f}%\")\n",
    "print(f\"- Accuracy by init model:     {acc_init:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e9044d",
   "metadata": {},
   "source": [
    "### Period 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e547015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 0\n",
      "    - Memory Used    : 9406 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n",
      "‚úÖ Loaded frozen LoRA model with 1 adapter group(s)\n",
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n",
      "üìã Total samples for known classes [0, 1, 2, 3]: 907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3955042/2779051602.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_model_path, map_location=device)\n",
      "/tmp/ipykernel_3955042/1383618855.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_known, dtype=torch.float32),\n",
      "/tmp/ipykernel_3955042/1383618855.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_known, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### üîç FWT Debug Info:\n",
      "- Total evaluated samples: 907\n",
      "- Accuracy by previous model: 86.55%\n",
      "- Accuracy by init model:     20.29%\n",
      "- FWT = Acc_prev - Acc_init = 66.26%\n",
      "\n",
      "### Period 3:\n",
      "- FWT (DynEx-CLoRA ECG Period 3, old classes [0, 1, 2, 3]): 66.26%\n",
      "- Accuracy by previous model: 86.55%\n",
      "- Accuracy by init model:     20.29%\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå FWT - Period 3 (DynEx-CLoRA - ECG)\n",
    "# ================================\n",
    "period = 3\n",
    "\n",
    "# === Load Validation Data ===\n",
    "X_val = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "device = auto_select_cuda_device()\n",
    "input_channels = X_val.shape[2]\n",
    "output_size_prev = 4 \n",
    "known_classes = [0, 1, 2, 3]\n",
    "\n",
    "# === Load Period 2 checkpoint ===\n",
    "prev_model_path = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v3\", \"Period_2\", \"ResNet18_1D_LoRA_best.pth\")\n",
    "checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "prev_state_dict = checkpoint[\"model_state_dict\"]\n",
    "num_lora_groups = checkpoint.get(\"num_lora_groups\", 0)\n",
    "\n",
    "# === Build frozen_model (previous) ===\n",
    "frozen_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size_prev).to(device)\n",
    "for _ in range(num_lora_groups):\n",
    "    frozen_model.add_lora_adapter()\n",
    "frozen_model.load_state_dict(prev_state_dict, strict=False)\n",
    "frozen_model.eval()\n",
    "frozen_model.output_size = output_size_prev\n",
    "print(f\"‚úÖ Loaded frozen LoRA model with {num_lora_groups} adapter group(s)\")\n",
    "\n",
    "# === Build init_model (untrained) ===\n",
    "init_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size_prev).to(device)\n",
    "for _ in range(num_lora_groups):\n",
    "    init_model.add_lora_adapter()\n",
    "init_model.output_size = output_size_prev\n",
    "\n",
    "# === Tensor Conversion ===\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# === Run FWT Evaluation ===\n",
    "fwt, acc_prev, acc_init = compute_fwt_ecg(frozen_model, init_model, X_val_tensor, y_val_tensor, known_classes)\n",
    "\n",
    "print(f\"\\n### Period 3:\")\n",
    "print(f\"- FWT (DynEx-CLoRA ECG Period 3, old classes {known_classes}): {fwt:.2f}%\")\n",
    "print(f\"- Accuracy by previous model: {acc_prev:.2f}%\")\n",
    "print(f\"- Accuracy by init model:     {acc_init:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a03350",
   "metadata": {},
   "source": [
    "### Period 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1de3cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 0\n",
      "    - Memory Used    : 9538 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3955042/808076442.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n",
      "‚úÖ Loaded frozen LoRA model with 1 adapter group(s)\n",
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n",
      "üìã Total samples for known classes [0, 1, 2, 3, 4, 5]: 947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3955042/1383618855.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_known, dtype=torch.float32),\n",
      "/tmp/ipykernel_3955042/1383618855.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_known, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### üîç FWT Debug Info:\n",
      "- Total evaluated samples: 947\n",
      "- Accuracy by previous model: 97.36%\n",
      "- Accuracy by init model:     22.81%\n",
      "- FWT = Acc_prev - Acc_init = 74.55%\n",
      "\n",
      "### Period 4:\n",
      "- FWT (DynEx-CLoRA ECG Period 4, old classes [0, 1, 2, 3, 4, 5]): 75%\n",
      "- Accuracy by previous model: 97%\n",
      "- Accuracy by init model:     23%\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå FWT - Period 4 (DynEx-CLoRA - ECG)\n",
    "# ================================\n",
    "period = 4\n",
    "\n",
    "# === Load Validation Data ===\n",
    "X_val = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "device = auto_select_cuda_device()\n",
    "input_channels = X_val.shape[2]\n",
    "output_size_prev = 6\n",
    "known_classes = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# === Load Period 3 checkpoint ===\n",
    "prev_model_path = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v3\", \"Period_3\", \"ResNet18_1D_LoRA_best.pth\")\n",
    "checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "prev_state_dict = checkpoint[\"model_state_dict\"]\n",
    "num_lora_groups = checkpoint.get(\"num_lora_groups\", 0)\n",
    "\n",
    "# === Build frozen_model (previous) ===\n",
    "frozen_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size_prev).to(device)\n",
    "for _ in range(num_lora_groups):\n",
    "    frozen_model.add_lora_adapter()\n",
    "frozen_model.load_state_dict(prev_state_dict, strict=False)\n",
    "frozen_model.eval()\n",
    "frozen_model.output_size = output_size_prev\n",
    "print(f\"‚úÖ Loaded frozen LoRA model with {num_lora_groups} adapter group(s)\")\n",
    "\n",
    "# === Build init_model (untrained) ===\n",
    "init_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size_prev).to(device)\n",
    "for _ in range(num_lora_groups):\n",
    "    init_model.add_lora_adapter()\n",
    "init_model.output_size = output_size_prev\n",
    "\n",
    "# === Tensor Conversion ===\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# === Run FWT Evaluation ===\n",
    "fwt, acc_prev, acc_init = compute_fwt_ecg(frozen_model, init_model, X_val_tensor, y_val_tensor, known_classes)\n",
    "\n",
    "print(f\"\\n### Period 4:\")\n",
    "print(f\"- FWT (DynEx-CLoRA ECG Period 4, old classes {known_classes}): {fwt:.0f}%\")\n",
    "print(f\"- Accuracy by previous model: {acc_prev:.0f}%\")\n",
    "print(f\"- Accuracy by init model:     {acc_init:.0f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4631cbbd",
   "metadata": {},
   "source": [
    "## üìä Summary: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f8faad",
   "metadata": {},
   "source": [
    "### ‚úîÔ∏è CPSC - DynEx-CLoRA: Validation Summary\n",
    "\n",
    "| Period | Training Time (s) | Validation Accuracy | Class-wise Accuracy                                                                 |\n",
    "|--------|-------------------|---------------------|--------------------------------------------------------------------------------------|\n",
    "| 1      | 134.66            | **88.86%**          | {0: 91.85%, 1: 85.87%}                                                              |\n",
    "| 2      | 323.71            | **88.97%**          | {0: 90.22%, 1: 81.15%, 2: 90.28%, 3: 95.08%}                                        |\n",
    "| 3      | 507.01            | **88.68%**          | {0: 82.61%, 1: 83.58%, 2: 89.58%, 3: 94.67%, 4: 82.50%, 5: 93.11%}                  |\n",
    "| 4      | 574.52            | **89.37%**          | {0: 89.67%, 2: 94.44%, 3: 97.95%, 4: 87.50%, 5: 98.81%, 6: 44.44%, 7: 88.80%, 8: 86.62%, 9: 72.97%} |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d645d4cc",
   "metadata": {},
   "source": [
    "### üß† Continual Learning Metrics\n",
    "\n",
    "| Period | AA_old (%) | AA_new (%) | BWT (%) | FWT (%) | FWT Classes        | Prev. Model Acc | Init Model Acc |\n",
    "|--------|------------|------------|---------|---------|---------------------|------------------|-----------------|\n",
    "| 2      | 85.69%     | 92.68%     | -3.17%  | 46.26%  | [0, 1]              | 89.25%           | 42.99%          |\n",
    "| 3      | 87.61%     | 87.81%     | -1.57%  | 66.26%  | [0, 1, 2, 3]        | 86.55%           | 20.29%          |\n",
    "| 4      | 93.67%     | 73.21%     | +5.18%  | 74.55%  | [0, 1, 2, 3, 4, 5]  | 97.36%           | 22.81%          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf44f62a",
   "metadata": {},
   "source": [
    "### üì¶ Model Size per Period\n",
    "\n",
    "| Period | Output Size | LoRA Added? | Total Params | Œî Params vs Prev | Œî % vs Prev | Model Size (float32) |\n",
    "|--------|-------------|-------------|--------------|------------------|-------------|-----------------------|\n",
    "| 1      | 2           | ‚úò No        | 3,857,026    | ‚Äî                | ‚Äî           | 14.71 MB              |\n",
    "| 2      | 4           | ‚úî Yes       | 3,889,796    | +32,770          | +0.85%      | 14.84 MB              |\n",
    "| 3      | 6           | ‚úò No        | 3,891,846    | +2,050           | +0.05%      | 14.85 MB              |\n",
    "| 4      | 10          | ‚úò No        | 3,895,946    | +4,100           | +0.11%      | 14.86 MB              |\n",
    "\n",
    "**üìà Model Growth Rate (MGR) = (3,895,946 - 3,857,026) / (3,857,026 √ó 3) ‚âà +0.34%**\n",
    "\n",
    "**üìà Max trainable ratio = 54.67%**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
