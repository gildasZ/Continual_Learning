{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07af288c",
   "metadata": {},
   "source": [
    "## __Check first before starting__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f00b1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /mnt/mydisk/Continual_Learning_JL/Continual_Learning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "Working_directory = os.path.normpath(\"/mnt/mydisk/Continual_Learning_JL/Continual_Learning/\")\n",
    "os.chdir(Working_directory)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97bdf56",
   "metadata": {},
   "source": [
    "## __All imports__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83b85d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating system and file management\n",
    "import os\n",
    "import shutil\n",
    "import contextlib\n",
    "import traceback\n",
    "import gc\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import subprocess\n",
    "import time\n",
    "import re, pickle\n",
    "import scipy.io\n",
    "from scipy.io import loadmat\n",
    "from glob import glob\n",
    "from math import ceil\n",
    "\n",
    "# Jupyter notebook widgets and display\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_interactions import zoom_factory, panhandler\n",
    "\n",
    "# Machine learning and preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from ta import trend, momentum, volatility, volume\n",
    "\n",
    "# Mathematical and scientific computing\n",
    "import math\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Type hinting\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "# Deep learning with PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a496fe70",
   "metadata": {},
   "source": [
    "## __üìÅ Path Settings and Constants__\n",
    "This cell defines essential paths and constants for the CPSC2018 ECG dataset processing:\n",
    "- `BASE_DIR`: Root directory of the project.\n",
    "- `save_dir`: Path to the preprocessed `.npy` files (one for each continual learning period).\n",
    "- `ECG_PATH`: Directory containing original `.mat` and `.hea` files.\n",
    "- `MAX_LEN`: Length of each ECG sample, fixed to 5000 time steps (i.e., 10 seconds at 500Hz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7748e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL\"\n",
    "save_dir = os.path.join(BASE_DIR, \"processed\")\n",
    "ECG_PATH = os.path.join(BASE_DIR, \"datas\")\n",
    "MAX_LEN = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba7249f",
   "metadata": {},
   "source": [
    "## __üè∑Ô∏è Label Mapping and Period Configuration__\n",
    "\n",
    "This section defines:\n",
    "- `snomed_map`: Mapping from SNOMED CT codes to readable class names for 9 major ECG conditions.\n",
    "- `period_label_map`: Incremental learning task structure across four periods.  \n",
    "  Class `1` is reserved for \"OTHER\" abnormalities until Period 4 when all 9 classes are explicitly categorized.\n",
    "- `print_class_distribution()`: Helper function to show class-wise data distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "103ca271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNOMED CT to readable names\n",
    "snomed_map = {\n",
    "    \"426783006\": \"NSR\",    # Ê≠£Â∏∏Á´áÊÄßÂøÉÂæã\n",
    "    \"270492004\": \"I-AVB\",  # ‰∏ÄÂ∫¶ÊàøÂÆ§ÂÇ≥Â∞éÈòªÊªØ\n",
    "    \"164889003\": \"AF\",     # ÂøÉÊàøÁ∫ñÁ∂≠È°´Âãï\n",
    "    \"164909002\": \"LBBB\",   # Â∑¶ÊùüÊîØÂÇ≥Â∞éÈòªÊªØ\n",
    "    \"59118001\":  \"RBBB\",   # Âè≥ÊùüÊîØÂÇ≥Â∞éÈòªÊªØ\n",
    "    \"284470004\": \"PAC\",    # ÂøÉÊàøÊó©ÊúüÊêèÂãï\n",
    "    \"164884008\": \"PVC\",    # ÂÆ§ÊÄßÊó©ÊúüÊêèÂãï\n",
    "    \"429622005\": \"STD\",    # ST ÊÆµÂ£ì‰Ωé\n",
    "    \"164931005\": \"STE\"     # ST ÊÆµÊä¨È´ò\n",
    "}\n",
    "\n",
    "# Period class mapping (Âõ∫ÂÆö class 1 ÊòØ„ÄåÂÖ∂‰ªñÁï∞Â∏∏„ÄçÁõ¥Âà∞ P4 ÁßªÈô§)\n",
    "period_label_map = {\n",
    "    1: {\"NSR\": 0, \"OTHER\": 1},\n",
    "    2: {\"NSR\": 0, \"I-AVB\": 2, \"AF\": 3, \"OTHER\": 1},\n",
    "    3: {\"NSR\": 0, \"I-AVB\": 2, \"AF\": 3, \"LBBB\": 4, \"RBBB\": 5, \"OTHER\": 1},\n",
    "    4: {\"NSR\": 0, \"I-AVB\": 2, \"AF\": 3, \"LBBB\": 4, \"RBBB\": 5, \"PAC\": 6, \"PVC\": 7, \"STD\": 8, \"STE\": 9}\n",
    "}\n",
    "\n",
    "def print_class_distribution(y, label_map):\n",
    "    y = np.array(y).flatten()\n",
    "    total = len(y)\n",
    "    all_labels = sorted(label_map.values())\n",
    "    print(\"\\nüìä Class Distribution\")\n",
    "    for lbl in all_labels:\n",
    "        count = np.sum(y == lbl)\n",
    "        label = [k for k, v in label_map.items() if v == lbl]\n",
    "        name = label[0] if label else str(lbl)\n",
    "        print(f\"  ‚îú‚îÄ Label {lbl:<2} ({name:<10}) ‚Üí {count:>5} samples ({(count/total)*100:5.2f}%)\")\n",
    "\n",
    "def ensure_folder(folder_path: str) -> None:\n",
    "    \"\"\"Ensure the given folder exists, create it if not.\"\"\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f95af3",
   "metadata": {},
   "source": [
    "## üì¶ EX. Load Example (Period 4) Data and View Format\n",
    "\n",
    "This example demonstrates how to load preprocessed `.npy` data for **Period 4**, and inspect the dataset shapes and label distribution.  \n",
    "Use this format as a reference when loading data in other methods (e.g., EWC, PNN, DynEx-CLoRA).\n",
    "\n",
    "Each ECG sample:\n",
    "- Has shape `(5000, 12)` ‚Üí represents 10 seconds (at 500Hz) across 12-lead channels.\n",
    "- Corresponding label is an integer ID (e.g., 0‚Äì9) defined by `period_label_map[4]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a401dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ÁØÑ‰æã:ËºâÂÖ• period 4\n",
    "# save_dir = os.path.join(BASE_DIR, \"processed\")\n",
    "# X_train = np.load(os.path.join(save_dir, \"X_train_p4.npy\"))\n",
    "# y_train = np.load(os.path.join(save_dir, \"y_train_p4.npy\"))\n",
    "# X_test = np.load(os.path.join(save_dir, \"X_test_p4.npy\"))\n",
    "# y_test = np.load(os.path.join(save_dir, \"y_test_p4.npy\"))\n",
    "\n",
    "# print(\"‚úÖ Loaded\")\n",
    "# print(\"X_train shape:\", X_train.shape)\n",
    "# print(\"y_train shape:\", y_train.shape)\n",
    "# print(\"X_test shape:\", X_test.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)\n",
    "# print_class_distribution(y_train, period_label_map[4])\n",
    "# print_class_distribution(y_test, period_label_map[4])\n",
    "\n",
    "# del X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d4b54b",
   "metadata": {},
   "source": [
    "## __Check GPU, CUDA, Pytorch__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50a987a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 10 15:56:30 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               Off |   00000000:2A:00.0 Off |                  Off |\n",
      "| 30%   47C    P2             80W /  300W |    3281MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               Off |   00000000:3D:00.0 Off |                  Off |\n",
      "| 30%   51C    P2             97W /  300W |   45329MiB /  49140MiB |    100%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000               Off |   00000000:AB:00.0 Off |                  Off |\n",
      "| 30%   45C    P2             96W /  300W |   45299MiB /  49140MiB |    100%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2845      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    0   N/A  N/A          114062      C   ...onda3/envs/CIL_env/bin/python        562MiB |\n",
      "|    0   N/A  N/A         2314326      C   python                                  766MiB |\n",
      "|    0   N/A  N/A         2514160      C   python                                  640MiB |\n",
      "|    0   N/A  N/A         4147895      C   ...onda3/envs/CIL_env/bin/python        660MiB |\n",
      "|    0   N/A  N/A         4159827      C   ...onda3/envs/CIL_env/bin/python        608MiB |\n",
      "|    1   N/A  N/A            2845      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    1   N/A  N/A           48977      C   python                                44814MiB |\n",
      "|    1   N/A  N/A         4147895      C   ...onda3/envs/CIL_env/bin/python        370MiB |\n",
      "|    2   N/A  N/A            2845      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    2   N/A  N/A           49785      C   python                                44812MiB |\n",
      "|    2   N/A  N/A         4147895      C   ...onda3/envs/CIL_env/bin/python        342MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f78325",
   "metadata": {},
   "source": [
    "### CUDA Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00875f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "             GPU Configuration Check              \n",
      "==================================================\n",
      "PyTorch Version          : 2.5.1\n",
      "GPU Available            : Yes\n",
      "--------------------------------------------------\n",
      "                   GPU Details                    \n",
      "--------------------------------------------------\n",
      "Device Name              : NVIDIA RTX A6000\n",
      "Number of GPUs           : 3\n",
      "Current Device Index     : 0\n",
      "Compute Capability       : 8.6\n",
      "Total CUDA Cores         : 10752\n",
      "Total Memory (GB)        : 47.41\n",
      "Allocated Memory (GB)    : 0.00\n",
      "Reserved Memory (GB)     : 0.00\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def check_gpu_config():\n",
    "    \"\"\"\n",
    "    Check GPU availability and display detailed configuration information.\n",
    "    \"\"\"\n",
    "    # Check if GPU is available\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    \n",
    "    # Print header\n",
    "    print(\"=\" * 50)\n",
    "    print(\"GPU Configuration Check\".center(50))\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic GPU availability\n",
    "    print(f\"{'PyTorch Version':<25}: {torch.__version__}\")\n",
    "    print(f\"{'GPU Available':<25}: {'Yes' if gpu_available else 'No'}\")\n",
    "    \n",
    "    # If GPU is available, print detailed info\n",
    "    if gpu_available:\n",
    "        print(\"-\" * 50)\n",
    "        print(\"GPU Details\".center(50))\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Device info\n",
    "        print(f\"{'Device Name':<25}: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"{'Number of GPUs':<25}: {torch.cuda.device_count()}\")\n",
    "        print(f\"{'Current Device Index':<25}: {torch.cuda.current_device()}\")\n",
    "        \n",
    "        # Compute capability and CUDA cores\n",
    "        props = torch.cuda.get_device_properties(0)\n",
    "        print(f\"{'Compute Capability':<25}: {props.major}.{props.minor}\")\n",
    "        print(f\"{'Total CUDA Cores':<25}: {props.multi_processor_count * 128}\")  # Approx. 128 cores per SM\n",
    "        \n",
    "        # Memory info\n",
    "        total_memory = props.total_memory / (1024 ** 3)  # Convert to GB\n",
    "        memory_allocated = torch.cuda.memory_allocated(0) / (1024 ** 3)\n",
    "        memory_reserved = torch.cuda.memory_reserved(0) / (1024 ** 3)\n",
    "        print(f\"{'Total Memory (GB)':<25}: {total_memory:.2f}\")\n",
    "        print(f\"{'Allocated Memory (GB)':<25}: {memory_allocated:.2f}\")\n",
    "        print(f\"{'Reserved Memory (GB)':<25}: {memory_reserved:.2f}\")\n",
    "    else:\n",
    "        print(\"-\" * 50)\n",
    "        print(\"No GPU detected. Running on CPU.\".center(50))\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_gpu_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a868ff5",
   "metadata": {},
   "source": [
    "### PyTorch Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d153a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "              PyTorch Configuration               \n",
      "==================================================\n",
      "PyTorch Version          : 2.5.1\n",
      "CUDA Compiled Version    : 12.1\n",
      "CUDA Available           : Yes\n",
      "Number of GPUs           : 3\n",
      "GPU Name                 : NVIDIA RTX A6000\n",
      "--------------------------------------------------\n",
      "Random Seed              : 42 (Seeding successful!)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def print_torch_config():\n",
    "    \"\"\"Print PyTorch and CUDA configuration in a formatted manner.\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"PyTorch Configuration\".center(50))\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic PyTorch and CUDA info\n",
    "    print(f\"{'PyTorch Version':<25}: {torch.__version__}\")\n",
    "    print(f\"{'CUDA Compiled Version':<25}: {torch.version.cuda}\")\n",
    "    print(f\"{'CUDA Available':<25}: {'Yes' if torch.cuda.is_available() else 'No'}\")\n",
    "    print(f\"{'Number of GPUs':<25}: {torch.cuda.device_count()}\")\n",
    "\n",
    "    # GPU details if available\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"{'GPU Name':<25}: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Seed setting\n",
    "    torch.manual_seed(42)\n",
    "    print(f\"{'Random Seed':<25}: 42 (Seeding successful!)\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print_torch_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbab549",
   "metadata": {},
   "source": [
    "## __‚öôÔ∏è GPU Selection ‚Äî Auto-select the least loaded GPU__\n",
    "This code automatically scans available GPUs and selects the one with the lowest current memory usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4ffdd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 0\n",
      "    - Memory Used    : 3281 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "def auto_select_cuda_device(verbose=True):\n",
    "    \"\"\"\n",
    "    Automatically selects the CUDA GPU with the least memory usage.\n",
    "    Falls back to CPU if no GPU is available.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"üö´ No CUDA GPU available. Using CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "    try:\n",
    "        # Run nvidia-smi to get memory usage of each GPU\n",
    "        smi_output = subprocess.check_output(\n",
    "            ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'],\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        memory_used = [int(x) for x in smi_output.strip().split('\\n')]\n",
    "        best_gpu = int(np.argmin(memory_used))\n",
    "\n",
    "        if verbose:\n",
    "            print(\"üéØ Automatically selected GPU:\")\n",
    "            print(f\"    - CUDA Device ID : {best_gpu}\")\n",
    "            print(f\"    - Memory Used    : {memory_used[best_gpu]} MiB\")\n",
    "            print(f\"    - Device Name    : {torch.cuda.get_device_name(best_gpu)}\")\n",
    "        return torch.device(f\"cuda:{best_gpu}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to auto-detect GPU. Falling back to cuda:0. ({e})\")\n",
    "        return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Execute and assign\n",
    "device = auto_select_cuda_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae7c5a",
   "metadata": {},
   "source": [
    "## __Model__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b4707e",
   "metadata": {},
   "source": [
    "### ResNet 18 - 1D (ResNet18_1D_big_inplane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cab8843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAConv1d(nn.Module):\n",
    "    def __init__(self, conv_layer: nn.Conv1d, rank: int):\n",
    "        super(LoRAConv1d, self).__init__()\n",
    "        self.conv = conv_layer\n",
    "        self.rank = rank\n",
    "        # ÁÇ∫ÈÅ©ÊáâLoRA‰ΩéÁß©ÂàÜËß£ÂâµÂª∫AÂíåBÁü©Èô£\n",
    "        self.lora_A = nn.Parameter(torch.zeros(conv_layer.out_channels, rank))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, conv_layer.in_channels * conv_layer.kernel_size[0]))\n",
    "        # ÂàùÂßãÂåñÊ¨äÈáçÔºöAÁî®Ê≠£ÊÖãÂàÜ‰ΩàÔºåBÁî®Èõ∂ÂàùÂßãÂåñ‰ª•Á¢∫‰øùË®ìÁ∑¥ÈñãÂßãÊôÇLoRAÁÑ°ÂΩ±Èüø\n",
    "        nn.init.normal_(self.lora_A, mean=0.0, std=0.01)\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ë®àÁÆóLoRAÊ¨äÈáç‰∏¶ÈáçÂ°ëÁÇ∫Âç∑Á©çÊ†∏ÂΩ¢ÁãÄ\n",
    "        lora_weight = torch.matmul(self.lora_A, self.lora_B).view(\n",
    "            self.conv.out_channels, self.conv.in_channels, self.conv.kernel_size[0]\n",
    "        )\n",
    "        # Â∞áLoRAÊ¨äÈáçÊ∑ªÂä†Âà∞ÂéüÂßãÊ¨äÈáç\n",
    "        adapted_weight = self.conv.weight + lora_weight\n",
    "        # ‰ΩøÁî®‰øÆÊîπÂæåÁöÑÊ¨äÈáçÂü∑Ë°åÂç∑Á©ç\n",
    "        return F.conv1d(x, adapted_weight, bias=self.conv.bias,\n",
    "                        stride=self.conv.stride, padding=self.conv.padding,\n",
    "                        dilation=self.conv.dilation, groups=self.conv.groups)\n",
    "                        \n",
    "    def parameters(self, recurse=True):\n",
    "        # Âè™ËøîÂõûLoRAÂèÉÊï∏Ôºå‰∏çÂåÖÊã¨ÂéüÂßãÂç∑Á©çÂ±§ÂèÉÊï∏\n",
    "        return [self.lora_A, self.lora_B]\n",
    "\n",
    "class BasicBlock1d_LoRA(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, lora_rank=None):\n",
    "        super(BasicBlock1d_LoRA, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "        self.lora_rank = lora_rank\n",
    "        self.lora_adapter = None  # Âª∂ÈÅ≤ÂàùÂßãÂåñLoRAÈÅ©ÈÖçÂô®\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        if self.lora_adapter is not None:\n",
    "            # Â¶ÇÊûúLoRAÈÅ©ÈÖçÂô®Â≠òÂú®Ôºå‰ΩøÁî®ÂÆÉËôïÁêÜ\n",
    "            out = self.lora_adapter(out)\n",
    "        else:\n",
    "            # Âê¶Ââá‰ΩøÁî®ÂéüÂßãconv2\n",
    "            out = self.conv2(out)\n",
    "\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18_1D_LoRA(nn.Module):\n",
    "    def __init__(self, input_channels=12, output_size=9, inplanes=64, lora_rank=4):\n",
    "        super(ResNet18_1D_LoRA, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.lora_rank = lora_rank\n",
    "\n",
    "        # ÂàùÂßãÂç∑Á©çÂ±§\n",
    "        self.conv1 = nn.Conv1d(input_channels, inplanes, kernel_size=15, stride=2, padding=7, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ÊÆòÂ∑ÆÂ±§\n",
    "        self.layer1 = self._make_layer(BasicBlock1d_LoRA, 64, 2)\n",
    "        self.layer2 = self._make_layer(BasicBlock1d_LoRA, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock1d_LoRA, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlock1d_LoRA, 512, 2, stride=2)\n",
    "\n",
    "        # Ëá™ÈÅ©ÊáâÊ±†ÂåñÔºàÂπ≥ÂùáÂíåÊúÄÂ§ßÔºâ\n",
    "        self.adaptiveavgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.adaptivemaxpool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # ÂÖ®ÈÄ£Êé•Â±§Ëàádropout\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(512 * 2, output_size)  # *2Âõ†ÁÇ∫concat‰∫ÜavgÂíåmaxÊ±†Âåñ\n",
    "\n",
    "        # ÂàùÂßãÂåñÊ¨äÈáç\n",
    "        self.init_weights()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.lora_rank))\n",
    "        self.inplanes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, lora_rank=self.lora_rank))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # È†êÊúüËº∏ÂÖ•ÂΩ¢ÁãÄ: (batch_size, time_steps, channels)\n",
    "        x = x.permute(0, 2, 1)  # ‚Üí (batch_size, channels, time_steps)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        # ÊáâÁî®Âπ≥ÂùáÂíåÊúÄÂ§ßÊ±†Âåñ\n",
    "        x1 = self.adaptiveavgpool(x)\n",
    "        x2 = self.adaptivemaxpool(x)\n",
    "        \n",
    "        # ÈÄ£Êé•Ê±†ÂåñÁµêÊûú\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        \n",
    "        # Â±ïÂπ≥\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # ÊáâÁî®dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # ÊúÄÁµÇÂàÜÈ°û\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"ÂàùÂßãÂåñÁ∂≤Áµ°Ê¨äÈáç\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def init_lora(self):\n",
    "        \"\"\"Period 2ÈñãÂßãÂëºÂè´‰∏ÄÊ¨°ÔºåÂàùÂßãÂåñ‰∏¶Âõ∫ÂÆö‰ΩøÁî®LoRAÈÅ©ÈÖçÂô®\"\"\"\n",
    "        lora_count = 0\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, BasicBlock1d_LoRA) and module.lora_adapter is None:\n",
    "                # ÁÇ∫ÊØèÂÄãBasicBlockÁöÑconv2Â±§ÂâµÂª∫LoRAÈÅ©ÈÖçÂô®\n",
    "                module.lora_adapter = LoRAConv1d(module.conv2, self.lora_rank).to(next(self.parameters()).device)\n",
    "                lora_count += 1\n",
    "        print(f\"‚úÖ LoRA adapters initialized for {lora_count} conv2 layers\")\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        \"\"\"ËøîÂõûÂèØË®ìÁ∑¥ÂèÉÊï∏ÂàóË°®ÔºàÁî®ÊñºÂÑ™ÂåñÂô®Ôºâ‰∏¶Êèê‰æõÂèÉÊï∏Áµ±Ë®à\"\"\"\n",
    "        lora_params = []\n",
    "        lora_names = []\n",
    "        fc_params = []\n",
    "        fc_names = []\n",
    "        \n",
    "        # Ë®àÁÆóÁ∏ΩÂèÉÊï∏Êï∏Èáè\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        \n",
    "        # Êî∂ÈõÜÊâÄÊúâLoRAÂèÉÊï∏\n",
    "        # for name, module in self.named_modules():\n",
    "        #     if isinstance(module, LoRAConv1d):\n",
    "        #         for param_name, param in module.named_parameters():\n",
    "        #             lora_params.append(param)\n",
    "        #             lora_names.append(f\"{name}.{param_name}\")\n",
    "        \n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, LoRAConv1d):\n",
    "                lora_params.append(module.lora_A)\n",
    "                lora_names.append(f\"{name}.lora_A\")\n",
    "                lora_params.append(module.lora_B)\n",
    "                lora_names.append(f\"{name}.lora_B\")\n",
    "\n",
    "        # Ê∑ªÂä†fcÂ±§ÂèÉÊï∏\n",
    "        for name, param in self.fc.named_parameters():\n",
    "            fc_params.append(param)\n",
    "            fc_names.append(f\"fc.{name}\")\n",
    "        \n",
    "        # Ë®àÁÆóÁµ±Ë®àÊï∏Êìö\n",
    "        trainable_params = lora_params + fc_params\n",
    "        frozen_params = total_params - sum(p.numel() for p in trainable_params)\n",
    "        lora_param_count = sum(p.numel() for p in lora_params)\n",
    "        fc_param_count = sum(p.numel() for p in fc_params)\n",
    "        trainable_param_count = lora_param_count + fc_param_count\n",
    "        \n",
    "        # ÊâìÂç∞Áµ±Ë®à‰ø°ÊÅØ\n",
    "        print(f\"üìä Parameter Statistics:\")\n",
    "        print(f\"  - Total parameters: {total_params:,}\")\n",
    "        print(f\"  - Trainable parameters: {trainable_param_count:,} ({trainable_param_count/total_params*100:.2f}%)\")\n",
    "        print(f\"    - LoRA parameters: {lora_param_count:,} ({lora_param_count/total_params*100:.2f}%)\")\n",
    "        print(f\"    - FC parameters: {fc_param_count:,} ({fc_param_count/total_params*100:.2f}%)\")\n",
    "        print(f\"  - Frozen parameters: {frozen_params:,} ({frozen_params/total_params*100:.2f}%)\")\n",
    "        \n",
    "        print(f\"üß† Trainable parameter names:\")\n",
    "        for name in lora_names:\n",
    "            print(f\"  ‚úÖ {name} (LoRA)\")\n",
    "        for name in fc_names:\n",
    "            print(f\"  ‚úÖ {name} (FC)\")\n",
    "        \n",
    "        return trainable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ebc8c6",
   "metadata": {},
   "source": [
    "## __Training and validation function__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2616acb4",
   "metadata": {},
   "source": [
    "### Extra Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3ce00ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classwise_accuracy(student_logits_flat, y_batch, class_correct, class_total):\n",
    "    \"\"\"\n",
    "    Computes per-class accuracy by accumulating correct and total samples for each class using vectorized operations.\n",
    "    \n",
    "    Args:\n",
    "        student_logits_flat (torch.Tensor): Model predictions (logits) in shape [batch_size * seq_len, output_size]\n",
    "        y_batch (torch.Tensor): True labels in shape [batch_size * seq_len]\n",
    "        class_correct (dict): Dictionary to store correct predictions per class\n",
    "        class_total (dict): Dictionary to store total samples per class\n",
    "    \"\"\"\n",
    "    # Ensure inputs are on the same device\n",
    "    if student_logits_flat.device != y_batch.device:\n",
    "        raise ValueError(\"student_logits_flat and y_batch must be on the same device\")\n",
    "\n",
    "    # Convert logits to predicted class indices\n",
    "    predictions = torch.argmax(student_logits_flat, dim=-1)  # Shape: [batch_size * seq_len]\n",
    "\n",
    "    # Compute correct predictions mask\n",
    "    correct_mask = (predictions == y_batch)  # Shape: [batch_size * seq_len], boolean\n",
    "\n",
    "    # Get unique labels in this batch\n",
    "    unique_labels = torch.unique(y_batch)\n",
    "\n",
    "    # Update class_total and class_correct using vectorized operations\n",
    "    for label in unique_labels:\n",
    "        label = label.item()  # Convert tensor to scalar\n",
    "        if label not in class_total:\n",
    "            class_total[label] = 0\n",
    "            class_correct[label] = 0\n",
    "        \n",
    "        # Count total samples for this label\n",
    "        label_mask = (y_batch == label)\n",
    "        class_total[label] += label_mask.sum().item()\n",
    "        \n",
    "        # Count correct predictions for this label\n",
    "        class_correct[label] += (label_mask & correct_mask).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "724647c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_parameter_info(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    param_size_bytes = total_params * 4\n",
    "    param_size_MB = param_size_bytes / (1024**2)\n",
    "    return total_params, param_size_MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b7f0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(y: np.ndarray, num_classes: int, exclude_classes: list = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Ë®àÁÆó class weightsÔºàinverse frequencyÔºâÈÅøÂÖç class imbalance„ÄÇ\n",
    "    ÂèØÊéíÈô§Êüê‰∫õÈ°ûÂà•ÔºàÂ¶Ç‰∏çÂ≠òÂú®ÁöÑÈ°ûÂà•ÔºâÔºåÈÄô‰∫õÈ°ûÂà•ÁöÑÊ¨äÈáçÂ∞áË®≠ÁÇ∫ 0„ÄÇ\n",
    "    \"\"\"\n",
    "    exclude_classes = set(exclude_classes or [])\n",
    "    class_sample_counts = np.bincount(y, minlength=num_classes)\n",
    "    total_samples = len(y)\n",
    "\n",
    "    weights = np.zeros(num_classes, dtype=np.float32)\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        if cls in exclude_classes:\n",
    "            weights[cls] = 0.0\n",
    "        else:\n",
    "            count = class_sample_counts[cls]\n",
    "            weights[cls] = total_samples / (count + 1e-6)\n",
    "\n",
    "    # Normalize only non-excluded weights\n",
    "    valid_mask = np.array([cls not in exclude_classes for cls in range(num_classes)])\n",
    "    norm_sum = weights[valid_mask].sum()\n",
    "    if norm_sum > 0:\n",
    "        weights[valid_mask] /= norm_sum\n",
    "\n",
    "    print(\"\\nüìä Class Weights (normalized):\")\n",
    "    for i, w in enumerate(weights):\n",
    "        status = \" (excluded)\" if i in exclude_classes else \"\"\n",
    "        print(f\"  - Class {i}: {w:.4f}{status}\")\n",
    "    \n",
    "    return torch.tensor(weights, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abe5f03",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7836ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lora_ecg(model, output_size, criterion, optimizer,\n",
    "                        X_train, y_train, X_val, y_val,\n",
    "                        scheduler=None, num_epochs=10, batch_size=64,\n",
    "                        model_saving_folder=None, model_name=None,\n",
    "                        stop_signal_file=None, device=None):\n",
    "\n",
    "    print(\"\\nüöÄ 'train_with_lora_ecg' started.\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    device = device or auto_select_cuda_device()\n",
    "    model_name = model_name or 'lora_model'\n",
    "    model_saving_folder = model_saving_folder or './saved_models'\n",
    "\n",
    "    if os.path.exists(model_saving_folder):\n",
    "        shutil.rmtree(model_saving_folder)\n",
    "        print(f\"‚úÖ Removed existing folder: {model_saving_folder}\")\n",
    "    os.makedirs(model_saving_folder, exist_ok=True)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"\\n‚úÖ Data Overview:\")\n",
    "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "\n",
    "    best_results = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if stop_signal_file and os.path.exists(stop_signal_file):\n",
    "            print(\"\\nüõë Stop signal detected. Exiting training loop.\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        class_correct, class_total = {}, {}\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * X_batch.size(0)\n",
    "            compute_classwise_accuracy(outputs, y_batch, class_correct, class_total)\n",
    "\n",
    "        train_loss = epoch_loss / len(train_loader.dataset)\n",
    "        train_acc = {int(c): f\"{(class_correct[c] / class_total[c]) * 100:.2f}%\" if class_total[c] > 0 else \"0.00%\"\n",
    "                     for c in sorted(class_total.keys())}\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        val_class_correct, val_class_total = {}, {}\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                outputs = model(X_batch)\n",
    "                val_loss += criterion(outputs, y_batch).item() * X_batch.size(0)\n",
    "                predictions = torch.argmax(outputs, dim=-1)\n",
    "                val_correct += (predictions == y_batch).sum().item()\n",
    "                val_total += y_batch.size(0)\n",
    "                compute_classwise_accuracy(outputs, y_batch, val_class_correct, val_class_total)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct / val_total\n",
    "        val_acc_cls = {int(c): f\"{(val_class_correct[c] / val_class_total[c]) * 100:.2f}%\" if val_class_total[c] > 0 else \"0.00%\"\n",
    "                       for c in sorted(val_class_total.keys())}\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Train-Class-Acc: {train_acc}\")\n",
    "        print(f\"Val Loss: {val_loss:.6f}, Val Acc: {val_acc * 100:.2f}%, Val-Class-Acc: {val_acc_cls}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        model_path = os.path.join(model_saving_folder, f\"{model_name}_epoch_{epoch+1}.pth\")\n",
    "        current = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_acc,\n",
    "            'train_classwise_accuracy': train_acc,\n",
    "            'val_classwise_accuracy': val_acc_cls,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "            'model_path': model_path\n",
    "        }\n",
    "\n",
    "        if len(best_results) < 5 or val_acc > best_results[-1]['val_accuracy']:\n",
    "            if len(best_results) == 5:\n",
    "                to_remove = best_results.pop()\n",
    "                if os.path.exists(to_remove['model_path']):\n",
    "                    os.remove(to_remove['model_path'])\n",
    "                    print(f\"üóë Removed: {to_remove['model_path']}\")\n",
    "            best_results.append(current)\n",
    "            best_results.sort(key=lambda x: (x['val_accuracy'], x['epoch']), reverse=True)\n",
    "            torch.save(current, model_path)\n",
    "            print(f\"‚úÖ Saved model: {model_path}\")\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    total_params, param_size_MB = get_model_parameter_info(model)\n",
    "\n",
    "    if best_results:\n",
    "        best = best_results[0]\n",
    "        best_model_path = os.path.join(model_saving_folder, f\"{model_name}_best.pth\")\n",
    "        torch.save(best, best_model_path)\n",
    "        print(f\"\\nüèÜ Best model saved as: {best_model_path} (Val Accuracy: {best['val_accuracy'] * 100:.2f}%)\")\n",
    "\n",
    "    final_model_path = os.path.join(model_saving_folder, f\"{model_name}_final.pth\")\n",
    "    torch.save(current, final_model_path)\n",
    "    print(f\"\\nüìå Final model saved as: {final_model_path}\")\n",
    "\n",
    "    print(\"\\nüéØ Top 5 Best Models:\")\n",
    "    for res in best_results:\n",
    "        print(f\"Epoch {res['epoch']}, Train Loss: {res['train_loss']:.6f}, Train-Acc: {res['train_classwise_accuracy']},\\n\"\n",
    "              f\"Val Loss: {res['val_loss']:.6f}, Val Acc: {res['val_accuracy']*100:.2f}%, Val-Acc: {res['val_classwise_accuracy']},\"\n",
    "              f\" Model Path: {res['model_path']}\")\n",
    "\n",
    "    match = re.search(r'Period_(\\d+)', model_saving_folder)\n",
    "    period_label = match.group(1) if match else \"?\"\n",
    "    model_name_str = model.__class__.__name__\n",
    "\n",
    "    print(f\"\"\"\n",
    "---\n",
    "### Period {period_label}\n",
    "+ ##### Total training time: {training_time:.2f} seconds\n",
    "+ ##### Model: {model_name_str}\n",
    "+ ##### Training and saving in *'{model_saving_folder}'*\n",
    "+ ##### Best Epoch: {best['epoch']}\n",
    "#### __Val Accuracy: {best['val_accuracy'] * 100:.2f}%__\n",
    "#### __Val-Class-Acc: {best['val_classwise_accuracy']}__\n",
    "#### __Total Parameters: {total_params:,}__\n",
    "#### __Model Size (float32): {param_size_MB:.2f} MB__\n",
    "\"\"\".strip())\n",
    "\n",
    "    del X_train, y_train, X_val, y_val, train_loader, val_loader, current, outputs, predictions\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1085b2",
   "metadata": {},
   "source": [
    "## __Training__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8ee5f8",
   "metadata": {},
   "source": [
    "### Period 1 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c948b89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n",
      "ResNet18_1D_LoRA(\n",
      "  (conv1): Conv1d(12, 64, kernel_size=(15,), stride=(2,), padding=(7,), bias=False)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (adaptiveavgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (adaptivemaxpool): AdaptiveMaxPool1d(output_size=1)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "üì¶ Model Summary from: Class_Incremental_CL/CPSC_CIL/ResNet18_Selection/ResNet18_big_inplane_v1/ResNet18_big_inplane_1D_best.pth\n",
      "üìå Epoch: 63\n",
      "üßÆ Train Loss: 0.007664\n",
      "üéØ Val Loss: 0.800983\n",
      "‚úÖ Val Accuracy: 88.86%\n",
      "üìé Learning Rate: 0.0006561000000000001\n",
      "üìÅ Stored Model Path: Class_Incremental_CL/CPSC_CIL/ResNet18_Selection/ResNet18_big_inplane_v1/ResNet18_big_inplane_1D_epoch_63.pth\n",
      "üß† Total Parameters: 3,857,026\n",
      "üìè Model Size (float32): 14.71 MB\n",
      "\n",
      "üìä Train Class-wise Accuracy:\n",
      "  ‚îî‚îÄ Class 0 : 99.86%\n",
      "  ‚îî‚îÄ Class 1 : 99.59%\n",
      "\n",
      "üìä Val Class-wise Accuracy:\n",
      "  ‚îî‚îÄ Class 0 : 91.85%\n",
      "  ‚îî‚îÄ Class 1 : 85.87%\n",
      "\n",
      "---\n",
      "### Period 1 Summary (Markdown Format)\n",
      "+ **Epoch:** 63\n",
      "+ **Train Loss:** 0.0076635455248283595\n",
      "+ **Val Loss:** 0.800983331773592\n",
      "+ **Val Accuracy:** 88.86%\n",
      "+ **Learning Rate:** 0.0006561000000000001\n",
      "+ **Stored Model Path:** `Class_Incremental_CL/CPSC_CIL/ResNet18_Selection/ResNet18_big_inplane_v1/ResNet18_big_inplane_1D_epoch_63.pth`\n",
      "+ **Total Parameters:** 3,857,026\n",
      "+ **Model Size (float32):** 14.71 MB\n",
      "+ **Train-Class-Acc:** {0: '99.86%', 1: '99.59%'}\n",
      "+ **Val-Class-Acc:** {0: '91.85%', 1: '85.87%'}\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_763947/2299765635.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "def display_model_summary_with_params(model_folder, model_filename=\"ResNet18_big_inplane_1D_best.pth\", input_channels=12, output_size=10):\n",
    "    model_path = os.path.join(model_folder, model_filename)\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"‚ùå File not found: {model_path}\")\n",
    "        return\n",
    "\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "    # === ÈÇÑÂéüÊ®°Âûã‰∏¶ËºâÂÖ•ÂèÉÊï∏ ===\n",
    "    model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    total_params, param_size_MB = get_model_parameter_info(model)\n",
    "\n",
    "    # === È°ØÁ§∫ÊëòË¶Å ===\n",
    "    epoch = checkpoint.get(\"epoch\", \"?\")\n",
    "    train_loss = checkpoint.get(\"train_loss\", \"?\")\n",
    "    val_loss = checkpoint.get(\"val_loss\", \"?\")\n",
    "    val_acc = checkpoint.get(\"val_accuracy\", \"?\")\n",
    "    train_acc_dict = checkpoint.get(\"train_classwise_accuracy\", {})\n",
    "    val_acc_dict = checkpoint.get(\"val_classwise_accuracy\", {})\n",
    "    lr = checkpoint.get(\"learning_rate\", \"?\")\n",
    "    stored_path = checkpoint.get(\"model_path\", \"N/A\")\n",
    "    print(f\"Model Architecture:\")\n",
    "    print(model)\n",
    "    print(f\"\\nüì¶ Model Summary from: {model_path}\")\n",
    "    print(f\"üìå Epoch: {epoch}\")\n",
    "    print(f\"üßÆ Train Loss: {train_loss:.6f}\" if isinstance(train_loss, float) else f\"üßÆ Train Loss: {train_loss}\")\n",
    "    print(f\"üéØ Val Loss: {val_loss:.6f}\" if isinstance(val_loss, float) else f\"üéØ Val Loss: {val_loss}\")\n",
    "    print(f\"‚úÖ Val Accuracy: {val_acc*100:.2f}%\" if isinstance(val_acc, float) else f\"‚úÖ Val Accuracy: {val_acc}\")\n",
    "    print(f\"üìé Learning Rate: {lr}\")\n",
    "    print(f\"üìÅ Stored Model Path: {stored_path}\")\n",
    "    print(f\"üß† Total Parameters: {total_params:,}\")\n",
    "    print(f\"üìè Model Size (float32): {param_size_MB:.2f} MB\")\n",
    "\n",
    "    print(\"\\nüìä Train Class-wise Accuracy:\")\n",
    "    for c, acc in train_acc_dict.items():\n",
    "        print(f\"  ‚îî‚îÄ Class {c:<2}: {acc}\")\n",
    "\n",
    "    print(\"\\nüìä Val Class-wise Accuracy:\")\n",
    "    for c, acc in val_acc_dict.items():\n",
    "        print(f\"  ‚îî‚îÄ Class {c:<2}: {acc}\")\n",
    "\n",
    "    print(\"\\n---\\n### Period 1 Summary (Markdown Format)\")\n",
    "    print(f\"+ **Epoch:** {epoch}\")\n",
    "    print(f\"+ **Train Loss:** {train_loss}\")\n",
    "    print(f\"+ **Val Loss:** {val_loss}\")\n",
    "    print(f\"+ **Val Accuracy:** {val_acc*100:.2f}%\" if isinstance(val_acc, float) else f\"+ **Val Accuracy:** {val_acc}\")\n",
    "    print(f\"+ **Learning Rate:** {lr}\")\n",
    "    print(f\"+ **Stored Model Path:** `{stored_path}`\")\n",
    "    print(f\"+ **Total Parameters:** {total_params:,}\")\n",
    "    print(f\"+ **Model Size (float32):** {param_size_MB:.2f} MB\")\n",
    "    print(f\"+ **Train-Class-Acc:** {train_acc_dict}\")\n",
    "    print(f\"+ **Val-Class-Acc:** {val_acc_dict}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# Example call:\n",
    "display_model_summary_with_params(\n",
    "    model_folder=os.path.join(\"Class_Incremental_CL\", \"CPSC_CIL\", \"ResNet18_Selection\", \"ResNet18_big_inplane_v1\"),\n",
    "    input_channels=12,\n",
    "    output_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e2f418",
   "metadata": {},
   "source": [
    "### Period 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d742a53",
   "metadata": {},
   "source": [
    "#### v1: Trainable parameterÊú™ÊéíÈô§`lora_adapter.conv`ÁöÑÁâàÊú¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f583d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 2\n",
      "    - Memory Used    : 1202 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_763947/3538613332.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  prev_checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Not loaded: fc.weight, shape=torch.Size([4, 1024])\n",
      "üîç Not loaded: fc.bias, shape=torch.Size([4])\n",
      "‚úÖ Loaded Period 1 weights (excluding final FC layer)\n",
      "‚úÖ LoRA adapters initialized for 8 conv2 layers\n",
      "üìä Parameter Statistics:\n",
      "  - Total parameters: 3,889,796\n",
      "  - Trainable parameters: 2,123,780 (54.60%)\n",
      "    - LoRA parameters: 2,119,680 (54.49%)\n",
      "    - FC parameters: 4,100 (0.11%)\n",
      "  - Frozen parameters: 1,766,016 (45.40%)\n",
      "üß† Trainable parameter names:\n",
      "  ‚úÖ layer1.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer1.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer1.0.lora_adapter.conv.weight (LoRA)\n",
      "  ‚úÖ layer1.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer1.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer1.1.lora_adapter.conv.weight (LoRA)\n",
      "  ‚úÖ layer2.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer2.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer2.0.lora_adapter.conv.weight (LoRA)\n",
      "  ‚úÖ layer2.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer2.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer2.1.lora_adapter.conv.weight (LoRA)\n",
      "  ‚úÖ layer3.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer3.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer3.0.lora_adapter.conv.weight (LoRA)\n",
      "  ‚úÖ layer3.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer3.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer3.1.lora_adapter.conv.weight (LoRA)\n",
      "  ‚úÖ layer4.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer4.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer4.0.lora_adapter.conv.weight (LoRA)\n",
      "  ‚úÖ layer4.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer4.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer4.1.lora_adapter.conv.weight (LoRA)\n",
      "  ‚úÖ fc.weight (FC)\n",
      "  ‚úÖ fc.bias (FC)\n",
      "\n",
      "üöÄ 'train_with_lora_ecg' started.\n",
      "‚úÖ Removed existing folder: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2\n",
      "\n",
      "‚úÖ Data Overview:\n",
      "X_train: torch.Size([3263, 5000, 12]), y_train: torch.Size([3263])\n",
      "X_val: torch.Size([816, 5000, 12]), y_val: torch.Size([816])\n",
      "Epoch 1/200, Train Loss: 21.662514, Train-Class-Acc: {0: '74.11%', 1: '54.71%', 2: '60.83%', 3: '63.42%'}\n",
      "Val Loss: 5.681706, Val Acc: 82.72%, Val-Class-Acc: {0: '86.96%', 1: '73.36%', 2: '77.78%', 3: '91.80%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_1.pth\n",
      "Epoch 2/200, Train Loss: 10.301444, Train-Class-Acc: {0: '76.16%', 1: '68.85%', 2: '76.78%', 3: '78.38%'}\n",
      "Val Loss: 13.773001, Val Acc: 77.08%, Val-Class-Acc: {0: '66.85%', 1: '86.48%', 2: '75.69%', 3: '76.23%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_2.pth\n",
      "Epoch 3/200, Train Loss: 7.223097, Train-Class-Acc: {0: '79.02%', 1: '71.72%', 2: '79.38%', 3: '84.43%'}\n",
      "Val Loss: 3.524492, Val Acc: 86.40%, Val-Class-Acc: {0: '91.30%', 1: '79.10%', 2: '79.86%', 3: '93.85%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_3.pth\n",
      "Epoch 4/200, Train Loss: 7.325075, Train-Class-Acc: {0: '79.97%', 1: '70.80%', 2: '81.80%', 3: '84.84%'}\n",
      "Val Loss: 3.969149, Val Acc: 85.78%, Val-Class-Acc: {0: '89.67%', 1: '77.05%', 2: '85.42%', 3: '91.80%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_4.pth\n",
      "Epoch 5/200, Train Loss: 5.205383, Train-Class-Acc: {0: '82.83%', 1: '78.48%', 2: '82.32%', 3: '86.68%'}\n",
      "Val Loss: 7.208787, Val Acc: 82.60%, Val-Class-Acc: {0: '85.33%', 1: '79.51%', 2: '87.50%', 3: '80.74%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_5.pth\n",
      "Epoch 6/200, Train Loss: 4.845548, Train-Class-Acc: {0: '81.61%', 1: '77.77%', 2: '85.96%', 3: '88.11%'}\n",
      "Val Loss: 4.170148, Val Acc: 84.07%, Val-Class-Acc: {0: '84.24%', 1: '77.05%', 2: '90.97%', 3: '86.89%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_2.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_6.pth\n",
      "Epoch 7/200, Train Loss: 3.363297, Train-Class-Acc: {0: '86.78%', 1: '82.38%', 2: '87.87%', 3: '90.37%'}\n",
      "Val Loss: 4.696389, Val Acc: 85.29%, Val-Class-Acc: {0: '91.30%', 1: '77.05%', 2: '84.03%', 3: '89.75%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_5.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_7.pth\n",
      "Epoch 8/200, Train Loss: 3.211641, Train-Class-Acc: {0: '85.15%', 1: '81.97%', 2: '88.04%', 3: '89.96%'}\n",
      "Val Loss: 3.231616, Val Acc: 86.52%, Val-Class-Acc: {0: '90.76%', 1: '76.64%', 2: '81.94%', 3: '95.90%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_1.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_8.pth\n",
      "Epoch 9/200, Train Loss: 2.493183, Train-Class-Acc: {0: '87.74%', 1: '85.25%', 2: '88.39%', 3: '92.52%'}\n",
      "Val Loss: 4.708895, Val Acc: 86.52%, Val-Class-Acc: {0: '94.57%', 1: '75.00%', 2: '84.03%', 3: '93.44%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_6.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_9.pth\n",
      "Epoch 10/200, Train Loss: 2.413882, Train-Class-Acc: {0: '88.56%', 1: '84.94%', 2: '90.47%', 3: '92.73%'}\n",
      "Val Loss: 3.409631, Val Acc: 87.50%, Val-Class-Acc: {0: '86.41%', 1: '83.20%', 2: '87.50%', 3: '92.62%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_7.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_10.pth\n",
      "Epoch 11/200, Train Loss: 2.479448, Train-Class-Acc: {0: '87.87%', 1: '85.55%', 2: '89.95%', 3: '93.03%'}\n",
      "Val Loss: 3.225006, Val Acc: 86.64%, Val-Class-Acc: {0: '85.33%', 1: '80.33%', 2: '84.03%', 3: '95.49%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_4.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_11.pth\n",
      "Epoch 12/200, Train Loss: 1.639061, Train-Class-Acc: {0: '90.46%', 1: '88.42%', 2: '92.20%', 3: '94.26%'}\n",
      "Val Loss: 3.564409, Val Acc: 85.05%, Val-Class-Acc: {0: '83.70%', 1: '72.54%', 2: '86.11%', 3: '97.95%'}, LR: 0.001000\n",
      "Epoch 13/200, Train Loss: 1.883551, Train-Class-Acc: {0: '90.33%', 1: '88.42%', 2: '91.16%', 3: '94.06%'}\n",
      "Val Loss: 5.158819, Val Acc: 85.29%, Val-Class-Acc: {0: '84.24%', 1: '78.28%', 2: '88.19%', 3: '91.39%'}, LR: 0.001000\n",
      "Epoch 14/200, Train Loss: 2.609724, Train-Class-Acc: {0: '88.28%', 1: '85.45%', 2: '89.95%', 3: '93.03%'}\n",
      "Val Loss: 3.722396, Val Acc: 84.80%, Val-Class-Acc: {0: '84.78%', 1: '71.72%', 2: '86.81%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 15/200, Train Loss: 1.463512, Train-Class-Acc: {0: '88.96%', 1: '89.34%', 2: '92.37%', 3: '95.39%'}\n",
      "Val Loss: 4.204310, Val Acc: 83.21%, Val-Class-Acc: {0: '96.20%', 1: '67.21%', 2: '85.42%', 3: '88.11%'}, LR: 0.001000\n",
      "Epoch 16/200, Train Loss: 1.109118, Train-Class-Acc: {0: '92.10%', 1: '90.68%', 2: '93.41%', 3: '95.80%'}\n",
      "Val Loss: 3.362353, Val Acc: 86.03%, Val-Class-Acc: {0: '84.78%', 1: '76.23%', 2: '87.50%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 17/200, Train Loss: 1.532286, Train-Class-Acc: {0: '92.51%', 1: '90.57%', 2: '93.41%', 3: '94.88%'}\n",
      "Val Loss: 3.471946, Val Acc: 84.68%, Val-Class-Acc: {0: '78.80%', 1: '79.51%', 2: '88.19%', 3: '92.21%'}, LR: 0.001000\n",
      "Epoch 18/200, Train Loss: 1.742891, Train-Class-Acc: {0: '92.51%', 1: '90.57%', 2: '93.76%', 3: '93.85%'}\n",
      "Val Loss: 3.345808, Val Acc: 86.76%, Val-Class-Acc: {0: '87.50%', 1: '76.64%', 2: '86.11%', 3: '96.72%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_3.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_18.pth\n",
      "Epoch 19/200, Train Loss: 1.037077, Train-Class-Acc: {0: '93.46%', 1: '92.32%', 2: '94.97%', 3: '95.90%'}\n",
      "Val Loss: 5.399586, Val Acc: 84.31%, Val-Class-Acc: {0: '88.59%', 1: '85.25%', 2: '77.08%', 3: '84.43%'}, LR: 0.001000\n",
      "Epoch 20/200, Train Loss: 1.001847, Train-Class-Acc: {0: '94.69%', 1: '92.42%', 2: '94.11%', 3: '95.59%'}\n",
      "Val Loss: 3.106847, Val Acc: 87.62%, Val-Class-Acc: {0: '89.13%', 1: '78.28%', 2: '87.50%', 3: '95.90%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_8.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_20.pth\n",
      "Epoch 21/200, Train Loss: 0.753383, Train-Class-Acc: {0: '93.87%', 1: '92.62%', 2: '94.97%', 3: '96.52%'}\n",
      "Val Loss: 3.504215, Val Acc: 87.13%, Val-Class-Acc: {0: '83.70%', 1: '83.20%', 2: '88.19%', 3: '93.03%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_9.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_21.pth\n",
      "Epoch 22/200, Train Loss: 0.598192, Train-Class-Acc: {0: '94.69%', 1: '93.65%', 2: '96.19%', 3: '97.13%'}\n",
      "Val Loss: 3.712488, Val Acc: 86.15%, Val-Class-Acc: {0: '88.04%', 1: '75.82%', 2: '84.03%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 23/200, Train Loss: 0.771997, Train-Class-Acc: {0: '94.28%', 1: '93.03%', 2: '95.67%', 3: '97.34%'}\n",
      "Val Loss: 4.147768, Val Acc: 86.52%, Val-Class-Acc: {0: '85.33%', 1: '83.20%', 2: '81.94%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 24/200, Train Loss: 0.767121, Train-Class-Acc: {0: '94.01%', 1: '93.44%', 2: '95.84%', 3: '97.13%'}\n",
      "Val Loss: 3.230651, Val Acc: 86.76%, Val-Class-Acc: {0: '88.59%', 1: '77.46%', 2: '83.33%', 3: '96.72%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_11.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_24.pth\n",
      "Epoch 25/200, Train Loss: 0.695079, Train-Class-Acc: {0: '94.69%', 1: '93.85%', 2: '95.49%', 3: '96.93%'}\n",
      "Val Loss: 3.318409, Val Acc: 86.52%, Val-Class-Acc: {0: '75.54%', 1: '86.48%', 2: '89.58%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 26/200, Train Loss: 1.143683, Train-Class-Acc: {0: '94.69%', 1: '92.52%', 2: '95.67%', 3: '96.41%'}\n",
      "Val Loss: 6.066843, Val Acc: 85.42%, Val-Class-Acc: {0: '87.50%', 1: '82.38%', 2: '79.86%', 3: '90.16%'}, LR: 0.001000\n",
      "Epoch 27/200, Train Loss: 1.279926, Train-Class-Acc: {0: '93.19%', 1: '92.52%', 2: '96.01%', 3: '96.93%'}\n",
      "Val Loss: 4.287782, Val Acc: 84.19%, Val-Class-Acc: {0: '86.96%', 1: '81.97%', 2: '81.25%', 3: '86.07%'}, LR: 0.001000\n",
      "Epoch 28/200, Train Loss: 0.850731, Train-Class-Acc: {0: '95.10%', 1: '94.16%', 2: '94.80%', 3: '96.93%'}\n",
      "Val Loss: 3.631411, Val Acc: 87.01%, Val-Class-Acc: {0: '82.61%', 1: '84.02%', 2: '82.64%', 3: '95.90%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_18.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_28.pth\n",
      "Epoch 29/200, Train Loss: 0.661760, Train-Class-Acc: {0: '95.37%', 1: '93.75%', 2: '95.84%', 3: '96.62%'}\n",
      "Val Loss: 3.395661, Val Acc: 87.01%, Val-Class-Acc: {0: '89.13%', 1: '82.79%', 2: '85.42%', 3: '90.57%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_24.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_29.pth\n",
      "Epoch 30/200, Train Loss: 0.776527, Train-Class-Acc: {0: '95.37%', 1: '94.36%', 2: '95.49%', 3: '97.64%'}\n",
      "Val Loss: 3.877829, Val Acc: 87.62%, Val-Class-Acc: {0: '92.39%', 1: '80.33%', 2: '88.19%', 3: '90.98%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_28.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_30.pth\n",
      "Epoch 31/200, Train Loss: 0.688721, Train-Class-Acc: {0: '94.55%', 1: '93.65%', 2: '96.19%', 3: '96.93%'}\n",
      "Val Loss: 4.199972, Val Acc: 86.89%, Val-Class-Acc: {0: '93.48%', 1: '74.59%', 2: '81.25%', 3: '97.54%'}, LR: 0.001000\n",
      "Epoch 32/200, Train Loss: 0.443195, Train-Class-Acc: {0: '95.91%', 1: '96.00%', 2: '97.05%', 3: '97.75%'}\n",
      "Val Loss: 3.893874, Val Acc: 87.75%, Val-Class-Acc: {0: '95.11%', 1: '75.00%', 2: '84.03%', 3: '97.13%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_29.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_32.pth\n",
      "Epoch 33/200, Train Loss: 0.650108, Train-Class-Acc: {0: '96.19%', 1: '94.47%', 2: '95.67%', 3: '97.34%'}\n",
      "Val Loss: 3.407180, Val Acc: 86.89%, Val-Class-Acc: {0: '87.50%', 1: '78.69%', 2: '82.64%', 3: '97.13%'}, LR: 0.000900\n",
      "Epoch 34/200, Train Loss: 0.570838, Train-Class-Acc: {0: '95.64%', 1: '96.21%', 2: '97.05%', 3: '98.57%'}\n",
      "Val Loss: 3.746744, Val Acc: 85.66%, Val-Class-Acc: {0: '75.54%', 1: '81.56%', 2: '86.11%', 3: '97.13%'}, LR: 0.000900\n",
      "Epoch 35/200, Train Loss: 0.577483, Train-Class-Acc: {0: '96.46%', 1: '96.00%', 2: '97.23%', 3: '98.46%'}\n",
      "Val Loss: 4.182368, Val Acc: 85.54%, Val-Class-Acc: {0: '85.87%', 1: '83.61%', 2: '85.42%', 3: '87.30%'}, LR: 0.000900\n",
      "Epoch 36/200, Train Loss: 0.486325, Train-Class-Acc: {0: '97.00%', 1: '96.00%', 2: '97.23%', 3: '98.26%'}\n",
      "Val Loss: 3.770137, Val Acc: 85.54%, Val-Class-Acc: {0: '77.17%', 1: '80.33%', 2: '85.42%', 3: '97.13%'}, LR: 0.000900\n",
      "Epoch 37/200, Train Loss: 0.566489, Train-Class-Acc: {0: '95.91%', 1: '96.00%', 2: '97.23%', 3: '97.75%'}\n",
      "Val Loss: 4.046100, Val Acc: 85.66%, Val-Class-Acc: {0: '77.72%', 1: '80.33%', 2: '86.11%', 3: '96.72%'}, LR: 0.000900\n",
      "Epoch 38/200, Train Loss: 0.522614, Train-Class-Acc: {0: '95.64%', 1: '96.00%', 2: '97.92%', 3: '98.26%'}\n",
      "Val Loss: 6.486963, Val Acc: 85.66%, Val-Class-Acc: {0: '82.07%', 1: '75.41%', 2: '86.81%', 3: '97.95%'}, LR: 0.000900\n",
      "Epoch 39/200, Train Loss: 0.384677, Train-Class-Acc: {0: '97.68%', 1: '96.52%', 2: '97.05%', 3: '97.85%'}\n",
      "Val Loss: 3.586828, Val Acc: 88.11%, Val-Class-Acc: {0: '92.93%', 1: '78.28%', 2: '83.33%', 3: '97.13%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_21.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_39.pth\n",
      "Epoch 40/200, Train Loss: 0.574230, Train-Class-Acc: {0: '96.59%', 1: '95.80%', 2: '97.05%', 3: '97.75%'}\n",
      "Val Loss: 4.377317, Val Acc: 86.64%, Val-Class-Acc: {0: '86.96%', 1: '79.92%', 2: '81.94%', 3: '95.90%'}, LR: 0.000900\n",
      "Epoch 41/200, Train Loss: 0.613749, Train-Class-Acc: {0: '96.59%', 1: '95.59%', 2: '97.57%', 3: '97.95%'}\n",
      "Val Loss: 4.259809, Val Acc: 87.13%, Val-Class-Acc: {0: '87.50%', 1: '82.38%', 2: '79.17%', 3: '96.31%'}, LR: 0.000900\n",
      "Epoch 42/200, Train Loss: 0.562669, Train-Class-Acc: {0: '96.32%', 1: '95.59%', 2: '96.53%', 3: '97.85%'}\n",
      "Val Loss: 5.970198, Val Acc: 85.91%, Val-Class-Acc: {0: '94.57%', 1: '75.41%', 2: '83.33%', 3: '91.39%'}, LR: 0.000900\n",
      "Epoch 43/200, Train Loss: 0.523237, Train-Class-Acc: {0: '97.41%', 1: '96.31%', 2: '97.23%', 3: '98.57%'}\n",
      "Val Loss: 4.591437, Val Acc: 87.50%, Val-Class-Acc: {0: '91.85%', 1: '75.82%', 2: '85.42%', 3: '97.13%'}, LR: 0.000810\n",
      "Epoch 44/200, Train Loss: 0.467956, Train-Class-Acc: {0: '97.00%', 1: '97.13%', 2: '96.88%', 3: '98.46%'}\n",
      "Val Loss: 5.213167, Val Acc: 87.62%, Val-Class-Acc: {0: '88.04%', 1: '80.74%', 2: '84.72%', 3: '95.90%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_10.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_44.pth\n",
      "Epoch 45/200, Train Loss: 0.410914, Train-Class-Acc: {0: '97.82%', 1: '97.13%', 2: '98.27%', 3: '98.87%'}\n",
      "Val Loss: 3.939963, Val Acc: 86.52%, Val-Class-Acc: {0: '84.78%', 1: '81.97%', 2: '83.33%', 3: '94.26%'}, LR: 0.000810\n",
      "Epoch 46/200, Train Loss: 0.375493, Train-Class-Acc: {0: '97.96%', 1: '96.93%', 2: '98.44%', 3: '97.95%'}\n",
      "Val Loss: 6.983690, Val Acc: 85.17%, Val-Class-Acc: {0: '85.87%', 1: '82.38%', 2: '81.94%', 3: '89.34%'}, LR: 0.000810\n",
      "Epoch 47/200, Train Loss: 0.731035, Train-Class-Acc: {0: '96.19%', 1: '95.59%', 2: '97.75%', 3: '97.95%'}\n",
      "Val Loss: 5.193706, Val Acc: 86.52%, Val-Class-Acc: {0: '84.78%', 1: '82.79%', 2: '76.39%', 3: '97.54%'}, LR: 0.000810\n",
      "Epoch 48/200, Train Loss: 0.534998, Train-Class-Acc: {0: '97.41%', 1: '96.93%', 2: '96.53%', 3: '97.44%'}\n",
      "Val Loss: 4.087388, Val Acc: 87.75%, Val-Class-Acc: {0: '90.76%', 1: '75.00%', 2: '89.58%', 3: '97.13%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_20.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_48.pth\n",
      "Epoch 49/200, Train Loss: 0.413135, Train-Class-Acc: {0: '96.05%', 1: '96.41%', 2: '97.05%', 3: '98.46%'}\n",
      "Val Loss: 4.545583, Val Acc: 86.89%, Val-Class-Acc: {0: '84.78%', 1: '79.10%', 2: '86.81%', 3: '96.31%'}, LR: 0.000810\n",
      "Epoch 50/200, Train Loss: 0.246062, Train-Class-Acc: {0: '96.59%', 1: '97.23%', 2: '98.61%', 3: '99.08%'}\n",
      "Val Loss: 4.019076, Val Acc: 87.13%, Val-Class-Acc: {0: '76.63%', 1: '87.30%', 2: '85.42%', 3: '95.90%'}, LR: 0.000810\n",
      "Epoch 51/200, Train Loss: 0.162798, Train-Class-Acc: {0: '97.82%', 1: '97.64%', 2: '98.96%', 3: '99.59%'}\n",
      "Val Loss: 3.750899, Val Acc: 87.50%, Val-Class-Acc: {0: '82.07%', 1: '82.38%', 2: '88.19%', 3: '96.31%'}, LR: 0.000810\n",
      "Epoch 52/200, Train Loss: 0.250222, Train-Class-Acc: {0: '98.23%', 1: '97.75%', 2: '98.27%', 3: '99.18%'}\n",
      "Val Loss: 5.081690, Val Acc: 87.50%, Val-Class-Acc: {0: '86.96%', 1: '75.82%', 2: '91.67%', 3: '97.13%'}, LR: 0.000810\n",
      "Epoch 53/200, Train Loss: 0.240924, Train-Class-Acc: {0: '98.09%', 1: '97.85%', 2: '98.27%', 3: '98.46%'}\n",
      "Val Loss: 4.153482, Val Acc: 86.89%, Val-Class-Acc: {0: '84.78%', 1: '79.51%', 2: '86.81%', 3: '95.90%'}, LR: 0.000810\n",
      "Epoch 54/200, Train Loss: 0.148529, Train-Class-Acc: {0: '98.23%', 1: '97.75%', 2: '99.13%', 3: '99.28%'}\n",
      "Val Loss: 4.269515, Val Acc: 86.52%, Val-Class-Acc: {0: '81.52%', 1: '81.15%', 2: '86.11%', 3: '95.90%'}, LR: 0.000729\n",
      "Epoch 55/200, Train Loss: 0.340735, Train-Class-Acc: {0: '98.37%', 1: '97.64%', 2: '98.27%', 3: '98.87%'}\n",
      "Val Loss: 4.275910, Val Acc: 85.05%, Val-Class-Acc: {0: '83.15%', 1: '78.69%', 2: '88.89%', 3: '90.57%'}, LR: 0.000729\n",
      "Epoch 56/200, Train Loss: 0.147490, Train-Class-Acc: {0: '97.68%', 1: '98.05%', 2: '99.13%', 3: '99.49%'}\n",
      "Val Loss: 5.177790, Val Acc: 86.52%, Val-Class-Acc: {0: '90.76%', 1: '72.13%', 2: '88.89%', 3: '96.31%'}, LR: 0.000729\n",
      "Epoch 57/200, Train Loss: 0.207372, Train-Class-Acc: {0: '98.77%', 1: '98.57%', 2: '98.61%', 3: '99.08%'}\n",
      "Val Loss: 5.005525, Val Acc: 86.64%, Val-Class-Acc: {0: '83.15%', 1: '77.87%', 2: '88.89%', 3: '96.72%'}, LR: 0.000729\n",
      "Epoch 58/200, Train Loss: 0.093229, Train-Class-Acc: {0: '99.18%', 1: '98.77%', 2: '98.27%', 3: '99.08%'}\n",
      "Val Loss: 4.229862, Val Acc: 87.13%, Val-Class-Acc: {0: '88.04%', 1: '77.05%', 2: '85.42%', 3: '97.54%'}, LR: 0.000729\n",
      "Epoch 59/200, Train Loss: 0.271443, Train-Class-Acc: {0: '98.64%', 1: '98.36%', 2: '98.96%', 3: '99.08%'}\n",
      "Val Loss: 4.617750, Val Acc: 86.52%, Val-Class-Acc: {0: '84.24%', 1: '78.69%', 2: '84.03%', 3: '97.54%'}, LR: 0.000729\n",
      "Epoch 60/200, Train Loss: 0.304883, Train-Class-Acc: {0: '98.37%', 1: '98.16%', 2: '98.61%', 3: '98.98%'}\n",
      "Val Loss: 5.400592, Val Acc: 86.64%, Val-Class-Acc: {0: '86.41%', 1: '77.46%', 2: '86.11%', 3: '96.31%'}, LR: 0.000729\n",
      "Epoch 61/200, Train Loss: 0.333616, Train-Class-Acc: {0: '96.59%', 1: '96.62%', 2: '97.92%', 3: '99.28%'}\n",
      "Val Loss: 4.742289, Val Acc: 85.42%, Val-Class-Acc: {0: '88.04%', 1: '75.41%', 2: '86.11%', 3: '93.03%'}, LR: 0.000729\n",
      "Epoch 62/200, Train Loss: 0.253119, Train-Class-Acc: {0: '97.28%', 1: '97.44%', 2: '97.92%', 3: '98.67%'}\n",
      "Val Loss: 3.797647, Val Acc: 88.24%, Val-Class-Acc: {0: '84.78%', 1: '82.79%', 2: '86.11%', 3: '97.54%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_30.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_62.pth\n",
      "Epoch 63/200, Train Loss: 0.303155, Train-Class-Acc: {0: '97.82%', 1: '97.54%', 2: '98.61%', 3: '98.87%'}\n",
      "Val Loss: 5.182899, Val Acc: 86.03%, Val-Class-Acc: {0: '81.52%', 1: '82.79%', 2: '87.50%', 3: '91.80%'}, LR: 0.000729\n",
      "Epoch 64/200, Train Loss: 0.500349, Train-Class-Acc: {0: '96.73%', 1: '96.82%', 2: '98.61%', 3: '98.77%'}\n",
      "Val Loss: 5.747892, Val Acc: 86.27%, Val-Class-Acc: {0: '77.17%', 1: '84.84%', 2: '82.64%', 3: '96.72%'}, LR: 0.000729\n",
      "Epoch 65/200, Train Loss: 0.236144, Train-Class-Acc: {0: '98.23%', 1: '97.64%', 2: '97.92%', 3: '99.28%'}\n",
      "Val Loss: 5.113878, Val Acc: 86.64%, Val-Class-Acc: {0: '79.89%', 1: '86.89%', 2: '82.64%', 3: '93.85%'}, LR: 0.000656\n",
      "Epoch 66/200, Train Loss: 0.213226, Train-Class-Acc: {0: '97.41%', 1: '97.34%', 2: '98.61%', 3: '99.08%'}\n",
      "Val Loss: 4.952840, Val Acc: 86.76%, Val-Class-Acc: {0: '88.59%', 1: '77.87%', 2: '87.50%', 3: '93.85%'}, LR: 0.000656\n",
      "Epoch 67/200, Train Loss: 0.121051, Train-Class-Acc: {0: '98.50%', 1: '98.77%', 2: '98.96%', 3: '99.18%'}\n",
      "Val Loss: 4.885515, Val Acc: 86.64%, Val-Class-Acc: {0: '88.04%', 1: '76.64%', 2: '86.81%', 3: '95.49%'}, LR: 0.000656\n",
      "Epoch 68/200, Train Loss: 0.124444, Train-Class-Acc: {0: '98.77%', 1: '98.57%', 2: '99.31%', 3: '99.28%'}\n",
      "Val Loss: 5.514650, Val Acc: 86.89%, Val-Class-Acc: {0: '85.33%', 1: '79.10%', 2: '85.42%', 3: '96.72%'}, LR: 0.000656\n",
      "Epoch 69/200, Train Loss: 0.175858, Train-Class-Acc: {0: '98.64%', 1: '98.57%', 2: '98.96%', 3: '99.69%'}\n",
      "Val Loss: 4.799601, Val Acc: 87.50%, Val-Class-Acc: {0: '79.89%', 1: '85.25%', 2: '86.11%', 3: '96.31%'}, LR: 0.000656\n",
      "Epoch 70/200, Train Loss: 0.246547, Train-Class-Acc: {0: '98.37%', 1: '98.05%', 2: '98.27%', 3: '98.87%'}\n",
      "Val Loss: 5.528666, Val Acc: 86.40%, Val-Class-Acc: {0: '86.41%', 1: '82.38%', 2: '77.08%', 3: '95.90%'}, LR: 0.000656\n",
      "Epoch 71/200, Train Loss: 0.112057, Train-Class-Acc: {0: '98.50%', 1: '98.16%', 2: '99.13%', 3: '99.39%'}\n",
      "Val Loss: 5.224372, Val Acc: 87.01%, Val-Class-Acc: {0: '85.33%', 1: '79.51%', 2: '88.89%', 3: '94.67%'}, LR: 0.000656\n",
      "Epoch 72/200, Train Loss: 0.087220, Train-Class-Acc: {0: '99.05%', 1: '99.08%', 2: '99.65%', 3: '99.69%'}\n",
      "Val Loss: 5.287271, Val Acc: 85.91%, Val-Class-Acc: {0: '82.07%', 1: '80.33%', 2: '81.94%', 3: '96.72%'}, LR: 0.000656\n",
      "Epoch 73/200, Train Loss: 0.159143, Train-Class-Acc: {0: '99.32%', 1: '98.77%', 2: '98.96%', 3: '98.87%'}\n",
      "Val Loss: 5.352964, Val Acc: 86.89%, Val-Class-Acc: {0: '87.50%', 1: '80.33%', 2: '81.25%', 3: '96.31%'}, LR: 0.000656\n",
      "Epoch 74/200, Train Loss: 0.141029, Train-Class-Acc: {0: '98.77%', 1: '98.87%', 2: '98.79%', 3: '99.69%'}\n",
      "Val Loss: 5.454295, Val Acc: 86.76%, Val-Class-Acc: {0: '92.93%', 1: '72.95%', 2: '86.81%', 3: '95.90%'}, LR: 0.000656\n",
      "Epoch 75/200, Train Loss: 0.106908, Train-Class-Acc: {0: '99.32%', 1: '98.77%', 2: '99.83%', 3: '99.28%'}\n",
      "Val Loss: 5.704502, Val Acc: 86.15%, Val-Class-Acc: {0: '78.80%', 1: '84.02%', 2: '83.33%', 3: '95.49%'}, LR: 0.000656\n",
      "Epoch 76/200, Train Loss: 0.126150, Train-Class-Acc: {0: '98.50%', 1: '98.67%', 2: '98.96%', 3: '99.49%'}\n",
      "Val Loss: 5.900135, Val Acc: 85.91%, Val-Class-Acc: {0: '89.13%', 1: '77.87%', 2: '82.64%', 3: '93.44%'}, LR: 0.000590\n",
      "Epoch 77/200, Train Loss: 0.091760, Train-Class-Acc: {0: '99.05%', 1: '98.87%', 2: '99.48%', 3: '99.69%'}\n",
      "Val Loss: 5.613017, Val Acc: 86.40%, Val-Class-Acc: {0: '89.13%', 1: '77.46%', 2: '82.64%', 3: '95.49%'}, LR: 0.000590\n",
      "Epoch 78/200, Train Loss: 0.134000, Train-Class-Acc: {0: '99.18%', 1: '98.98%', 2: '99.48%', 3: '99.28%'}\n",
      "Val Loss: 5.292777, Val Acc: 86.76%, Val-Class-Acc: {0: '84.78%', 1: '78.28%', 2: '86.81%', 3: '96.72%'}, LR: 0.000590\n",
      "Epoch 79/200, Train Loss: 0.047491, Train-Class-Acc: {0: '99.59%', 1: '99.39%', 2: '99.48%', 3: '99.80%'}\n",
      "Val Loss: 5.633325, Val Acc: 86.64%, Val-Class-Acc: {0: '89.67%', 1: '76.23%', 2: '84.72%', 3: '95.90%'}, LR: 0.000590\n",
      "Epoch 80/200, Train Loss: 0.115239, Train-Class-Acc: {0: '99.18%', 1: '99.28%', 2: '98.96%', 3: '99.49%'}\n",
      "Val Loss: 5.443731, Val Acc: 86.15%, Val-Class-Acc: {0: '90.76%', 1: '77.05%', 2: '84.72%', 3: '92.62%'}, LR: 0.000590\n",
      "Epoch 81/200, Train Loss: 0.087170, Train-Class-Acc: {0: '98.91%', 1: '98.77%', 2: '99.31%', 3: '99.80%'}\n",
      "Val Loss: 4.888262, Val Acc: 86.64%, Val-Class-Acc: {0: '86.41%', 1: '77.87%', 2: '88.89%', 3: '94.26%'}, LR: 0.000590\n",
      "Epoch 82/200, Train Loss: 0.234833, Train-Class-Acc: {0: '99.18%', 1: '98.98%', 2: '99.31%', 3: '99.18%'}\n",
      "Val Loss: 5.612121, Val Acc: 86.15%, Val-Class-Acc: {0: '85.87%', 1: '79.10%', 2: '84.03%', 3: '94.67%'}, LR: 0.000590\n",
      "Epoch 83/200, Train Loss: 0.169118, Train-Class-Acc: {0: '99.05%', 1: '98.36%', 2: '98.96%', 3: '99.39%'}\n",
      "Val Loss: 5.843650, Val Acc: 86.52%, Val-Class-Acc: {0: '85.87%', 1: '74.59%', 2: '88.89%', 3: '97.54%'}, LR: 0.000590\n",
      "Epoch 84/200, Train Loss: 0.079169, Train-Class-Acc: {0: '98.91%', 1: '99.08%', 2: '99.31%', 3: '99.59%'}\n",
      "Val Loss: 5.530100, Val Acc: 87.50%, Val-Class-Acc: {0: '87.50%', 1: '80.33%', 2: '83.33%', 3: '97.13%'}, LR: 0.000590\n",
      "Epoch 85/200, Train Loss: 0.091378, Train-Class-Acc: {0: '99.05%', 1: '99.08%', 2: '98.96%', 3: '99.49%'}\n",
      "Val Loss: 7.500537, Val Acc: 86.52%, Val-Class-Acc: {0: '86.41%', 1: '75.41%', 2: '88.19%', 3: '96.72%'}, LR: 0.000590\n",
      "Epoch 86/200, Train Loss: 0.539222, Train-Class-Acc: {0: '97.96%', 1: '97.54%', 2: '98.44%', 3: '98.77%'}\n",
      "Val Loss: 7.230286, Val Acc: 87.50%, Val-Class-Acc: {0: '89.13%', 1: '79.92%', 2: '84.03%', 3: '95.90%'}, LR: 0.000590\n",
      "Epoch 87/200, Train Loss: 0.232931, Train-Class-Acc: {0: '99.05%', 1: '98.57%', 2: '98.09%', 3: '99.08%'}\n",
      "Val Loss: 5.605006, Val Acc: 87.01%, Val-Class-Acc: {0: '79.89%', 1: '81.56%', 2: '88.89%', 3: '96.72%'}, LR: 0.000531\n",
      "Epoch 88/200, Train Loss: 0.204246, Train-Class-Acc: {0: '97.96%', 1: '97.75%', 2: '98.96%', 3: '99.39%'}\n",
      "Val Loss: 5.491636, Val Acc: 86.52%, Val-Class-Acc: {0: '85.33%', 1: '77.87%', 2: '90.28%', 3: '93.85%'}, LR: 0.000531\n",
      "Epoch 89/200, Train Loss: 0.114103, Train-Class-Acc: {0: '99.32%', 1: '99.18%', 2: '99.31%', 3: '99.49%'}\n",
      "Val Loss: 6.096846, Val Acc: 86.27%, Val-Class-Acc: {0: '83.70%', 1: '79.10%', 2: '89.58%', 3: '93.44%'}, LR: 0.000531\n",
      "Epoch 90/200, Train Loss: 0.144234, Train-Class-Acc: {0: '98.77%', 1: '98.67%', 2: '99.13%', 3: '99.18%'}\n",
      "Val Loss: 5.578258, Val Acc: 86.64%, Val-Class-Acc: {0: '77.72%', 1: '84.43%', 2: '86.11%', 3: '95.90%'}, LR: 0.000531\n",
      "Epoch 91/200, Train Loss: 0.078263, Train-Class-Acc: {0: '98.50%', 1: '98.77%', 2: '99.83%', 3: '99.69%'}\n",
      "Val Loss: 5.714015, Val Acc: 86.64%, Val-Class-Acc: {0: '83.70%', 1: '81.15%', 2: '84.72%', 3: '95.49%'}, LR: 0.000531\n",
      "Epoch 92/200, Train Loss: 0.086413, Train-Class-Acc: {0: '99.05%', 1: '99.08%', 2: '99.31%', 3: '99.80%'}\n",
      "Val Loss: 5.647472, Val Acc: 87.62%, Val-Class-Acc: {0: '86.41%', 1: '81.15%', 2: '86.81%', 3: '95.49%'}, LR: 0.000531\n",
      "Epoch 93/200, Train Loss: 0.060218, Train-Class-Acc: {0: '99.59%', 1: '99.39%', 2: '99.13%', 3: '99.69%'}\n",
      "Val Loss: 5.476304, Val Acc: 86.52%, Val-Class-Acc: {0: '78.26%', 1: '82.38%', 2: '88.89%', 3: '95.49%'}, LR: 0.000531\n",
      "Epoch 94/200, Train Loss: 0.213204, Train-Class-Acc: {0: '99.32%', 1: '99.28%', 2: '98.96%', 3: '99.39%'}\n",
      "Val Loss: 7.584354, Val Acc: 86.64%, Val-Class-Acc: {0: '88.59%', 1: '73.77%', 2: '88.89%', 3: '96.72%'}, LR: 0.000531\n",
      "Epoch 95/200, Train Loss: 0.116598, Train-Class-Acc: {0: '99.32%', 1: '98.67%', 2: '99.48%', 3: '99.49%'}\n",
      "Val Loss: 5.841688, Val Acc: 87.50%, Val-Class-Acc: {0: '84.24%', 1: '81.56%', 2: '86.81%', 3: '96.31%'}, LR: 0.000531\n",
      "Epoch 96/200, Train Loss: 0.118054, Train-Class-Acc: {0: '99.18%', 1: '98.87%', 2: '99.13%', 3: '98.98%'}\n",
      "Val Loss: 5.998168, Val Acc: 86.27%, Val-Class-Acc: {0: '78.80%', 1: '82.79%', 2: '84.72%', 3: '96.31%'}, LR: 0.000531\n",
      "Epoch 97/200, Train Loss: 0.069029, Train-Class-Acc: {0: '98.91%', 1: '99.59%', 2: '99.48%', 3: '99.59%'}\n",
      "Val Loss: 6.008742, Val Acc: 86.64%, Val-Class-Acc: {0: '85.87%', 1: '77.46%', 2: '84.03%', 3: '97.95%'}, LR: 0.000531\n",
      "Epoch 98/200, Train Loss: 0.280830, Train-Class-Acc: {0: '99.05%', 1: '98.67%', 2: '98.96%', 3: '99.18%'}\n",
      "Val Loss: 5.748895, Val Acc: 87.25%, Val-Class-Acc: {0: '88.04%', 1: '79.51%', 2: '84.03%', 3: '96.31%'}, LR: 0.000478\n",
      "Epoch 99/200, Train Loss: 0.079133, Train-Class-Acc: {0: '99.18%', 1: '98.87%', 2: '99.65%', 3: '99.69%'}\n",
      "Val Loss: 5.754571, Val Acc: 86.52%, Val-Class-Acc: {0: '83.15%', 1: '80.33%', 2: '83.33%', 3: '97.13%'}, LR: 0.000478\n",
      "Epoch 100/200, Train Loss: 0.078470, Train-Class-Acc: {0: '98.91%', 1: '98.98%', 2: '99.13%', 3: '99.69%'}\n",
      "Val Loss: 5.315287, Val Acc: 87.01%, Val-Class-Acc: {0: '85.87%', 1: '79.92%', 2: '85.42%', 3: '95.90%'}, LR: 0.000478\n",
      "Epoch 101/200, Train Loss: 0.036679, Train-Class-Acc: {0: '99.32%', 1: '99.39%', 2: '99.65%', 3: '99.49%'}\n",
      "Val Loss: 5.467148, Val Acc: 86.64%, Val-Class-Acc: {0: '81.52%', 1: '80.74%', 2: '86.81%', 3: '96.31%'}, LR: 0.000478\n",
      "Epoch 102/200, Train Loss: 0.072139, Train-Class-Acc: {0: '98.91%', 1: '99.49%', 2: '99.48%', 3: '99.90%'}\n",
      "Val Loss: 5.304335, Val Acc: 87.38%, Val-Class-Acc: {0: '84.24%', 1: '84.43%', 2: '81.25%', 3: '96.31%'}, LR: 0.000478\n",
      "Epoch 103/200, Train Loss: 0.030672, Train-Class-Acc: {0: '99.86%', 1: '99.59%', 2: '99.65%', 3: '99.59%'}\n",
      "Val Loss: 5.292199, Val Acc: 86.89%, Val-Class-Acc: {0: '86.41%', 1: '79.10%', 2: '84.72%', 3: '96.31%'}, LR: 0.000478\n",
      "Epoch 104/200, Train Loss: 0.074696, Train-Class-Acc: {0: '99.18%', 1: '99.18%', 2: '99.65%', 3: '100.00%'}\n",
      "Val Loss: 5.089379, Val Acc: 86.76%, Val-Class-Acc: {0: '80.98%', 1: '84.43%', 2: '83.33%', 3: '95.49%'}, LR: 0.000478\n",
      "Epoch 105/200, Train Loss: 0.040182, Train-Class-Acc: {0: '99.73%', 1: '99.49%', 2: '99.83%', 3: '99.80%'}\n",
      "Val Loss: 5.944128, Val Acc: 87.25%, Val-Class-Acc: {0: '88.59%', 1: '79.51%', 2: '84.03%', 3: '95.90%'}, LR: 0.000478\n",
      "Epoch 106/200, Train Loss: 0.060210, Train-Class-Acc: {0: '99.59%', 1: '99.49%', 2: '99.65%', 3: '99.59%'}\n",
      "Val Loss: 5.868864, Val Acc: 85.29%, Val-Class-Acc: {0: '87.50%', 1: '76.23%', 2: '84.72%', 3: '93.03%'}, LR: 0.000478\n",
      "Epoch 107/200, Train Loss: 0.155640, Train-Class-Acc: {0: '99.05%', 1: '99.18%', 2: '99.65%', 3: '99.28%'}\n",
      "Val Loss: 5.967796, Val Acc: 87.38%, Val-Class-Acc: {0: '89.67%', 1: '77.05%', 2: '84.03%', 3: '97.95%'}, LR: 0.000478\n",
      "Epoch 108/200, Train Loss: 0.045899, Train-Class-Acc: {0: '99.46%', 1: '99.39%', 2: '99.31%', 3: '99.49%'}\n",
      "Val Loss: 6.096491, Val Acc: 86.89%, Val-Class-Acc: {0: '88.04%', 1: '78.28%', 2: '83.33%', 3: '96.72%'}, LR: 0.000478\n",
      "Epoch 109/200, Train Loss: 0.035072, Train-Class-Acc: {0: '99.59%', 1: '99.18%', 2: '99.83%', 3: '99.90%'}\n",
      "Val Loss: 5.293425, Val Acc: 87.75%, Val-Class-Acc: {0: '90.22%', 1: '78.69%', 2: '85.42%', 3: '96.31%'}, LR: 0.000430\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_44.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_109.pth\n",
      "Epoch 110/200, Train Loss: 0.053182, Train-Class-Acc: {0: '99.18%', 1: '99.28%', 2: '99.48%', 3: '99.90%'}\n",
      "Val Loss: 5.039366, Val Acc: 87.13%, Val-Class-Acc: {0: '84.24%', 1: '81.56%', 2: '83.33%', 3: '97.13%'}, LR: 0.000430\n",
      "Epoch 111/200, Train Loss: 0.071608, Train-Class-Acc: {0: '98.64%', 1: '98.77%', 2: '99.31%', 3: '99.90%'}\n",
      "Val Loss: 5.443412, Val Acc: 87.50%, Val-Class-Acc: {0: '90.76%', 1: '79.51%', 2: '83.33%', 3: '95.49%'}, LR: 0.000430\n",
      "Epoch 112/200, Train Loss: 0.053371, Train-Class-Acc: {0: '99.59%', 1: '99.39%', 2: '99.48%', 3: '99.59%'}\n",
      "Val Loss: 5.180703, Val Acc: 88.60%, Val-Class-Acc: {0: '92.39%', 1: '79.51%', 2: '84.03%', 3: '97.54%'}, LR: 0.000430\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_32.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_112.pth\n",
      "Epoch 113/200, Train Loss: 0.141981, Train-Class-Acc: {0: '98.50%', 1: '99.18%', 2: '99.48%', 3: '99.69%'}\n",
      "Val Loss: 5.427702, Val Acc: 86.15%, Val-Class-Acc: {0: '86.41%', 1: '75.82%', 2: '84.72%', 3: '97.13%'}, LR: 0.000430\n",
      "Epoch 114/200, Train Loss: 0.051928, Train-Class-Acc: {0: '99.46%', 1: '99.39%', 2: '99.83%', 3: '99.59%'}\n",
      "Val Loss: 4.926001, Val Acc: 88.36%, Val-Class-Acc: {0: '89.13%', 1: '81.56%', 2: '84.03%', 3: '97.13%'}, LR: 0.000430\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_48.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_114.pth\n",
      "Epoch 115/200, Train Loss: 0.026971, Train-Class-Acc: {0: '99.32%', 1: '99.69%', 2: '99.65%', 3: '99.80%'}\n",
      "Val Loss: 5.242550, Val Acc: 88.36%, Val-Class-Acc: {0: '87.50%', 1: '81.97%', 2: '84.72%', 3: '97.54%'}, LR: 0.000430\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_109.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_115.pth\n",
      "Epoch 116/200, Train Loss: 0.044874, Train-Class-Acc: {0: '99.32%', 1: '99.28%', 2: '99.83%', 3: '99.80%'}\n",
      "Val Loss: 5.206062, Val Acc: 87.25%, Val-Class-Acc: {0: '84.78%', 1: '82.38%', 2: '85.42%', 3: '95.08%'}, LR: 0.000430\n",
      "Epoch 117/200, Train Loss: 0.089956, Train-Class-Acc: {0: '99.86%', 1: '99.28%', 2: '99.13%', 3: '99.59%'}\n",
      "Val Loss: 8.965479, Val Acc: 85.29%, Val-Class-Acc: {0: '86.41%', 1: '70.49%', 2: '88.19%', 3: '97.54%'}, LR: 0.000430\n",
      "Epoch 118/200, Train Loss: 0.091459, Train-Class-Acc: {0: '99.05%', 1: '99.28%', 2: '99.31%', 3: '99.49%'}\n",
      "Val Loss: 6.676454, Val Acc: 86.64%, Val-Class-Acc: {0: '86.41%', 1: '77.46%', 2: '84.03%', 3: '97.54%'}, LR: 0.000430\n",
      "Epoch 119/200, Train Loss: 0.072833, Train-Class-Acc: {0: '99.59%', 1: '99.69%', 2: '99.31%', 3: '99.59%'}\n",
      "Val Loss: 6.234802, Val Acc: 86.76%, Val-Class-Acc: {0: '83.15%', 1: '81.97%', 2: '82.64%', 3: '96.72%'}, LR: 0.000430\n",
      "Epoch 120/200, Train Loss: 0.022645, Train-Class-Acc: {0: '99.59%', 1: '99.59%', 2: '99.48%', 3: '100.00%'}\n",
      "Val Loss: 5.939068, Val Acc: 87.38%, Val-Class-Acc: {0: '87.50%', 1: '80.33%', 2: '85.42%', 3: '95.49%'}, LR: 0.000387\n",
      "Epoch 121/200, Train Loss: 0.122691, Train-Class-Acc: {0: '99.59%', 1: '99.39%', 2: '99.65%', 3: '99.49%'}\n",
      "Val Loss: 6.146290, Val Acc: 87.62%, Val-Class-Acc: {0: '86.41%', 1: '79.10%', 2: '86.81%', 3: '97.54%'}, LR: 0.000387\n",
      "Epoch 122/200, Train Loss: 0.141570, Train-Class-Acc: {0: '99.32%', 1: '99.18%', 2: '99.13%', 3: '99.49%'}\n",
      "Val Loss: 7.017757, Val Acc: 86.27%, Val-Class-Acc: {0: '84.24%', 1: '78.28%', 2: '82.64%', 3: '97.95%'}, LR: 0.000387\n",
      "Epoch 123/200, Train Loss: 0.078222, Train-Class-Acc: {0: '99.73%', 1: '99.28%', 2: '98.96%', 3: '99.90%'}\n",
      "Val Loss: 6.531556, Val Acc: 86.27%, Val-Class-Acc: {0: '83.70%', 1: '80.74%', 2: '84.72%', 3: '94.67%'}, LR: 0.000387\n",
      "Epoch 124/200, Train Loss: 0.045973, Train-Class-Acc: {0: '99.46%', 1: '99.49%', 2: '99.65%', 3: '99.69%'}\n",
      "Val Loss: 6.450742, Val Acc: 86.64%, Val-Class-Acc: {0: '86.41%', 1: '78.28%', 2: '85.42%', 3: '95.90%'}, LR: 0.000387\n",
      "Epoch 125/200, Train Loss: 0.078466, Train-Class-Acc: {0: '99.59%', 1: '99.08%', 2: '99.83%', 3: '99.59%'}\n",
      "Val Loss: 5.964353, Val Acc: 87.13%, Val-Class-Acc: {0: '86.41%', 1: '82.79%', 2: '84.72%', 3: '93.44%'}, LR: 0.000387\n",
      "Epoch 126/200, Train Loss: 0.046217, Train-Class-Acc: {0: '99.32%', 1: '99.39%', 2: '99.48%', 3: '99.69%'}\n",
      "Val Loss: 5.967096, Val Acc: 86.76%, Val-Class-Acc: {0: '88.04%', 1: '75.41%', 2: '87.50%', 3: '96.72%'}, LR: 0.000387\n",
      "Epoch 127/200, Train Loss: 0.145435, Train-Class-Acc: {0: '99.46%', 1: '99.08%', 2: '99.31%', 3: '99.18%'}\n",
      "Val Loss: 6.292521, Val Acc: 88.24%, Val-Class-Acc: {0: '84.78%', 1: '82.38%', 2: '88.19%', 3: '96.72%'}, LR: 0.000387\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_39.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_127.pth\n",
      "Epoch 128/200, Train Loss: 0.106523, Train-Class-Acc: {0: '99.46%', 1: '99.28%', 2: '99.48%', 3: '99.49%'}\n",
      "Val Loss: 6.316510, Val Acc: 87.01%, Val-Class-Acc: {0: '81.52%', 1: '85.66%', 2: '81.25%', 3: '95.90%'}, LR: 0.000387\n",
      "Epoch 129/200, Train Loss: 0.087134, Train-Class-Acc: {0: '99.32%', 1: '99.28%', 2: '99.48%', 3: '99.59%'}\n",
      "Val Loss: 5.769009, Val Acc: 88.11%, Val-Class-Acc: {0: '88.04%', 1: '81.15%', 2: '86.11%', 3: '96.31%'}, LR: 0.000387\n",
      "Epoch 130/200, Train Loss: 0.064856, Train-Class-Acc: {0: '99.73%', 1: '99.28%', 2: '99.13%', 3: '99.69%'}\n",
      "Val Loss: 6.513144, Val Acc: 86.76%, Val-Class-Acc: {0: '81.52%', 1: '78.69%', 2: '90.28%', 3: '96.72%'}, LR: 0.000387\n",
      "Epoch 131/200, Train Loss: 0.094771, Train-Class-Acc: {0: '99.46%', 1: '99.39%', 2: '99.13%', 3: '99.69%'}\n",
      "Val Loss: 6.586492, Val Acc: 87.50%, Val-Class-Acc: {0: '88.59%', 1: '78.69%', 2: '88.89%', 3: '94.67%'}, LR: 0.000349\n",
      "Epoch 132/200, Train Loss: 0.028161, Train-Class-Acc: {0: '99.46%', 1: '99.39%', 2: '100.00%', 3: '100.00%'}\n",
      "Val Loss: 6.882259, Val Acc: 87.62%, Val-Class-Acc: {0: '85.87%', 1: '85.25%', 2: '85.42%', 3: '92.62%'}, LR: 0.000349\n",
      "Epoch 133/200, Train Loss: 0.113266, Train-Class-Acc: {0: '99.59%', 1: '99.28%', 2: '99.65%', 3: '99.39%'}\n",
      "Val Loss: 6.042976, Val Acc: 86.89%, Val-Class-Acc: {0: '89.67%', 1: '75.00%', 2: '88.89%', 3: '95.49%'}, LR: 0.000349\n",
      "Epoch 134/200, Train Loss: 0.073673, Train-Class-Acc: {0: '99.46%', 1: '98.98%', 2: '99.65%', 3: '99.69%'}\n",
      "Val Loss: 6.054652, Val Acc: 87.87%, Val-Class-Acc: {0: '85.33%', 1: '82.79%', 2: '87.50%', 3: '95.08%'}, LR: 0.000349\n",
      "Epoch 135/200, Train Loss: 0.029429, Train-Class-Acc: {0: '99.32%', 1: '99.39%', 2: '99.65%', 3: '99.90%'}\n",
      "Val Loss: 6.695367, Val Acc: 87.62%, Val-Class-Acc: {0: '84.78%', 1: '81.97%', 2: '87.50%', 3: '95.49%'}, LR: 0.000349\n",
      "Epoch 136/200, Train Loss: 0.094735, Train-Class-Acc: {0: '99.73%', 1: '99.49%', 2: '99.65%', 3: '99.59%'}\n",
      "Val Loss: 6.445995, Val Acc: 87.62%, Val-Class-Acc: {0: '82.07%', 1: '85.66%', 2: '82.64%', 3: '96.72%'}, LR: 0.000349\n",
      "Epoch 137/200, Train Loss: 0.021455, Train-Class-Acc: {0: '99.86%', 1: '99.69%', 2: '99.65%', 3: '100.00%'}\n",
      "Val Loss: 6.033670, Val Acc: 87.75%, Val-Class-Acc: {0: '82.07%', 1: '85.25%', 2: '86.11%', 3: '95.49%'}, LR: 0.000349\n",
      "Epoch 138/200, Train Loss: 0.071872, Train-Class-Acc: {0: '99.18%', 1: '99.28%', 2: '99.48%', 3: '99.90%'}\n",
      "Val Loss: 7.493706, Val Acc: 87.13%, Val-Class-Acc: {0: '82.07%', 1: '80.33%', 2: '89.58%', 3: '96.31%'}, LR: 0.000349\n",
      "Epoch 139/200, Train Loss: 0.048421, Train-Class-Acc: {0: '99.73%', 1: '99.28%', 2: '99.83%', 3: '99.90%'}\n",
      "Val Loss: 6.542764, Val Acc: 87.13%, Val-Class-Acc: {0: '83.15%', 1: '84.02%', 2: '82.64%', 3: '95.90%'}, LR: 0.000349\n",
      "Epoch 140/200, Train Loss: 0.072587, Train-Class-Acc: {0: '99.59%', 1: '99.28%', 2: '99.48%', 3: '99.49%'}\n",
      "Val Loss: 6.533193, Val Acc: 86.40%, Val-Class-Acc: {0: '88.04%', 1: '74.59%', 2: '85.42%', 3: '97.54%'}, LR: 0.000349\n",
      "Epoch 141/200, Train Loss: 0.108813, Train-Class-Acc: {0: '99.32%', 1: '99.28%', 2: '99.48%', 3: '99.90%'}\n",
      "Val Loss: 4.895235, Val Acc: 87.62%, Val-Class-Acc: {0: '85.87%', 1: '81.15%', 2: '87.50%', 3: '95.49%'}, LR: 0.000349\n",
      "Epoch 142/200, Train Loss: 0.063779, Train-Class-Acc: {0: '99.18%', 1: '99.49%', 2: '99.65%', 3: '99.39%'}\n",
      "Val Loss: 5.845107, Val Acc: 85.91%, Val-Class-Acc: {0: '85.87%', 1: '77.87%', 2: '82.64%', 3: '95.90%'}, LR: 0.000314\n",
      "Epoch 143/200, Train Loss: 0.063704, Train-Class-Acc: {0: '99.32%', 1: '98.98%', 2: '99.65%', 3: '100.00%'}\n",
      "Val Loss: 6.030185, Val Acc: 87.13%, Val-Class-Acc: {0: '88.04%', 1: '78.69%', 2: '86.81%', 3: '95.08%'}, LR: 0.000314\n",
      "Epoch 144/200, Train Loss: 0.020510, Train-Class-Acc: {0: '99.59%', 1: '99.80%', 2: '99.83%', 3: '99.80%'}\n",
      "Val Loss: 5.818605, Val Acc: 86.52%, Val-Class-Acc: {0: '83.70%', 1: '78.69%', 2: '88.89%', 3: '95.08%'}, LR: 0.000314\n",
      "Epoch 145/200, Train Loss: 0.026753, Train-Class-Acc: {0: '99.73%', 1: '99.59%', 2: '99.83%', 3: '99.90%'}\n",
      "Val Loss: 5.712788, Val Acc: 87.13%, Val-Class-Acc: {0: '87.50%', 1: '80.33%', 2: '83.33%', 3: '95.90%'}, LR: 0.000314\n",
      "Epoch 146/200, Train Loss: 0.059220, Train-Class-Acc: {0: '99.86%', 1: '99.69%', 2: '99.65%', 3: '99.90%'}\n",
      "Val Loss: 7.442676, Val Acc: 86.64%, Val-Class-Acc: {0: '84.78%', 1: '77.46%', 2: '88.19%', 3: '96.31%'}, LR: 0.000314\n",
      "Epoch 147/200, Train Loss: 0.037970, Train-Class-Acc: {0: '99.86%', 1: '99.69%', 2: '99.65%', 3: '99.59%'}\n",
      "Val Loss: 6.631152, Val Acc: 87.25%, Val-Class-Acc: {0: '89.13%', 1: '77.87%', 2: '86.11%', 3: '95.90%'}, LR: 0.000314\n",
      "Epoch 148/200, Train Loss: 0.116647, Train-Class-Acc: {0: '99.59%', 1: '99.49%', 2: '99.83%', 3: '99.69%'}\n",
      "Val Loss: 7.341654, Val Acc: 87.38%, Val-Class-Acc: {0: '88.04%', 1: '78.28%', 2: '84.03%', 3: '97.95%'}, LR: 0.000314\n",
      "Epoch 149/200, Train Loss: 0.041802, Train-Class-Acc: {0: '99.59%', 1: '99.59%', 2: '99.65%', 3: '99.80%'}\n",
      "Val Loss: 6.737298, Val Acc: 87.75%, Val-Class-Acc: {0: '86.96%', 1: '83.61%', 2: '81.25%', 3: '96.31%'}, LR: 0.000314\n",
      "Epoch 150/200, Train Loss: 0.054472, Train-Class-Acc: {0: '99.59%', 1: '99.69%', 2: '99.65%', 3: '99.69%'}\n",
      "Val Loss: 6.696641, Val Acc: 86.76%, Val-Class-Acc: {0: '90.76%', 1: '78.28%', 2: '84.03%', 3: '93.85%'}, LR: 0.000314\n",
      "Epoch 151/200, Train Loss: 0.308388, Train-Class-Acc: {0: '99.59%', 1: '99.49%', 2: '99.65%', 3: '98.87%'}\n",
      "Val Loss: 7.640464, Val Acc: 87.01%, Val-Class-Acc: {0: '88.59%', 1: '82.38%', 2: '81.94%', 3: '93.44%'}, LR: 0.000314\n",
      "Epoch 152/200, Train Loss: 0.042134, Train-Class-Acc: {0: '99.86%', 1: '99.49%', 2: '98.79%', 3: '99.90%'}\n",
      "Val Loss: 7.151016, Val Acc: 87.38%, Val-Class-Acc: {0: '86.96%', 1: '79.51%', 2: '88.19%', 3: '95.08%'}, LR: 0.000314\n",
      "Epoch 153/200, Train Loss: 0.052082, Train-Class-Acc: {0: '99.46%', 1: '99.69%', 2: '99.83%', 3: '99.80%'}\n",
      "Val Loss: 7.127752, Val Acc: 87.13%, Val-Class-Acc: {0: '85.87%', 1: '78.69%', 2: '86.81%', 3: '96.72%'}, LR: 0.000282\n",
      "Epoch 154/200, Train Loss: 0.051313, Train-Class-Acc: {0: '99.86%', 1: '99.59%', 2: '99.48%', 3: '99.59%'}\n",
      "Val Loss: 6.226601, Val Acc: 87.01%, Val-Class-Acc: {0: '84.24%', 1: '79.92%', 2: '88.19%', 3: '95.49%'}, LR: 0.000282\n",
      "Epoch 155/200, Train Loss: 0.025335, Train-Class-Acc: {0: '99.59%', 1: '99.69%', 2: '99.65%', 3: '99.90%'}\n",
      "Val Loss: 5.959474, Val Acc: 87.50%, Val-Class-Acc: {0: '89.13%', 1: '79.10%', 2: '86.11%', 3: '95.49%'}, LR: 0.000282\n",
      "Epoch 156/200, Train Loss: 0.027149, Train-Class-Acc: {0: '99.73%', 1: '99.90%', 2: '99.83%', 3: '99.69%'}\n",
      "Val Loss: 6.116544, Val Acc: 87.13%, Val-Class-Acc: {0: '86.41%', 1: '77.05%', 2: '87.50%', 3: '97.54%'}, LR: 0.000282\n",
      "Epoch 157/200, Train Loss: 0.033357, Train-Class-Acc: {0: '99.59%', 1: '99.28%', 2: '99.48%', 3: '99.80%'}\n",
      "Val Loss: 5.983855, Val Acc: 87.87%, Val-Class-Acc: {0: '85.87%', 1: '82.38%', 2: '86.81%', 3: '95.49%'}, LR: 0.000282\n",
      "Epoch 158/200, Train Loss: 0.012289, Train-Class-Acc: {0: '99.73%', 1: '99.90%', 2: '99.83%', 3: '99.90%'}\n",
      "Val Loss: 6.539404, Val Acc: 87.62%, Val-Class-Acc: {0: '89.67%', 1: '78.69%', 2: '84.03%', 3: '97.13%'}, LR: 0.000282\n",
      "Epoch 159/200, Train Loss: 0.021056, Train-Class-Acc: {0: '99.86%', 1: '99.69%', 2: '99.48%', 3: '99.90%'}\n",
      "Val Loss: 6.020889, Val Acc: 87.38%, Val-Class-Acc: {0: '85.87%', 1: '79.92%', 2: '88.19%', 3: '95.49%'}, LR: 0.000282\n",
      "Epoch 160/200, Train Loss: 0.008955, Train-Class-Acc: {0: '99.73%', 1: '99.80%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 6.225178, Val Acc: 87.25%, Val-Class-Acc: {0: '89.67%', 1: '78.28%', 2: '87.50%', 3: '94.26%'}, LR: 0.000282\n",
      "Epoch 161/200, Train Loss: 0.025784, Train-Class-Acc: {0: '100.00%', 1: '99.69%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 6.093623, Val Acc: 87.25%, Val-Class-Acc: {0: '84.78%', 1: '83.61%', 2: '86.11%', 3: '93.44%'}, LR: 0.000282\n",
      "Epoch 162/200, Train Loss: 0.016059, Train-Class-Acc: {0: '99.73%', 1: '99.90%', 2: '99.83%', 3: '99.80%'}\n",
      "Val Loss: 6.298590, Val Acc: 87.01%, Val-Class-Acc: {0: '86.96%', 1: '78.69%', 2: '86.81%', 3: '95.49%'}, LR: 0.000282\n",
      "Epoch 163/200, Train Loss: 0.033270, Train-Class-Acc: {0: '99.86%', 1: '99.80%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 7.007747, Val Acc: 85.54%, Val-Class-Acc: {0: '85.87%', 1: '79.51%', 2: '86.11%', 3: '90.98%'}, LR: 0.000282\n",
      "Epoch 164/200, Train Loss: 0.030436, Train-Class-Acc: {0: '99.86%', 1: '99.80%', 2: '99.83%', 3: '99.90%'}\n",
      "Val Loss: 6.186052, Val Acc: 86.52%, Val-Class-Acc: {0: '85.87%', 1: '76.23%', 2: '88.19%', 3: '96.31%'}, LR: 0.000254\n",
      "Epoch 165/200, Train Loss: 0.019759, Train-Class-Acc: {0: '99.86%', 1: '99.69%', 2: '99.83%', 3: '99.90%'}\n",
      "Val Loss: 6.023771, Val Acc: 86.03%, Val-Class-Acc: {0: '84.78%', 1: '77.87%', 2: '86.11%', 3: '95.08%'}, LR: 0.000254\n",
      "Epoch 166/200, Train Loss: 0.018061, Train-Class-Acc: {0: '99.86%', 1: '99.90%', 2: '99.83%', 3: '99.90%'}\n",
      "Val Loss: 5.871097, Val Acc: 86.64%, Val-Class-Acc: {0: '84.78%', 1: '81.56%', 2: '84.03%', 3: '94.67%'}, LR: 0.000254\n",
      "Epoch 167/200, Train Loss: 0.020456, Train-Class-Acc: {0: '99.73%', 1: '99.80%', 2: '99.48%', 3: '100.00%'}\n",
      "Val Loss: 6.341614, Val Acc: 87.38%, Val-Class-Acc: {0: '90.22%', 1: '76.64%', 2: '85.42%', 3: '97.13%'}, LR: 0.000254\n",
      "Epoch 168/200, Train Loss: 0.005258, Train-Class-Acc: {0: '99.86%', 1: '99.80%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 5.813244, Val Acc: 87.13%, Val-Class-Acc: {0: '82.61%', 1: '81.15%', 2: '87.50%', 3: '96.31%'}, LR: 0.000254\n",
      "Epoch 169/200, Train Loss: 0.005448, Train-Class-Acc: {0: '99.86%', 1: '100.00%', 2: '99.83%', 3: '100.00%'}\n",
      "Val Loss: 6.095858, Val Acc: 87.50%, Val-Class-Acc: {0: '88.59%', 1: '77.46%', 2: '87.50%', 3: '96.72%'}, LR: 0.000254\n",
      "Epoch 170/200, Train Loss: 0.000807, Train-Class-Acc: {0: '100.00%', 1: '99.90%', 2: '100.00%', 3: '100.00%'}\n",
      "Val Loss: 5.817205, Val Acc: 87.75%, Val-Class-Acc: {0: '85.87%', 1: '81.15%', 2: '86.11%', 3: '96.72%'}, LR: 0.000254\n",
      "Epoch 171/200, Train Loss: 0.005498, Train-Class-Acc: {0: '100.00%', 1: '99.90%', 2: '100.00%', 3: '100.00%'}\n",
      "Val Loss: 6.000939, Val Acc: 86.40%, Val-Class-Acc: {0: '84.78%', 1: '81.97%', 2: '85.42%', 3: '92.62%'}, LR: 0.000254\n",
      "Epoch 172/200, Train Loss: 0.006252, Train-Class-Acc: {0: '99.86%', 1: '100.00%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 6.014193, Val Acc: 87.50%, Val-Class-Acc: {0: '90.22%', 1: '79.51%', 2: '85.42%', 3: '94.67%'}, LR: 0.000254\n",
      "Epoch 173/200, Train Loss: 0.014268, Train-Class-Acc: {0: '100.00%', 1: '99.90%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 5.762366, Val Acc: 87.01%, Val-Class-Acc: {0: '84.24%', 1: '80.74%', 2: '84.72%', 3: '96.72%'}, LR: 0.000254\n",
      "Epoch 174/200, Train Loss: 0.009428, Train-Class-Acc: {0: '100.00%', 1: '99.80%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 5.668487, Val Acc: 87.25%, Val-Class-Acc: {0: '83.15%', 1: '82.79%', 2: '85.42%', 3: '95.90%'}, LR: 0.000254\n",
      "Epoch 175/200, Train Loss: 0.000531, Train-Class-Acc: {0: '99.86%', 1: '100.00%', 2: '100.00%', 3: '100.00%'}\n",
      "Val Loss: 5.711034, Val Acc: 87.25%, Val-Class-Acc: {0: '84.78%', 1: '81.97%', 2: '86.11%', 3: '95.08%'}, LR: 0.000229\n",
      "Epoch 176/200, Train Loss: 0.008386, Train-Class-Acc: {0: '99.86%', 1: '99.80%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 5.927978, Val Acc: 87.38%, Val-Class-Acc: {0: '86.96%', 1: '79.51%', 2: '86.11%', 3: '96.31%'}, LR: 0.000229\n",
      "Epoch 177/200, Train Loss: 0.016376, Train-Class-Acc: {0: '99.73%', 1: '99.90%', 2: '100.00%', 3: '100.00%'}\n",
      "Val Loss: 6.173034, Val Acc: 87.01%, Val-Class-Acc: {0: '85.87%', 1: '80.74%', 2: '84.72%', 3: '95.49%'}, LR: 0.000229\n",
      "Epoch 178/200, Train Loss: 0.010255, Train-Class-Acc: {0: '99.86%', 1: '99.80%', 2: '100.00%', 3: '100.00%'}\n",
      "Val Loss: 5.735135, Val Acc: 87.38%, Val-Class-Acc: {0: '84.78%', 1: '81.97%', 2: '86.81%', 3: '95.08%'}, LR: 0.000229\n",
      "Epoch 179/200, Train Loss: 0.012655, Train-Class-Acc: {0: '100.00%', 1: '99.80%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 6.049065, Val Acc: 87.62%, Val-Class-Acc: {0: '83.70%', 1: '81.97%', 2: '88.19%', 3: '95.90%'}, LR: 0.000229\n",
      "Epoch 180/200, Train Loss: 0.016204, Train-Class-Acc: {0: '99.86%', 1: '100.00%', 2: '99.83%', 3: '99.69%'}\n",
      "Val Loss: 5.531887, Val Acc: 87.62%, Val-Class-Acc: {0: '89.67%', 1: '77.05%', 2: '88.89%', 3: '95.90%'}, LR: 0.000229\n",
      "Epoch 181/200, Train Loss: 0.054510, Train-Class-Acc: {0: '99.73%', 1: '99.69%', 2: '99.83%', 3: '99.59%'}\n",
      "Val Loss: 8.190695, Val Acc: 85.78%, Val-Class-Acc: {0: '84.24%', 1: '75.82%', 2: '84.03%', 3: '97.95%'}, LR: 0.000229\n",
      "Epoch 182/200, Train Loss: 0.164178, Train-Class-Acc: {0: '99.46%', 1: '99.28%', 2: '99.65%', 3: '99.90%'}\n",
      "Val Loss: 6.972647, Val Acc: 86.76%, Val-Class-Acc: {0: '87.50%', 1: '77.87%', 2: '86.81%', 3: '95.08%'}, LR: 0.000229\n",
      "Epoch 183/200, Train Loss: 0.006362, Train-Class-Acc: {0: '99.73%', 1: '99.69%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 6.826588, Val Acc: 87.13%, Val-Class-Acc: {0: '82.61%', 1: '81.97%', 2: '85.42%', 3: '96.72%'}, LR: 0.000229\n",
      "Epoch 184/200, Train Loss: 0.024311, Train-Class-Acc: {0: '99.73%', 1: '99.80%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 6.797963, Val Acc: 87.38%, Val-Class-Acc: {0: '87.50%', 1: '79.51%', 2: '86.81%', 3: '95.49%'}, LR: 0.000229\n",
      "Epoch 185/200, Train Loss: 0.075198, Train-Class-Acc: {0: '99.73%', 1: '99.39%', 2: '99.48%', 3: '99.69%'}\n",
      "Val Loss: 6.460068, Val Acc: 87.62%, Val-Class-Acc: {0: '89.67%', 1: '83.20%', 2: '86.11%', 3: '91.39%'}, LR: 0.000229\n",
      "Epoch 186/200, Train Loss: 0.028683, Train-Class-Acc: {0: '100.00%', 1: '99.80%', 2: '99.65%', 3: '99.80%'}\n",
      "Val Loss: 6.467184, Val Acc: 87.87%, Val-Class-Acc: {0: '84.78%', 1: '80.74%', 2: '88.89%', 3: '96.72%'}, LR: 0.000206\n",
      "Epoch 187/200, Train Loss: 0.040656, Train-Class-Acc: {0: '99.32%', 1: '99.69%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 6.566571, Val Acc: 88.24%, Val-Class-Acc: {0: '88.59%', 1: '79.51%', 2: '87.50%', 3: '97.13%'}, LR: 0.000206\n",
      "Epoch 188/200, Train Loss: 0.019887, Train-Class-Acc: {0: '99.86%', 1: '99.59%', 2: '99.83%', 3: '100.00%'}\n",
      "Val Loss: 6.517642, Val Acc: 86.76%, Val-Class-Acc: {0: '85.87%', 1: '81.56%', 2: '86.11%', 3: '93.03%'}, LR: 0.000206\n",
      "Epoch 189/200, Train Loss: 0.069319, Train-Class-Acc: {0: '99.86%', 1: '99.90%', 2: '99.83%', 3: '99.90%'}\n",
      "Val Loss: 6.310853, Val Acc: 86.89%, Val-Class-Acc: {0: '86.41%', 1: '78.69%', 2: '87.50%', 3: '95.08%'}, LR: 0.000206\n",
      "Epoch 190/200, Train Loss: 0.005764, Train-Class-Acc: {0: '99.86%', 1: '99.80%', 2: '100.00%', 3: '100.00%'}\n",
      "Val Loss: 5.896970, Val Acc: 87.38%, Val-Class-Acc: {0: '86.41%', 1: '79.92%', 2: '88.19%', 3: '95.08%'}, LR: 0.000206\n",
      "Epoch 191/200, Train Loss: 0.033433, Train-Class-Acc: {0: '99.86%', 1: '99.69%', 2: '99.65%', 3: '99.80%'}\n",
      "Val Loss: 6.512643, Val Acc: 86.40%, Val-Class-Acc: {0: '90.22%', 1: '75.41%', 2: '88.89%', 3: '93.03%'}, LR: 0.000206\n",
      "Epoch 192/200, Train Loss: 0.009550, Train-Class-Acc: {0: '99.86%', 1: '99.80%', 2: '100.00%', 3: '99.80%'}\n",
      "Val Loss: 6.076721, Val Acc: 87.62%, Val-Class-Acc: {0: '89.67%', 1: '78.28%', 2: '86.81%', 3: '95.90%'}, LR: 0.000206\n",
      "Epoch 193/200, Train Loss: 0.000015, Train-Class-Acc: {0: '100.00%', 1: '100.00%', 2: '100.00%', 3: '100.00%'}\n",
      "Val Loss: 6.139001, Val Acc: 87.38%, Val-Class-Acc: {0: '87.50%', 1: '79.92%', 2: '86.11%', 3: '95.49%'}, LR: 0.000206\n",
      "Epoch 194/200, Train Loss: 0.002397, Train-Class-Acc: {0: '100.00%', 1: '100.00%', 2: '99.83%', 3: '100.00%'}\n",
      "Val Loss: 6.368114, Val Acc: 87.75%, Val-Class-Acc: {0: '89.67%', 1: '78.69%', 2: '87.50%', 3: '95.49%'}, LR: 0.000206\n",
      "Epoch 195/200, Train Loss: 0.016623, Train-Class-Acc: {0: '100.00%', 1: '99.69%', 2: '99.83%', 3: '100.00%'}\n",
      "Val Loss: 6.217190, Val Acc: 87.75%, Val-Class-Acc: {0: '86.96%', 1: '82.79%', 2: '86.81%', 3: '93.85%'}, LR: 0.000206\n",
      "Epoch 196/200, Train Loss: 0.012319, Train-Class-Acc: {0: '99.59%', 1: '99.90%', 2: '99.83%', 3: '99.80%'}\n",
      "Val Loss: 6.232961, Val Acc: 86.27%, Val-Class-Acc: {0: '86.96%', 1: '79.51%', 2: '86.11%', 3: '92.62%'}, LR: 0.000206\n",
      "Epoch 197/200, Train Loss: 0.002478, Train-Class-Acc: {0: '99.86%', 1: '100.00%', 2: '100.00%', 3: '99.90%'}\n",
      "Val Loss: 6.401203, Val Acc: 86.76%, Val-Class-Acc: {0: '91.30%', 1: '78.28%', 2: '84.03%', 3: '93.44%'}, LR: 0.000185\n",
      "Epoch 198/200, Train Loss: 0.023535, Train-Class-Acc: {0: '99.73%', 1: '99.69%', 2: '99.83%', 3: '100.00%'}\n",
      "Val Loss: 6.706873, Val Acc: 86.89%, Val-Class-Acc: {0: '92.93%', 1: '79.92%', 2: '84.03%', 3: '90.98%'}, LR: 0.000185\n",
      "Epoch 199/200, Train Loss: 0.027511, Train-Class-Acc: {0: '99.59%', 1: '99.80%', 2: '100.00%', 3: '99.80%'}\n",
      "Val Loss: 6.130005, Val Acc: 87.75%, Val-Class-Acc: {0: '89.67%', 1: '79.51%', 2: '86.11%', 3: '95.49%'}, LR: 0.000185\n",
      "Epoch 200/200, Train Loss: 0.146994, Train-Class-Acc: {0: '99.73%', 1: '98.98%', 2: '100.00%', 3: '99.59%'}\n",
      "Val Loss: 6.928359, Val Acc: 87.87%, Val-Class-Acc: {0: '86.96%', 1: '84.02%', 2: '84.72%', 3: '94.26%'}, LR: 0.000185\n",
      "\n",
      "üèÜ Best model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_best.pth (Val Accuracy: 88.60%)\n",
      "\n",
      "üìå Final model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_final.pth\n",
      "\n",
      "üéØ Top 5 Best Models:\n",
      "Epoch 112, Train Loss: 0.053371, Train-Acc: {0: '99.59%', 1: '99.39%', 2: '99.48%', 3: '99.59%'},\n",
      "Val Loss: 5.180703, Val Acc: 88.60%, Val-Acc: {0: '92.39%', 1: '79.51%', 2: '84.03%', 3: '97.54%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_112.pth\n",
      "Epoch 115, Train Loss: 0.026971, Train-Acc: {0: '99.32%', 1: '99.69%', 2: '99.65%', 3: '99.80%'},\n",
      "Val Loss: 5.242550, Val Acc: 88.36%, Val-Acc: {0: '87.50%', 1: '81.97%', 2: '84.72%', 3: '97.54%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_115.pth\n",
      "Epoch 114, Train Loss: 0.051928, Train-Acc: {0: '99.46%', 1: '99.39%', 2: '99.83%', 3: '99.59%'},\n",
      "Val Loss: 4.926001, Val Acc: 88.36%, Val-Acc: {0: '89.13%', 1: '81.56%', 2: '84.03%', 3: '97.13%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_114.pth\n",
      "Epoch 127, Train Loss: 0.145435, Train-Acc: {0: '99.46%', 1: '99.08%', 2: '99.31%', 3: '99.18%'},\n",
      "Val Loss: 6.292521, Val Acc: 88.24%, Val-Acc: {0: '84.78%', 1: '82.38%', 2: '88.19%', 3: '96.72%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_127.pth\n",
      "Epoch 62, Train Loss: 0.253119, Train-Acc: {0: '97.28%', 1: '97.44%', 2: '97.92%', 3: '98.67%'},\n",
      "Val Loss: 3.797647, Val Acc: 88.24%, Val-Acc: {0: '84.78%', 1: '82.79%', 2: '86.11%', 3: '97.54%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2/ResNet18_1D_LoRA_epoch_62.pth\n",
      "---\n",
      "### Period 2\n",
      "+ ##### Total training time: 300.28 seconds\n",
      "+ ##### Model: ResNet18_1D_LoRA\n",
      "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v1/Period_2'*\n",
      "+ ##### Best Epoch: 112\n",
      "#### __Val Accuracy: 88.60%__\n",
      "#### __Val-Class-Acc: {0: '92.39%', 1: '79.51%', 2: '84.03%', 3: '97.54%'}__\n",
      "#### __Total Parameters: 3,889,796__\n",
      "#### __Model Size (float32): 14.84 MB__\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå Period 2: Standard LoRA Training (ECG)\n",
    "# ================================\n",
    "period = 2\n",
    "\n",
    "# ==== Paths ====\n",
    "stop_signal_file = os.path.join(BASE_DIR, \"stop_training.txt\")\n",
    "model_saving_folder = os.path.join(BASE_DIR, \"Trained_models\", \"Standard_LoRA_CIL_v1\", f\"Period_{period}\")\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# ==== Load Period 2 Data ====\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{period}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{period}.npy\"))\n",
    "X_val   = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val   = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "# ==== Device ====\n",
    "device = auto_select_cuda_device()\n",
    "\n",
    "# ==== Model Configuration ====\n",
    "input_channels = X_train.shape[2]  # ECG 12-lead\n",
    "output_size = len(np.unique(y_train))  # Êñ∞Â¢ûÈ°ûÂà•Êï∏\n",
    "model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size, lora_rank=4).to(device)\n",
    "\n",
    "# ==== Load Period 1 Best Model Weights (excluding FC but keeping internal layers) ====\n",
    "prev_model_path = os.path.join(BASE_DIR, \"ResNet18_Selection\", \"ResNet18_big_inplane_v1\", \"ResNet18_big_inplane_1D_best.pth\")\n",
    "prev_checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "prev_state_dict = prev_checkpoint[\"model_state_dict\"]\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "filtered_state_dict = {\n",
    "    k: v for k, v in prev_state_dict.items()\n",
    "    if k in model_dict and model_dict[k].shape == v.shape and not k.startswith(\"fc\")\n",
    "}\n",
    "model.load_state_dict(filtered_state_dict, strict=False)\n",
    "for k in model_dict:\n",
    "    if k not in filtered_state_dict:\n",
    "        print(f\"üîç Not loaded: {k}, shape={model_dict[k].shape}\")\n",
    "print(\"‚úÖ Loaded Period 1 weights (excluding final FC layer)\")\n",
    "\n",
    "# ==== Initialize LoRA Adapters ====\n",
    "model.init_lora()\n",
    "\n",
    "# ==== Optimizer / Scheduler ====\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.get_trainable_parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# ==== Start Training ====\n",
    "train_with_lora_ecg(\n",
    "    model=model,\n",
    "    output_size=output_size,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    model_saving_folder=model_saving_folder,\n",
    "    model_name=\"ResNet18_1D_LoRA\",\n",
    "    stop_signal_file=stop_signal_file,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# ‚úÖ Cleanup\n",
    "# ================================\n",
    "del X_train, y_train, X_val, y_val\n",
    "del prev_model_path, prev_checkpoint, prev_state_dict, filtered_state_dict\n",
    "del model, criterion, optimizer, scheduler\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78caece5",
   "metadata": {},
   "source": [
    "#### v2: `lora_adapter.conv`Êú™Ë¢´ËºâÂÖ•ÁöÑÁâàÊú¨ (‰ΩÜÈÄôÈÇä‰∏¶‰∏çÂΩ±ÈüøÔºåÂõ†ÁÇ∫period 1Ê≤íÊúâLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94faba3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 1\n",
      "    - Memory Used    : 1409 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_763947/3172304596.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  prev_checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Not loaded: fc.weight, shape=torch.Size([4, 1024])\n",
      "üîç Not loaded: fc.bias, shape=torch.Size([4])\n",
      "‚úÖ Loaded Period 1 weights (excluding final FC layer)\n",
      "‚úÖ LoRA adapters initialized for 8 conv2 layers\n",
      "üìä Parameter Statistics:\n",
      "  - Total parameters: 3,889,796\n",
      "  - Trainable parameters: 34,820 (0.90%)\n",
      "    - LoRA parameters: 30,720 (0.79%)\n",
      "    - FC parameters: 4,100 (0.11%)\n",
      "  - Frozen parameters: 3,854,976 (99.10%)\n",
      "üß† Trainable parameter names:\n",
      "  ‚úÖ layer1.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer1.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer1.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer1.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer2.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer2.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer2.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer2.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer3.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer3.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer3.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer3.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer4.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer4.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer4.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer4.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ fc.weight (FC)\n",
      "  ‚úÖ fc.bias (FC)\n",
      "\n",
      "üöÄ 'train_with_lora_ecg' started.\n",
      "‚úÖ Removed existing folder: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2\n",
      "\n",
      "‚úÖ Data Overview:\n",
      "X_train: torch.Size([3263, 5000, 12]), y_train: torch.Size([3263])\n",
      "X_val: torch.Size([816, 5000, 12]), y_val: torch.Size([816])\n",
      "Epoch 1/200, Train Loss: 72.039804, Train-Class-Acc: {0: '52.04%', 1: '31.15%', 2: '20.62%', 3: '24.90%'}\n",
      "Val Loss: 18.556009, Val Acc: 58.82%, Val-Class-Acc: {0: '86.41%', 1: '59.43%', 2: '35.42%', 3: '51.23%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_1.pth\n",
      "Epoch 2/200, Train Loss: 25.264387, Train-Class-Acc: {0: '71.93%', 1: '48.87%', 2: '38.13%', 3: '45.39%'}\n",
      "Val Loss: 13.135515, Val Acc: 63.48%, Val-Class-Acc: {0: '87.50%', 1: '64.34%', 2: '58.33%', 3: '47.54%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_2.pth\n",
      "Epoch 3/200, Train Loss: 20.571803, Train-Class-Acc: {0: '74.39%', 1: '49.90%', 2: '47.31%', 3: '51.74%'}\n",
      "Val Loss: 8.557251, Val Acc: 71.20%, Val-Class-Acc: {0: '85.33%', 1: '69.26%', 2: '66.67%', 3: '65.16%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_3.pth\n",
      "Epoch 4/200, Train Loss: 16.128804, Train-Class-Acc: {0: '76.57%', 1: '53.48%', 2: '56.85%', 3: '57.38%'}\n",
      "Val Loss: 6.362445, Val Acc: 74.88%, Val-Class-Acc: {0: '90.76%', 1: '62.30%', 2: '71.53%', 3: '77.46%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_4.pth\n",
      "Epoch 5/200, Train Loss: 12.391421, Train-Class-Acc: {0: '76.98%', 1: '57.48%', 2: '61.35%', 3: '64.65%'}\n",
      "Val Loss: 5.031834, Val Acc: 76.47%, Val-Class-Acc: {0: '89.67%', 1: '54.92%', 2: '79.86%', 3: '86.07%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_5.pth\n",
      "Epoch 6/200, Train Loss: 12.025142, Train-Class-Acc: {0: '76.16%', 1: '57.79%', 2: '65.51%', 3: '66.70%'}\n",
      "Val Loss: 5.875543, Val Acc: 75.61%, Val-Class-Acc: {0: '91.30%', 1: '48.36%', 2: '68.75%', 3: '95.08%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_1.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_6.pth\n",
      "Epoch 7/200, Train Loss: 9.834262, Train-Class-Acc: {0: '76.70%', 1: '58.71%', 2: '68.46%', 3: '67.42%'}\n",
      "Val Loss: 5.766961, Val Acc: 78.68%, Val-Class-Acc: {0: '85.33%', 1: '72.54%', 2: '71.53%', 3: '84.02%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_2.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_7.pth\n",
      "Epoch 8/200, Train Loss: 9.410800, Train-Class-Acc: {0: '74.66%', 1: '62.50%', 2: '68.63%', 3: '71.62%'}\n",
      "Val Loss: 5.822134, Val Acc: 79.66%, Val-Class-Acc: {0: '89.67%', 1: '69.26%', 2: '75.00%', 3: '85.25%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_3.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_8.pth\n",
      "Epoch 9/200, Train Loss: 9.535752, Train-Class-Acc: {0: '75.89%', 1: '59.63%', 2: '70.54%', 3: '71.11%'}\n",
      "Val Loss: 4.658244, Val Acc: 79.66%, Val-Class-Acc: {0: '90.76%', 1: '65.98%', 2: '75.69%', 3: '87.30%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_4.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_9.pth\n",
      "Epoch 10/200, Train Loss: 8.819534, Train-Class-Acc: {0: '76.43%', 1: '63.22%', 2: '72.79%', 3: '70.90%'}\n",
      "Val Loss: 5.036119, Val Acc: 80.02%, Val-Class-Acc: {0: '83.70%', 1: '75.00%', 2: '72.92%', 3: '86.48%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_6.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_10.pth\n",
      "Epoch 11/200, Train Loss: 7.469645, Train-Class-Acc: {0: '77.11%', 1: '64.65%', 2: '74.87%', 3: '76.84%'}\n",
      "Val Loss: 3.778604, Val Acc: 81.13%, Val-Class-Acc: {0: '88.04%', 1: '74.59%', 2: '73.61%', 3: '86.89%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_5.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_11.pth\n",
      "Epoch 12/200, Train Loss: 7.550989, Train-Class-Acc: {0: '77.11%', 1: '64.75%', 2: '71.58%', 3: '73.98%'}\n",
      "Val Loss: 4.097113, Val Acc: 80.64%, Val-Class-Acc: {0: '87.50%', 1: '66.39%', 2: '75.00%', 3: '93.03%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_7.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_12.pth\n",
      "Epoch 13/200, Train Loss: 7.021710, Train-Class-Acc: {0: '76.57%', 1: '63.32%', 2: '73.83%', 3: '76.84%'}\n",
      "Val Loss: 4.103744, Val Acc: 80.39%, Val-Class-Acc: {0: '86.96%', 1: '65.57%', 2: '72.92%', 3: '94.67%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_8.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_13.pth\n",
      "Epoch 14/200, Train Loss: 6.122269, Train-Class-Acc: {0: '77.11%', 1: '66.39%', 2: '74.18%', 3: '79.41%'}\n",
      "Val Loss: 3.712601, Val Acc: 81.86%, Val-Class-Acc: {0: '88.59%', 1: '73.77%', 2: '75.00%', 3: '88.93%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_9.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_14.pth\n",
      "Epoch 15/200, Train Loss: 5.391948, Train-Class-Acc: {0: '79.84%', 1: '69.67%', 2: '77.64%', 3: '79.61%'}\n",
      "Val Loss: 3.125309, Val Acc: 82.60%, Val-Class-Acc: {0: '86.96%', 1: '70.90%', 2: '81.25%', 3: '91.80%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_10.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_15.pth\n",
      "Epoch 16/200, Train Loss: 5.901815, Train-Class-Acc: {0: '77.38%', 1: '66.60%', 2: '75.04%', 3: '78.59%'}\n",
      "Val Loss: 3.693137, Val Acc: 82.60%, Val-Class-Acc: {0: '90.76%', 1: '72.13%', 2: '76.39%', 3: '90.57%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_13.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_16.pth\n",
      "Epoch 17/200, Train Loss: 5.495462, Train-Class-Acc: {0: '77.38%', 1: '68.24%', 2: '78.86%', 3: '79.00%'}\n",
      "Val Loss: 3.455023, Val Acc: 83.21%, Val-Class-Acc: {0: '85.33%', 1: '75.82%', 2: '83.33%', 3: '88.93%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_12.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_17.pth\n",
      "Epoch 18/200, Train Loss: 5.447272, Train-Class-Acc: {0: '76.84%', 1: '68.44%', 2: '76.78%', 3: '80.12%'}\n",
      "Val Loss: 3.110216, Val Acc: 81.25%, Val-Class-Acc: {0: '86.96%', 1: '67.21%', 2: '76.39%', 3: '93.85%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_11.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_18.pth\n",
      "Epoch 19/200, Train Loss: 5.139449, Train-Class-Acc: {0: '77.93%', 1: '67.73%', 2: '78.16%', 3: '81.86%'}\n",
      "Val Loss: 3.403423, Val Acc: 83.21%, Val-Class-Acc: {0: '85.33%', 1: '75.41%', 2: '81.94%', 3: '90.16%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_18.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_19.pth\n",
      "Epoch 20/200, Train Loss: 4.919444, Train-Class-Acc: {0: '76.16%', 1: '69.26%', 2: '79.90%', 3: '81.45%'}\n",
      "Val Loss: 5.187028, Val Acc: 81.25%, Val-Class-Acc: {0: '91.30%', 1: '76.23%', 2: '81.25%', 3: '78.69%'}, LR: 0.001000\n",
      "Epoch 21/200, Train Loss: 4.659947, Train-Class-Acc: {0: '77.52%', 1: '70.18%', 2: '78.68%', 3: '80.94%'}\n",
      "Val Loss: 2.871761, Val Acc: 84.44%, Val-Class-Acc: {0: '91.85%', 1: '75.00%', 2: '78.47%', 3: '91.80%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_14.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_21.pth\n",
      "Epoch 22/200, Train Loss: 4.735050, Train-Class-Acc: {0: '79.16%', 1: '68.75%', 2: '76.60%', 3: '81.97%'}\n",
      "Val Loss: 2.527826, Val Acc: 84.31%, Val-Class-Acc: {0: '88.04%', 1: '77.87%', 2: '80.56%', 3: '90.16%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_15.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_22.pth\n",
      "Epoch 23/200, Train Loss: 4.294019, Train-Class-Acc: {0: '79.02%', 1: '70.70%', 2: '79.38%', 3: '83.81%'}\n",
      "Val Loss: 3.354470, Val Acc: 83.82%, Val-Class-Acc: {0: '88.59%', 1: '79.51%', 2: '78.47%', 3: '87.70%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_16.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_23.pth\n",
      "Epoch 24/200, Train Loss: 4.536197, Train-Class-Acc: {0: '77.66%', 1: '71.93%', 2: '79.20%', 3: '82.27%'}\n",
      "Val Loss: 2.382175, Val Acc: 84.19%, Val-Class-Acc: {0: '88.59%', 1: '75.41%', 2: '86.11%', 3: '88.52%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_17.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_24.pth\n",
      "Epoch 25/200, Train Loss: 4.291674, Train-Class-Acc: {0: '78.20%', 1: '70.80%', 2: '80.07%', 3: '81.76%'}\n",
      "Val Loss: 2.990030, Val Acc: 84.31%, Val-Class-Acc: {0: '90.76%', 1: '80.33%', 2: '72.22%', 3: '90.57%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_19.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_25.pth\n",
      "Epoch 26/200, Train Loss: 3.817210, Train-Class-Acc: {0: '81.34%', 1: '71.11%', 2: '79.55%', 3: '84.12%'}\n",
      "Val Loss: 2.169835, Val Acc: 84.68%, Val-Class-Acc: {0: '92.39%', 1: '72.13%', 2: '85.42%', 3: '90.98%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_23.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_26.pth\n",
      "Epoch 27/200, Train Loss: 3.289961, Train-Class-Acc: {0: '78.61%', 1: '74.39%', 2: '80.07%', 3: '84.84%'}\n",
      "Val Loss: 2.319923, Val Acc: 85.05%, Val-Class-Acc: {0: '91.30%', 1: '75.00%', 2: '84.72%', 3: '90.57%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_24.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_27.pth\n",
      "Epoch 28/200, Train Loss: 3.585972, Train-Class-Acc: {0: '79.84%', 1: '72.13%', 2: '80.42%', 3: '84.43%'}\n",
      "Val Loss: 2.203596, Val Acc: 84.19%, Val-Class-Acc: {0: '84.24%', 1: '75.00%', 2: '90.28%', 3: '89.75%'}, LR: 0.001000\n",
      "Epoch 29/200, Train Loss: 3.513581, Train-Class-Acc: {0: '79.16%', 1: '71.62%', 2: '80.07%', 3: '83.91%'}\n",
      "Val Loss: 4.169696, Val Acc: 82.97%, Val-Class-Acc: {0: '87.50%', 1: '81.56%', 2: '79.17%', 3: '83.20%'}, LR: 0.001000\n",
      "Epoch 30/200, Train Loss: 3.246430, Train-Class-Acc: {0: '80.52%', 1: '73.46%', 2: '80.76%', 3: '84.73%'}\n",
      "Val Loss: 2.525838, Val Acc: 85.05%, Val-Class-Acc: {0: '85.33%', 1: '75.00%', 2: '83.33%', 3: '95.90%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_22.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_30.pth\n",
      "Epoch 31/200, Train Loss: 3.246519, Train-Class-Acc: {0: '79.16%', 1: '74.49%', 2: '82.84%', 3: '86.89%'}\n",
      "Val Loss: 2.783891, Val Acc: 84.07%, Val-Class-Acc: {0: '90.76%', 1: '80.74%', 2: '81.94%', 3: '83.61%'}, LR: 0.001000\n",
      "Epoch 32/200, Train Loss: 3.341954, Train-Class-Acc: {0: '81.88%', 1: '73.46%', 2: '81.11%', 3: '84.73%'}\n",
      "Val Loss: 2.456737, Val Acc: 83.46%, Val-Class-Acc: {0: '83.15%', 1: '70.90%', 2: '82.64%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 33/200, Train Loss: 3.405951, Train-Class-Acc: {0: '77.66%', 1: '73.05%', 2: '81.80%', 3: '85.35%'}\n",
      "Val Loss: 3.314313, Val Acc: 84.07%, Val-Class-Acc: {0: '93.48%', 1: '76.23%', 2: '77.08%', 3: '88.93%'}, LR: 0.001000\n",
      "Epoch 34/200, Train Loss: 3.392009, Train-Class-Acc: {0: '79.43%', 1: '74.39%', 2: '83.02%', 3: '86.07%'}\n",
      "Val Loss: 2.474998, Val Acc: 83.70%, Val-Class-Acc: {0: '84.78%', 1: '78.69%', 2: '78.47%', 3: '90.98%'}, LR: 0.001000\n",
      "Epoch 35/200, Train Loss: 2.818690, Train-Class-Acc: {0: '81.34%', 1: '76.23%', 2: '80.94%', 3: '86.89%'}\n",
      "Val Loss: 2.956662, Val Acc: 84.44%, Val-Class-Acc: {0: '86.96%', 1: '77.05%', 2: '82.64%', 3: '90.98%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_25.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_35.pth\n",
      "Epoch 36/200, Train Loss: 2.887924, Train-Class-Acc: {0: '79.43%', 1: '76.64%', 2: '82.84%', 3: '86.58%'}\n",
      "Val Loss: 2.485425, Val Acc: 84.19%, Val-Class-Acc: {0: '87.50%', 1: '75.00%', 2: '77.78%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 37/200, Train Loss: 2.739823, Train-Class-Acc: {0: '79.97%', 1: '75.10%', 2: '84.23%', 3: '86.37%'}\n",
      "Val Loss: 3.517773, Val Acc: 83.82%, Val-Class-Acc: {0: '86.41%', 1: '82.38%', 2: '85.42%', 3: '82.38%'}, LR: 0.001000\n",
      "Epoch 38/200, Train Loss: 3.125971, Train-Class-Acc: {0: '80.38%', 1: '73.16%', 2: '83.02%', 3: '85.55%'}\n",
      "Val Loss: 2.175774, Val Acc: 84.80%, Val-Class-Acc: {0: '79.35%', 1: '79.10%', 2: '87.50%', 3: '93.03%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_21.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_38.pth\n",
      "Epoch 39/200, Train Loss: 2.482918, Train-Class-Acc: {0: '80.38%', 1: '76.13%', 2: '84.23%', 3: '88.22%'}\n",
      "Val Loss: 2.079336, Val Acc: 86.03%, Val-Class-Acc: {0: '88.59%', 1: '78.28%', 2: '84.03%', 3: '93.03%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_35.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_39.pth\n",
      "Epoch 40/200, Train Loss: 2.783772, Train-Class-Acc: {0: '81.47%', 1: '76.84%', 2: '83.19%', 3: '86.37%'}\n",
      "Val Loss: 2.387718, Val Acc: 83.09%, Val-Class-Acc: {0: '82.07%', 1: '71.72%', 2: '79.17%', 3: '97.54%'}, LR: 0.000900\n",
      "Epoch 41/200, Train Loss: 2.831793, Train-Class-Acc: {0: '80.38%', 1: '74.49%', 2: '82.50%', 3: '86.68%'}\n",
      "Val Loss: 4.977298, Val Acc: 83.82%, Val-Class-Acc: {0: '84.78%', 1: '83.61%', 2: '82.64%', 3: '84.02%'}, LR: 0.000900\n",
      "Epoch 42/200, Train Loss: 2.959319, Train-Class-Acc: {0: '82.02%', 1: '76.33%', 2: '82.84%', 3: '87.19%'}\n",
      "Val Loss: 2.488813, Val Acc: 84.93%, Val-Class-Acc: {0: '85.87%', 1: '79.92%', 2: '81.25%', 3: '91.39%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_26.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_42.pth\n",
      "Epoch 43/200, Train Loss: 2.211903, Train-Class-Acc: {0: '83.51%', 1: '78.48%', 2: '83.36%', 3: '87.09%'}\n",
      "Val Loss: 1.883401, Val Acc: 86.15%, Val-Class-Acc: {0: '86.41%', 1: '78.69%', 2: '84.72%', 3: '94.26%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_38.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_43.pth\n",
      "Epoch 44/200, Train Loss: 2.269388, Train-Class-Acc: {0: '78.34%', 1: '78.07%', 2: '83.36%', 3: '88.32%'}\n",
      "Val Loss: 2.913600, Val Acc: 84.68%, Val-Class-Acc: {0: '83.70%', 1: '80.33%', 2: '84.03%', 3: '90.16%'}, LR: 0.000900\n",
      "Epoch 45/200, Train Loss: 2.711516, Train-Class-Acc: {0: '82.15%', 1: '75.92%', 2: '83.02%', 3: '87.60%'}\n",
      "Val Loss: 4.074460, Val Acc: 83.09%, Val-Class-Acc: {0: '82.61%', 1: '80.33%', 2: '84.03%', 3: '85.66%'}, LR: 0.000900\n",
      "Epoch 46/200, Train Loss: 2.580506, Train-Class-Acc: {0: '83.24%', 1: '77.97%', 2: '87.69%', 3: '88.11%'}\n",
      "Val Loss: 2.156747, Val Acc: 85.54%, Val-Class-Acc: {0: '84.78%', 1: '79.10%', 2: '86.11%', 3: '92.21%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_42.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_46.pth\n",
      "Epoch 47/200, Train Loss: 2.647933, Train-Class-Acc: {0: '82.29%', 1: '76.74%', 2: '83.54%', 3: '88.32%'}\n",
      "Val Loss: 3.053206, Val Acc: 83.70%, Val-Class-Acc: {0: '78.26%', 1: '77.05%', 2: '83.33%', 3: '94.67%'}, LR: 0.000900\n",
      "Epoch 48/200, Train Loss: 2.369906, Train-Class-Acc: {0: '80.65%', 1: '76.64%', 2: '84.75%', 3: '88.32%'}\n",
      "Val Loss: 1.842139, Val Acc: 84.31%, Val-Class-Acc: {0: '79.35%', 1: '77.05%', 2: '82.64%', 3: '96.31%'}, LR: 0.000900\n",
      "Epoch 49/200, Train Loss: 2.106127, Train-Class-Acc: {0: '84.20%', 1: '78.18%', 2: '85.27%', 3: '88.01%'}\n",
      "Val Loss: 2.024489, Val Acc: 84.80%, Val-Class-Acc: {0: '80.98%', 1: '82.38%', 2: '86.11%', 3: '89.34%'}, LR: 0.000900\n",
      "Epoch 50/200, Train Loss: 2.385265, Train-Class-Acc: {0: '81.20%', 1: '76.54%', 2: '84.06%', 3: '89.04%'}\n",
      "Val Loss: 2.294932, Val Acc: 85.17%, Val-Class-Acc: {0: '79.89%', 1: '82.79%', 2: '84.03%', 3: '92.21%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_27.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_50.pth\n",
      "Epoch 51/200, Train Loss: 2.443582, Train-Class-Acc: {0: '83.38%', 1: '76.84%', 2: '85.79%', 3: '87.30%'}\n",
      "Val Loss: 3.427108, Val Acc: 83.58%, Val-Class-Acc: {0: '77.72%', 1: '82.38%', 2: '79.86%', 3: '91.39%'}, LR: 0.000900\n",
      "Epoch 52/200, Train Loss: 2.040405, Train-Class-Acc: {0: '80.38%', 1: '78.07%', 2: '84.75%', 3: '89.86%'}\n",
      "Val Loss: 2.521712, Val Acc: 85.66%, Val-Class-Acc: {0: '84.78%', 1: '79.10%', 2: '86.11%', 3: '92.62%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_30.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_52.pth\n",
      "Epoch 53/200, Train Loss: 2.005749, Train-Class-Acc: {0: '83.38%', 1: '78.38%', 2: '85.44%', 3: '88.63%'}\n",
      "Val Loss: 1.998205, Val Acc: 86.03%, Val-Class-Acc: {0: '90.76%', 1: '74.59%', 2: '88.19%', 3: '92.62%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_50.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_53.pth\n",
      "Epoch 54/200, Train Loss: 2.256594, Train-Class-Acc: {0: '83.11%', 1: '77.66%', 2: '85.27%', 3: '90.47%'}\n",
      "Val Loss: 2.044745, Val Acc: 85.29%, Val-Class-Acc: {0: '79.89%', 1: '82.38%', 2: '83.33%', 3: '93.44%'}, LR: 0.000900\n",
      "Epoch 55/200, Train Loss: 1.636336, Train-Class-Acc: {0: '84.06%', 1: '79.92%', 2: '86.31%', 3: '89.96%'}\n",
      "Val Loss: 1.934104, Val Acc: 86.27%, Val-Class-Acc: {0: '84.24%', 1: '81.15%', 2: '85.42%', 3: '93.44%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_46.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_55.pth\n",
      "Epoch 56/200, Train Loss: 1.895566, Train-Class-Acc: {0: '84.20%', 1: '80.02%', 2: '84.06%', 3: '88.83%'}\n",
      "Val Loss: 2.004037, Val Acc: 85.66%, Val-Class-Acc: {0: '83.70%', 1: '80.33%', 2: '84.03%', 3: '93.44%'}, LR: 0.000900\n",
      "Epoch 57/200, Train Loss: 2.733789, Train-Class-Acc: {0: '84.88%', 1: '78.28%', 2: '84.75%', 3: '88.73%'}\n",
      "Val Loss: 1.836807, Val Acc: 84.93%, Val-Class-Acc: {0: '78.80%', 1: '79.51%', 2: '82.64%', 3: '96.31%'}, LR: 0.000900\n",
      "Epoch 58/200, Train Loss: 2.150783, Train-Class-Acc: {0: '82.43%', 1: '79.20%', 2: '83.02%', 3: '90.68%'}\n",
      "Val Loss: 2.479738, Val Acc: 85.78%, Val-Class-Acc: {0: '89.13%', 1: '78.69%', 2: '86.11%', 3: '90.16%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_52.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_58.pth\n",
      "Epoch 59/200, Train Loss: 1.828755, Train-Class-Acc: {0: '83.92%', 1: '80.12%', 2: '88.56%', 3: '89.24%'}\n",
      "Val Loss: 1.898859, Val Acc: 84.44%, Val-Class-Acc: {0: '80.43%', 1: '74.59%', 2: '86.11%', 3: '96.31%'}, LR: 0.000900\n",
      "Epoch 60/200, Train Loss: 1.918261, Train-Class-Acc: {0: '83.11%', 1: '79.20%', 2: '85.96%', 3: '91.19%'}\n",
      "Val Loss: 1.928262, Val Acc: 85.66%, Val-Class-Acc: {0: '84.24%', 1: '77.05%', 2: '87.50%', 3: '94.26%'}, LR: 0.000900\n",
      "Epoch 61/200, Train Loss: 2.073848, Train-Class-Acc: {0: '84.06%', 1: '79.41%', 2: '87.87%', 3: '90.16%'}\n",
      "Val Loss: 1.751671, Val Acc: 86.03%, Val-Class-Acc: {0: '86.41%', 1: '80.33%', 2: '81.94%', 3: '93.85%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_58.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_61.pth\n",
      "Epoch 62/200, Train Loss: 1.992661, Train-Class-Acc: {0: '82.02%', 1: '81.05%', 2: '86.14%', 3: '88.73%'}\n",
      "Val Loss: 4.548862, Val Acc: 83.33%, Val-Class-Acc: {0: '84.78%', 1: '82.38%', 2: '84.03%', 3: '82.79%'}, LR: 0.000900\n",
      "Epoch 63/200, Train Loss: 1.518189, Train-Class-Acc: {0: '85.15%', 1: '81.45%', 2: '87.00%', 3: '91.09%'}\n",
      "Val Loss: 2.940732, Val Acc: 84.93%, Val-Class-Acc: {0: '82.61%', 1: '81.56%', 2: '84.03%', 3: '90.57%'}, LR: 0.000900\n",
      "Epoch 64/200, Train Loss: 1.908059, Train-Class-Acc: {0: '84.60%', 1: '80.02%', 2: '87.87%', 3: '90.68%'}\n",
      "Val Loss: 1.773732, Val Acc: 85.91%, Val-Class-Acc: {0: '82.61%', 1: '77.46%', 2: '85.42%', 3: '97.13%'}, LR: 0.000900\n",
      "Epoch 65/200, Train Loss: 1.608835, Train-Class-Acc: {0: '84.60%', 1: '80.84%', 2: '87.69%', 3: '90.88%'}\n",
      "Val Loss: 2.193505, Val Acc: 84.93%, Val-Class-Acc: {0: '85.33%', 1: '70.49%', 2: '86.81%', 3: '97.95%'}, LR: 0.000900\n",
      "Epoch 66/200, Train Loss: 1.777231, Train-Class-Acc: {0: '84.06%', 1: '80.43%', 2: '86.48%', 3: '90.57%'}\n",
      "Val Loss: 2.058726, Val Acc: 84.80%, Val-Class-Acc: {0: '78.26%', 1: '79.92%', 2: '85.42%', 3: '94.26%'}, LR: 0.000900\n",
      "Epoch 67/200, Train Loss: 1.434579, Train-Class-Acc: {0: '85.29%', 1: '81.97%', 2: '87.35%', 3: '90.16%'}\n",
      "Val Loss: 2.292403, Val Acc: 85.78%, Val-Class-Acc: {0: '82.61%', 1: '82.79%', 2: '85.42%', 3: '91.39%'}, LR: 0.000900\n",
      "Epoch 68/200, Train Loss: 1.618677, Train-Class-Acc: {0: '85.15%', 1: '81.15%', 2: '86.14%', 3: '89.96%'}\n",
      "Val Loss: 2.036109, Val Acc: 84.68%, Val-Class-Acc: {0: '85.87%', 1: '72.13%', 2: '86.81%', 3: '95.08%'}, LR: 0.000900\n",
      "Epoch 69/200, Train Loss: 1.601640, Train-Class-Acc: {0: '87.06%', 1: '81.97%', 2: '88.73%', 3: '91.60%'}\n",
      "Val Loss: 2.484404, Val Acc: 85.29%, Val-Class-Acc: {0: '81.52%', 1: '83.20%', 2: '80.56%', 3: '93.03%'}, LR: 0.000900\n",
      "Epoch 70/200, Train Loss: 1.487798, Train-Class-Acc: {0: '87.47%', 1: '82.79%', 2: '88.21%', 3: '90.88%'}\n",
      "Val Loss: 1.769676, Val Acc: 85.05%, Val-Class-Acc: {0: '81.52%', 1: '77.46%', 2: '81.25%', 3: '97.54%'}, LR: 0.000900\n",
      "Epoch 71/200, Train Loss: 1.425248, Train-Class-Acc: {0: '87.33%', 1: '83.91%', 2: '87.00%', 3: '92.11%'}\n",
      "Val Loss: 1.516768, Val Acc: 85.91%, Val-Class-Acc: {0: '85.33%', 1: '77.05%', 2: '86.11%', 3: '95.08%'}, LR: 0.000900\n",
      "Epoch 72/200, Train Loss: 1.531363, Train-Class-Acc: {0: '85.15%', 1: '82.79%', 2: '88.04%', 3: '91.91%'}\n",
      "Val Loss: 1.620338, Val Acc: 85.78%, Val-Class-Acc: {0: '81.52%', 1: '80.74%', 2: '84.72%', 3: '94.67%'}, LR: 0.000900\n",
      "Epoch 73/200, Train Loss: 1.385999, Train-Class-Acc: {0: '86.78%', 1: '83.09%', 2: '89.25%', 3: '92.01%'}\n",
      "Val Loss: 1.969595, Val Acc: 86.64%, Val-Class-Acc: {0: '85.33%', 1: '80.74%', 2: '86.81%', 3: '93.44%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_39.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_73.pth\n",
      "Epoch 74/200, Train Loss: 1.718577, Train-Class-Acc: {0: '86.38%', 1: '81.97%', 2: '88.21%', 3: '90.78%'}\n",
      "Val Loss: 1.745604, Val Acc: 85.91%, Val-Class-Acc: {0: '82.07%', 1: '78.69%', 2: '86.81%', 3: '95.49%'}, LR: 0.000900\n",
      "Epoch 75/200, Train Loss: 1.655744, Train-Class-Acc: {0: '86.24%', 1: '84.94%', 2: '88.73%', 3: '91.29%'}\n",
      "Val Loss: 2.713174, Val Acc: 86.52%, Val-Class-Acc: {0: '84.24%', 1: '84.43%', 2: '84.72%', 3: '91.39%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_53.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_75.pth\n",
      "Epoch 76/200, Train Loss: 1.613661, Train-Class-Acc: {0: '86.78%', 1: '82.07%', 2: '88.56%', 3: '91.19%'}\n",
      "Val Loss: 1.842998, Val Acc: 84.56%, Val-Class-Acc: {0: '79.89%', 1: '75.41%', 2: '86.81%', 3: '95.90%'}, LR: 0.000900\n",
      "Epoch 77/200, Train Loss: 1.264781, Train-Class-Acc: {0: '84.74%', 1: '83.20%', 2: '89.77%', 3: '91.70%'}\n",
      "Val Loss: 1.671194, Val Acc: 86.52%, Val-Class-Acc: {0: '84.24%', 1: '79.51%', 2: '84.72%', 3: '96.31%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_61.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_77.pth\n",
      "Epoch 78/200, Train Loss: 1.152499, Train-Class-Acc: {0: '86.51%', 1: '82.38%', 2: '89.25%', 3: '93.14%'}\n",
      "Val Loss: 2.190495, Val Acc: 84.07%, Val-Class-Acc: {0: '83.15%', 1: '72.95%', 2: '81.94%', 3: '97.13%'}, LR: 0.000900\n",
      "Epoch 79/200, Train Loss: 1.343598, Train-Class-Acc: {0: '86.51%', 1: '84.12%', 2: '88.56%', 3: '91.91%'}\n",
      "Val Loss: 2.116646, Val Acc: 86.15%, Val-Class-Acc: {0: '87.50%', 1: '81.56%', 2: '83.33%', 3: '91.39%'}, LR: 0.000900\n",
      "Epoch 80/200, Train Loss: 1.770103, Train-Class-Acc: {0: '85.69%', 1: '81.05%', 2: '88.73%', 3: '90.88%'}\n",
      "Val Loss: 2.291243, Val Acc: 84.93%, Val-Class-Acc: {0: '79.89%', 1: '83.20%', 2: '83.33%', 3: '91.39%'}, LR: 0.000900\n",
      "Epoch 81/200, Train Loss: 1.512755, Train-Class-Acc: {0: '87.06%', 1: '82.89%', 2: '87.52%', 3: '91.09%'}\n",
      "Val Loss: 1.951850, Val Acc: 85.78%, Val-Class-Acc: {0: '82.61%', 1: '83.61%', 2: '82.64%', 3: '92.21%'}, LR: 0.000900\n",
      "Epoch 82/200, Train Loss: 1.404977, Train-Class-Acc: {0: '88.69%', 1: '83.61%', 2: '87.52%', 3: '92.11%'}\n",
      "Val Loss: 1.948209, Val Acc: 87.01%, Val-Class-Acc: {0: '84.24%', 1: '81.56%', 2: '89.58%', 3: '93.03%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_43.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_82.pth\n",
      "Epoch 83/200, Train Loss: 1.224142, Train-Class-Acc: {0: '86.78%', 1: '84.02%', 2: '89.60%', 3: '93.03%'}\n",
      "Val Loss: 1.666243, Val Acc: 86.64%, Val-Class-Acc: {0: '81.52%', 1: '82.79%', 2: '87.50%', 3: '93.85%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_55.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_83.pth\n",
      "Epoch 84/200, Train Loss: 1.222286, Train-Class-Acc: {0: '88.28%', 1: '83.61%', 2: '90.29%', 3: '91.29%'}\n",
      "Val Loss: 1.694017, Val Acc: 86.03%, Val-Class-Acc: {0: '79.89%', 1: '80.33%', 2: '86.11%', 3: '96.31%'}, LR: 0.000810\n",
      "Epoch 85/200, Train Loss: 1.246258, Train-Class-Acc: {0: '84.74%', 1: '84.32%', 2: '89.95%', 3: '93.14%'}\n",
      "Val Loss: 1.705775, Val Acc: 86.76%, Val-Class-Acc: {0: '85.87%', 1: '79.92%', 2: '84.03%', 3: '95.90%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_75.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_85.pth\n",
      "Epoch 86/200, Train Loss: 1.313511, Train-Class-Acc: {0: '87.60%', 1: '86.37%', 2: '89.43%', 3: '92.93%'}\n",
      "Val Loss: 1.902396, Val Acc: 86.15%, Val-Class-Acc: {0: '84.24%', 1: '78.69%', 2: '86.11%', 3: '95.08%'}, LR: 0.000810\n",
      "Epoch 87/200, Train Loss: 1.174880, Train-Class-Acc: {0: '86.38%', 1: '85.86%', 2: '90.64%', 3: '93.03%'}\n",
      "Val Loss: 3.764686, Val Acc: 84.56%, Val-Class-Acc: {0: '81.52%', 1: '84.02%', 2: '85.42%', 3: '86.89%'}, LR: 0.000810\n",
      "Epoch 88/200, Train Loss: 1.246692, Train-Class-Acc: {0: '88.83%', 1: '82.79%', 2: '90.81%', 3: '91.91%'}\n",
      "Val Loss: 2.490000, Val Acc: 84.44%, Val-Class-Acc: {0: '80.43%', 1: '74.59%', 2: '87.50%', 3: '95.49%'}, LR: 0.000810\n",
      "Epoch 89/200, Train Loss: 1.212230, Train-Class-Acc: {0: '87.06%', 1: '85.25%', 2: '90.12%', 3: '92.83%'}\n",
      "Val Loss: 2.776789, Val Acc: 84.44%, Val-Class-Acc: {0: '75.54%', 1: '86.48%', 2: '81.25%', 3: '90.98%'}, LR: 0.000810\n",
      "Epoch 90/200, Train Loss: 1.087037, Train-Class-Acc: {0: '87.87%', 1: '85.96%', 2: '90.81%', 3: '93.95%'}\n",
      "Val Loss: 1.961830, Val Acc: 85.17%, Val-Class-Acc: {0: '79.89%', 1: '85.25%', 2: '83.33%', 3: '90.16%'}, LR: 0.000810\n",
      "Epoch 91/200, Train Loss: 1.342139, Train-Class-Acc: {0: '88.69%', 1: '84.73%', 2: '90.12%', 3: '92.42%'}\n",
      "Val Loss: 1.842317, Val Acc: 86.03%, Val-Class-Acc: {0: '85.87%', 1: '77.46%', 2: '86.81%', 3: '94.26%'}, LR: 0.000810\n",
      "Epoch 92/200, Train Loss: 1.044322, Train-Class-Acc: {0: '86.92%', 1: '86.27%', 2: '89.77%', 3: '92.83%'}\n",
      "Val Loss: 1.895262, Val Acc: 86.15%, Val-Class-Acc: {0: '83.15%', 1: '79.92%', 2: '85.42%', 3: '95.08%'}, LR: 0.000810\n",
      "Epoch 93/200, Train Loss: 0.904793, Train-Class-Acc: {0: '88.69%', 1: '85.86%', 2: '90.99%', 3: '93.55%'}\n",
      "Val Loss: 2.010764, Val Acc: 84.31%, Val-Class-Acc: {0: '79.89%', 1: '76.23%', 2: '81.25%', 3: '97.54%'}, LR: 0.000810\n",
      "Epoch 94/200, Train Loss: 0.917354, Train-Class-Acc: {0: '89.24%', 1: '86.68%', 2: '88.91%', 3: '93.85%'}\n",
      "Val Loss: 2.202148, Val Acc: 85.17%, Val-Class-Acc: {0: '83.70%', 1: '76.64%', 2: '83.33%', 3: '95.90%'}, LR: 0.000729\n",
      "Epoch 95/200, Train Loss: 1.197953, Train-Class-Acc: {0: '86.92%', 1: '85.04%', 2: '90.47%', 3: '92.73%'}\n",
      "Val Loss: 2.332159, Val Acc: 86.15%, Val-Class-Acc: {0: '80.98%', 1: '82.38%', 2: '87.50%', 3: '93.03%'}, LR: 0.000729\n",
      "Epoch 96/200, Train Loss: 1.097367, Train-Class-Acc: {0: '88.83%', 1: '85.86%', 2: '90.12%', 3: '93.14%'}\n",
      "Val Loss: 2.935157, Val Acc: 85.29%, Val-Class-Acc: {0: '80.43%', 1: '83.20%', 2: '84.72%', 3: '91.39%'}, LR: 0.000729\n",
      "Epoch 97/200, Train Loss: 1.185449, Train-Class-Acc: {0: '89.10%', 1: '85.86%', 2: '89.60%', 3: '92.73%'}\n",
      "Val Loss: 1.794019, Val Acc: 86.40%, Val-Class-Acc: {0: '83.15%', 1: '79.10%', 2: '86.11%', 3: '96.31%'}, LR: 0.000729\n",
      "Epoch 98/200, Train Loss: 1.143817, Train-Class-Acc: {0: '89.24%', 1: '86.78%', 2: '90.99%', 3: '93.65%'}\n",
      "Val Loss: 2.575886, Val Acc: 83.58%, Val-Class-Acc: {0: '81.52%', 1: '70.49%', 2: '84.72%', 3: '97.54%'}, LR: 0.000729\n",
      "Epoch 99/200, Train Loss: 1.249110, Train-Class-Acc: {0: '87.47%', 1: '83.91%', 2: '90.81%', 3: '93.44%'}\n",
      "Val Loss: 2.190863, Val Acc: 86.03%, Val-Class-Acc: {0: '84.24%', 1: '80.74%', 2: '84.03%', 3: '93.85%'}, LR: 0.000729\n",
      "Epoch 100/200, Train Loss: 1.095855, Train-Class-Acc: {0: '89.51%', 1: '87.70%', 2: '91.68%', 3: '93.24%'}\n",
      "Val Loss: 2.122360, Val Acc: 85.29%, Val-Class-Acc: {0: '85.87%', 1: '72.95%', 2: '86.81%', 3: '96.31%'}, LR: 0.000729\n",
      "Epoch 101/200, Train Loss: 1.023088, Train-Class-Acc: {0: '89.51%', 1: '86.27%', 2: '90.12%', 3: '93.55%'}\n",
      "Val Loss: 2.432180, Val Acc: 85.17%, Val-Class-Acc: {0: '85.87%', 1: '77.05%', 2: '80.56%', 3: '95.49%'}, LR: 0.000729\n",
      "Epoch 102/200, Train Loss: 1.094625, Train-Class-Acc: {0: '89.10%', 1: '86.58%', 2: '89.77%', 3: '93.55%'}\n",
      "Val Loss: 1.992447, Val Acc: 86.52%, Val-Class-Acc: {0: '87.50%', 1: '76.64%', 2: '86.81%', 3: '95.49%'}, LR: 0.000729\n",
      "Epoch 103/200, Train Loss: 0.973714, Train-Class-Acc: {0: '89.92%', 1: '87.40%', 2: '92.20%', 3: '94.06%'}\n",
      "Val Loss: 2.574534, Val Acc: 85.78%, Val-Class-Acc: {0: '79.89%', 1: '82.38%', 2: '85.42%', 3: '93.85%'}, LR: 0.000729\n",
      "Epoch 104/200, Train Loss: 0.934803, Train-Class-Acc: {0: '88.15%', 1: '85.96%', 2: '91.68%', 3: '93.75%'}\n",
      "Val Loss: 1.849386, Val Acc: 85.29%, Val-Class-Acc: {0: '82.07%', 1: '82.38%', 2: '83.33%', 3: '91.80%'}, LR: 0.000729\n",
      "Epoch 105/200, Train Loss: 0.953704, Train-Class-Acc: {0: '90.74%', 1: '86.68%', 2: '91.51%', 3: '93.65%'}\n",
      "Val Loss: 2.074090, Val Acc: 86.03%, Val-Class-Acc: {0: '82.07%', 1: '81.97%', 2: '83.33%', 3: '94.67%'}, LR: 0.000656\n",
      "Epoch 106/200, Train Loss: 0.887354, Train-Class-Acc: {0: '90.33%', 1: '87.30%', 2: '89.60%', 3: '93.55%'}\n",
      "Val Loss: 1.688404, Val Acc: 86.03%, Val-Class-Acc: {0: '82.61%', 1: '79.51%', 2: '86.81%', 3: '94.67%'}, LR: 0.000656\n",
      "Epoch 107/200, Train Loss: 0.935871, Train-Class-Acc: {0: '90.33%', 1: '86.89%', 2: '91.68%', 3: '94.16%'}\n",
      "Val Loss: 2.037963, Val Acc: 85.42%, Val-Class-Acc: {0: '79.35%', 1: '80.74%', 2: '86.81%', 3: '93.85%'}, LR: 0.000656\n",
      "Epoch 108/200, Train Loss: 0.845994, Train-Class-Acc: {0: '88.28%', 1: '88.52%', 2: '90.29%', 3: '94.36%'}\n",
      "Val Loss: 1.889471, Val Acc: 86.64%, Val-Class-Acc: {0: '82.61%', 1: '81.15%', 2: '86.11%', 3: '95.49%'}, LR: 0.000656\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_77.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_108.pth\n",
      "Epoch 109/200, Train Loss: 0.969628, Train-Class-Acc: {0: '90.87%', 1: '88.11%', 2: '92.89%', 3: '93.34%'}\n",
      "Val Loss: 1.957008, Val Acc: 85.42%, Val-Class-Acc: {0: '83.70%', 1: '78.69%', 2: '80.56%', 3: '96.31%'}, LR: 0.000656\n",
      "Epoch 110/200, Train Loss: 0.647947, Train-Class-Acc: {0: '88.69%', 1: '88.52%', 2: '92.20%', 3: '95.70%'}\n",
      "Val Loss: 2.358279, Val Acc: 85.91%, Val-Class-Acc: {0: '81.52%', 1: '82.38%', 2: '84.72%', 3: '93.44%'}, LR: 0.000656\n",
      "Epoch 111/200, Train Loss: 0.795623, Train-Class-Acc: {0: '89.92%', 1: '87.30%', 2: '92.03%', 3: '94.77%'}\n",
      "Val Loss: 2.406218, Val Acc: 84.56%, Val-Class-Acc: {0: '84.24%', 1: '71.31%', 2: '84.72%', 3: '97.95%'}, LR: 0.000656\n",
      "Epoch 112/200, Train Loss: 0.916063, Train-Class-Acc: {0: '90.33%', 1: '88.63%', 2: '92.20%', 3: '94.57%'}\n",
      "Val Loss: 2.029524, Val Acc: 85.66%, Val-Class-Acc: {0: '79.35%', 1: '80.74%', 2: '86.11%', 3: '95.08%'}, LR: 0.000656\n",
      "Epoch 113/200, Train Loss: 0.810612, Train-Class-Acc: {0: '91.28%', 1: '88.52%', 2: '91.51%', 3: '94.98%'}\n",
      "Val Loss: 3.157964, Val Acc: 84.80%, Val-Class-Acc: {0: '79.35%', 1: '83.61%', 2: '85.42%', 3: '89.75%'}, LR: 0.000656\n",
      "Epoch 114/200, Train Loss: 1.098947, Train-Class-Acc: {0: '89.92%', 1: '86.78%', 2: '91.68%', 3: '93.03%'}\n",
      "Val Loss: 2.277345, Val Acc: 86.15%, Val-Class-Acc: {0: '82.61%', 1: '81.56%', 2: '87.50%', 3: '92.62%'}, LR: 0.000656\n",
      "Epoch 115/200, Train Loss: 0.703008, Train-Class-Acc: {0: '90.46%', 1: '89.55%', 2: '92.37%', 3: '94.57%'}\n",
      "Val Loss: 1.799892, Val Acc: 86.27%, Val-Class-Acc: {0: '82.61%', 1: '79.10%', 2: '85.42%', 3: '96.72%'}, LR: 0.000656\n",
      "Epoch 116/200, Train Loss: 0.563416, Train-Class-Acc: {0: '92.10%', 1: '89.24%', 2: '91.85%', 3: '94.98%'}\n",
      "Val Loss: 1.806271, Val Acc: 86.03%, Val-Class-Acc: {0: '80.43%', 1: '79.92%', 2: '85.42%', 3: '96.72%'}, LR: 0.000590\n",
      "Epoch 117/200, Train Loss: 0.810657, Train-Class-Acc: {0: '91.69%', 1: '87.70%', 2: '93.24%', 3: '94.57%'}\n",
      "Val Loss: 2.019427, Val Acc: 85.29%, Val-Class-Acc: {0: '78.80%', 1: '78.28%', 2: '86.11%', 3: '96.72%'}, LR: 0.000590\n",
      "Epoch 118/200, Train Loss: 0.667848, Train-Class-Acc: {0: '90.87%', 1: '90.06%', 2: '92.20%', 3: '94.26%'}\n",
      "Val Loss: 1.859688, Val Acc: 85.78%, Val-Class-Acc: {0: '82.61%', 1: '77.87%', 2: '87.50%', 3: '95.08%'}, LR: 0.000590\n",
      "Epoch 119/200, Train Loss: 0.816696, Train-Class-Acc: {0: '92.37%', 1: '88.63%', 2: '91.51%', 3: '94.88%'}\n",
      "Val Loss: 1.960966, Val Acc: 86.40%, Val-Class-Acc: {0: '82.61%', 1: '82.38%', 2: '86.81%', 3: '93.03%'}, LR: 0.000590\n",
      "Epoch 120/200, Train Loss: 0.775816, Train-Class-Acc: {0: '90.60%', 1: '87.70%', 2: '92.55%', 3: '94.36%'}\n",
      "Val Loss: 1.856591, Val Acc: 86.27%, Val-Class-Acc: {0: '80.98%', 1: '80.33%', 2: '87.50%', 3: '95.49%'}, LR: 0.000590\n",
      "Epoch 121/200, Train Loss: 1.067812, Train-Class-Acc: {0: '91.42%', 1: '88.83%', 2: '92.72%', 3: '94.06%'}\n",
      "Val Loss: 2.113229, Val Acc: 85.91%, Val-Class-Acc: {0: '79.89%', 1: '78.28%', 2: '88.19%', 3: '96.72%'}, LR: 0.000590\n",
      "Epoch 122/200, Train Loss: 0.683622, Train-Class-Acc: {0: '91.83%', 1: '89.45%', 2: '93.24%', 3: '94.16%'}\n",
      "Val Loss: 2.920902, Val Acc: 85.66%, Val-Class-Acc: {0: '82.61%', 1: '83.20%', 2: '85.42%', 3: '90.57%'}, LR: 0.000590\n",
      "Epoch 123/200, Train Loss: 0.791421, Train-Class-Acc: {0: '91.55%', 1: '89.45%', 2: '92.89%', 3: '95.49%'}\n",
      "Val Loss: 2.196975, Val Acc: 85.66%, Val-Class-Acc: {0: '80.98%', 1: '79.10%', 2: '86.11%', 3: '95.49%'}, LR: 0.000590\n",
      "Epoch 124/200, Train Loss: 0.579523, Train-Class-Acc: {0: '91.14%', 1: '89.96%', 2: '93.24%', 3: '95.49%'}\n",
      "Val Loss: 2.454525, Val Acc: 86.40%, Val-Class-Acc: {0: '78.80%', 1: '81.97%', 2: '85.42%', 3: '97.13%'}, LR: 0.000590\n",
      "Epoch 125/200, Train Loss: 0.715093, Train-Class-Acc: {0: '91.69%', 1: '89.75%', 2: '92.72%', 3: '94.47%'}\n",
      "Val Loss: 1.893514, Val Acc: 86.76%, Val-Class-Acc: {0: '81.52%', 1: '83.20%', 2: '86.11%', 3: '94.67%'}, LR: 0.000590\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_73.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_125.pth\n",
      "Epoch 126/200, Train Loss: 0.650396, Train-Class-Acc: {0: '92.37%', 1: '88.52%', 2: '93.76%', 3: '95.08%'}\n",
      "Val Loss: 1.943359, Val Acc: 86.15%, Val-Class-Acc: {0: '80.98%', 1: '80.33%', 2: '86.11%', 3: '95.90%'}, LR: 0.000590\n",
      "Epoch 127/200, Train Loss: 0.675924, Train-Class-Acc: {0: '89.51%', 1: '89.86%', 2: '94.63%', 3: '96.11%'}\n",
      "Val Loss: 1.993287, Val Acc: 86.64%, Val-Class-Acc: {0: '84.24%', 1: '80.74%', 2: '86.81%', 3: '94.26%'}, LR: 0.000531\n",
      "Epoch 128/200, Train Loss: 0.662826, Train-Class-Acc: {0: '93.05%', 1: '90.27%', 2: '93.93%', 3: '95.18%'}\n",
      "Val Loss: 2.253654, Val Acc: 86.89%, Val-Class-Acc: {0: '83.70%', 1: '82.38%', 2: '86.11%', 3: '94.26%'}, LR: 0.000531\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_83.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_128.pth\n",
      "Epoch 129/200, Train Loss: 0.555797, Train-Class-Acc: {0: '91.28%', 1: '89.96%', 2: '94.28%', 3: '95.80%'}\n",
      "Val Loss: 2.088194, Val Acc: 87.25%, Val-Class-Acc: {0: '84.24%', 1: '83.61%', 2: '85.42%', 3: '94.26%'}, LR: 0.000531\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_108.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_129.pth\n",
      "Epoch 130/200, Train Loss: 0.824106, Train-Class-Acc: {0: '91.14%', 1: '89.45%', 2: '93.24%', 3: '94.36%'}\n",
      "Val Loss: 2.098383, Val Acc: 86.15%, Val-Class-Acc: {0: '84.78%', 1: '77.46%', 2: '85.42%', 3: '96.31%'}, LR: 0.000531\n",
      "Epoch 131/200, Train Loss: 0.686328, Train-Class-Acc: {0: '92.23%', 1: '89.34%', 2: '93.24%', 3: '96.21%'}\n",
      "Val Loss: 2.483047, Val Acc: 85.91%, Val-Class-Acc: {0: '83.15%', 1: '76.64%', 2: '84.72%', 3: '97.95%'}, LR: 0.000531\n",
      "Epoch 132/200, Train Loss: 0.698945, Train-Class-Acc: {0: '92.37%', 1: '90.88%', 2: '92.20%', 3: '95.80%'}\n",
      "Val Loss: 1.909625, Val Acc: 86.15%, Val-Class-Acc: {0: '82.07%', 1: '79.10%', 2: '87.50%', 3: '95.49%'}, LR: 0.000531\n",
      "Epoch 133/200, Train Loss: 0.645121, Train-Class-Acc: {0: '91.96%', 1: '91.39%', 2: '94.28%', 3: '95.08%'}\n",
      "Val Loss: 2.108240, Val Acc: 86.27%, Val-Class-Acc: {0: '83.70%', 1: '80.33%', 2: '84.03%', 3: '95.49%'}, LR: 0.000531\n",
      "Epoch 134/200, Train Loss: 0.585839, Train-Class-Acc: {0: '92.78%', 1: '89.86%', 2: '93.24%', 3: '95.08%'}\n",
      "Val Loss: 2.326134, Val Acc: 87.13%, Val-Class-Acc: {0: '83.70%', 1: '83.61%', 2: '86.81%', 3: '93.44%'}, LR: 0.000531\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_85.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_134.pth\n",
      "Epoch 135/200, Train Loss: 0.514899, Train-Class-Acc: {0: '93.87%', 1: '91.19%', 2: '92.72%', 3: '94.98%'}\n",
      "Val Loss: 2.020257, Val Acc: 87.01%, Val-Class-Acc: {0: '85.87%', 1: '81.56%', 2: '82.64%', 3: '95.90%'}, LR: 0.000531\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_125.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_135.pth\n",
      "Epoch 136/200, Train Loss: 0.661738, Train-Class-Acc: {0: '90.74%', 1: '90.98%', 2: '93.24%', 3: '95.70%'}\n",
      "Val Loss: 2.086865, Val Acc: 85.78%, Val-Class-Acc: {0: '79.35%', 1: '84.43%', 2: '84.03%', 3: '93.03%'}, LR: 0.000531\n",
      "Epoch 137/200, Train Loss: 0.581026, Train-Class-Acc: {0: '93.46%', 1: '90.16%', 2: '92.37%', 3: '96.11%'}\n",
      "Val Loss: 2.291963, Val Acc: 86.76%, Val-Class-Acc: {0: '82.61%', 1: '80.33%', 2: '86.11%', 3: '96.72%'}, LR: 0.000531\n",
      "Epoch 138/200, Train Loss: 0.527215, Train-Class-Acc: {0: '92.10%', 1: '91.70%', 2: '94.63%', 3: '95.49%'}\n",
      "Val Loss: 2.147050, Val Acc: 86.27%, Val-Class-Acc: {0: '84.24%', 1: '79.10%', 2: '85.42%', 3: '95.49%'}, LR: 0.000478\n",
      "Epoch 139/200, Train Loss: 0.592230, Train-Class-Acc: {0: '92.10%', 1: '91.29%', 2: '93.59%', 3: '95.80%'}\n",
      "Val Loss: 1.951229, Val Acc: 86.89%, Val-Class-Acc: {0: '86.41%', 1: '79.51%', 2: '84.72%', 3: '95.90%'}, LR: 0.000478\n",
      "Epoch 140/200, Train Loss: 0.631532, Train-Class-Acc: {0: '93.32%', 1: '90.98%', 2: '93.76%', 3: '95.90%'}\n",
      "Val Loss: 2.135546, Val Acc: 87.25%, Val-Class-Acc: {0: '84.78%', 1: '82.38%', 2: '84.72%', 3: '95.49%'}, LR: 0.000478\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_128.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_140.pth\n",
      "Epoch 141/200, Train Loss: 0.601600, Train-Class-Acc: {0: '91.96%', 1: '90.68%', 2: '93.59%', 3: '96.11%'}\n",
      "Val Loss: 2.551108, Val Acc: 86.27%, Val-Class-Acc: {0: '84.78%', 1: '76.64%', 2: '86.11%', 3: '97.13%'}, LR: 0.000478\n",
      "Epoch 142/200, Train Loss: 0.686715, Train-Class-Acc: {0: '91.83%', 1: '91.19%', 2: '94.11%', 3: '95.90%'}\n",
      "Val Loss: 2.010182, Val Acc: 87.25%, Val-Class-Acc: {0: '85.33%', 1: '80.33%', 2: '85.42%', 3: '96.72%'}, LR: 0.000478\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_82.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_142.pth\n",
      "Epoch 143/200, Train Loss: 0.412097, Train-Class-Acc: {0: '92.92%', 1: '91.91%', 2: '95.32%', 3: '96.93%'}\n",
      "Val Loss: 2.216712, Val Acc: 86.03%, Val-Class-Acc: {0: '81.52%', 1: '79.10%', 2: '87.50%', 3: '95.49%'}, LR: 0.000478\n",
      "Epoch 144/200, Train Loss: 0.571429, Train-Class-Acc: {0: '91.83%', 1: '91.29%', 2: '94.11%', 3: '96.93%'}\n",
      "Val Loss: 2.725674, Val Acc: 86.40%, Val-Class-Acc: {0: '79.89%', 1: '86.07%', 2: '84.03%', 3: '93.03%'}, LR: 0.000478\n",
      "Epoch 145/200, Train Loss: 0.631584, Train-Class-Acc: {0: '92.92%', 1: '90.98%', 2: '93.59%', 3: '94.88%'}\n",
      "Val Loss: 2.862675, Val Acc: 85.42%, Val-Class-Acc: {0: '84.24%', 1: '84.84%', 2: '79.86%', 3: '90.16%'}, LR: 0.000478\n",
      "Epoch 146/200, Train Loss: 0.575609, Train-Class-Acc: {0: '95.10%', 1: '92.01%', 2: '94.11%', 3: '96.62%'}\n",
      "Val Loss: 2.087714, Val Acc: 86.40%, Val-Class-Acc: {0: '84.24%', 1: '77.87%', 2: '87.50%', 3: '95.90%'}, LR: 0.000478\n",
      "Epoch 147/200, Train Loss: 0.486004, Train-Class-Acc: {0: '92.78%', 1: '91.50%', 2: '94.45%', 3: '96.21%'}\n",
      "Val Loss: 2.159543, Val Acc: 86.64%, Val-Class-Acc: {0: '83.15%', 1: '80.74%', 2: '85.42%', 3: '95.90%'}, LR: 0.000478\n",
      "Epoch 148/200, Train Loss: 0.460407, Train-Class-Acc: {0: '93.87%', 1: '91.29%', 2: '94.80%', 3: '96.52%'}\n",
      "Val Loss: 3.165488, Val Acc: 86.40%, Val-Class-Acc: {0: '80.43%', 1: '85.66%', 2: '86.81%', 3: '91.39%'}, LR: 0.000478\n",
      "Epoch 149/200, Train Loss: 0.490720, Train-Class-Acc: {0: '92.92%', 1: '92.01%', 2: '93.41%', 3: '96.72%'}\n",
      "Val Loss: 2.235967, Val Acc: 86.52%, Val-Class-Acc: {0: '84.78%', 1: '78.28%', 2: '86.81%', 3: '95.90%'}, LR: 0.000430\n",
      "Epoch 150/200, Train Loss: 0.632583, Train-Class-Acc: {0: '93.73%', 1: '90.37%', 2: '94.45%', 3: '95.39%'}\n",
      "Val Loss: 2.228503, Val Acc: 86.76%, Val-Class-Acc: {0: '79.35%', 1: '84.84%', 2: '85.42%', 3: '95.08%'}, LR: 0.000430\n",
      "Epoch 151/200, Train Loss: 0.566313, Train-Class-Acc: {0: '93.05%', 1: '91.60%', 2: '94.28%', 3: '95.90%'}\n",
      "Val Loss: 2.265432, Val Acc: 86.52%, Val-Class-Acc: {0: '83.70%', 1: '82.38%', 2: '86.11%', 3: '93.03%'}, LR: 0.000430\n",
      "Epoch 152/200, Train Loss: 0.368032, Train-Class-Acc: {0: '93.87%', 1: '92.01%', 2: '95.15%', 3: '96.93%'}\n",
      "Val Loss: 2.180788, Val Acc: 85.78%, Val-Class-Acc: {0: '83.70%', 1: '80.33%', 2: '79.86%', 3: '96.31%'}, LR: 0.000430\n",
      "Epoch 153/200, Train Loss: 0.503926, Train-Class-Acc: {0: '93.73%', 1: '92.73%', 2: '93.76%', 3: '96.62%'}\n",
      "Val Loss: 2.451200, Val Acc: 86.15%, Val-Class-Acc: {0: '82.07%', 1: '79.10%', 2: '84.72%', 3: '97.13%'}, LR: 0.000430\n",
      "Epoch 154/200, Train Loss: 0.409156, Train-Class-Acc: {0: '94.14%', 1: '92.93%', 2: '94.80%', 3: '96.11%'}\n",
      "Val Loss: 2.649983, Val Acc: 84.56%, Val-Class-Acc: {0: '80.98%', 1: '72.54%', 2: '88.89%', 3: '96.72%'}, LR: 0.000430\n",
      "Epoch 155/200, Train Loss: 0.466586, Train-Class-Acc: {0: '94.01%', 1: '93.03%', 2: '94.63%', 3: '97.44%'}\n",
      "Val Loss: 2.131729, Val Acc: 86.27%, Val-Class-Acc: {0: '83.15%', 1: '79.51%', 2: '84.72%', 3: '96.31%'}, LR: 0.000430\n",
      "Epoch 156/200, Train Loss: 0.493406, Train-Class-Acc: {0: '94.55%', 1: '91.39%', 2: '94.45%', 3: '96.82%'}\n",
      "Val Loss: 2.110791, Val Acc: 87.13%, Val-Class-Acc: {0: '83.70%', 1: '81.56%', 2: '86.81%', 3: '95.49%'}, LR: 0.000430\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_135.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_156.pth\n",
      "Epoch 157/200, Train Loss: 0.530231, Train-Class-Acc: {0: '93.05%', 1: '93.03%', 2: '95.49%', 3: '95.90%'}\n",
      "Val Loss: 2.174586, Val Acc: 87.01%, Val-Class-Acc: {0: '83.15%', 1: '82.79%', 2: '83.33%', 3: '96.31%'}, LR: 0.000430\n",
      "Epoch 158/200, Train Loss: 0.389620, Train-Class-Acc: {0: '94.96%', 1: '92.73%', 2: '94.28%', 3: '96.52%'}\n",
      "Val Loss: 2.599966, Val Acc: 86.15%, Val-Class-Acc: {0: '82.61%', 1: '81.97%', 2: '85.42%', 3: '93.44%'}, LR: 0.000430\n",
      "Epoch 159/200, Train Loss: 0.547306, Train-Class-Acc: {0: '92.51%', 1: '92.42%', 2: '94.80%', 3: '96.72%'}\n",
      "Val Loss: 1.969820, Val Acc: 87.13%, Val-Class-Acc: {0: '80.43%', 1: '84.84%', 2: '85.42%', 3: '95.49%'}, LR: 0.000430\n",
      "Epoch 160/200, Train Loss: 0.439968, Train-Class-Acc: {0: '94.96%', 1: '92.21%', 2: '95.15%', 3: '96.52%'}\n",
      "Val Loss: 2.024716, Val Acc: 86.15%, Val-Class-Acc: {0: '77.72%', 1: '84.43%', 2: '83.33%', 3: '95.90%'}, LR: 0.000387\n",
      "Epoch 161/200, Train Loss: 0.495909, Train-Class-Acc: {0: '95.23%', 1: '92.73%', 2: '94.45%', 3: '95.29%'}\n",
      "Val Loss: 2.202760, Val Acc: 86.52%, Val-Class-Acc: {0: '81.52%', 1: '82.38%', 2: '84.03%', 3: '95.90%'}, LR: 0.000387\n",
      "Epoch 162/200, Train Loss: 0.650427, Train-Class-Acc: {0: '93.60%', 1: '92.73%', 2: '94.28%', 3: '97.44%'}\n",
      "Val Loss: 2.113730, Val Acc: 86.27%, Val-Class-Acc: {0: '81.52%', 1: '80.33%', 2: '86.11%', 3: '95.90%'}, LR: 0.000387\n",
      "Epoch 163/200, Train Loss: 0.299145, Train-Class-Acc: {0: '94.82%', 1: '94.47%', 2: '96.36%', 3: '96.93%'}\n",
      "Val Loss: 2.327431, Val Acc: 86.76%, Val-Class-Acc: {0: '82.07%', 1: '84.84%', 2: '81.25%', 3: '95.49%'}, LR: 0.000387\n",
      "Epoch 164/200, Train Loss: 0.606809, Train-Class-Acc: {0: '93.87%', 1: '93.24%', 2: '93.93%', 3: '97.03%'}\n",
      "Val Loss: 2.796287, Val Acc: 85.91%, Val-Class-Acc: {0: '80.98%', 1: '83.61%', 2: '84.72%', 3: '92.62%'}, LR: 0.000387\n",
      "Epoch 165/200, Train Loss: 0.464215, Train-Class-Acc: {0: '94.96%', 1: '92.42%', 2: '94.28%', 3: '95.70%'}\n",
      "Val Loss: 2.441608, Val Acc: 86.52%, Val-Class-Acc: {0: '80.98%', 1: '84.84%', 2: '84.03%', 3: '93.85%'}, LR: 0.000387\n",
      "Epoch 166/200, Train Loss: 0.434059, Train-Class-Acc: {0: '94.55%', 1: '92.21%', 2: '95.67%', 3: '97.13%'}\n",
      "Val Loss: 2.132113, Val Acc: 87.01%, Val-Class-Acc: {0: '82.61%', 1: '81.97%', 2: '86.81%', 3: '95.49%'}, LR: 0.000387\n",
      "Epoch 167/200, Train Loss: 0.527524, Train-Class-Acc: {0: '95.50%', 1: '93.75%', 2: '94.45%', 3: '96.52%'}\n",
      "Val Loss: 2.582231, Val Acc: 86.03%, Val-Class-Acc: {0: '83.15%', 1: '78.69%', 2: '85.42%', 3: '95.90%'}, LR: 0.000387\n",
      "Epoch 168/200, Train Loss: 0.398469, Train-Class-Acc: {0: '93.60%', 1: '93.85%', 2: '96.19%', 3: '97.34%'}\n",
      "Val Loss: 2.229452, Val Acc: 86.52%, Val-Class-Acc: {0: '83.70%', 1: '79.10%', 2: '86.81%', 3: '95.90%'}, LR: 0.000387\n",
      "Epoch 169/200, Train Loss: 0.421572, Train-Class-Acc: {0: '94.96%', 1: '94.26%', 2: '96.01%', 3: '96.00%'}\n",
      "Val Loss: 2.161367, Val Acc: 86.27%, Val-Class-Acc: {0: '84.78%', 1: '79.92%', 2: '86.11%', 3: '93.85%'}, LR: 0.000387\n",
      "Epoch 170/200, Train Loss: 0.350274, Train-Class-Acc: {0: '94.82%', 1: '93.95%', 2: '94.80%', 3: '97.34%'}\n",
      "Val Loss: 2.129480, Val Acc: 86.15%, Val-Class-Acc: {0: '83.70%', 1: '78.28%', 2: '86.81%', 3: '95.49%'}, LR: 0.000387\n",
      "Epoch 171/200, Train Loss: 0.340691, Train-Class-Acc: {0: '95.23%', 1: '93.14%', 2: '95.32%', 3: '97.03%'}\n",
      "Val Loss: 2.355650, Val Acc: 86.03%, Val-Class-Acc: {0: '82.07%', 1: '77.46%', 2: '87.50%', 3: '96.72%'}, LR: 0.000349\n",
      "Epoch 172/200, Train Loss: 0.398789, Train-Class-Acc: {0: '94.96%', 1: '93.24%', 2: '95.84%', 3: '96.11%'}\n",
      "Val Loss: 2.268619, Val Acc: 85.54%, Val-Class-Acc: {0: '80.43%', 1: '80.74%', 2: '84.72%', 3: '94.67%'}, LR: 0.000349\n",
      "Epoch 173/200, Train Loss: 0.398734, Train-Class-Acc: {0: '94.14%', 1: '93.65%', 2: '95.49%', 3: '97.23%'}\n",
      "Val Loss: 2.374349, Val Acc: 86.89%, Val-Class-Acc: {0: '84.24%', 1: '79.51%', 2: '85.42%', 3: '97.13%'}, LR: 0.000349\n",
      "Epoch 174/200, Train Loss: 0.444604, Train-Class-Acc: {0: '95.64%', 1: '93.55%', 2: '95.15%', 3: '97.03%'}\n",
      "Val Loss: 2.298621, Val Acc: 86.40%, Val-Class-Acc: {0: '83.70%', 1: '79.10%', 2: '85.42%', 3: '96.31%'}, LR: 0.000349\n",
      "Epoch 175/200, Train Loss: 0.344072, Train-Class-Acc: {0: '96.32%', 1: '94.36%', 2: '95.32%', 3: '97.23%'}\n",
      "Val Loss: 2.247331, Val Acc: 85.66%, Val-Class-Acc: {0: '79.89%', 1: '82.38%', 2: '82.64%', 3: '95.08%'}, LR: 0.000349\n",
      "Epoch 176/200, Train Loss: 0.322225, Train-Class-Acc: {0: '94.82%', 1: '94.47%', 2: '95.49%', 3: '97.64%'}\n",
      "Val Loss: 2.182491, Val Acc: 86.15%, Val-Class-Acc: {0: '82.61%', 1: '81.56%', 2: '84.72%', 3: '94.26%'}, LR: 0.000349\n",
      "Epoch 177/200, Train Loss: 0.359737, Train-Class-Acc: {0: '94.41%', 1: '93.65%', 2: '95.32%', 3: '97.03%'}\n",
      "Val Loss: 2.399602, Val Acc: 85.66%, Val-Class-Acc: {0: '84.78%', 1: '77.05%', 2: '84.03%', 3: '95.90%'}, LR: 0.000349\n",
      "Epoch 178/200, Train Loss: 0.587503, Train-Class-Acc: {0: '94.96%', 1: '93.44%', 2: '95.49%', 3: '96.72%'}\n",
      "Val Loss: 2.263688, Val Acc: 86.03%, Val-Class-Acc: {0: '83.70%', 1: '80.33%', 2: '82.64%', 3: '95.49%'}, LR: 0.000349\n",
      "Epoch 179/200, Train Loss: 0.338625, Train-Class-Acc: {0: '94.14%', 1: '92.52%', 2: '96.71%', 3: '96.52%'}\n",
      "Val Loss: 2.263013, Val Acc: 86.15%, Val-Class-Acc: {0: '83.70%', 1: '80.33%', 2: '83.33%', 3: '95.49%'}, LR: 0.000349\n",
      "Epoch 180/200, Train Loss: 0.343951, Train-Class-Acc: {0: '95.23%', 1: '93.34%', 2: '95.49%', 3: '97.03%'}\n",
      "Val Loss: 2.354756, Val Acc: 86.40%, Val-Class-Acc: {0: '82.07%', 1: '80.33%', 2: '85.42%', 3: '96.31%'}, LR: 0.000349\n",
      "Epoch 181/200, Train Loss: 0.460697, Train-Class-Acc: {0: '93.32%', 1: '92.21%', 2: '95.15%', 3: '97.03%'}\n",
      "Val Loss: 2.258363, Val Acc: 87.13%, Val-Class-Acc: {0: '84.78%', 1: '80.74%', 2: '84.72%', 3: '96.72%'}, LR: 0.000349\n",
      "Epoch 182/200, Train Loss: 0.394620, Train-Class-Acc: {0: '95.37%', 1: '93.44%', 2: '95.15%', 3: '97.03%'}\n",
      "Val Loss: 2.298074, Val Acc: 86.52%, Val-Class-Acc: {0: '85.33%', 1: '80.33%', 2: '84.72%', 3: '94.67%'}, LR: 0.000314\n",
      "Epoch 183/200, Train Loss: 0.386310, Train-Class-Acc: {0: '94.69%', 1: '93.95%', 2: '95.49%', 3: '97.85%'}\n",
      "Val Loss: 2.186595, Val Acc: 86.40%, Val-Class-Acc: {0: '83.70%', 1: '83.61%', 2: '82.64%', 3: '93.44%'}, LR: 0.000314\n",
      "Epoch 184/200, Train Loss: 0.315808, Train-Class-Acc: {0: '95.23%', 1: '93.24%', 2: '96.36%', 3: '97.95%'}\n",
      "Val Loss: 2.226073, Val Acc: 86.40%, Val-Class-Acc: {0: '82.07%', 1: '83.20%', 2: '83.33%', 3: '94.67%'}, LR: 0.000314\n",
      "Epoch 185/200, Train Loss: 0.378508, Train-Class-Acc: {0: '94.96%', 1: '94.57%', 2: '95.84%', 3: '96.93%'}\n",
      "Val Loss: 2.819944, Val Acc: 84.68%, Val-Class-Acc: {0: '79.89%', 1: '75.41%', 2: '84.72%', 3: '97.54%'}, LR: 0.000314\n",
      "Epoch 186/200, Train Loss: 0.355949, Train-Class-Acc: {0: '94.41%', 1: '94.36%', 2: '95.32%', 3: '97.13%'}\n",
      "Val Loss: 2.924793, Val Acc: 86.15%, Val-Class-Acc: {0: '83.15%', 1: '76.64%', 2: '86.11%', 3: '97.95%'}, LR: 0.000314\n",
      "Epoch 187/200, Train Loss: 0.321995, Train-Class-Acc: {0: '94.41%', 1: '94.36%', 2: '97.05%', 3: '98.16%'}\n",
      "Val Loss: 2.331525, Val Acc: 86.52%, Val-Class-Acc: {0: '85.33%', 1: '80.74%', 2: '83.33%', 3: '95.08%'}, LR: 0.000314\n",
      "Epoch 188/200, Train Loss: 0.339931, Train-Class-Acc: {0: '94.82%', 1: '94.06%', 2: '95.49%', 3: '97.23%'}\n",
      "Val Loss: 2.354365, Val Acc: 86.76%, Val-Class-Acc: {0: '82.07%', 1: '80.74%', 2: '86.81%', 3: '96.31%'}, LR: 0.000314\n",
      "Epoch 189/200, Train Loss: 0.378938, Train-Class-Acc: {0: '95.23%', 1: '93.14%', 2: '96.19%', 3: '96.41%'}\n",
      "Val Loss: 2.268236, Val Acc: 86.76%, Val-Class-Acc: {0: '82.61%', 1: '82.79%', 2: '84.72%', 3: '95.08%'}, LR: 0.000314\n",
      "Epoch 190/200, Train Loss: 0.277686, Train-Class-Acc: {0: '95.64%', 1: '94.36%', 2: '95.49%', 3: '98.16%'}\n",
      "Val Loss: 2.490514, Val Acc: 86.27%, Val-Class-Acc: {0: '83.70%', 1: '77.46%', 2: '86.11%', 3: '97.13%'}, LR: 0.000314\n",
      "Epoch 191/200, Train Loss: 0.387842, Train-Class-Acc: {0: '96.05%', 1: '93.65%', 2: '95.15%', 3: '96.31%'}\n",
      "Val Loss: 2.479273, Val Acc: 86.64%, Val-Class-Acc: {0: '84.78%', 1: '78.69%', 2: '84.72%', 3: '97.13%'}, LR: 0.000314\n",
      "Epoch 192/200, Train Loss: 0.270144, Train-Class-Acc: {0: '94.82%', 1: '95.18%', 2: '96.53%', 3: '97.44%'}\n",
      "Val Loss: 2.629408, Val Acc: 85.91%, Val-Class-Acc: {0: '84.24%', 1: '75.41%', 2: '85.42%', 3: '97.95%'}, LR: 0.000314\n",
      "Epoch 193/200, Train Loss: 0.341312, Train-Class-Acc: {0: '95.91%', 1: '95.08%', 2: '96.19%', 3: '97.64%'}\n",
      "Val Loss: 2.632959, Val Acc: 86.52%, Val-Class-Acc: {0: '84.78%', 1: '81.15%', 2: '84.72%', 3: '94.26%'}, LR: 0.000282\n",
      "Epoch 194/200, Train Loss: 0.339074, Train-Class-Acc: {0: '95.10%', 1: '93.85%', 2: '95.67%', 3: '97.44%'}\n",
      "Val Loss: 2.482698, Val Acc: 85.42%, Val-Class-Acc: {0: '81.52%', 1: '79.51%', 2: '85.42%', 3: '94.26%'}, LR: 0.000282\n",
      "Epoch 195/200, Train Loss: 0.298089, Train-Class-Acc: {0: '95.78%', 1: '93.75%', 2: '96.71%', 3: '97.23%'}\n",
      "Val Loss: 2.376842, Val Acc: 86.64%, Val-Class-Acc: {0: '83.15%', 1: '82.79%', 2: '83.33%', 3: '95.08%'}, LR: 0.000282\n",
      "Epoch 196/200, Train Loss: 0.314646, Train-Class-Acc: {0: '94.96%', 1: '95.08%', 2: '95.49%', 3: '98.26%'}\n",
      "Val Loss: 2.507513, Val Acc: 86.52%, Val-Class-Acc: {0: '83.70%', 1: '83.20%', 2: '82.64%', 3: '94.26%'}, LR: 0.000282\n",
      "Epoch 197/200, Train Loss: 0.216175, Train-Class-Acc: {0: '97.00%', 1: '95.29%', 2: '96.53%', 3: '98.16%'}\n",
      "Val Loss: 2.451815, Val Acc: 85.91%, Val-Class-Acc: {0: '80.98%', 1: '80.33%', 2: '84.72%', 3: '95.90%'}, LR: 0.000282\n",
      "Epoch 198/200, Train Loss: 0.298661, Train-Class-Acc: {0: '95.10%', 1: '94.57%', 2: '97.05%', 3: '97.75%'}\n",
      "Val Loss: 2.390330, Val Acc: 86.03%, Val-Class-Acc: {0: '79.35%', 1: '81.15%', 2: '86.81%', 3: '95.49%'}, LR: 0.000282\n",
      "Epoch 199/200, Train Loss: 0.452573, Train-Class-Acc: {0: '95.78%', 1: '92.93%', 2: '97.05%', 3: '97.23%'}\n",
      "Val Loss: 2.557275, Val Acc: 85.78%, Val-Class-Acc: {0: '78.80%', 1: '84.84%', 2: '81.25%', 3: '94.67%'}, LR: 0.000282\n",
      "Epoch 200/200, Train Loss: 0.291939, Train-Class-Acc: {0: '96.73%', 1: '96.00%', 2: '96.19%', 3: '97.54%'}\n",
      "Val Loss: 2.570388, Val Acc: 86.40%, Val-Class-Acc: {0: '82.61%', 1: '79.51%', 2: '86.11%', 3: '96.31%'}, LR: 0.000282\n",
      "\n",
      "üèÜ Best model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_best.pth (Val Accuracy: 87.25%)\n",
      "\n",
      "üìå Final model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_final.pth\n",
      "\n",
      "üéØ Top 5 Best Models:\n",
      "Epoch 142, Train Loss: 0.686715, Train-Acc: {0: '91.83%', 1: '91.19%', 2: '94.11%', 3: '95.90%'},\n",
      "Val Loss: 2.010182, Val Acc: 87.25%, Val-Acc: {0: '85.33%', 1: '80.33%', 2: '85.42%', 3: '96.72%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_142.pth\n",
      "Epoch 140, Train Loss: 0.631532, Train-Acc: {0: '93.32%', 1: '90.98%', 2: '93.76%', 3: '95.90%'},\n",
      "Val Loss: 2.135546, Val Acc: 87.25%, Val-Acc: {0: '84.78%', 1: '82.38%', 2: '84.72%', 3: '95.49%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_140.pth\n",
      "Epoch 129, Train Loss: 0.555797, Train-Acc: {0: '91.28%', 1: '89.96%', 2: '94.28%', 3: '95.80%'},\n",
      "Val Loss: 2.088194, Val Acc: 87.25%, Val-Acc: {0: '84.24%', 1: '83.61%', 2: '85.42%', 3: '94.26%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_129.pth\n",
      "Epoch 156, Train Loss: 0.493406, Train-Acc: {0: '94.55%', 1: '91.39%', 2: '94.45%', 3: '96.82%'},\n",
      "Val Loss: 2.110791, Val Acc: 87.13%, Val-Acc: {0: '83.70%', 1: '81.56%', 2: '86.81%', 3: '95.49%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_156.pth\n",
      "Epoch 134, Train Loss: 0.585839, Train-Acc: {0: '92.78%', 1: '89.86%', 2: '93.24%', 3: '95.08%'},\n",
      "Val Loss: 2.326134, Val Acc: 87.13%, Val-Acc: {0: '83.70%', 1: '83.61%', 2: '86.81%', 3: '93.44%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2/ResNet18_1D_LoRA_epoch_134.pth\n",
      "---\n",
      "### Period 2\n",
      "+ ##### Total training time: 298.35 seconds\n",
      "+ ##### Model: ResNet18_1D_LoRA\n",
      "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_2'*\n",
      "+ ##### Best Epoch: 142\n",
      "#### __Val Accuracy: 87.25%__\n",
      "#### __Val-Class-Acc: {0: '85.33%', 1: '80.33%', 2: '85.42%', 3: '96.72%'}__\n",
      "#### __Total Parameters: 3,889,796__\n",
      "#### __Model Size (float32): 14.84 MB__\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå Period 2: Standard LoRA Training (ECG)\n",
    "# ================================\n",
    "period = 2\n",
    "\n",
    "# ==== Paths ====\n",
    "stop_signal_file = os.path.join(BASE_DIR, \"stop_training.txt\")\n",
    "model_saving_folder = os.path.join(BASE_DIR, \"Trained_models\", \"Standard_LoRA_CIL_v2\", f\"Period_{period}\")\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# ==== Load Period 2 Data ====\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{period}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{period}.npy\"))\n",
    "X_val   = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val   = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "# ==== Device ====\n",
    "device = auto_select_cuda_device()\n",
    "\n",
    "# ==== Model Configuration ====\n",
    "input_channels = X_train.shape[2]  # ECG 12-lead\n",
    "output_size = len(np.unique(y_train))  # Êñ∞Â¢ûÈ°ûÂà•Êï∏\n",
    "model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size, lora_rank=4).to(device)\n",
    "\n",
    "# ==== Load Period 1 Best Model Weights (excluding FC but keeping internal layers) ====\n",
    "prev_model_path = os.path.join(BASE_DIR, \"ResNet18_Selection\", \"ResNet18_big_inplane_v1\", \"ResNet18_big_inplane_1D_best.pth\")\n",
    "prev_checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "prev_state_dict = prev_checkpoint[\"model_state_dict\"]\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "filtered_state_dict = {\n",
    "    k: v for k, v in prev_state_dict.items()\n",
    "    if k in model_dict and model_dict[k].shape == v.shape and not k.startswith(\"fc\")\n",
    "}\n",
    "model.load_state_dict(filtered_state_dict, strict=False)\n",
    "for k in model_dict:\n",
    "    if k not in filtered_state_dict:\n",
    "        print(f\"üîç Not loaded: {k}, shape={model_dict[k].shape}\")\n",
    "print(\"‚úÖ Loaded Period 1 weights (excluding final FC layer)\")\n",
    "\n",
    "# ==== Initialize LoRA Adapters ====\n",
    "model.init_lora()\n",
    "\n",
    "# ==== Optimizer / Scheduler ====\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.get_trainable_parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# ==== Start Training ====\n",
    "train_with_lora_ecg(\n",
    "    model=model,\n",
    "    output_size=output_size,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    model_saving_folder=model_saving_folder,\n",
    "    model_name=\"ResNet18_1D_LoRA\",\n",
    "    stop_signal_file=stop_signal_file,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# ‚úÖ Cleanup\n",
    "# ================================\n",
    "del X_train, y_train, X_val, y_val\n",
    "del prev_model_path, prev_checkpoint, prev_state_dict, filtered_state_dict\n",
    "del model, criterion, optimizer, scheduler\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad6ab5d",
   "metadata": {},
   "source": [
    "### Period 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08221a9c",
   "metadata": {},
   "source": [
    "#### v2: `lora_adapter.conv`Êú™Ë¢´ËºâÂÖ•ÁöÑÁâàÊú¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90039f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 2\n",
      "    - Memory Used    : 1597 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ LoRA adapters initialized for 8 conv2 layers\n",
      "üîç Not loaded: layer1.0.lora_adapter.lora_A, shape=torch.Size([64, 4])\n",
      "üîç Not loaded: layer1.0.lora_adapter.lora_B, shape=torch.Size([4, 192])\n",
      "üîç Not loaded: layer1.0.lora_adapter.conv.weight, shape=torch.Size([64, 64, 3])\n",
      "üîç Not loaded: layer1.1.lora_adapter.lora_A, shape=torch.Size([64, 4])\n",
      "üîç Not loaded: layer1.1.lora_adapter.lora_B, shape=torch.Size([4, 192])\n",
      "üîç Not loaded: layer1.1.lora_adapter.conv.weight, shape=torch.Size([64, 64, 3])\n",
      "üîç Not loaded: layer2.0.lora_adapter.lora_A, shape=torch.Size([128, 4])\n",
      "üîç Not loaded: layer2.0.lora_adapter.lora_B, shape=torch.Size([4, 384])\n",
      "üîç Not loaded: layer2.0.lora_adapter.conv.weight, shape=torch.Size([128, 128, 3])\n",
      "üîç Not loaded: layer2.1.lora_adapter.lora_A, shape=torch.Size([128, 4])\n",
      "üîç Not loaded: layer2.1.lora_adapter.lora_B, shape=torch.Size([4, 384])\n",
      "üîç Not loaded: layer2.1.lora_adapter.conv.weight, shape=torch.Size([128, 128, 3])\n",
      "üîç Not loaded: layer3.0.lora_adapter.lora_A, shape=torch.Size([256, 4])\n",
      "üîç Not loaded: layer3.0.lora_adapter.lora_B, shape=torch.Size([4, 768])\n",
      "üîç Not loaded: layer3.0.lora_adapter.conv.weight, shape=torch.Size([256, 256, 3])\n",
      "üîç Not loaded: layer3.1.lora_adapter.lora_A, shape=torch.Size([256, 4])\n",
      "üîç Not loaded: layer3.1.lora_adapter.lora_B, shape=torch.Size([4, 768])\n",
      "üîç Not loaded: layer3.1.lora_adapter.conv.weight, shape=torch.Size([256, 256, 3])\n",
      "üîç Not loaded: layer4.0.lora_adapter.lora_A, shape=torch.Size([512, 4])\n",
      "üîç Not loaded: layer4.0.lora_adapter.lora_B, shape=torch.Size([4, 1536])\n",
      "üîç Not loaded: layer4.0.lora_adapter.conv.weight, shape=torch.Size([512, 512, 3])\n",
      "üîç Not loaded: layer4.1.lora_adapter.lora_A, shape=torch.Size([512, 4])\n",
      "üîç Not loaded: layer4.1.lora_adapter.lora_B, shape=torch.Size([4, 1536])\n",
      "üîç Not loaded: layer4.1.lora_adapter.conv.weight, shape=torch.Size([512, 512, 3])\n",
      "üîç Not loaded: fc.weight, shape=torch.Size([6, 1024])\n",
      "üîç Not loaded: fc.bias, shape=torch.Size([6])\n",
      "‚úÖ Loaded Period 2 weights (excluding FC & LoRA)\n",
      "üìä Parameter Statistics:\n",
      "  - Total parameters: 3,891,846\n",
      "  - Trainable parameters: 36,870 (0.95%)\n",
      "    - LoRA parameters: 30,720 (0.79%)\n",
      "    - FC parameters: 6,150 (0.16%)\n",
      "  - Frozen parameters: 3,854,976 (99.05%)\n",
      "üß† Trainable parameter names:\n",
      "  ‚úÖ layer1.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer1.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer1.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer1.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer2.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer2.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer2.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer2.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer3.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer3.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer3.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer3.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer4.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer4.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer4.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer4.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ fc.weight (FC)\n",
      "  ‚úÖ fc.bias (FC)\n",
      "\n",
      "üöÄ 'train_with_lora_ecg' started.\n",
      "‚úÖ Removed existing folder: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_763947/1785557546.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  prev_checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Data Overview:\n",
      "X_train: torch.Size([5120, 5000, 12]), y_train: torch.Size([5120])\n",
      "X_val: torch.Size([1281, 5000, 12]), y_val: torch.Size([1281])\n",
      "Epoch 1/200, Train Loss: 57.088251, Train-Class-Acc: {0: '39.65%', 1: '28.42%', 2: '18.37%', 3: '27.25%', 4: '12.66%', 5: '43.20%'}\n",
      "Val Loss: 13.715727, Val Acc: 58.16%, Val-Class-Acc: {0: '63.04%', 1: '58.21%', 2: '22.92%', 3: '45.49%', 4: '7.50%', 5: '85.93%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_1.pth\n",
      "Epoch 2/200, Train Loss: 15.742894, Train-Class-Acc: {0: '61.44%', 1: '46.75%', 2: '30.68%', 3: '38.42%', 4: '20.89%', 5: '71.00%'}\n",
      "Val Loss: 6.599931, Val Acc: 66.82%, Val-Class-Acc: {0: '89.13%', 1: '60.90%', 2: '48.61%', 3: '51.23%', 4: '32.50%', 5: '83.83%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_2.pth\n",
      "Epoch 3/200, Train Loss: 11.824591, Train-Class-Acc: {0: '63.08%', 1: '51.91%', 2: '36.92%', 3: '43.75%', 4: '27.22%', 5: '74.44%'}\n",
      "Val Loss: 6.899810, Val Acc: 69.95%, Val-Class-Acc: {0: '86.41%', 1: '74.93%', 2: '53.47%', 3: '45.90%', 4: '25.00%', 5: '85.93%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_3.pth\n",
      "Epoch 4/200, Train Loss: 9.185968, Train-Class-Acc: {0: '62.81%', 1: '53.10%', 2: '45.75%', 3: '50.20%', 4: '24.05%', 5: '75.71%'}\n",
      "Val Loss: 4.012288, Val Acc: 72.76%, Val-Class-Acc: {0: '84.78%', 1: '75.82%', 2: '61.81%', 3: '67.21%', 4: '25.00%', 5: '77.54%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_4.pth\n",
      "Epoch 5/200, Train Loss: 7.934286, Train-Class-Acc: {0: '62.81%', 1: '53.63%', 2: '47.83%', 3: '48.16%', 4: '27.85%', 5: '75.34%'}\n",
      "Val Loss: 4.263618, Val Acc: 73.30%, Val-Class-Acc: {0: '81.52%', 1: '75.22%', 2: '69.44%', 3: '57.79%', 4: '40.00%', 5: '83.83%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_5.pth\n",
      "Epoch 6/200, Train Loss: 6.905748, Train-Class-Acc: {0: '62.40%', 1: '55.27%', 2: '54.94%', 3: '54.92%', 4: '31.01%', 5: '76.98%'}\n",
      "Val Loss: 3.963920, Val Acc: 73.77%, Val-Class-Acc: {0: '77.72%', 1: '80.30%', 2: '57.64%', 3: '60.66%', 4: '47.50%', 5: '84.73%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_1.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_6.pth\n",
      "Epoch 7/200, Train Loss: 6.063903, Train-Class-Acc: {0: '64.85%', 1: '56.99%', 2: '57.71%', 3: '55.64%', 4: '36.71%', 5: '77.35%'}\n",
      "Val Loss: 3.533071, Val Acc: 74.71%, Val-Class-Acc: {0: '70.11%', 1: '76.42%', 2: '68.75%', 3: '66.80%', 4: '62.50%', 5: '85.33%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_2.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_7.pth\n",
      "Epoch 8/200, Train Loss: 5.085532, Train-Class-Acc: {0: '64.58%', 1: '58.49%', 2: '59.10%', 3: '60.14%', 4: '47.47%', 5: '77.88%'}\n",
      "Val Loss: 2.983347, Val Acc: 75.72%, Val-Class-Acc: {0: '80.43%', 1: '75.22%', 2: '66.67%', 3: '74.59%', 4: '42.50%', 5: '82.34%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_3.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_8.pth\n",
      "Epoch 9/200, Train Loss: 4.921773, Train-Class-Acc: {0: '63.35%', 1: '58.94%', 2: '61.18%', 3: '60.45%', 4: '39.24%', 5: '76.01%'}\n",
      "Val Loss: 2.592355, Val Acc: 76.42%, Val-Class-Acc: {0: '77.72%', 1: '74.03%', 2: '68.06%', 3: '81.15%', 4: '50.00%', 5: '81.44%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_4.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_9.pth\n",
      "Epoch 10/200, Train Loss: 4.564614, Train-Class-Acc: {0: '64.17%', 1: '56.62%', 2: '58.41%', 3: '59.73%', 4: '36.71%', 5: '76.98%'}\n",
      "Val Loss: 3.090672, Val Acc: 75.80%, Val-Class-Acc: {0: '82.07%', 1: '65.97%', 2: '59.72%', 3: '79.51%', 4: '62.50%', 5: '88.02%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_5.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_10.pth\n",
      "Epoch 11/200, Train Loss: 3.851113, Train-Class-Acc: {0: '61.85%', 1: '60.43%', 2: '63.26%', 3: '62.91%', 4: '44.30%', 5: '78.40%'}\n",
      "Val Loss: 2.745279, Val Acc: 78.30%, Val-Class-Acc: {0: '73.91%', 1: '76.72%', 2: '67.36%', 3: '81.15%', 4: '60.00%', 5: '87.13%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_6.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_11.pth\n",
      "Epoch 12/200, Train Loss: 3.902855, Train-Class-Acc: {0: '62.67%', 1: '61.41%', 2: '64.30%', 3: '63.93%', 4: '37.34%', 5: '77.50%'}\n",
      "Val Loss: 3.026468, Val Acc: 77.13%, Val-Class-Acc: {0: '78.26%', 1: '78.81%', 2: '71.53%', 3: '67.62%', 4: '65.00%', 5: '85.63%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_7.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_12.pth\n",
      "Epoch 13/200, Train Loss: 3.522364, Train-Class-Acc: {0: '64.99%', 1: '58.79%', 2: '65.51%', 3: '64.65%', 4: '39.87%', 5: '77.73%'}\n",
      "Val Loss: 2.859215, Val Acc: 76.81%, Val-Class-Acc: {0: '59.78%', 1: '82.69%', 2: '68.06%', 3: '78.69%', 4: '65.00%', 5: '84.13%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_8.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_13.pth\n",
      "Epoch 14/200, Train Loss: 3.404618, Train-Class-Acc: {0: '63.22%', 1: '59.31%', 2: '65.34%', 3: '64.86%', 4: '41.77%', 5: '78.70%'}\n",
      "Val Loss: 2.835591, Val Acc: 78.45%, Val-Class-Acc: {0: '73.91%', 1: '78.51%', 2: '73.61%', 3: '78.69%', 4: '57.50%', 5: '85.33%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_10.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_14.pth\n",
      "Epoch 15/200, Train Loss: 3.190331, Train-Class-Acc: {0: '65.67%', 1: '59.54%', 2: '67.76%', 3: '67.01%', 4: '43.67%', 5: '78.18%'}\n",
      "Val Loss: 2.296987, Val Acc: 77.99%, Val-Class-Acc: {0: '71.20%', 1: '83.58%', 2: '72.92%', 3: '74.18%', 4: '65.00%', 5: '82.63%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_9.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_15.pth\n",
      "Epoch 16/200, Train Loss: 2.936546, Train-Class-Acc: {0: '64.44%', 1: '60.51%', 2: '68.11%', 3: '67.21%', 4: '41.14%', 5: '80.42%'}\n",
      "Val Loss: 2.181627, Val Acc: 79.16%, Val-Class-Acc: {0: '73.91%', 1: '79.40%', 2: '70.83%', 3: '83.61%', 4: '65.00%', 5: '83.83%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_13.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_16.pth\n",
      "Epoch 17/200, Train Loss: 2.719442, Train-Class-Acc: {0: '61.31%', 1: '60.28%', 2: '66.38%', 3: '67.93%', 4: '47.47%', 5: '79.52%'}\n",
      "Val Loss: 2.013479, Val Acc: 79.86%, Val-Class-Acc: {0: '78.80%', 1: '77.61%', 2: '72.92%', 3: '84.02%', 4: '65.00%', 5: '84.43%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_12.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_17.pth\n",
      "Epoch 18/200, Train Loss: 2.615334, Train-Class-Acc: {0: '66.89%', 1: '64.10%', 2: '69.84%', 3: '70.18%', 4: '46.20%', 5: '81.39%'}\n",
      "Val Loss: 1.887001, Val Acc: 80.02%, Val-Class-Acc: {0: '75.54%', 1: '76.12%', 2: '77.78%', 3: '83.20%', 4: '67.50%', 5: '86.53%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_15.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_18.pth\n",
      "Epoch 19/200, Train Loss: 2.476297, Train-Class-Acc: {0: '64.03%', 1: '62.98%', 2: '71.06%', 3: '71.82%', 4: '55.06%', 5: '81.46%'}\n",
      "Val Loss: 1.906558, Val Acc: 77.36%, Val-Class-Acc: {0: '64.67%', 1: '82.99%', 2: '71.53%', 3: '86.07%', 4: '60.00%', 5: '76.95%'}, LR: 0.001000\n",
      "Epoch 20/200, Train Loss: 2.762703, Train-Class-Acc: {0: '64.17%', 1: '63.80%', 2: '68.63%', 3: '70.49%', 4: '50.63%', 5: '79.45%'}\n",
      "Val Loss: 2.120779, Val Acc: 79.47%, Val-Class-Acc: {0: '75.54%', 1: '76.72%', 2: '75.69%', 3: '84.84%', 4: '70.00%', 5: '83.23%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_11.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_20.pth\n",
      "Epoch 21/200, Train Loss: 2.365495, Train-Class-Acc: {0: '67.57%', 1: '63.80%', 2: '71.40%', 3: '69.36%', 4: '48.10%', 5: '81.09%'}\n",
      "Val Loss: 2.001627, Val Acc: 80.41%, Val-Class-Acc: {0: '78.26%', 1: '77.31%', 2: '72.22%', 3: '84.84%', 4: '72.50%', 5: '85.93%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_14.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_21.pth\n",
      "Epoch 22/200, Train Loss: 2.309440, Train-Class-Acc: {0: '67.71%', 1: '63.65%', 2: '68.98%', 3: '72.23%', 4: '53.16%', 5: '81.91%'}\n",
      "Val Loss: 1.380399, Val Acc: 77.21%, Val-Class-Acc: {0: '74.46%', 1: '78.21%', 2: '75.69%', 3: '84.43%', 4: '75.00%', 5: '73.35%'}, LR: 0.001000\n",
      "Epoch 23/200, Train Loss: 2.186276, Train-Class-Acc: {0: '68.80%', 1: '64.40%', 2: '71.75%', 3: '72.44%', 4: '51.27%', 5: '81.61%'}\n",
      "Val Loss: 1.806571, Val Acc: 79.39%, Val-Class-Acc: {0: '71.20%', 1: '79.10%', 2: '73.61%', 3: '84.02%', 4: '70.00%', 5: '84.43%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_16.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_23.pth\n",
      "Epoch 24/200, Train Loss: 2.112219, Train-Class-Acc: {0: '65.53%', 1: '65.52%', 2: '72.44%', 3: '73.57%', 4: '51.27%', 5: '80.79%'}\n",
      "Val Loss: 2.453420, Val Acc: 79.31%, Val-Class-Acc: {0: '77.72%', 1: '71.64%', 2: '72.22%', 3: '81.97%', 4: '77.50%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 25/200, Train Loss: 2.135507, Train-Class-Acc: {0: '69.35%', 1: '65.30%', 2: '71.40%', 3: '72.64%', 4: '55.70%', 5: '81.76%'}\n",
      "Val Loss: 1.715508, Val Acc: 79.47%, Val-Class-Acc: {0: '73.37%', 1: '79.10%', 2: '72.92%', 3: '77.46%', 4: '75.00%', 5: '88.02%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_23.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_25.pth\n",
      "Epoch 26/200, Train Loss: 1.878083, Train-Class-Acc: {0: '67.44%', 1: '65.89%', 2: '74.35%', 3: '74.59%', 4: '54.43%', 5: '82.81%'}\n",
      "Val Loss: 1.743590, Val Acc: 79.31%, Val-Class-Acc: {0: '66.85%', 1: '76.12%', 2: '73.61%', 3: '86.89%', 4: '77.50%', 5: '86.53%'}, LR: 0.001000\n",
      "Epoch 27/200, Train Loss: 1.732661, Train-Class-Acc: {0: '69.75%', 1: '68.06%', 2: '72.96%', 3: '73.36%', 4: '60.13%', 5: '82.81%'}\n",
      "Val Loss: 1.360908, Val Acc: 80.02%, Val-Class-Acc: {0: '70.11%', 1: '76.12%', 2: '74.31%', 3: '88.11%', 4: '75.00%', 5: '86.53%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_20.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_27.pth\n",
      "Epoch 28/200, Train Loss: 1.967402, Train-Class-Acc: {0: '67.44%', 1: '66.42%', 2: '75.74%', 3: '74.69%', 4: '55.70%', 5: '83.71%'}\n",
      "Val Loss: 2.810681, Val Acc: 79.47%, Val-Class-Acc: {0: '72.83%', 1: '74.93%', 2: '74.31%', 3: '79.10%', 4: '77.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 29/200, Train Loss: 1.700441, Train-Class-Acc: {0: '69.62%', 1: '67.31%', 2: '72.27%', 3: '73.67%', 4: '55.06%', 5: '83.33%'}\n",
      "Val Loss: 1.879780, Val Acc: 79.08%, Val-Class-Acc: {0: '66.30%', 1: '80.90%', 2: '74.31%', 3: '81.56%', 4: '77.50%', 5: '84.73%'}, LR: 0.001000\n",
      "Epoch 30/200, Train Loss: 1.617008, Train-Class-Acc: {0: '70.03%', 1: '68.89%', 2: '75.22%', 3: '77.97%', 4: '66.46%', 5: '84.08%'}\n",
      "Val Loss: 2.794775, Val Acc: 79.55%, Val-Class-Acc: {0: '69.02%', 1: '79.10%', 2: '75.00%', 3: '80.33%', 4: '77.50%', 5: '87.43%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_25.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_30.pth\n",
      "Epoch 31/200, Train Loss: 1.498197, Train-Class-Acc: {0: '69.62%', 1: '70.31%', 2: '72.79%', 3: '79.20%', 4: '60.13%', 5: '84.38%'}\n",
      "Val Loss: 2.000751, Val Acc: 79.16%, Val-Class-Acc: {0: '76.63%', 1: '74.03%', 2: '75.69%', 3: '76.23%', 4: '77.50%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 32/200, Train Loss: 1.474197, Train-Class-Acc: {0: '71.66%', 1: '68.36%', 2: '75.22%', 3: '77.97%', 4: '62.66%', 5: '84.98%'}\n",
      "Val Loss: 1.770859, Val Acc: 79.55%, Val-Class-Acc: {0: '67.93%', 1: '81.19%', 2: '75.00%', 3: '82.79%', 4: '77.50%', 5: '84.13%'}, LR: 0.001000\n",
      "Epoch 33/200, Train Loss: 1.659783, Train-Class-Acc: {0: '70.30%', 1: '65.97%', 2: '76.60%', 3: '75.10%', 4: '56.96%', 5: '83.18%'}\n",
      "Val Loss: 1.546449, Val Acc: 78.84%, Val-Class-Acc: {0: '77.17%', 1: '75.52%', 2: '77.08%', 3: '84.02%', 4: '77.50%', 5: '80.24%'}, LR: 0.001000\n",
      "Epoch 34/200, Train Loss: 1.632788, Train-Class-Acc: {0: '70.44%', 1: '68.21%', 2: '76.60%', 3: '78.38%', 4: '62.66%', 5: '83.48%'}\n",
      "Val Loss: 2.401011, Val Acc: 80.17%, Val-Class-Acc: {0: '78.26%', 1: '73.43%', 2: '75.00%', 3: '83.20%', 4: '77.50%', 5: '88.32%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_30.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_34.pth\n",
      "Epoch 35/200, Train Loss: 1.603636, Train-Class-Acc: {0: '70.98%', 1: '70.68%', 2: '76.43%', 3: '77.05%', 4: '68.99%', 5: '84.75%'}\n",
      "Val Loss: 1.752495, Val Acc: 79.78%, Val-Class-Acc: {0: '78.80%', 1: '71.94%', 2: '75.00%', 3: '79.92%', 4: '77.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 36/200, Train Loss: 1.513971, Train-Class-Acc: {0: '71.12%', 1: '69.41%', 2: '75.91%', 3: '76.33%', 4: '62.03%', 5: '84.53%'}\n",
      "Val Loss: 2.554452, Val Acc: 79.78%, Val-Class-Acc: {0: '65.22%', 1: '81.19%', 2: '68.75%', 3: '84.84%', 4: '77.50%', 5: '87.72%'}, LR: 0.001000\n",
      "Epoch 37/200, Train Loss: 1.432986, Train-Class-Acc: {0: '72.34%', 1: '69.04%', 2: '76.08%', 3: '78.89%', 4: '67.72%', 5: '84.45%'}\n",
      "Val Loss: 1.604938, Val Acc: 79.23%, Val-Class-Acc: {0: '72.28%', 1: '74.93%', 2: '77.08%', 3: '81.56%', 4: '77.50%', 5: '86.83%'}, LR: 0.001000\n",
      "Epoch 38/200, Train Loss: 1.403786, Train-Class-Acc: {0: '72.21%', 1: '70.38%', 2: '75.39%', 3: '76.74%', 4: '63.29%', 5: '84.16%'}\n",
      "Val Loss: 1.429392, Val Acc: 80.72%, Val-Class-Acc: {0: '67.93%', 1: '77.31%', 2: '82.64%', 3: '85.25%', 4: '77.50%', 5: '87.43%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_17.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_38.pth\n",
      "Epoch 39/200, Train Loss: 1.196863, Train-Class-Acc: {0: '70.57%', 1: '71.05%', 2: '77.47%', 3: '80.74%', 4: '67.09%', 5: '86.02%'}\n",
      "Val Loss: 1.504498, Val Acc: 80.41%, Val-Class-Acc: {0: '72.83%', 1: '77.01%', 2: '72.22%', 3: '86.48%', 4: '77.50%', 5: '87.43%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_18.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_39.pth\n",
      "Epoch 40/200, Train Loss: 1.280846, Train-Class-Acc: {0: '71.93%', 1: '70.98%', 2: '76.08%', 3: '79.41%', 4: '62.03%', 5: '85.95%'}\n",
      "Val Loss: 1.022023, Val Acc: 80.25%, Val-Class-Acc: {0: '67.93%', 1: '81.49%', 2: '74.31%', 3: '86.07%', 4: '77.50%', 5: '84.43%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_27.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_40.pth\n",
      "Epoch 41/200, Train Loss: 1.366407, Train-Class-Acc: {0: '70.57%', 1: '71.20%', 2: '78.51%', 3: '79.71%', 4: '65.82%', 5: '84.60%'}\n",
      "Val Loss: 1.794194, Val Acc: 79.94%, Val-Class-Acc: {0: '69.57%', 1: '77.01%', 2: '72.92%', 3: '82.38%', 4: '77.50%', 5: '90.12%'}, LR: 0.000900\n",
      "Epoch 42/200, Train Loss: 1.247919, Train-Class-Acc: {0: '74.11%', 1: '71.20%', 2: '75.74%', 3: '78.89%', 4: '60.76%', 5: '85.72%'}\n",
      "Val Loss: 1.341704, Val Acc: 80.09%, Val-Class-Acc: {0: '72.28%', 1: '77.01%', 2: '75.69%', 3: '86.07%', 4: '75.00%', 5: '85.63%'}, LR: 0.000900\n",
      "Epoch 43/200, Train Loss: 1.110185, Train-Class-Acc: {0: '72.21%', 1: '71.88%', 2: '77.64%', 3: '82.07%', 4: '69.62%', 5: '85.58%'}\n",
      "Val Loss: 1.696040, Val Acc: 79.86%, Val-Class-Acc: {0: '73.37%', 1: '79.40%', 2: '72.22%', 3: '78.69%', 4: '77.50%', 5: '88.32%'}, LR: 0.000900\n",
      "Epoch 44/200, Train Loss: 1.080796, Train-Class-Acc: {0: '72.07%', 1: '72.85%', 2: '78.16%', 3: '80.84%', 4: '65.19%', 5: '87.14%'}\n",
      "Val Loss: 1.568431, Val Acc: 80.48%, Val-Class-Acc: {0: '72.28%', 1: '75.82%', 2: '76.39%', 3: '87.30%', 4: '77.50%', 5: '86.83%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_34.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_44.pth\n",
      "Epoch 45/200, Train Loss: 1.277664, Train-Class-Acc: {0: '73.84%', 1: '73.90%', 2: '78.34%', 3: '81.15%', 4: '72.78%', 5: '85.95%'}\n",
      "Val Loss: 1.218170, Val Acc: 80.80%, Val-Class-Acc: {0: '79.35%', 1: '74.33%', 2: '75.00%', 3: '82.38%', 4: '70.00%', 5: '90.72%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_40.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_45.pth\n",
      "Epoch 46/200, Train Loss: 1.206182, Train-Class-Acc: {0: '72.34%', 1: '71.73%', 2: '78.86%', 3: '80.43%', 4: '69.62%', 5: '86.62%'}\n",
      "Val Loss: 2.478014, Val Acc: 80.33%, Val-Class-Acc: {0: '72.83%', 1: '75.22%', 2: '75.00%', 3: '81.56%', 4: '77.50%', 5: '91.32%'}, LR: 0.000900\n",
      "Epoch 47/200, Train Loss: 0.972410, Train-Class-Acc: {0: '74.25%', 1: '74.35%', 2: '78.86%', 3: '80.94%', 4: '70.25%', 5: '86.92%'}\n",
      "Val Loss: 2.491273, Val Acc: 80.17%, Val-Class-Acc: {0: '67.39%', 1: '78.21%', 2: '76.39%', 3: '84.43%', 4: '77.50%', 5: '88.02%'}, LR: 0.000900\n",
      "Epoch 48/200, Train Loss: 1.183560, Train-Class-Acc: {0: '73.57%', 1: '73.30%', 2: '80.42%', 3: '81.15%', 4: '66.46%', 5: '87.00%'}\n",
      "Val Loss: 4.043357, Val Acc: 79.55%, Val-Class-Acc: {0: '72.83%', 1: '77.31%', 2: '77.08%', 3: '83.20%', 4: '77.50%', 5: '84.13%'}, LR: 0.000900\n",
      "Epoch 49/200, Train Loss: 1.415864, Train-Class-Acc: {0: '72.34%', 1: '69.86%', 2: '78.51%', 3: '81.05%', 4: '72.15%', 5: '84.68%'}\n",
      "Val Loss: 1.116362, Val Acc: 80.09%, Val-Class-Acc: {0: '70.65%', 1: '80.00%', 2: '75.00%', 3: '83.61%', 4: '75.00%', 5: '85.63%'}, LR: 0.000900\n",
      "Epoch 50/200, Train Loss: 1.001692, Train-Class-Acc: {0: '72.75%', 1: '75.17%', 2: '81.11%', 3: '83.09%', 4: '73.42%', 5: '86.85%'}\n",
      "Val Loss: 0.994836, Val Acc: 80.95%, Val-Class-Acc: {0: '70.65%', 1: '76.72%', 2: '77.08%', 3: '86.48%', 4: '77.50%', 5: '88.92%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_21.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_50.pth\n",
      "Epoch 51/200, Train Loss: 0.942713, Train-Class-Acc: {0: '73.43%', 1: '72.25%', 2: '80.07%', 3: '81.66%', 4: '66.46%', 5: '87.37%'}\n",
      "Val Loss: 0.853220, Val Acc: 80.48%, Val-Class-Acc: {0: '69.02%', 1: '79.40%', 2: '77.08%', 3: '88.52%', 4: '75.00%', 5: '84.13%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_39.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_51.pth\n",
      "Epoch 52/200, Train Loss: 1.011529, Train-Class-Acc: {0: '71.39%', 1: '76.66%', 2: '78.16%', 3: '83.20%', 4: '71.52%', 5: '87.89%'}\n",
      "Val Loss: 2.090559, Val Acc: 80.56%, Val-Class-Acc: {0: '66.85%', 1: '80.00%', 2: '77.08%', 3: '82.79%', 4: '75.00%', 5: '89.22%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_44.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_52.pth\n",
      "Epoch 53/200, Train Loss: 0.934637, Train-Class-Acc: {0: '76.70%', 1: '75.09%', 2: '82.50%', 3: '81.25%', 4: '71.52%', 5: '87.44%'}\n",
      "Val Loss: 0.850080, Val Acc: 79.94%, Val-Class-Acc: {0: '68.48%', 1: '79.70%', 2: '74.31%', 3: '86.07%', 4: '72.50%', 5: '85.33%'}, LR: 0.000900\n",
      "Epoch 54/200, Train Loss: 0.944444, Train-Class-Acc: {0: '75.75%', 1: '75.32%', 2: '81.46%', 3: '82.68%', 4: '71.52%', 5: '87.59%'}\n",
      "Val Loss: 1.121430, Val Acc: 80.72%, Val-Class-Acc: {0: '77.72%', 1: '73.13%', 2: '79.86%', 3: '85.25%', 4: '77.50%', 5: '87.43%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_51.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_54.pth\n",
      "Epoch 55/200, Train Loss: 0.850741, Train-Class-Acc: {0: '74.39%', 1: '74.50%', 2: '79.20%', 3: '85.04%', 4: '72.78%', 5: '88.49%'}\n",
      "Val Loss: 1.575726, Val Acc: 80.41%, Val-Class-Acc: {0: '71.74%', 1: '78.51%', 2: '74.31%', 3: '88.11%', 4: '72.50%', 5: '85.03%'}, LR: 0.000900\n",
      "Epoch 56/200, Train Loss: 0.917697, Train-Class-Acc: {0: '75.48%', 1: '76.89%', 2: '80.59%', 3: '85.04%', 4: '69.62%', 5: '88.34%'}\n",
      "Val Loss: 0.724592, Val Acc: 80.72%, Val-Class-Acc: {0: '70.65%', 1: '79.40%', 2: '77.08%', 3: '87.70%', 4: '70.00%', 5: '85.33%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_52.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_56.pth\n",
      "Epoch 57/200, Train Loss: 1.051910, Train-Class-Acc: {0: '75.34%', 1: '73.90%', 2: '81.11%', 3: '82.48%', 4: '71.52%', 5: '87.29%'}\n",
      "Val Loss: 1.040465, Val Acc: 80.80%, Val-Class-Acc: {0: '60.33%', 1: '81.19%', 2: '75.69%', 3: '86.48%', 4: '75.00%', 5: '90.42%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_38.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_57.pth\n",
      "Epoch 58/200, Train Loss: 0.943531, Train-Class-Acc: {0: '76.98%', 1: '75.54%', 2: '81.46%', 3: '84.43%', 4: '75.95%', 5: '88.86%'}\n",
      "Val Loss: 1.321643, Val Acc: 79.94%, Val-Class-Acc: {0: '64.67%', 1: '80.00%', 2: '75.00%', 3: '86.89%', 4: '72.50%', 5: '86.23%'}, LR: 0.000900\n",
      "Epoch 59/200, Train Loss: 0.765529, Train-Class-Acc: {0: '77.79%', 1: '76.74%', 2: '82.84%', 3: '85.35%', 4: '70.89%', 5: '88.49%'}\n",
      "Val Loss: 1.274215, Val Acc: 80.95%, Val-Class-Acc: {0: '67.93%', 1: '77.61%', 2: '79.17%', 3: '85.25%', 4: '77.50%', 5: '89.52%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_54.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_59.pth\n",
      "Epoch 60/200, Train Loss: 1.276029, Train-Class-Acc: {0: '75.34%', 1: '75.02%', 2: '83.36%', 3: '83.50%', 4: '70.89%', 5: '86.47%'}\n",
      "Val Loss: 1.479908, Val Acc: 80.09%, Val-Class-Acc: {0: '78.80%', 1: '72.84%', 2: '75.00%', 3: '86.89%', 4: '77.50%', 5: '85.63%'}, LR: 0.000900\n",
      "Epoch 61/200, Train Loss: 0.950619, Train-Class-Acc: {0: '74.93%', 1: '76.14%', 2: '81.46%', 3: '83.30%', 4: '70.89%', 5: '88.04%'}\n",
      "Val Loss: 0.825836, Val Acc: 81.34%, Val-Class-Acc: {0: '72.83%', 1: '77.31%', 2: '79.17%', 3: '86.48%', 4: '75.00%', 5: '88.02%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_56.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_61.pth\n",
      "Epoch 62/200, Train Loss: 0.742790, Train-Class-Acc: {0: '76.16%', 1: '77.71%', 2: '82.50%', 3: '86.48%', 4: '71.52%', 5: '89.09%'}\n",
      "Val Loss: 1.019054, Val Acc: 80.56%, Val-Class-Acc: {0: '63.04%', 1: '81.19%', 2: '77.08%', 3: '87.70%', 4: '70.00%', 5: '87.13%'}, LR: 0.000900\n",
      "Epoch 63/200, Train Loss: 0.837994, Train-Class-Acc: {0: '78.20%', 1: '76.07%', 2: '82.50%', 3: '84.84%', 4: '74.68%', 5: '89.39%'}\n",
      "Val Loss: 1.783460, Val Acc: 80.41%, Val-Class-Acc: {0: '73.91%', 1: '75.52%', 2: '78.47%', 3: '85.66%', 4: '75.00%', 5: '86.53%'}, LR: 0.000900\n",
      "Epoch 64/200, Train Loss: 0.883653, Train-Class-Acc: {0: '76.02%', 1: '78.09%', 2: '84.06%', 3: '85.76%', 4: '78.48%', 5: '89.09%'}\n",
      "Val Loss: 1.526921, Val Acc: 80.87%, Val-Class-Acc: {0: '71.74%', 1: '78.21%', 2: '75.00%', 3: '86.89%', 4: '75.00%', 5: '87.43%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_45.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_64.pth\n",
      "Epoch 65/200, Train Loss: 0.822616, Train-Class-Acc: {0: '79.02%', 1: '77.71%', 2: '81.46%', 3: '85.86%', 4: '78.48%', 5: '88.71%'}\n",
      "Val Loss: 1.643164, Val Acc: 80.95%, Val-Class-Acc: {0: '70.65%', 1: '79.40%', 2: '76.39%', 3: '80.74%', 4: '75.00%', 5: '91.02%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_57.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_65.pth\n",
      "Epoch 66/200, Train Loss: 0.871756, Train-Class-Acc: {0: '74.39%', 1: '76.36%', 2: '81.63%', 3: '84.73%', 4: '75.95%', 5: '88.64%'}\n",
      "Val Loss: 0.847794, Val Acc: 79.94%, Val-Class-Acc: {0: '76.09%', 1: '76.12%', 2: '77.78%', 3: '85.25%', 4: '75.00%', 5: '83.53%'}, LR: 0.000900\n",
      "Epoch 67/200, Train Loss: 0.777081, Train-Class-Acc: {0: '78.34%', 1: '77.11%', 2: '83.19%', 3: '85.45%', 4: '78.48%', 5: '88.34%'}\n",
      "Val Loss: 0.934913, Val Acc: 80.95%, Val-Class-Acc: {0: '70.11%', 1: '78.51%', 2: '77.08%', 3: '85.25%', 4: '80.00%', 5: '88.02%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_64.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_67.pth\n",
      "Epoch 68/200, Train Loss: 0.671897, Train-Class-Acc: {0: '76.43%', 1: '80.18%', 2: '81.98%', 3: '85.35%', 4: '77.85%', 5: '89.91%'}\n",
      "Val Loss: 1.247248, Val Acc: 79.63%, Val-Class-Acc: {0: '67.93%', 1: '74.93%', 2: '69.44%', 3: '87.70%', 4: '75.00%', 5: '89.82%'}, LR: 0.000810\n",
      "Epoch 69/200, Train Loss: 0.731992, Train-Class-Acc: {0: '78.07%', 1: '76.22%', 2: '82.50%', 3: '87.19%', 4: '79.11%', 5: '90.13%'}\n",
      "Val Loss: 0.998026, Val Acc: 80.48%, Val-Class-Acc: {0: '67.93%', 1: '77.91%', 2: '75.00%', 3: '86.89%', 4: '77.50%', 5: '88.02%'}, LR: 0.000810\n",
      "Epoch 70/200, Train Loss: 0.718612, Train-Class-Acc: {0: '78.75%', 1: '78.76%', 2: '82.84%', 3: '85.96%', 4: '77.85%', 5: '89.31%'}\n",
      "Val Loss: 1.918399, Val Acc: 79.94%, Val-Class-Acc: {0: '65.76%', 1: '78.51%', 2: '72.92%', 3: '84.84%', 4: '75.00%', 5: '89.22%'}, LR: 0.000810\n",
      "Epoch 71/200, Train Loss: 0.888328, Train-Class-Acc: {0: '78.61%', 1: '78.61%', 2: '84.92%', 3: '86.48%', 4: '78.48%', 5: '89.09%'}\n",
      "Val Loss: 1.711854, Val Acc: 80.02%, Val-Class-Acc: {0: '63.59%', 1: '82.39%', 2: '75.69%', 3: '86.48%', 4: '75.00%', 5: '84.43%'}, LR: 0.000810\n",
      "Epoch 72/200, Train Loss: 0.777230, Train-Class-Acc: {0: '77.38%', 1: '79.13%', 2: '83.54%', 3: '88.22%', 4: '75.32%', 5: '90.06%'}\n",
      "Val Loss: 1.141076, Val Acc: 80.41%, Val-Class-Acc: {0: '71.74%', 1: '76.12%', 2: '75.00%', 3: '85.25%', 4: '75.00%', 5: '88.92%'}, LR: 0.000810\n",
      "Epoch 73/200, Train Loss: 0.759500, Train-Class-Acc: {0: '78.47%', 1: '78.91%', 2: '83.19%', 3: '84.94%', 4: '75.32%', 5: '89.91%'}\n",
      "Val Loss: 2.291307, Val Acc: 80.56%, Val-Class-Acc: {0: '73.91%', 1: '74.33%', 2: '73.61%', 3: '82.38%', 4: '82.50%', 5: '91.92%'}, LR: 0.000810\n",
      "Epoch 74/200, Train Loss: 0.784535, Train-Class-Acc: {0: '77.38%', 1: '76.51%', 2: '84.40%', 3: '83.91%', 4: '81.01%', 5: '89.31%'}\n",
      "Val Loss: 1.564376, Val Acc: 80.95%, Val-Class-Acc: {0: '72.83%', 1: '76.42%', 2: '77.08%', 3: '81.97%', 4: '80.00%', 5: '91.02%'}, LR: 0.000810\n",
      "Epoch 75/200, Train Loss: 0.728959, Train-Class-Acc: {0: '79.70%', 1: '79.51%', 2: '84.92%', 3: '86.78%', 4: '75.32%', 5: '89.31%'}\n",
      "Val Loss: 1.485239, Val Acc: 80.41%, Val-Class-Acc: {0: '69.57%', 1: '77.91%', 2: '75.00%', 3: '84.84%', 4: '75.00%', 5: '88.62%'}, LR: 0.000810\n",
      "Epoch 76/200, Train Loss: 0.711299, Train-Class-Acc: {0: '79.29%', 1: '80.63%', 2: '83.88%', 3: '86.68%', 4: '81.01%', 5: '89.69%'}\n",
      "Val Loss: 0.820272, Val Acc: 80.02%, Val-Class-Acc: {0: '64.67%', 1: '78.81%', 2: '75.00%', 3: '86.89%', 4: '77.50%', 5: '87.13%'}, LR: 0.000810\n",
      "Epoch 77/200, Train Loss: 0.704001, Train-Class-Acc: {0: '80.93%', 1: '81.30%', 2: '84.23%', 3: '86.48%', 4: '80.38%', 5: '90.96%'}\n",
      "Val Loss: 0.966655, Val Acc: 80.17%, Val-Class-Acc: {0: '68.48%', 1: '77.31%', 2: '74.31%', 3: '86.89%', 4: '77.50%', 5: '87.43%'}, LR: 0.000810\n",
      "Epoch 78/200, Train Loss: 0.552841, Train-Class-Acc: {0: '81.34%', 1: '81.82%', 2: '83.36%', 3: '88.63%', 4: '80.38%', 5: '91.18%'}\n",
      "Val Loss: 0.901314, Val Acc: 81.11%, Val-Class-Acc: {0: '70.65%', 1: '79.40%', 2: '77.78%', 3: '86.07%', 4: '75.00%', 5: '87.13%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_50.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_78.pth\n",
      "Epoch 79/200, Train Loss: 0.694664, Train-Class-Acc: {0: '81.34%', 1: '80.40%', 2: '84.58%', 3: '87.91%', 4: '80.38%', 5: '88.94%'}\n",
      "Val Loss: 1.252962, Val Acc: 79.70%, Val-Class-Acc: {0: '60.87%', 1: '80.90%', 2: '74.31%', 3: '84.84%', 4: '77.50%', 5: '87.72%'}, LR: 0.000729\n",
      "Epoch 80/200, Train Loss: 0.692082, Train-Class-Acc: {0: '79.29%', 1: '80.78%', 2: '85.10%', 3: '87.70%', 4: '81.65%', 5: '89.99%'}\n",
      "Val Loss: 0.861014, Val Acc: 80.87%, Val-Class-Acc: {0: '72.28%', 1: '78.51%', 2: '74.31%', 3: '86.89%', 4: '75.00%', 5: '87.13%'}, LR: 0.000729\n",
      "Epoch 81/200, Train Loss: 0.659592, Train-Class-Acc: {0: '80.11%', 1: '81.08%', 2: '85.27%', 3: '89.45%', 4: '82.91%', 5: '90.81%'}\n",
      "Val Loss: 1.170032, Val Acc: 80.95%, Val-Class-Acc: {0: '66.85%', 1: '80.00%', 2: '77.78%', 3: '84.84%', 4: '75.00%', 5: '88.92%'}, LR: 0.000729\n",
      "Epoch 82/200, Train Loss: 0.597200, Train-Class-Acc: {0: '81.74%', 1: '80.48%', 2: '85.62%', 3: '88.32%', 4: '77.85%', 5: '90.73%'}\n",
      "Val Loss: 0.997494, Val Acc: 80.56%, Val-Class-Acc: {0: '72.28%', 1: '77.61%', 2: '77.78%', 3: '83.61%', 4: '77.50%', 5: '87.43%'}, LR: 0.000729\n",
      "Epoch 83/200, Train Loss: 0.503714, Train-Class-Acc: {0: '81.74%', 1: '81.90%', 2: '88.04%', 3: '88.22%', 4: '82.91%', 5: '90.13%'}\n",
      "Val Loss: 1.293122, Val Acc: 80.09%, Val-Class-Acc: {0: '62.50%', 1: '78.51%', 2: '76.39%', 3: '86.89%', 4: '77.50%', 5: '88.32%'}, LR: 0.000729\n",
      "Epoch 84/200, Train Loss: 0.578031, Train-Class-Acc: {0: '82.15%', 1: '81.82%', 2: '85.10%', 3: '88.22%', 4: '80.38%', 5: '90.96%'}\n",
      "Val Loss: 1.286256, Val Acc: 80.87%, Val-Class-Acc: {0: '71.74%', 1: '77.01%', 2: '75.00%', 3: '87.70%', 4: '77.50%', 5: '87.72%'}, LR: 0.000729\n",
      "Epoch 85/200, Train Loss: 0.555434, Train-Class-Acc: {0: '82.15%', 1: '81.53%', 2: '87.18%', 3: '88.52%', 4: '82.91%', 5: '91.33%'}\n",
      "Val Loss: 1.088595, Val Acc: 80.25%, Val-Class-Acc: {0: '76.63%', 1: '73.73%', 2: '75.69%', 3: '88.52%', 4: '77.50%', 5: '85.03%'}, LR: 0.000729\n",
      "Epoch 86/200, Train Loss: 0.649067, Train-Class-Acc: {0: '82.43%', 1: '82.20%', 2: '85.79%', 3: '87.81%', 4: '81.01%', 5: '90.73%'}\n",
      "Val Loss: 1.305884, Val Acc: 81.26%, Val-Class-Acc: {0: '75.00%', 1: '74.93%', 2: '77.78%', 3: '87.30%', 4: '77.50%', 5: '88.62%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_59.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_86.pth\n",
      "Epoch 87/200, Train Loss: 0.682151, Train-Class-Acc: {0: '82.97%', 1: '83.40%', 2: '85.10%', 3: '88.22%', 4: '78.48%', 5: '90.36%'}\n",
      "Val Loss: 1.689512, Val Acc: 80.48%, Val-Class-Acc: {0: '72.28%', 1: '76.42%', 2: '75.00%', 3: '82.38%', 4: '82.50%', 5: '89.82%'}, LR: 0.000729\n",
      "Epoch 88/200, Train Loss: 0.530575, Train-Class-Acc: {0: '80.25%', 1: '81.38%', 2: '86.14%', 3: '88.01%', 4: '79.11%', 5: '90.51%'}\n",
      "Val Loss: 1.416820, Val Acc: 81.26%, Val-Class-Acc: {0: '73.91%', 1: '76.72%', 2: '75.69%', 3: '84.84%', 4: '80.00%', 5: '89.82%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_65.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_88.pth\n",
      "Epoch 89/200, Train Loss: 0.664173, Train-Class-Acc: {0: '82.29%', 1: '81.60%', 2: '86.48%', 3: '89.14%', 4: '82.91%', 5: '91.33%'}\n",
      "Val Loss: 0.824264, Val Acc: 80.02%, Val-Class-Acc: {0: '66.30%', 1: '80.30%', 2: '79.86%', 3: '88.52%', 4: '75.00%', 5: '81.74%'}, LR: 0.000729\n",
      "Epoch 90/200, Train Loss: 0.517155, Train-Class-Acc: {0: '81.20%', 1: '82.95%', 2: '87.35%', 3: '90.06%', 4: '84.81%', 5: '90.81%'}\n",
      "Val Loss: 0.997059, Val Acc: 81.19%, Val-Class-Acc: {0: '74.46%', 1: '74.93%', 2: '78.47%', 3: '86.07%', 4: '77.50%', 5: '89.22%'}, LR: 0.000656\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_67.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_90.pth\n",
      "Epoch 91/200, Train Loss: 0.520653, Train-Class-Acc: {0: '80.52%', 1: '81.68%', 2: '86.31%', 3: '89.65%', 4: '81.65%', 5: '91.70%'}\n",
      "Val Loss: 1.122280, Val Acc: 80.87%, Val-Class-Acc: {0: '76.63%', 1: '76.72%', 2: '80.56%', 3: '84.43%', 4: '77.50%', 5: '85.33%'}, LR: 0.000656\n",
      "Epoch 92/200, Train Loss: 0.526292, Train-Class-Acc: {0: '82.29%', 1: '84.29%', 2: '87.52%', 3: '89.24%', 4: '83.54%', 5: '90.58%'}\n",
      "Val Loss: 1.099571, Val Acc: 80.33%, Val-Class-Acc: {0: '67.39%', 1: '76.72%', 2: '78.47%', 3: '86.48%', 4: '75.00%', 5: '88.02%'}, LR: 0.000656\n",
      "Epoch 93/200, Train Loss: 0.733034, Train-Class-Acc: {0: '82.29%', 1: '81.75%', 2: '86.66%', 3: '88.83%', 4: '82.91%', 5: '90.96%'}\n",
      "Val Loss: 1.016396, Val Acc: 79.47%, Val-Class-Acc: {0: '67.39%', 1: '77.91%', 2: '75.69%', 3: '88.93%', 4: '80.00%', 5: '82.34%'}, LR: 0.000656\n",
      "Epoch 94/200, Train Loss: 0.542277, Train-Class-Acc: {0: '82.83%', 1: '81.68%', 2: '88.21%', 3: '89.55%', 4: '78.48%', 5: '91.63%'}\n",
      "Val Loss: 1.088460, Val Acc: 80.17%, Val-Class-Acc: {0: '66.85%', 1: '78.21%', 2: '75.00%', 3: '85.25%', 4: '75.00%', 5: '88.62%'}, LR: 0.000656\n",
      "Epoch 95/200, Train Loss: 0.614086, Train-Class-Acc: {0: '83.11%', 1: '82.05%', 2: '88.04%', 3: '88.83%', 4: '84.18%', 5: '91.03%'}\n",
      "Val Loss: 0.986495, Val Acc: 80.87%, Val-Class-Acc: {0: '75.00%', 1: '76.12%', 2: '75.69%', 3: '86.07%', 4: '80.00%', 5: '87.43%'}, LR: 0.000656\n",
      "Epoch 96/200, Train Loss: 0.487613, Train-Class-Acc: {0: '83.92%', 1: '81.75%', 2: '87.18%', 3: '89.65%', 4: '81.01%', 5: '92.00%'}\n",
      "Val Loss: 1.445839, Val Acc: 81.50%, Val-Class-Acc: {0: '71.74%', 1: '77.61%', 2: '75.00%', 3: '87.70%', 4: '77.50%', 5: '89.52%'}, LR: 0.000656\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_78.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_96.pth\n",
      "Epoch 97/200, Train Loss: 0.463280, Train-Class-Acc: {0: '82.83%', 1: '83.62%', 2: '87.69%', 3: '88.83%', 4: '84.18%', 5: '91.26%'}\n",
      "Val Loss: 1.582970, Val Acc: 81.03%, Val-Class-Acc: {0: '69.02%', 1: '77.01%', 2: '78.47%', 3: '86.48%', 4: '77.50%', 5: '89.22%'}, LR: 0.000656\n",
      "Epoch 98/200, Train Loss: 0.445705, Train-Class-Acc: {0: '85.01%', 1: '83.77%', 2: '87.35%', 3: '89.75%', 4: '83.54%', 5: '91.48%'}\n",
      "Val Loss: 1.314374, Val Acc: 81.34%, Val-Class-Acc: {0: '73.37%', 1: '76.42%', 2: '79.17%', 3: '84.43%', 4: '77.50%', 5: '89.82%'}, LR: 0.000656\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_90.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_98.pth\n",
      "Epoch 99/200, Train Loss: 0.481234, Train-Class-Acc: {0: '84.33%', 1: '82.95%', 2: '85.79%', 3: '88.11%', 4: '82.91%', 5: '91.48%'}\n",
      "Val Loss: 1.126474, Val Acc: 80.95%, Val-Class-Acc: {0: '71.20%', 1: '78.81%', 2: '79.17%', 3: '84.02%', 4: '75.00%', 5: '87.72%'}, LR: 0.000656\n",
      "Epoch 100/200, Train Loss: 0.469369, Train-Class-Acc: {0: '81.61%', 1: '82.27%', 2: '87.52%', 3: '90.16%', 4: '83.54%', 5: '91.70%'}\n",
      "Val Loss: 0.921033, Val Acc: 81.03%, Val-Class-Acc: {0: '76.09%', 1: '76.72%', 2: '77.78%', 3: '88.11%', 4: '75.00%', 5: '85.03%'}, LR: 0.000656\n",
      "Epoch 101/200, Train Loss: 0.437795, Train-Class-Acc: {0: '85.01%', 1: '83.10%', 2: '87.35%', 3: '90.98%', 4: '86.08%', 5: '91.48%'}\n",
      "Val Loss: 0.998006, Val Acc: 79.55%, Val-Class-Acc: {0: '60.87%', 1: '77.31%', 2: '77.78%', 3: '87.70%', 4: '77.50%', 5: '87.13%'}, LR: 0.000590\n",
      "Epoch 102/200, Train Loss: 0.425333, Train-Class-Acc: {0: '84.06%', 1: '83.55%', 2: '89.08%', 3: '89.65%', 4: '80.38%', 5: '92.30%'}\n",
      "Val Loss: 1.240791, Val Acc: 80.72%, Val-Class-Acc: {0: '70.11%', 1: '78.21%', 2: '76.39%', 3: '86.89%', 4: '80.00%', 5: '86.53%'}, LR: 0.000590\n",
      "Epoch 103/200, Train Loss: 0.524733, Train-Class-Acc: {0: '83.79%', 1: '85.19%', 2: '89.77%', 3: '90.16%', 4: '84.18%', 5: '92.30%'}\n",
      "Val Loss: 1.181572, Val Acc: 81.42%, Val-Class-Acc: {0: '78.26%', 1: '74.63%', 2: '77.08%', 3: '86.07%', 4: '82.50%', 5: '88.32%'}, LR: 0.000590\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_86.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_103.pth\n",
      "Epoch 104/200, Train Loss: 0.615000, Train-Class-Acc: {0: '82.56%', 1: '83.02%', 2: '87.00%', 3: '89.65%', 4: '83.54%', 5: '90.96%'}\n",
      "Val Loss: 1.188956, Val Acc: 81.11%, Val-Class-Acc: {0: '67.93%', 1: '78.21%', 2: '76.39%', 3: '87.70%', 4: '82.50%', 5: '88.32%'}, LR: 0.000590\n",
      "Epoch 105/200, Train Loss: 0.666742, Train-Class-Acc: {0: '81.61%', 1: '83.10%', 2: '87.00%', 3: '89.65%', 4: '86.71%', 5: '91.26%'}\n",
      "Val Loss: 1.538805, Val Acc: 79.70%, Val-Class-Acc: {0: '64.13%', 1: '74.33%', 2: '73.61%', 3: '90.16%', 4: '75.00%', 5: '89.22%'}, LR: 0.000590\n",
      "Epoch 106/200, Train Loss: 0.438429, Train-Class-Acc: {0: '86.24%', 1: '83.32%', 2: '89.08%', 3: '90.57%', 4: '83.54%', 5: '92.00%'}\n",
      "Val Loss: 2.120835, Val Acc: 81.34%, Val-Class-Acc: {0: '76.63%', 1: '77.31%', 2: '76.39%', 3: '83.20%', 4: '82.50%', 5: '88.62%'}, LR: 0.000590\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_88.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_106.pth\n",
      "Epoch 107/200, Train Loss: 0.477596, Train-Class-Acc: {0: '84.20%', 1: '84.59%', 2: '86.83%', 3: '92.01%', 4: '82.91%', 5: '92.90%'}\n",
      "Val Loss: 1.218661, Val Acc: 81.34%, Val-Class-Acc: {0: '72.83%', 1: '77.31%', 2: '77.78%', 3: '85.66%', 4: '82.50%', 5: '88.32%'}, LR: 0.000590\n",
      "Epoch 108/200, Train Loss: 0.556345, Train-Class-Acc: {0: '86.24%', 1: '85.34%', 2: '88.56%', 3: '90.16%', 4: '84.18%', 5: '91.55%'}\n",
      "Val Loss: 0.879408, Val Acc: 81.03%, Val-Class-Acc: {0: '68.48%', 1: '78.21%', 2: '74.31%', 3: '86.07%', 4: '80.00%', 5: '90.12%'}, LR: 0.000590\n",
      "Epoch 109/200, Train Loss: 0.468213, Train-Class-Acc: {0: '83.92%', 1: '85.19%', 2: '89.25%', 3: '90.78%', 4: '86.71%', 5: '91.93%'}\n",
      "Val Loss: 0.952754, Val Acc: 80.09%, Val-Class-Acc: {0: '65.22%', 1: '78.51%', 2: '80.56%', 3: '87.70%', 4: '77.50%', 5: '84.43%'}, LR: 0.000590\n",
      "Epoch 110/200, Train Loss: 0.637741, Train-Class-Acc: {0: '82.70%', 1: '82.87%', 2: '88.39%', 3: '89.04%', 4: '85.44%', 5: '90.96%'}\n",
      "Val Loss: 1.090547, Val Acc: 80.72%, Val-Class-Acc: {0: '74.46%', 1: '74.93%', 2: '76.39%', 3: '83.61%', 4: '80.00%', 5: '89.82%'}, LR: 0.000590\n",
      "Epoch 111/200, Train Loss: 0.398704, Train-Class-Acc: {0: '86.10%', 1: '85.34%', 2: '88.39%', 3: '90.27%', 4: '85.44%', 5: '92.60%'}\n",
      "Val Loss: 1.398981, Val Acc: 80.48%, Val-Class-Acc: {0: '69.57%', 1: '74.93%', 2: '72.92%', 3: '86.07%', 4: '77.50%', 5: '91.62%'}, LR: 0.000590\n",
      "Epoch 112/200, Train Loss: 0.343609, Train-Class-Acc: {0: '87.33%', 1: '85.42%', 2: '89.25%', 3: '92.01%', 4: '83.54%', 5: '93.50%'}\n",
      "Val Loss: 1.804203, Val Acc: 80.87%, Val-Class-Acc: {0: '71.20%', 1: '76.12%', 2: '75.69%', 3: '85.25%', 4: '77.50%', 5: '90.42%'}, LR: 0.000531\n",
      "Epoch 113/200, Train Loss: 0.407359, Train-Class-Acc: {0: '85.29%', 1: '84.59%', 2: '88.56%', 3: '92.52%', 4: '87.97%', 5: '93.72%'}\n",
      "Val Loss: 1.185577, Val Acc: 79.08%, Val-Class-Acc: {0: '59.78%', 1: '78.51%', 2: '79.17%', 3: '87.70%', 4: '77.50%', 5: '84.13%'}, LR: 0.000531\n",
      "Epoch 114/200, Train Loss: 0.434299, Train-Class-Acc: {0: '86.92%', 1: '85.34%', 2: '89.08%', 3: '89.45%', 4: '88.61%', 5: '92.23%'}\n",
      "Val Loss: 1.729454, Val Acc: 80.80%, Val-Class-Acc: {0: '71.74%', 1: '77.31%', 2: '75.00%', 3: '84.84%', 4: '80.00%', 5: '88.92%'}, LR: 0.000531\n",
      "Epoch 115/200, Train Loss: 0.359101, Train-Class-Acc: {0: '85.97%', 1: '85.86%', 2: '89.95%', 3: '93.34%', 4: '86.08%', 5: '93.42%'}\n",
      "Val Loss: 1.474810, Val Acc: 80.48%, Val-Class-Acc: {0: '69.02%', 1: '76.72%', 2: '77.78%', 3: '85.66%', 4: '77.50%', 5: '88.32%'}, LR: 0.000531\n",
      "Epoch 116/200, Train Loss: 0.324385, Train-Class-Acc: {0: '85.42%', 1: '85.56%', 2: '90.64%', 3: '92.73%', 4: '84.81%', 5: '93.72%'}\n",
      "Val Loss: 1.594027, Val Acc: 80.41%, Val-Class-Acc: {0: '69.02%', 1: '76.12%', 2: '75.00%', 3: '86.89%', 4: '80.00%', 5: '88.62%'}, LR: 0.000531\n",
      "Epoch 117/200, Train Loss: 0.332835, Train-Class-Acc: {0: '86.38%', 1: '85.71%', 2: '91.16%', 3: '93.55%', 4: '86.71%', 5: '93.50%'}\n",
      "Val Loss: 2.049688, Val Acc: 81.19%, Val-Class-Acc: {0: '72.28%', 1: '76.42%', 2: '76.39%', 3: '86.48%', 4: '77.50%', 5: '89.52%'}, LR: 0.000531\n",
      "Epoch 118/200, Train Loss: 0.508918, Train-Class-Acc: {0: '87.74%', 1: '86.01%', 2: '88.73%', 3: '92.21%', 4: '86.71%', 5: '92.97%'}\n",
      "Val Loss: 1.502832, Val Acc: 80.95%, Val-Class-Acc: {0: '70.65%', 1: '77.61%', 2: '77.78%', 3: '84.84%', 4: '77.50%', 5: '88.92%'}, LR: 0.000531\n",
      "Epoch 119/200, Train Loss: 0.347164, Train-Class-Acc: {0: '86.51%', 1: '87.21%', 2: '91.33%', 3: '92.73%', 4: '86.08%', 5: '93.27%'}\n",
      "Val Loss: 2.391956, Val Acc: 81.11%, Val-Class-Acc: {0: '76.09%', 1: '75.22%', 2: '77.08%', 3: '84.02%', 4: '77.50%', 5: '89.82%'}, LR: 0.000531\n",
      "Epoch 120/200, Train Loss: 0.441430, Train-Class-Acc: {0: '87.74%', 1: '86.39%', 2: '89.25%', 3: '91.70%', 4: '87.97%', 5: '92.97%'}\n",
      "Val Loss: 2.446079, Val Acc: 80.41%, Val-Class-Acc: {0: '68.48%', 1: '76.72%', 2: '75.00%', 3: '82.79%', 4: '77.50%', 5: '91.62%'}, LR: 0.000531\n",
      "Epoch 121/200, Train Loss: 0.343825, Train-Class-Acc: {0: '87.47%', 1: '87.14%', 2: '90.64%', 3: '92.32%', 4: '86.08%', 5: '93.35%'}\n",
      "Val Loss: 1.237298, Val Acc: 81.03%, Val-Class-Acc: {0: '73.91%', 1: '73.43%', 2: '80.56%', 3: '85.66%', 4: '80.00%', 5: '89.52%'}, LR: 0.000531\n",
      "Epoch 122/200, Train Loss: 0.317197, Train-Class-Acc: {0: '87.19%', 1: '87.14%', 2: '90.12%', 3: '92.93%', 4: '87.34%', 5: '92.75%'}\n",
      "Val Loss: 0.947783, Val Acc: 80.41%, Val-Class-Acc: {0: '71.20%', 1: '74.63%', 2: '79.17%', 3: '86.07%', 4: '77.50%', 5: '88.02%'}, LR: 0.000531\n",
      "Epoch 123/200, Train Loss: 0.359282, Train-Class-Acc: {0: '88.01%', 1: '87.28%', 2: '89.95%', 3: '91.60%', 4: '84.81%', 5: '93.80%'}\n",
      "Val Loss: 0.934594, Val Acc: 80.41%, Val-Class-Acc: {0: '69.57%', 1: '76.12%', 2: '76.39%', 3: '84.43%', 4: '80.00%', 5: '89.52%'}, LR: 0.000478\n",
      "Epoch 124/200, Train Loss: 0.405699, Train-Class-Acc: {0: '85.56%', 1: '86.01%', 2: '88.39%', 3: '92.42%', 4: '88.61%', 5: '93.27%'}\n",
      "Val Loss: 1.318430, Val Acc: 81.19%, Val-Class-Acc: {0: '75.54%', 1: '75.82%', 2: '75.00%', 3: '86.48%', 4: '77.50%', 5: '88.92%'}, LR: 0.000478\n",
      "Epoch 125/200, Train Loss: 0.405983, Train-Class-Acc: {0: '87.47%', 1: '86.69%', 2: '88.73%', 3: '92.11%', 4: '83.54%', 5: '93.20%'}\n",
      "Val Loss: 1.002324, Val Acc: 80.64%, Val-Class-Acc: {0: '69.02%', 1: '76.72%', 2: '78.47%', 3: '86.89%', 4: '77.50%', 5: '87.72%'}, LR: 0.000478\n",
      "Epoch 126/200, Train Loss: 0.361961, Train-Class-Acc: {0: '88.15%', 1: '86.61%', 2: '88.91%', 3: '92.21%', 4: '87.34%', 5: '92.83%'}\n",
      "Val Loss: 1.071148, Val Acc: 80.80%, Val-Class-Acc: {0: '72.83%', 1: '75.52%', 2: '75.69%', 3: '86.07%', 4: '77.50%', 5: '89.22%'}, LR: 0.000478\n",
      "Epoch 127/200, Train Loss: 0.288006, Train-Class-Acc: {0: '88.01%', 1: '88.41%', 2: '89.95%', 3: '92.93%', 4: '87.34%', 5: '94.10%'}\n",
      "Val Loss: 1.080386, Val Acc: 81.11%, Val-Class-Acc: {0: '72.83%', 1: '76.12%', 2: '79.17%', 3: '85.25%', 4: '77.50%', 5: '88.92%'}, LR: 0.000478\n",
      "Epoch 128/200, Train Loss: 0.344207, Train-Class-Acc: {0: '88.96%', 1: '86.54%', 2: '92.03%', 3: '92.21%', 4: '86.08%', 5: '94.02%'}\n",
      "Val Loss: 1.991229, Val Acc: 80.56%, Val-Class-Acc: {0: '70.11%', 1: '74.63%', 2: '78.47%', 3: '83.20%', 4: '80.00%', 5: '91.32%'}, LR: 0.000478\n",
      "Epoch 129/200, Train Loss: 0.461287, Train-Class-Acc: {0: '87.47%', 1: '85.64%', 2: '89.60%', 3: '91.50%', 4: '89.24%', 5: '93.05%'}\n",
      "Val Loss: 1.395895, Val Acc: 80.87%, Val-Class-Acc: {0: '76.09%', 1: '76.12%', 2: '77.78%', 3: '83.20%', 4: '80.00%', 5: '88.02%'}, LR: 0.000478\n",
      "Epoch 130/200, Train Loss: 0.326986, Train-Class-Acc: {0: '88.42%', 1: '87.88%', 2: '89.95%', 3: '92.11%', 4: '86.08%', 5: '93.50%'}\n",
      "Val Loss: 1.439698, Val Acc: 80.41%, Val-Class-Acc: {0: '75.54%', 1: '74.93%', 2: '75.00%', 3: '86.48%', 4: '75.00%', 5: '87.13%'}, LR: 0.000478\n",
      "Epoch 131/200, Train Loss: 0.314085, Train-Class-Acc: {0: '88.83%', 1: '88.03%', 2: '90.81%', 3: '93.34%', 4: '89.24%', 5: '94.10%'}\n",
      "Val Loss: 0.954258, Val Acc: 80.87%, Val-Class-Acc: {0: '73.91%', 1: '76.12%', 2: '79.17%', 3: '85.66%', 4: '75.00%', 5: '87.43%'}, LR: 0.000478\n",
      "Epoch 132/200, Train Loss: 0.328540, Train-Class-Acc: {0: '87.60%', 1: '88.93%', 2: '91.33%', 3: '92.93%', 4: '85.44%', 5: '93.65%'}\n",
      "Val Loss: 1.400549, Val Acc: 80.95%, Val-Class-Acc: {0: '78.26%', 1: '72.54%', 2: '80.56%', 3: '84.43%', 4: '77.50%', 5: '88.92%'}, LR: 0.000478\n",
      "Epoch 133/200, Train Loss: 0.434125, Train-Class-Acc: {0: '90.19%', 1: '88.48%', 2: '90.12%', 3: '92.11%', 4: '85.44%', 5: '93.12%'}\n",
      "Val Loss: 1.086632, Val Acc: 80.33%, Val-Class-Acc: {0: '67.93%', 1: '77.01%', 2: '78.47%', 3: '86.89%', 4: '77.50%', 5: '86.83%'}, LR: 0.000478\n",
      "Epoch 134/200, Train Loss: 0.320661, Train-Class-Acc: {0: '88.69%', 1: '88.33%', 2: '91.85%', 3: '92.52%', 4: '89.87%', 5: '94.54%'}\n",
      "Val Loss: 1.739227, Val Acc: 81.11%, Val-Class-Acc: {0: '73.91%', 1: '74.93%', 2: '81.94%', 3: '84.84%', 4: '80.00%', 5: '88.32%'}, LR: 0.000430\n",
      "Epoch 135/200, Train Loss: 0.283584, Train-Class-Acc: {0: '89.78%', 1: '87.96%', 2: '91.51%', 3: '91.70%', 4: '86.08%', 5: '94.17%'}\n",
      "Val Loss: 1.543672, Val Acc: 80.80%, Val-Class-Acc: {0: '71.20%', 1: '77.31%', 2: '74.31%', 3: '84.84%', 4: '80.00%', 5: '89.52%'}, LR: 0.000430\n",
      "Epoch 136/200, Train Loss: 0.302009, Train-Class-Acc: {0: '89.65%', 1: '89.01%', 2: '92.55%', 3: '93.44%', 4: '87.34%', 5: '94.17%'}\n",
      "Val Loss: 2.092026, Val Acc: 80.25%, Val-Class-Acc: {0: '69.57%', 1: '76.42%', 2: '77.08%', 3: '81.97%', 4: '77.50%', 5: '90.42%'}, LR: 0.000430\n",
      "Epoch 137/200, Train Loss: 0.265446, Train-Class-Acc: {0: '90.46%', 1: '88.33%', 2: '88.91%', 3: '92.52%', 4: '88.61%', 5: '94.17%'}\n",
      "Val Loss: 1.411706, Val Acc: 80.64%, Val-Class-Acc: {0: '70.65%', 1: '76.72%', 2: '77.08%', 3: '84.84%', 4: '77.50%', 5: '88.92%'}, LR: 0.000430\n",
      "Epoch 138/200, Train Loss: 0.362370, Train-Class-Acc: {0: '90.05%', 1: '88.33%', 2: '90.81%', 3: '93.24%', 4: '87.34%', 5: '93.65%'}\n",
      "Val Loss: 2.029041, Val Acc: 80.72%, Val-Class-Acc: {0: '66.85%', 1: '77.31%', 2: '79.86%', 3: '84.43%', 4: '80.00%', 5: '89.52%'}, LR: 0.000430\n",
      "Epoch 139/200, Train Loss: 0.290803, Train-Class-Acc: {0: '89.51%', 1: '88.56%', 2: '91.51%', 3: '91.91%', 4: '89.24%', 5: '94.62%'}\n",
      "Val Loss: 1.688213, Val Acc: 80.56%, Val-Class-Acc: {0: '65.22%', 1: '76.72%', 2: '79.17%', 3: '85.66%', 4: '80.00%', 5: '89.82%'}, LR: 0.000430\n",
      "Epoch 140/200, Train Loss: 0.289462, Train-Class-Acc: {0: '90.05%', 1: '88.93%', 2: '92.20%', 3: '93.95%', 4: '89.24%', 5: '94.47%'}\n",
      "Val Loss: 1.485272, Val Acc: 80.17%, Val-Class-Acc: {0: '69.57%', 1: '74.93%', 2: '79.17%', 3: '85.66%', 4: '80.00%', 5: '87.72%'}, LR: 0.000430\n",
      "Epoch 141/200, Train Loss: 0.313368, Train-Class-Acc: {0: '89.37%', 1: '89.15%', 2: '90.81%', 3: '92.52%', 4: '88.61%', 5: '93.95%'}\n",
      "Val Loss: 1.604868, Val Acc: 81.03%, Val-Class-Acc: {0: '74.46%', 1: '73.13%', 2: '83.33%', 3: '85.25%', 4: '77.50%', 5: '88.92%'}, LR: 0.000430\n",
      "Epoch 142/200, Train Loss: 0.291147, Train-Class-Acc: {0: '91.28%', 1: '89.30%', 2: '90.64%', 3: '92.21%', 4: '87.34%', 5: '94.25%'}\n",
      "Val Loss: 1.225694, Val Acc: 80.64%, Val-Class-Acc: {0: '68.48%', 1: '76.12%', 2: '78.47%', 3: '86.89%', 4: '77.50%', 5: '88.62%'}, LR: 0.000430\n",
      "Epoch 143/200, Train Loss: 0.254365, Train-Class-Acc: {0: '91.55%', 1: '89.30%', 2: '93.07%', 3: '93.75%', 4: '88.61%', 5: '94.99%'}\n",
      "Val Loss: 2.131049, Val Acc: 80.87%, Val-Class-Acc: {0: '67.93%', 1: '77.61%', 2: '77.78%', 3: '86.48%', 4: '77.50%', 5: '88.92%'}, LR: 0.000430\n",
      "Epoch 144/200, Train Loss: 0.281670, Train-Class-Acc: {0: '89.65%', 1: '88.63%', 2: '90.64%', 3: '93.03%', 4: '86.71%', 5: '94.02%'}\n",
      "Val Loss: 1.723024, Val Acc: 80.56%, Val-Class-Acc: {0: '66.85%', 1: '77.01%', 2: '78.47%', 3: '86.07%', 4: '77.50%', 5: '88.92%'}, LR: 0.000430\n",
      "Epoch 145/200, Train Loss: 0.230302, Train-Class-Acc: {0: '92.92%', 1: '90.50%', 2: '92.55%', 3: '94.06%', 4: '86.71%', 5: '94.77%'}\n",
      "Val Loss: 1.536142, Val Acc: 80.33%, Val-Class-Acc: {0: '70.65%', 1: '75.52%', 2: '72.22%', 3: '86.07%', 4: '80.00%', 5: '89.82%'}, LR: 0.000387\n",
      "Epoch 146/200, Train Loss: 0.319508, Train-Class-Acc: {0: '89.51%', 1: '90.05%', 2: '92.20%', 3: '93.24%', 4: '89.87%', 5: '94.77%'}\n",
      "Val Loss: 1.388797, Val Acc: 80.87%, Val-Class-Acc: {0: '69.57%', 1: '76.12%', 2: '79.17%', 3: '87.30%', 4: '80.00%', 5: '88.02%'}, LR: 0.000387\n",
      "Epoch 147/200, Train Loss: 0.257948, Train-Class-Acc: {0: '90.87%', 1: '88.63%', 2: '91.68%', 3: '94.06%', 4: '88.61%', 5: '94.32%'}\n",
      "Val Loss: 2.160832, Val Acc: 80.56%, Val-Class-Acc: {0: '64.67%', 1: '77.31%', 2: '79.86%', 3: '85.66%', 4: '77.50%', 5: '89.52%'}, LR: 0.000387\n",
      "Epoch 148/200, Train Loss: 0.297910, Train-Class-Acc: {0: '90.05%', 1: '89.83%', 2: '92.03%', 3: '92.01%', 4: '88.61%', 5: '93.72%'}\n",
      "Val Loss: 1.818266, Val Acc: 80.95%, Val-Class-Acc: {0: '72.83%', 1: '74.63%', 2: '81.25%', 3: '84.43%', 4: '77.50%', 5: '89.52%'}, LR: 0.000387\n",
      "Epoch 149/200, Train Loss: 0.228508, Train-Class-Acc: {0: '91.28%', 1: '90.28%', 2: '91.68%', 3: '94.26%', 4: '90.51%', 5: '94.99%'}\n",
      "Val Loss: 1.724661, Val Acc: 81.19%, Val-Class-Acc: {0: '69.57%', 1: '77.61%', 2: '79.86%', 3: '85.25%', 4: '77.50%', 5: '89.22%'}, LR: 0.000387\n",
      "Epoch 150/200, Train Loss: 0.274056, Train-Class-Acc: {0: '91.28%', 1: '88.86%', 2: '92.37%', 3: '93.34%', 4: '90.51%', 5: '94.77%'}\n",
      "Val Loss: 2.187788, Val Acc: 80.80%, Val-Class-Acc: {0: '72.83%', 1: '76.72%', 2: '75.69%', 3: '84.02%', 4: '77.50%', 5: '89.52%'}, LR: 0.000387\n",
      "Epoch 151/200, Train Loss: 0.395500, Train-Class-Acc: {0: '91.55%', 1: '89.30%', 2: '93.41%', 3: '93.14%', 4: '91.14%', 5: '93.72%'}\n",
      "Val Loss: 0.922831, Val Acc: 80.72%, Val-Class-Acc: {0: '74.46%', 1: '74.33%', 2: '80.56%', 3: '86.89%', 4: '75.00%', 5: '86.83%'}, LR: 0.000387\n",
      "Epoch 152/200, Train Loss: 0.235319, Train-Class-Acc: {0: '91.69%', 1: '90.58%', 2: '92.72%', 3: '93.75%', 4: '87.97%', 5: '94.77%'}\n",
      "Val Loss: 1.020629, Val Acc: 80.80%, Val-Class-Acc: {0: '66.85%', 1: '77.61%', 2: '81.94%', 3: '87.70%', 4: '77.50%', 5: '86.53%'}, LR: 0.000387\n",
      "Epoch 153/200, Train Loss: 0.246336, Train-Class-Acc: {0: '90.87%', 1: '89.01%', 2: '92.20%', 3: '94.16%', 4: '91.14%', 5: '94.69%'}\n",
      "Val Loss: 1.523447, Val Acc: 80.64%, Val-Class-Acc: {0: '65.22%', 1: '76.72%', 2: '77.78%', 3: '85.66%', 4: '80.00%', 5: '90.72%'}, LR: 0.000387\n",
      "Epoch 154/200, Train Loss: 0.261965, Train-Class-Acc: {0: '91.28%', 1: '91.02%', 2: '93.24%', 3: '92.73%', 4: '90.51%', 5: '95.67%'}\n",
      "Val Loss: 1.284028, Val Acc: 80.25%, Val-Class-Acc: {0: '69.02%', 1: '75.22%', 2: '75.69%', 3: '86.07%', 4: '80.00%', 5: '89.22%'}, LR: 0.000387\n",
      "Epoch 155/200, Train Loss: 0.262659, Train-Class-Acc: {0: '90.60%', 1: '89.68%', 2: '91.68%', 3: '94.06%', 4: '89.24%', 5: '95.52%'}\n",
      "Val Loss: 1.230029, Val Acc: 80.17%, Val-Class-Acc: {0: '70.11%', 1: '74.33%', 2: '76.39%', 3: '88.11%', 4: '75.00%', 5: '88.02%'}, LR: 0.000387\n",
      "Epoch 156/200, Train Loss: 0.245875, Train-Class-Acc: {0: '93.60%', 1: '90.80%', 2: '92.89%', 3: '94.06%', 4: '91.77%', 5: '95.14%'}\n",
      "Val Loss: 1.258336, Val Acc: 81.03%, Val-Class-Acc: {0: '73.37%', 1: '74.03%', 2: '81.94%', 3: '86.89%', 4: '77.50%', 5: '88.02%'}, LR: 0.000349\n",
      "Epoch 157/200, Train Loss: 0.210349, Train-Class-Acc: {0: '92.37%', 1: '89.60%', 2: '91.51%', 3: '93.75%', 4: '89.87%', 5: '95.67%'}\n",
      "Val Loss: 1.484864, Val Acc: 80.87%, Val-Class-Acc: {0: '70.65%', 1: '75.22%', 2: '77.78%', 3: '86.07%', 4: '80.00%', 5: '89.82%'}, LR: 0.000349\n",
      "Epoch 158/200, Train Loss: 0.258238, Train-Class-Acc: {0: '92.23%', 1: '90.43%', 2: '92.55%', 3: '93.44%', 4: '88.61%', 5: '94.47%'}\n",
      "Val Loss: 1.703502, Val Acc: 80.95%, Val-Class-Acc: {0: '69.02%', 1: '77.31%', 2: '79.17%', 3: '84.84%', 4: '80.00%', 5: '89.22%'}, LR: 0.000349\n",
      "Epoch 159/200, Train Loss: 0.372684, Train-Class-Acc: {0: '90.60%', 1: '89.45%', 2: '94.45%', 3: '92.83%', 4: '91.14%', 5: '95.44%'}\n",
      "Val Loss: 2.111985, Val Acc: 80.64%, Val-Class-Acc: {0: '72.28%', 1: '74.03%', 2: '78.47%', 3: '87.30%', 4: '75.00%', 5: '88.62%'}, LR: 0.000349\n",
      "Epoch 160/200, Train Loss: 0.334171, Train-Class-Acc: {0: '92.10%', 1: '90.73%', 2: '92.37%', 3: '93.14%', 4: '90.51%', 5: '93.95%'}\n",
      "Val Loss: 1.785050, Val Acc: 80.87%, Val-Class-Acc: {0: '64.13%', 1: '77.01%', 2: '80.56%', 3: '88.52%', 4: '82.50%', 5: '88.32%'}, LR: 0.000349\n",
      "Epoch 161/200, Train Loss: 0.275669, Train-Class-Acc: {0: '91.14%', 1: '90.20%', 2: '92.89%', 3: '94.67%', 4: '92.41%', 5: '95.07%'}\n",
      "Val Loss: 1.486101, Val Acc: 81.11%, Val-Class-Acc: {0: '71.74%', 1: '77.01%', 2: '79.17%', 3: '85.25%', 4: '80.00%', 5: '88.32%'}, LR: 0.000349\n",
      "Epoch 162/200, Train Loss: 0.225190, Train-Class-Acc: {0: '93.32%', 1: '91.77%', 2: '92.20%', 3: '94.26%', 4: '90.51%', 5: '95.44%'}\n",
      "Val Loss: 1.566793, Val Acc: 80.48%, Val-Class-Acc: {0: '64.67%', 1: '77.01%', 2: '77.78%', 3: '86.89%', 4: '75.00%', 5: '89.82%'}, LR: 0.000349\n",
      "Epoch 163/200, Train Loss: 0.369685, Train-Class-Acc: {0: '93.32%', 1: '90.95%', 2: '94.28%', 3: '93.95%', 4: '92.41%', 5: '94.84%'}\n",
      "Val Loss: 2.056290, Val Acc: 80.48%, Val-Class-Acc: {0: '70.11%', 1: '75.22%', 2: '77.78%', 3: '83.61%', 4: '85.00%', 5: '89.82%'}, LR: 0.000349\n",
      "Epoch 164/200, Train Loss: 0.293325, Train-Class-Acc: {0: '91.42%', 1: '90.65%', 2: '91.85%', 3: '94.06%', 4: '89.24%', 5: '95.29%'}\n",
      "Val Loss: 1.195996, Val Acc: 81.19%, Val-Class-Acc: {0: '70.11%', 1: '77.31%', 2: '78.47%', 3: '84.84%', 4: '82.50%', 5: '89.52%'}, LR: 0.000349\n",
      "Epoch 165/200, Train Loss: 0.202949, Train-Class-Acc: {0: '92.37%', 1: '90.80%', 2: '93.76%', 3: '94.47%', 4: '93.04%', 5: '95.29%'}\n",
      "Val Loss: 1.672650, Val Acc: 81.11%, Val-Class-Acc: {0: '70.11%', 1: '76.12%', 2: '78.47%', 3: '86.07%', 4: '82.50%', 5: '89.52%'}, LR: 0.000349\n",
      "Epoch 166/200, Train Loss: 0.208321, Train-Class-Acc: {0: '93.05%', 1: '90.95%', 2: '93.07%', 3: '93.95%', 4: '86.08%', 5: '95.22%'}\n",
      "Val Loss: 1.514795, Val Acc: 81.03%, Val-Class-Acc: {0: '71.20%', 1: '75.82%', 2: '77.78%', 3: '85.25%', 4: '80.00%', 5: '90.12%'}, LR: 0.000349\n",
      "Epoch 167/200, Train Loss: 0.348214, Train-Class-Acc: {0: '92.51%', 1: '90.28%', 2: '92.89%', 3: '94.36%', 4: '89.87%', 5: '94.77%'}\n",
      "Val Loss: 1.219353, Val Acc: 80.80%, Val-Class-Acc: {0: '69.57%', 1: '75.82%', 2: '77.08%', 3: '86.07%', 4: '80.00%', 5: '89.82%'}, LR: 0.000314\n",
      "Epoch 168/200, Train Loss: 0.184309, Train-Class-Acc: {0: '93.32%', 1: '92.15%', 2: '94.11%', 3: '95.39%', 4: '89.87%', 5: '95.96%'}\n",
      "Val Loss: 1.413786, Val Acc: 81.26%, Val-Class-Acc: {0: '71.20%', 1: '74.93%', 2: '80.56%', 3: '88.11%', 4: '80.00%', 5: '88.62%'}, LR: 0.000314\n",
      "Epoch 169/200, Train Loss: 0.237278, Train-Class-Acc: {0: '93.32%', 1: '91.55%', 2: '93.41%', 3: '94.06%', 4: '87.34%', 5: '95.07%'}\n",
      "Val Loss: 1.296910, Val Acc: 80.72%, Val-Class-Acc: {0: '69.57%', 1: '74.33%', 2: '78.47%', 3: '88.11%', 4: '80.00%', 5: '88.92%'}, LR: 0.000314\n",
      "Epoch 170/200, Train Loss: 0.251759, Train-Class-Acc: {0: '93.32%', 1: '90.50%', 2: '90.81%', 3: '94.67%', 4: '90.51%', 5: '95.81%'}\n",
      "Val Loss: 1.114166, Val Acc: 80.64%, Val-Class-Acc: {0: '73.91%', 1: '73.13%', 2: '78.47%', 3: '86.89%', 4: '80.00%', 5: '88.32%'}, LR: 0.000314\n",
      "Epoch 171/200, Train Loss: 0.257158, Train-Class-Acc: {0: '93.32%', 1: '91.47%', 2: '92.37%', 3: '94.77%', 4: '93.04%', 5: '95.22%'}\n",
      "Val Loss: 1.313145, Val Acc: 80.41%, Val-Class-Acc: {0: '69.57%', 1: '75.52%', 2: '79.86%', 3: '84.43%', 4: '77.50%', 5: '88.92%'}, LR: 0.000314\n",
      "Epoch 172/200, Train Loss: 0.256429, Train-Class-Acc: {0: '92.10%', 1: '88.71%', 2: '93.24%', 3: '93.85%', 4: '90.51%', 5: '95.14%'}\n",
      "Val Loss: 1.406420, Val Acc: 80.41%, Val-Class-Acc: {0: '71.20%', 1: '75.22%', 2: '79.17%', 3: '85.66%', 4: '80.00%', 5: '87.43%'}, LR: 0.000314\n",
      "Epoch 173/200, Train Loss: 0.189856, Train-Class-Acc: {0: '93.19%', 1: '92.07%', 2: '93.24%', 3: '94.67%', 4: '89.24%', 5: '95.59%'}\n",
      "Val Loss: 1.761214, Val Acc: 81.26%, Val-Class-Acc: {0: '71.74%', 1: '73.73%', 2: '81.94%', 3: '88.11%', 4: '80.00%', 5: '88.92%'}, LR: 0.000314\n",
      "Epoch 174/200, Train Loss: 0.301054, Train-Class-Acc: {0: '93.46%', 1: '92.67%', 2: '92.72%', 3: '94.57%', 4: '93.67%', 5: '94.84%'}\n",
      "Val Loss: 1.213301, Val Acc: 80.33%, Val-Class-Acc: {0: '70.65%', 1: '75.52%', 2: '81.25%', 3: '87.30%', 4: '80.00%', 5: '85.03%'}, LR: 0.000314\n",
      "Epoch 175/200, Train Loss: 0.286117, Train-Class-Acc: {0: '92.37%', 1: '90.80%', 2: '94.11%', 3: '94.57%', 4: '90.51%', 5: '94.92%'}\n",
      "Val Loss: 1.254658, Val Acc: 80.80%, Val-Class-Acc: {0: '70.11%', 1: '76.42%', 2: '78.47%', 3: '87.30%', 4: '77.50%', 5: '87.72%'}, LR: 0.000314\n",
      "Epoch 176/200, Train Loss: 0.185247, Train-Class-Acc: {0: '93.05%', 1: '91.40%', 2: '93.59%', 3: '94.88%', 4: '91.14%', 5: '95.59%'}\n",
      "Val Loss: 1.639183, Val Acc: 80.56%, Val-Class-Acc: {0: '71.74%', 1: '75.52%', 2: '79.86%', 3: '85.66%', 4: '77.50%', 5: '87.43%'}, LR: 0.000314\n",
      "Epoch 177/200, Train Loss: 0.181090, Train-Class-Acc: {0: '93.73%', 1: '92.00%', 2: '94.45%', 3: '95.08%', 4: '91.14%', 5: '95.37%'}\n",
      "Val Loss: 1.697731, Val Acc: 80.17%, Val-Class-Acc: {0: '70.11%', 1: '74.63%', 2: '79.17%', 3: '86.89%', 4: '77.50%', 5: '87.13%'}, LR: 0.000314\n",
      "Epoch 178/200, Train Loss: 0.197466, Train-Class-Acc: {0: '94.14%', 1: '93.04%', 2: '93.24%', 3: '95.39%', 4: '92.41%', 5: '95.96%'}\n",
      "Val Loss: 2.073772, Val Acc: 80.41%, Val-Class-Acc: {0: '69.57%', 1: '73.43%', 2: '81.25%', 3: '85.66%', 4: '77.50%', 5: '89.52%'}, LR: 0.000282\n",
      "Epoch 179/200, Train Loss: 0.165155, Train-Class-Acc: {0: '93.19%', 1: '92.97%', 2: '93.59%', 3: '94.67%', 4: '90.51%', 5: '95.81%'}\n",
      "Val Loss: 1.432766, Val Acc: 80.41%, Val-Class-Acc: {0: '64.13%', 1: '74.33%', 2: '81.25%', 3: '87.30%', 4: '82.50%', 5: '89.82%'}, LR: 0.000282\n",
      "Epoch 180/200, Train Loss: 0.175712, Train-Class-Acc: {0: '93.32%', 1: '91.70%', 2: '93.76%', 3: '95.70%', 4: '93.04%', 5: '96.11%'}\n",
      "Val Loss: 1.974936, Val Acc: 80.09%, Val-Class-Acc: {0: '68.48%', 1: '74.63%', 2: '79.17%', 3: '84.43%', 4: '77.50%', 5: '89.52%'}, LR: 0.000282\n",
      "Epoch 181/200, Train Loss: 0.278176, Train-Class-Acc: {0: '93.32%', 1: '91.02%', 2: '93.24%', 3: '95.39%', 4: '90.51%', 5: '95.59%'}\n",
      "Val Loss: 1.707551, Val Acc: 80.25%, Val-Class-Acc: {0: '67.39%', 1: '74.33%', 2: '80.56%', 3: '87.70%', 4: '80.00%', 5: '87.72%'}, LR: 0.000282\n",
      "Epoch 182/200, Train Loss: 0.186442, Train-Class-Acc: {0: '94.01%', 1: '92.60%', 2: '93.93%', 3: '94.98%', 4: '89.87%', 5: '95.52%'}\n",
      "Val Loss: 1.612862, Val Acc: 80.41%, Val-Class-Acc: {0: '67.39%', 1: '76.72%', 2: '78.47%', 3: '86.89%', 4: '77.50%', 5: '87.72%'}, LR: 0.000282\n",
      "Epoch 183/200, Train Loss: 0.237956, Train-Class-Acc: {0: '93.87%', 1: '91.70%', 2: '92.89%', 3: '95.39%', 4: '92.41%', 5: '95.29%'}\n",
      "Val Loss: 1.447482, Val Acc: 80.72%, Val-Class-Acc: {0: '69.02%', 1: '76.12%', 2: '77.78%', 3: '87.70%', 4: '77.50%', 5: '88.32%'}, LR: 0.000282\n",
      "Epoch 184/200, Train Loss: 0.216974, Train-Class-Acc: {0: '94.28%', 1: '92.82%', 2: '93.41%', 3: '95.18%', 4: '91.14%', 5: '95.96%'}\n",
      "Val Loss: 1.788752, Val Acc: 80.64%, Val-Class-Acc: {0: '64.67%', 1: '76.42%', 2: '82.64%', 3: '87.70%', 4: '80.00%', 5: '87.72%'}, LR: 0.000282\n",
      "Epoch 185/200, Train Loss: 0.176751, Train-Class-Acc: {0: '95.37%', 1: '92.89%', 2: '93.59%', 3: '94.77%', 4: '93.67%', 5: '95.81%'}\n",
      "Val Loss: 2.349929, Val Acc: 80.80%, Val-Class-Acc: {0: '70.11%', 1: '74.63%', 2: '80.56%', 3: '86.48%', 4: '80.00%', 5: '88.92%'}, LR: 0.000282\n",
      "Epoch 186/200, Train Loss: 0.165829, Train-Class-Acc: {0: '95.10%', 1: '92.52%', 2: '94.63%', 3: '95.29%', 4: '92.41%', 5: '96.49%'}\n",
      "Val Loss: 1.976078, Val Acc: 80.72%, Val-Class-Acc: {0: '67.93%', 1: '75.22%', 2: '79.17%', 3: '87.30%', 4: '80.00%', 5: '89.22%'}, LR: 0.000282\n",
      "Epoch 187/200, Train Loss: 0.238632, Train-Class-Acc: {0: '93.05%', 1: '92.97%', 2: '94.80%', 3: '96.72%', 4: '92.41%', 5: '95.59%'}\n",
      "Val Loss: 1.919545, Val Acc: 80.64%, Val-Class-Acc: {0: '71.20%', 1: '74.33%', 2: '79.17%', 3: '86.48%', 4: '80.00%', 5: '88.62%'}, LR: 0.000282\n",
      "Epoch 188/200, Train Loss: 0.192919, Train-Class-Acc: {0: '93.46%', 1: '92.30%', 2: '94.11%', 3: '94.98%', 4: '87.97%', 5: '95.74%'}\n",
      "Val Loss: 2.055668, Val Acc: 79.78%, Val-Class-Acc: {0: '71.20%', 1: '73.13%', 2: '78.47%', 3: '84.84%', 4: '80.00%', 5: '88.02%'}, LR: 0.000282\n",
      "Epoch 189/200, Train Loss: 0.196597, Train-Class-Acc: {0: '94.82%', 1: '93.49%', 2: '93.76%', 3: '95.70%', 4: '94.30%', 5: '95.81%'}\n",
      "Val Loss: 1.560844, Val Acc: 81.50%, Val-Class-Acc: {0: '69.02%', 1: '76.72%', 2: '81.25%', 3: '86.89%', 4: '80.00%', 5: '89.52%'}, LR: 0.000254\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_61.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_189.pth\n",
      "Epoch 190/200, Train Loss: 0.389489, Train-Class-Acc: {0: '92.37%', 1: '90.95%', 2: '93.07%', 3: '94.36%', 4: '89.87%', 5: '95.44%'}\n",
      "Val Loss: 1.611998, Val Acc: 80.56%, Val-Class-Acc: {0: '70.65%', 1: '75.22%', 2: '78.47%', 3: '88.11%', 4: '80.00%', 5: '86.83%'}, LR: 0.000254\n",
      "Epoch 191/200, Train Loss: 0.170977, Train-Class-Acc: {0: '94.55%', 1: '92.37%', 2: '94.63%', 3: '95.18%', 4: '89.24%', 5: '96.34%'}\n",
      "Val Loss: 1.496024, Val Acc: 80.95%, Val-Class-Acc: {0: '69.57%', 1: '75.82%', 2: '78.47%', 3: '87.70%', 4: '80.00%', 5: '88.62%'}, LR: 0.000254\n",
      "Epoch 192/200, Train Loss: 0.166129, Train-Class-Acc: {0: '95.23%', 1: '92.89%', 2: '93.93%', 3: '95.70%', 4: '90.51%', 5: '95.37%'}\n",
      "Val Loss: 1.239298, Val Acc: 79.86%, Val-Class-Acc: {0: '68.48%', 1: '74.03%', 2: '77.08%', 3: '85.25%', 4: '80.00%', 5: '89.22%'}, LR: 0.000254\n",
      "Epoch 193/200, Train Loss: 0.187223, Train-Class-Acc: {0: '92.78%', 1: '92.97%', 2: '93.07%', 3: '95.18%', 4: '91.77%', 5: '95.89%'}\n",
      "Val Loss: 1.440706, Val Acc: 81.03%, Val-Class-Acc: {0: '71.20%', 1: '74.93%', 2: '79.17%', 3: '87.70%', 4: '80.00%', 5: '88.62%'}, LR: 0.000254\n",
      "Epoch 194/200, Train Loss: 0.162026, Train-Class-Acc: {0: '95.78%', 1: '92.97%', 2: '93.24%', 3: '95.39%', 4: '94.30%', 5: '95.74%'}\n",
      "Val Loss: 2.079469, Val Acc: 80.72%, Val-Class-Acc: {0: '67.39%', 1: '74.93%', 2: '80.56%', 3: '87.30%', 4: '80.00%', 5: '89.22%'}, LR: 0.000254\n",
      "Epoch 195/200, Train Loss: 0.160566, Train-Class-Acc: {0: '95.37%', 1: '92.97%', 2: '94.80%', 3: '95.39%', 4: '89.87%', 5: '96.71%'}\n",
      "Val Loss: 1.818960, Val Acc: 80.95%, Val-Class-Acc: {0: '74.46%', 1: '74.03%', 2: '78.47%', 3: '85.66%', 4: '77.50%', 5: '89.52%'}, LR: 0.000254\n",
      "Epoch 196/200, Train Loss: 0.186895, Train-Class-Acc: {0: '94.82%', 1: '92.82%', 2: '94.28%', 3: '95.80%', 4: '93.67%', 5: '95.67%'}\n",
      "Val Loss: 1.523581, Val Acc: 80.56%, Val-Class-Acc: {0: '71.74%', 1: '74.33%', 2: '79.86%', 3: '85.25%', 4: '77.50%', 5: '88.92%'}, LR: 0.000254\n",
      "Epoch 197/200, Train Loss: 0.147425, Train-Class-Acc: {0: '95.78%', 1: '93.12%', 2: '93.59%', 3: '95.80%', 4: '93.04%', 5: '96.34%'}\n",
      "Val Loss: 1.698202, Val Acc: 80.95%, Val-Class-Acc: {0: '71.20%', 1: '74.63%', 2: '79.86%', 3: '85.25%', 4: '77.50%', 5: '90.42%'}, LR: 0.000254\n",
      "Epoch 198/200, Train Loss: 0.207564, Train-Class-Acc: {0: '94.55%', 1: '92.97%', 2: '94.45%', 3: '95.59%', 4: '90.51%', 5: '96.19%'}\n",
      "Val Loss: 1.101705, Val Acc: 81.03%, Val-Class-Acc: {0: '68.48%', 1: '78.21%', 2: '83.33%', 3: '88.52%', 4: '77.50%', 5: '84.73%'}, LR: 0.000254\n",
      "Epoch 199/200, Train Loss: 0.222322, Train-Class-Acc: {0: '94.41%', 1: '92.52%', 2: '93.93%', 3: '95.29%', 4: '89.24%', 5: '95.29%'}\n",
      "Val Loss: 1.328998, Val Acc: 81.34%, Val-Class-Acc: {0: '69.02%', 1: '77.01%', 2: '82.64%', 3: '86.89%', 4: '77.50%', 5: '88.32%'}, LR: 0.000254\n",
      "Epoch 200/200, Train Loss: 0.233600, Train-Class-Acc: {0: '95.64%', 1: '93.04%', 2: '94.28%', 3: '95.29%', 4: '92.41%', 5: '95.07%'}\n",
      "Val Loss: 1.790049, Val Acc: 80.80%, Val-Class-Acc: {0: '75.00%', 1: '73.43%', 2: '77.78%', 3: '85.66%', 4: '77.50%', 5: '89.52%'}, LR: 0.000229\n",
      "\n",
      "üèÜ Best model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_best.pth (Val Accuracy: 81.50%)\n",
      "\n",
      "üìå Final model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_final.pth\n",
      "\n",
      "üéØ Top 5 Best Models:\n",
      "Epoch 189, Train Loss: 0.196597, Train-Acc: {0: '94.82%', 1: '93.49%', 2: '93.76%', 3: '95.70%', 4: '94.30%', 5: '95.81%'},\n",
      "Val Loss: 1.560844, Val Acc: 81.50%, Val-Acc: {0: '69.02%', 1: '76.72%', 2: '81.25%', 3: '86.89%', 4: '80.00%', 5: '89.52%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_189.pth\n",
      "Epoch 96, Train Loss: 0.487613, Train-Acc: {0: '83.92%', 1: '81.75%', 2: '87.18%', 3: '89.65%', 4: '81.01%', 5: '92.00%'},\n",
      "Val Loss: 1.445839, Val Acc: 81.50%, Val-Acc: {0: '71.74%', 1: '77.61%', 2: '75.00%', 3: '87.70%', 4: '77.50%', 5: '89.52%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_96.pth\n",
      "Epoch 103, Train Loss: 0.524733, Train-Acc: {0: '83.79%', 1: '85.19%', 2: '89.77%', 3: '90.16%', 4: '84.18%', 5: '92.30%'},\n",
      "Val Loss: 1.181572, Val Acc: 81.42%, Val-Acc: {0: '78.26%', 1: '74.63%', 2: '77.08%', 3: '86.07%', 4: '82.50%', 5: '88.32%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_103.pth\n",
      "Epoch 106, Train Loss: 0.438429, Train-Acc: {0: '86.24%', 1: '83.32%', 2: '89.08%', 3: '90.57%', 4: '83.54%', 5: '92.00%'},\n",
      "Val Loss: 2.120835, Val Acc: 81.34%, Val-Acc: {0: '76.63%', 1: '77.31%', 2: '76.39%', 3: '83.20%', 4: '82.50%', 5: '88.62%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_106.pth\n",
      "Epoch 98, Train Loss: 0.445705, Train-Acc: {0: '85.01%', 1: '83.77%', 2: '87.35%', 3: '89.75%', 4: '83.54%', 5: '91.48%'},\n",
      "Val Loss: 1.314374, Val Acc: 81.34%, Val-Acc: {0: '73.37%', 1: '76.42%', 2: '79.17%', 3: '84.43%', 4: '77.50%', 5: '89.82%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3/ResNet18_1D_LoRA_epoch_98.pth\n",
      "---\n",
      "### Period 3\n",
      "+ ##### Total training time: 467.66 seconds\n",
      "+ ##### Model: ResNet18_1D_LoRA\n",
      "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v2/Period_3'*\n",
      "+ ##### Best Epoch: 189\n",
      "#### __Val Accuracy: 81.50%__\n",
      "#### __Val-Class-Acc: {0: '69.02%', 1: '76.72%', 2: '81.25%', 3: '86.89%', 4: '80.00%', 5: '89.52%'}__\n",
      "#### __Total Parameters: 3,891,846__\n",
      "#### __Model Size (float32): 14.85 MB__\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå Period 3: Standard LoRA Training (ECG)\n",
    "# ================================\n",
    "period = 3\n",
    "\n",
    "# ==== Paths ====\n",
    "stop_signal_file = os.path.join(BASE_DIR, \"stop_training.txt\")\n",
    "model_saving_folder = os.path.join(BASE_DIR, \"Trained_models\", \"Standard_LoRA_CIL_v2\", f\"Period_{period}\")\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# ==== Load Period 3 Data ====\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{period}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{period}.npy\"))\n",
    "X_val   = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val   = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "# ==== Device ====\n",
    "device = auto_select_cuda_device()\n",
    "\n",
    "# ==== Model Configuration ====\n",
    "input_channels = X_train.shape[2]  # ECG 12-lead\n",
    "output_size = len(np.unique(y_train))  # Êñ∞Â¢ûÈ°ûÂà•Êï∏\n",
    "model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size, lora_rank=4).to(device)\n",
    "\n",
    "# ==== Initialize LoRA Adapters FIRST ====\n",
    "model.init_lora()\n",
    "\n",
    "# ==== Load Period 2 Best Model Weights (excluding FC & LoRA) ====\n",
    "prev_model_path = os.path.join(BASE_DIR, \"Trained_models\", \"Standard_LoRA_CIL_v2\", f\"Period_{period - 1}\", \"ResNet18_1D_LoRA_best.pth\")\n",
    "prev_checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "prev_state_dict = prev_checkpoint[\"model_state_dict\"]\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "filtered_state_dict = {\n",
    "    k: v for k, v in prev_state_dict.items()\n",
    "    if k in model_dict and model_dict[k].shape == v.shape and not (k.startswith(\"fc\") or \"lora_adapter\" in k)\n",
    "}\n",
    "model.load_state_dict(filtered_state_dict, strict=False)\n",
    "for k in model_dict:\n",
    "    if k not in filtered_state_dict:\n",
    "        print(f\"üîç Not loaded: {k}, shape={model_dict[k].shape}\")\n",
    "print(\"‚úÖ Loaded Period 2 weights (excluding FC & LoRA)\")\n",
    "\n",
    "# ==== Optimizer / Scheduler ====\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.get_trainable_parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# ==== Start Training ====\n",
    "train_with_lora_ecg(\n",
    "    model=model,\n",
    "    output_size=output_size,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    model_saving_folder=model_saving_folder,\n",
    "    model_name=\"ResNet18_1D_LoRA\",\n",
    "    stop_signal_file=stop_signal_file,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# ‚úÖ Cleanup\n",
    "# ================================\n",
    "del X_train, y_train, X_val, y_val\n",
    "del prev_model_path, prev_checkpoint, prev_state_dict, filtered_state_dict\n",
    "del model, criterion, optimizer, scheduler\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ee646f",
   "metadata": {},
   "source": [
    "#### v3: ‰øÆÊîπËºâÂÖ•ÂèÉÊï∏ÈÇèËºØÔºå`lora_adapter.conv`‰πüË¶ÅËºâÂÖ•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e83d7561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 2\n",
      "    - Memory Used    : 1603 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ LoRA adapters initialized for 8 conv2 layers\n",
      "üîç Not loaded: layer1.0.lora_adapter.lora_A, shape=torch.Size([64, 4])\n",
      "üîç Not loaded: layer1.0.lora_adapter.lora_B, shape=torch.Size([4, 192])\n",
      "üîç Not loaded: layer1.1.lora_adapter.lora_A, shape=torch.Size([64, 4])\n",
      "üîç Not loaded: layer1.1.lora_adapter.lora_B, shape=torch.Size([4, 192])\n",
      "üîç Not loaded: layer2.0.lora_adapter.lora_A, shape=torch.Size([128, 4])\n",
      "üîç Not loaded: layer2.0.lora_adapter.lora_B, shape=torch.Size([4, 384])\n",
      "üîç Not loaded: layer2.1.lora_adapter.lora_A, shape=torch.Size([128, 4])\n",
      "üîç Not loaded: layer2.1.lora_adapter.lora_B, shape=torch.Size([4, 384])\n",
      "üîç Not loaded: layer3.0.lora_adapter.lora_A, shape=torch.Size([256, 4])\n",
      "üîç Not loaded: layer3.0.lora_adapter.lora_B, shape=torch.Size([4, 768])\n",
      "üîç Not loaded: layer3.1.lora_adapter.lora_A, shape=torch.Size([256, 4])\n",
      "üîç Not loaded: layer3.1.lora_adapter.lora_B, shape=torch.Size([4, 768])\n",
      "üîç Not loaded: layer4.0.lora_adapter.lora_A, shape=torch.Size([512, 4])\n",
      "üîç Not loaded: layer4.0.lora_adapter.lora_B, shape=torch.Size([4, 1536])\n",
      "üîç Not loaded: layer4.1.lora_adapter.lora_A, shape=torch.Size([512, 4])\n",
      "üîç Not loaded: layer4.1.lora_adapter.lora_B, shape=torch.Size([4, 1536])\n",
      "üîç Not loaded: fc.weight, shape=torch.Size([6, 1024])\n",
      "üîç Not loaded: fc.bias, shape=torch.Size([6])\n",
      "‚úÖ Loaded Period 2 weights (excluding FC & LoRA)\n",
      "üìä Parameter Statistics:\n",
      "  - Total parameters: 3,891,846\n",
      "  - Trainable parameters: 36,870 (0.95%)\n",
      "    - LoRA parameters: 30,720 (0.79%)\n",
      "    - FC parameters: 6,150 (0.16%)\n",
      "  - Frozen parameters: 3,854,976 (99.05%)\n",
      "üß† Trainable parameter names:\n",
      "  ‚úÖ layer1.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer1.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer1.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer1.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer2.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer2.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer2.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer2.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer3.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer3.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer3.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer3.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer4.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer4.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer4.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer4.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ fc.weight (FC)\n",
      "  ‚úÖ fc.bias (FC)\n",
      "\n",
      "üöÄ 'train_with_lora_ecg' started.\n",
      "‚úÖ Removed existing folder: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_763947/394820996.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  prev_checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Data Overview:\n",
      "X_train: torch.Size([5120, 5000, 12]), y_train: torch.Size([5120])\n",
      "X_val: torch.Size([1281, 5000, 12]), y_val: torch.Size([1281])\n",
      "Epoch 1/200, Train Loss: 39.377799, Train-Class-Acc: {0: '47.00%', 1: '37.10%', 2: '16.29%', 3: '25.20%', 4: '5.70%', 5: '52.09%'}\n",
      "Val Loss: 11.803610, Val Acc: 61.20%, Val-Class-Acc: {0: '69.57%', 1: '54.93%', 2: '36.11%', 3: '50.82%', 4: '0.00%', 5: '88.62%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_1.pth\n",
      "Epoch 2/200, Train Loss: 15.199642, Train-Class-Acc: {0: '63.35%', 1: '47.49%', 2: '35.01%', 3: '37.60%', 4: '10.13%', 5: '72.50%'}\n",
      "Val Loss: 7.343332, Val Acc: 67.14%, Val-Class-Acc: {0: '79.35%', 1: '60.00%', 2: '56.25%', 3: '53.28%', 4: '22.50%', 5: '87.72%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_2.pth\n",
      "Epoch 3/200, Train Loss: 11.043849, Train-Class-Acc: {0: '67.03%', 1: '51.53%', 2: '46.27%', 3: '42.93%', 4: '18.35%', 5: '74.89%'}\n",
      "Val Loss: 5.977223, Val Acc: 70.80%, Val-Class-Acc: {0: '76.63%', 1: '79.10%', 2: '57.64%', 3: '49.18%', 4: '25.00%', 5: '86.23%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_3.pth\n",
      "Epoch 4/200, Train Loss: 8.682405, Train-Class-Acc: {0: '63.76%', 1: '53.55%', 2: '50.26%', 3: '51.13%', 4: '27.85%', 5: '76.68%'}\n",
      "Val Loss: 5.714480, Val Acc: 73.46%, Val-Class-Acc: {0: '77.72%', 1: '72.84%', 2: '65.28%', 3: '59.43%', 4: '50.00%', 5: '88.32%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_4.pth\n",
      "Epoch 5/200, Train Loss: 7.513821, Train-Class-Acc: {0: '63.90%', 1: '53.18%', 2: '55.46%', 3: '53.07%', 4: '29.11%', 5: '76.16%'}\n",
      "Val Loss: 4.262769, Val Acc: 74.00%, Val-Class-Acc: {0: '78.26%', 1: '69.25%', 2: '66.67%', 3: '69.26%', 4: '55.00%', 5: '85.33%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_5.pth\n",
      "Epoch 6/200, Train Loss: 6.251635, Train-Class-Acc: {0: '68.53%', 1: '56.10%', 2: '59.27%', 3: '57.79%', 4: '28.48%', 5: '77.58%'}\n",
      "Val Loss: 3.959103, Val Acc: 76.11%, Val-Class-Acc: {0: '78.80%', 1: '74.03%', 2: '69.44%', 3: '66.80%', 4: '65.00%', 5: '87.72%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_1.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_6.pth\n",
      "Epoch 7/200, Train Loss: 5.849242, Train-Class-Acc: {0: '63.22%', 1: '55.80%', 2: '60.66%', 3: '58.81%', 4: '38.61%', 5: '77.20%'}\n",
      "Val Loss: 3.232534, Val Acc: 76.50%, Val-Class-Acc: {0: '65.76%', 1: '81.49%', 2: '75.00%', 3: '70.49%', 4: '62.50%', 5: '84.13%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_2.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_7.pth\n",
      "Epoch 8/200, Train Loss: 5.133608, Train-Class-Acc: {0: '64.71%', 1: '55.12%', 2: '63.60%', 3: '62.40%', 4: '38.61%', 5: '76.46%'}\n",
      "Val Loss: 3.273706, Val Acc: 76.27%, Val-Class-Acc: {0: '71.74%', 1: '68.66%', 2: '67.36%', 3: '82.38%', 4: '62.50%', 5: '87.43%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_3.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_8.pth\n",
      "Epoch 9/200, Train Loss: 4.460909, Train-Class-Acc: {0: '66.62%', 1: '57.37%', 2: '62.74%', 3: '63.32%', 4: '42.41%', 5: '76.76%'}\n",
      "Val Loss: 3.105662, Val Acc: 75.88%, Val-Class-Acc: {0: '67.93%', 1: '71.04%', 2: '73.61%', 3: '78.28%', 4: '67.50%', 5: '85.33%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_4.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_9.pth\n",
      "Epoch 10/200, Train Loss: 3.894810, Train-Class-Acc: {0: '65.40%', 1: '59.69%', 2: '63.43%', 3: '63.52%', 4: '41.14%', 5: '77.65%'}\n",
      "Val Loss: 2.245535, Val Acc: 76.58%, Val-Class-Acc: {0: '67.39%', 1: '77.31%', 2: '70.83%', 3: '80.33%', 4: '67.50%', 5: '81.74%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_5.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_10.pth\n",
      "Epoch 11/200, Train Loss: 3.620543, Train-Class-Acc: {0: '66.62%', 1: '59.91%', 2: '66.20%', 3: '64.14%', 4: '40.51%', 5: '78.10%'}\n",
      "Val Loss: 2.314982, Val Acc: 76.42%, Val-Class-Acc: {0: '71.74%', 1: '70.75%', 2: '76.39%', 3: '78.28%', 4: '77.50%', 5: '83.23%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_9.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_11.pth\n",
      "Epoch 12/200, Train Loss: 3.525943, Train-Class-Acc: {0: '65.80%', 1: '57.59%', 2: '66.72%', 3: '67.62%', 4: '46.84%', 5: '77.58%'}\n",
      "Val Loss: 2.219624, Val Acc: 76.50%, Val-Class-Acc: {0: '75.00%', 1: '72.84%', 2: '74.31%', 3: '84.84%', 4: '70.00%', 5: '76.65%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_6.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_12.pth\n",
      "Epoch 13/200, Train Loss: 3.614189, Train-Class-Acc: {0: '64.17%', 1: '59.24%', 2: '67.24%', 3: '67.11%', 4: '39.24%', 5: '77.43%'}\n",
      "Val Loss: 2.581871, Val Acc: 76.58%, Val-Class-Acc: {0: '69.57%', 1: '71.04%', 2: '71.53%', 3: '83.61%', 4: '72.50%', 5: '83.53%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_8.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_13.pth\n",
      "Epoch 14/200, Train Loss: 3.094129, Train-Class-Acc: {0: '65.53%', 1: '59.46%', 2: '68.46%', 3: '67.62%', 4: '50.00%', 5: '78.40%'}\n",
      "Val Loss: 2.969972, Val Acc: 75.96%, Val-Class-Acc: {0: '73.91%', 1: '73.43%', 2: '72.22%', 3: '72.54%', 4: '72.50%', 5: '84.13%'}, LR: 0.001000\n",
      "Epoch 15/200, Train Loss: 2.716700, Train-Class-Acc: {0: '66.08%', 1: '61.03%', 2: '69.84%', 3: '68.85%', 4: '48.73%', 5: '79.15%'}\n",
      "Val Loss: 2.397709, Val Acc: 77.13%, Val-Class-Acc: {0: '61.41%', 1: '77.01%', 2: '72.92%', 3: '83.20%', 4: '70.00%', 5: '84.13%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_11.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_15.pth\n",
      "Epoch 16/200, Train Loss: 2.793426, Train-Class-Acc: {0: '66.08%', 1: '61.48%', 2: '70.88%', 3: '69.16%', 4: '50.00%', 5: '78.70%'}\n",
      "Val Loss: 1.629392, Val Acc: 76.74%, Val-Class-Acc: {0: '60.33%', 1: '78.81%', 2: '77.78%', 3: '84.43%', 4: '75.00%', 5: '77.84%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_7.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_16.pth\n",
      "Epoch 17/200, Train Loss: 2.650805, Train-Class-Acc: {0: '69.35%', 1: '62.75%', 2: '67.24%', 3: '69.57%', 4: '48.73%', 5: '79.00%'}\n",
      "Val Loss: 2.333808, Val Acc: 76.50%, Val-Class-Acc: {0: '65.76%', 1: '76.42%', 2: '75.69%', 3: '77.46%', 4: '67.50%', 5: '83.23%'}, LR: 0.001000\n",
      "Epoch 18/200, Train Loss: 2.442724, Train-Class-Acc: {0: '64.31%', 1: '62.15%', 2: '72.10%', 3: '72.95%', 4: '54.43%', 5: '80.49%'}\n",
      "Val Loss: 1.947539, Val Acc: 78.45%, Val-Class-Acc: {0: '74.46%', 1: '73.43%', 2: '74.31%', 3: '81.15%', 4: '72.50%', 5: '86.23%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_12.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_18.pth\n",
      "Epoch 19/200, Train Loss: 2.629812, Train-Class-Acc: {0: '67.03%', 1: '63.50%', 2: '71.75%', 3: '70.90%', 4: '44.30%', 5: '80.49%'}\n",
      "Val Loss: 2.656353, Val Acc: 77.75%, Val-Class-Acc: {0: '69.57%', 1: '76.12%', 2: '70.83%', 3: '76.64%', 4: '75.00%', 5: '88.02%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_10.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_19.pth\n",
      "Epoch 20/200, Train Loss: 2.122996, Train-Class-Acc: {0: '64.85%', 1: '61.48%', 2: '69.84%', 3: '72.44%', 4: '56.96%', 5: '80.49%'}\n",
      "Val Loss: 2.081004, Val Acc: 77.36%, Val-Class-Acc: {0: '68.48%', 1: '74.33%', 2: '72.22%', 3: '83.61%', 4: '72.50%', 5: '83.53%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_13.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_20.pth\n",
      "Epoch 21/200, Train Loss: 2.258460, Train-Class-Acc: {0: '64.85%', 1: '64.32%', 2: '72.44%', 3: '74.49%', 4: '55.70%', 5: '81.46%'}\n",
      "Val Loss: 2.344175, Val Acc: 77.83%, Val-Class-Acc: {0: '70.65%', 1: '74.03%', 2: '70.83%', 3: '80.33%', 4: '75.00%', 5: '87.13%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_16.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_21.pth\n",
      "Epoch 22/200, Train Loss: 2.180765, Train-Class-Acc: {0: '68.26%', 1: '65.37%', 2: '71.75%', 3: '75.41%', 4: '55.70%', 5: '81.09%'}\n",
      "Val Loss: 1.643672, Val Acc: 79.23%, Val-Class-Acc: {0: '66.85%', 1: '77.61%', 2: '72.92%', 3: '86.07%', 4: '72.50%', 5: '86.23%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_15.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_22.pth\n",
      "Epoch 23/200, Train Loss: 2.345385, Train-Class-Acc: {0: '66.21%', 1: '64.70%', 2: '71.75%', 3: '72.95%', 4: '51.90%', 5: '81.91%'}\n",
      "Val Loss: 2.281462, Val Acc: 79.23%, Val-Class-Acc: {0: '75.54%', 1: '71.34%', 2: '77.08%', 3: '84.43%', 4: '75.00%', 5: '86.83%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_20.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_23.pth\n",
      "Epoch 24/200, Train Loss: 2.156516, Train-Class-Acc: {0: '65.40%', 1: '64.55%', 2: '74.18%', 3: '75.41%', 4: '57.59%', 5: '81.61%'}\n",
      "Val Loss: 2.159207, Val Acc: 78.22%, Val-Class-Acc: {0: '68.48%', 1: '72.24%', 2: '72.92%', 3: '86.07%', 4: '75.00%', 5: '86.53%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_19.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_24.pth\n",
      "Epoch 25/200, Train Loss: 2.103941, Train-Class-Acc: {0: '66.76%', 1: '66.49%', 2: '72.79%', 3: '74.18%', 4: '55.06%', 5: '81.24%'}\n",
      "Val Loss: 1.839227, Val Acc: 77.67%, Val-Class-Acc: {0: '56.52%', 1: '77.01%', 2: '70.14%', 3: '86.07%', 4: '72.50%', 5: '87.72%'}, LR: 0.001000\n",
      "Epoch 26/200, Train Loss: 1.828018, Train-Class-Acc: {0: '67.30%', 1: '65.30%', 2: '73.31%', 3: '76.02%', 4: '53.80%', 5: '83.63%'}\n",
      "Val Loss: 1.722629, Val Acc: 78.45%, Val-Class-Acc: {0: '68.48%', 1: '76.72%', 2: '76.39%', 3: '86.07%', 4: '75.00%', 5: '81.44%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_21.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_26.pth\n",
      "Epoch 27/200, Train Loss: 1.901795, Train-Class-Acc: {0: '69.89%', 1: '67.02%', 2: '72.44%', 3: '75.20%', 4: '52.53%', 5: '83.33%'}\n",
      "Val Loss: 1.726596, Val Acc: 79.31%, Val-Class-Acc: {0: '66.30%', 1: '77.01%', 2: '72.92%', 3: '85.66%', 4: '75.00%', 5: '87.43%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_24.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_27.pth\n",
      "Epoch 28/200, Train Loss: 1.813774, Train-Class-Acc: {0: '66.21%', 1: '65.07%', 2: '72.62%', 3: '75.41%', 4: '59.49%', 5: '81.69%'}\n",
      "Val Loss: 2.192640, Val Acc: 79.16%, Val-Class-Acc: {0: '71.20%', 1: '75.82%', 2: '76.39%', 3: '78.28%', 4: '75.00%', 5: '89.22%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_18.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_28.pth\n",
      "Epoch 29/200, Train Loss: 1.708661, Train-Class-Acc: {0: '67.98%', 1: '66.57%', 2: '75.74%', 3: '77.15%', 4: '58.23%', 5: '83.11%'}\n",
      "Val Loss: 2.135569, Val Acc: 77.99%, Val-Class-Acc: {0: '59.24%', 1: '81.79%', 2: '75.69%', 3: '79.92%', 4: '75.00%', 5: '84.43%'}, LR: 0.000900\n",
      "Epoch 30/200, Train Loss: 1.698018, Train-Class-Acc: {0: '68.39%', 1: '66.57%', 2: '72.79%', 3: '77.05%', 4: '63.92%', 5: '83.18%'}\n",
      "Val Loss: 1.826586, Val Acc: 79.78%, Val-Class-Acc: {0: '66.30%', 1: '77.61%', 2: '72.92%', 3: '85.66%', 4: '75.00%', 5: '88.62%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_26.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_30.pth\n",
      "Epoch 31/200, Train Loss: 1.617721, Train-Class-Acc: {0: '66.35%', 1: '65.89%', 2: '74.18%', 3: '79.71%', 4: '56.96%', 5: '84.30%'}\n",
      "Val Loss: 1.941084, Val Acc: 78.84%, Val-Class-Acc: {0: '63.04%', 1: '77.61%', 2: '71.53%', 3: '87.70%', 4: '75.00%', 5: '85.93%'}, LR: 0.000900\n",
      "Epoch 32/200, Train Loss: 1.645003, Train-Class-Acc: {0: '67.17%', 1: '68.74%', 2: '73.66%', 3: '77.87%', 4: '62.66%', 5: '83.56%'}\n",
      "Val Loss: 1.184575, Val Acc: 78.77%, Val-Class-Acc: {0: '66.30%', 1: '79.40%', 2: '79.17%', 3: '83.61%', 4: '75.00%', 5: '81.74%'}, LR: 0.000900\n",
      "Epoch 33/200, Train Loss: 1.496163, Train-Class-Acc: {0: '69.48%', 1: '67.17%', 2: '76.26%', 3: '79.61%', 4: '59.49%', 5: '84.38%'}\n",
      "Val Loss: 1.325955, Val Acc: 80.09%, Val-Class-Acc: {0: '75.00%', 1: '75.22%', 2: '79.86%', 3: '84.84%', 4: '75.00%', 5: '85.03%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_28.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_33.pth\n",
      "Epoch 34/200, Train Loss: 1.337239, Train-Class-Acc: {0: '69.48%', 1: '67.76%', 2: '74.52%', 3: '79.71%', 4: '61.39%', 5: '83.86%'}\n",
      "Val Loss: 2.499580, Val Acc: 80.17%, Val-Class-Acc: {0: '69.57%', 1: '76.72%', 2: '80.56%', 3: '81.97%', 4: '75.00%', 5: '88.62%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_22.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_34.pth\n",
      "Epoch 35/200, Train Loss: 1.678608, Train-Class-Acc: {0: '70.03%', 1: '67.69%', 2: '73.83%', 3: '77.87%', 4: '65.82%', 5: '84.01%'}\n",
      "Val Loss: 2.155140, Val Acc: 79.55%, Val-Class-Acc: {0: '71.20%', 1: '76.12%', 2: '72.92%', 3: '84.84%', 4: '75.00%', 5: '87.13%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_23.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_35.pth\n",
      "Epoch 36/200, Train Loss: 1.288812, Train-Class-Acc: {0: '71.66%', 1: '70.61%', 2: '76.78%', 3: '79.20%', 4: '60.13%', 5: '85.13%'}\n",
      "Val Loss: 1.336006, Val Acc: 79.70%, Val-Class-Acc: {0: '71.20%', 1: '76.12%', 2: '81.25%', 3: '86.07%', 4: '75.00%', 5: '83.23%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_27.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_36.pth\n",
      "Epoch 37/200, Train Loss: 1.363259, Train-Class-Acc: {0: '68.94%', 1: '69.78%', 2: '74.52%', 3: '80.02%', 4: '58.86%', 5: '85.28%'}\n",
      "Val Loss: 1.826787, Val Acc: 79.94%, Val-Class-Acc: {0: '65.76%', 1: '81.49%', 2: '81.25%', 3: '81.56%', 4: '75.00%', 5: '85.03%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_35.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_37.pth\n",
      "Epoch 38/200, Train Loss: 1.386963, Train-Class-Acc: {0: '71.80%', 1: '68.44%', 2: '76.26%', 3: '77.56%', 4: '63.92%', 5: '84.30%'}\n",
      "Val Loss: 1.572720, Val Acc: 80.33%, Val-Class-Acc: {0: '70.65%', 1: '80.30%', 2: '80.56%', 3: '81.56%', 4: '75.00%', 5: '85.33%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_36.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_38.pth\n",
      "Epoch 39/200, Train Loss: 1.515361, Train-Class-Acc: {0: '69.62%', 1: '67.84%', 2: '76.26%', 3: '76.33%', 4: '60.76%', 5: '83.18%'}\n",
      "Val Loss: 1.016354, Val Acc: 77.83%, Val-Class-Acc: {0: '74.46%', 1: '75.82%', 2: '78.47%', 3: '86.89%', 4: '75.00%', 5: '75.15%'}, LR: 0.000900\n",
      "Epoch 40/200, Train Loss: 1.366757, Train-Class-Acc: {0: '70.44%', 1: '69.11%', 2: '75.56%', 3: '79.61%', 4: '68.99%', 5: '84.53%'}\n",
      "Val Loss: 1.660719, Val Acc: 80.41%, Val-Class-Acc: {0: '69.57%', 1: '80.60%', 2: '75.69%', 3: '82.38%', 4: '75.00%', 5: '87.43%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_30.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_40.pth\n",
      "Epoch 41/200, Train Loss: 1.334708, Train-Class-Acc: {0: '70.16%', 1: '70.08%', 2: '75.91%', 3: '79.51%', 4: '62.03%', 5: '85.20%'}\n",
      "Val Loss: 1.366214, Val Acc: 79.55%, Val-Class-Acc: {0: '58.70%', 1: '82.09%', 2: '72.92%', 3: '84.84%', 4: '75.00%', 5: '88.02%'}, LR: 0.000900\n",
      "Epoch 42/200, Train Loss: 1.307192, Train-Class-Acc: {0: '72.89%', 1: '71.13%', 2: '75.91%', 3: '82.17%', 4: '65.82%', 5: '84.68%'}\n",
      "Val Loss: 1.515757, Val Acc: 80.48%, Val-Class-Acc: {0: '66.30%', 1: '77.01%', 2: '79.86%', 3: '84.43%', 4: '77.50%', 5: '89.52%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_37.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_42.pth\n",
      "Epoch 43/200, Train Loss: 1.635307, Train-Class-Acc: {0: '69.21%', 1: '68.14%', 2: '76.26%', 3: '79.61%', 4: '63.92%', 5: '84.23%'}\n",
      "Val Loss: 1.449025, Val Acc: 80.09%, Val-Class-Acc: {0: '74.46%', 1: '76.12%', 2: '79.86%', 3: '84.43%', 4: '67.50%', 5: '85.63%'}, LR: 0.000900\n",
      "Epoch 44/200, Train Loss: 1.407716, Train-Class-Acc: {0: '71.39%', 1: '70.76%', 2: '73.31%', 3: '80.33%', 4: '67.72%', 5: '85.05%'}\n",
      "Val Loss: 2.028210, Val Acc: 79.86%, Val-Class-Acc: {0: '67.93%', 1: '78.21%', 2: '78.47%', 3: '83.61%', 4: '75.00%', 5: '86.53%'}, LR: 0.000900\n",
      "Epoch 45/200, Train Loss: 1.321576, Train-Class-Acc: {0: '73.02%', 1: '72.85%', 2: '77.82%', 3: '79.20%', 4: '69.62%', 5: '85.95%'}\n",
      "Val Loss: 1.882366, Val Acc: 80.02%, Val-Class-Acc: {0: '73.91%', 1: '77.91%', 2: '83.33%', 3: '83.20%', 4: '75.00%', 5: '82.34%'}, LR: 0.000900\n",
      "Epoch 46/200, Train Loss: 1.150501, Train-Class-Acc: {0: '73.43%', 1: '71.65%', 2: '77.99%', 3: '82.27%', 4: '68.99%', 5: '85.43%'}\n",
      "Val Loss: 1.481227, Val Acc: 80.87%, Val-Class-Acc: {0: '66.85%', 1: '77.61%', 2: '81.94%', 3: '85.25%', 4: '77.50%', 5: '88.62%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_33.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_46.pth\n",
      "Epoch 47/200, Train Loss: 1.128598, Train-Class-Acc: {0: '72.07%', 1: '71.13%', 2: '77.47%', 3: '82.17%', 4: '66.46%', 5: '86.92%'}\n",
      "Val Loss: 1.290348, Val Acc: 80.17%, Val-Class-Acc: {0: '67.93%', 1: '78.81%', 2: '78.47%', 3: '79.92%', 4: '77.50%', 5: '89.52%'}, LR: 0.000900\n",
      "Epoch 48/200, Train Loss: 1.058337, Train-Class-Acc: {0: '72.21%', 1: '71.58%', 2: '78.51%', 3: '82.58%', 4: '68.99%', 5: '86.77%'}\n",
      "Val Loss: 1.633568, Val Acc: 80.95%, Val-Class-Acc: {0: '70.11%', 1: '79.10%', 2: '77.08%', 3: '83.20%', 4: '77.50%', 5: '89.22%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_34.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_48.pth\n",
      "Epoch 49/200, Train Loss: 0.940094, Train-Class-Acc: {0: '70.57%', 1: '71.58%', 2: '76.95%', 3: '83.20%', 4: '69.62%', 5: '86.55%'}\n",
      "Val Loss: 1.404377, Val Acc: 80.17%, Val-Class-Acc: {0: '71.20%', 1: '78.81%', 2: '77.08%', 3: '80.33%', 4: '77.50%', 5: '88.02%'}, LR: 0.000900\n",
      "Epoch 50/200, Train Loss: 1.046112, Train-Class-Acc: {0: '73.02%', 1: '73.90%', 2: '79.03%', 3: '80.94%', 4: '72.78%', 5: '86.55%'}\n",
      "Val Loss: 1.471897, Val Acc: 79.94%, Val-Class-Acc: {0: '63.59%', 1: '80.00%', 2: '79.17%', 3: '81.56%', 4: '77.50%', 5: '88.32%'}, LR: 0.000900\n",
      "Epoch 51/200, Train Loss: 1.244710, Train-Class-Acc: {0: '74.52%', 1: '72.85%', 2: '80.59%', 3: '83.40%', 4: '73.42%', 5: '87.07%'}\n",
      "Val Loss: 1.397325, Val Acc: 80.25%, Val-Class-Acc: {0: '70.11%', 1: '77.31%', 2: '82.64%', 3: '84.43%', 4: '77.50%', 5: '85.03%'}, LR: 0.000810\n",
      "Epoch 52/200, Train Loss: 1.057681, Train-Class-Acc: {0: '73.71%', 1: '74.50%', 2: '79.20%', 3: '81.56%', 4: '71.52%', 5: '87.14%'}\n",
      "Val Loss: 1.079748, Val Acc: 80.56%, Val-Class-Acc: {0: '71.74%', 1: '77.31%', 2: '81.94%', 3: '85.25%', 4: '77.50%', 5: '85.03%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_38.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_52.pth\n",
      "Epoch 53/200, Train Loss: 1.074907, Train-Class-Acc: {0: '73.84%', 1: '75.47%', 2: '77.99%', 3: '83.30%', 4: '72.15%', 5: '87.29%'}\n",
      "Val Loss: 0.971320, Val Acc: 80.02%, Val-Class-Acc: {0: '66.30%', 1: '80.00%', 2: '80.56%', 3: '83.61%', 4: '77.50%', 5: '85.03%'}, LR: 0.000810\n",
      "Epoch 54/200, Train Loss: 1.120474, Train-Class-Acc: {0: '72.75%', 1: '73.67%', 2: '79.20%', 3: '82.58%', 4: '72.15%', 5: '86.92%'}\n",
      "Val Loss: 1.137809, Val Acc: 81.19%, Val-Class-Acc: {0: '78.80%', 1: '75.22%', 2: '79.86%', 3: '86.48%', 4: '77.50%', 5: '85.63%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_40.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_54.pth\n",
      "Epoch 55/200, Train Loss: 1.166612, Train-Class-Acc: {0: '73.16%', 1: '74.87%', 2: '80.24%', 3: '84.32%', 4: '68.35%', 5: '87.00%'}\n",
      "Val Loss: 2.581758, Val Acc: 80.41%, Val-Class-Acc: {0: '63.59%', 1: '79.70%', 2: '73.61%', 3: '79.51%', 4: '77.50%', 5: '94.31%'}, LR: 0.000810\n",
      "Epoch 56/200, Train Loss: 1.208473, Train-Class-Acc: {0: '73.02%', 1: '71.80%', 2: '79.03%', 3: '82.38%', 4: '74.68%', 5: '87.00%'}\n",
      "Val Loss: 0.935368, Val Acc: 79.94%, Val-Class-Acc: {0: '65.22%', 1: '80.00%', 2: '79.17%', 3: '84.43%', 4: '80.00%', 5: '85.03%'}, LR: 0.000810\n",
      "Epoch 57/200, Train Loss: 0.984928, Train-Class-Acc: {0: '72.75%', 1: '74.50%', 2: '80.59%', 3: '83.50%', 4: '70.89%', 5: '86.47%'}\n",
      "Val Loss: 1.940414, Val Acc: 80.72%, Val-Class-Acc: {0: '64.13%', 1: '78.21%', 2: '79.17%', 3: '82.79%', 4: '77.50%', 5: '91.92%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_42.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_57.pth\n",
      "Epoch 58/200, Train Loss: 0.993957, Train-Class-Acc: {0: '71.66%', 1: '71.50%', 2: '80.24%', 3: '83.50%', 4: '69.62%', 5: '86.55%'}\n",
      "Val Loss: 1.417349, Val Acc: 81.42%, Val-Class-Acc: {0: '77.17%', 1: '74.33%', 2: '81.25%', 3: '83.20%', 4: '77.50%', 5: '90.12%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_52.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_58.pth\n",
      "Epoch 59/200, Train Loss: 0.876191, Train-Class-Acc: {0: '73.57%', 1: '74.72%', 2: '82.32%', 3: '84.84%', 4: '67.09%', 5: '87.29%'}\n",
      "Val Loss: 2.215698, Val Acc: 80.80%, Val-Class-Acc: {0: '80.43%', 1: '75.52%', 2: '80.56%', 3: '78.28%', 4: '77.50%', 5: '88.62%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_57.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_59.pth\n",
      "Epoch 60/200, Train Loss: 0.852802, Train-Class-Acc: {0: '74.80%', 1: '75.24%', 2: '83.02%', 3: '84.43%', 4: '70.25%', 5: '87.82%'}\n",
      "Val Loss: 1.059439, Val Acc: 80.87%, Val-Class-Acc: {0: '71.74%', 1: '77.01%', 2: '81.94%', 3: '86.48%', 4: '77.50%', 5: '85.63%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_59.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_60.pth\n",
      "Epoch 61/200, Train Loss: 0.815734, Train-Class-Acc: {0: '75.34%', 1: '73.97%', 2: '79.03%', 3: '82.89%', 4: '70.89%', 5: '88.49%'}\n",
      "Val Loss: 1.722629, Val Acc: 81.11%, Val-Class-Acc: {0: '75.00%', 1: '78.81%', 2: '77.08%', 3: '81.56%', 4: '75.00%', 5: '88.92%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_46.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_61.pth\n",
      "Epoch 62/200, Train Loss: 0.939345, Train-Class-Acc: {0: '76.16%', 1: '75.54%', 2: '79.03%', 3: '85.14%', 4: '64.56%', 5: '87.97%'}\n",
      "Val Loss: 0.939264, Val Acc: 81.42%, Val-Class-Acc: {0: '69.57%', 1: '77.61%', 2: '79.86%', 3: '87.30%', 4: '77.50%', 5: '88.62%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_60.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_62.pth\n",
      "Epoch 63/200, Train Loss: 1.048365, Train-Class-Acc: {0: '75.48%', 1: '75.09%', 2: '79.72%', 3: '83.09%', 4: '73.42%', 5: '88.57%'}\n",
      "Val Loss: 1.343308, Val Acc: 80.48%, Val-Class-Acc: {0: '70.11%', 1: '78.21%', 2: '74.31%', 3: '84.43%', 4: '77.50%', 5: '88.62%'}, LR: 0.000810\n",
      "Epoch 64/200, Train Loss: 0.823376, Train-Class-Acc: {0: '74.52%', 1: '74.50%', 2: '80.59%', 3: '83.40%', 4: '71.52%', 5: '88.27%'}\n",
      "Val Loss: 1.011943, Val Acc: 79.78%, Val-Class-Acc: {0: '67.39%', 1: '76.72%', 2: '82.64%', 3: '85.25%', 4: '80.00%', 5: '84.43%'}, LR: 0.000810\n",
      "Epoch 65/200, Train Loss: 0.775421, Train-Class-Acc: {0: '77.11%', 1: '77.19%', 2: '80.42%', 3: '85.04%', 4: '73.42%', 5: '88.71%'}\n",
      "Val Loss: 1.256090, Val Acc: 81.26%, Val-Class-Acc: {0: '73.37%', 1: '75.52%', 2: '83.33%', 3: '86.07%', 4: '77.50%', 5: '87.43%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_48.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_65.pth\n",
      "Epoch 66/200, Train Loss: 0.855991, Train-Class-Acc: {0: '76.29%', 1: '74.64%', 2: '81.98%', 3: '85.55%', 4: '69.62%', 5: '87.82%'}\n",
      "Val Loss: 1.120262, Val Acc: 80.87%, Val-Class-Acc: {0: '71.20%', 1: '78.81%', 2: '82.64%', 3: '84.02%', 4: '75.00%', 5: '85.93%'}, LR: 0.000810\n",
      "Epoch 67/200, Train Loss: 0.772531, Train-Class-Acc: {0: '76.43%', 1: '75.32%', 2: '80.94%', 3: '84.63%', 4: '75.95%', 5: '89.61%'}\n",
      "Val Loss: 1.833858, Val Acc: 80.56%, Val-Class-Acc: {0: '64.67%', 1: '81.49%', 2: '76.39%', 3: '81.97%', 4: '77.50%', 5: '89.52%'}, LR: 0.000810\n",
      "Epoch 68/200, Train Loss: 0.813143, Train-Class-Acc: {0: '75.75%', 1: '76.44%', 2: '83.02%', 3: '86.37%', 4: '74.05%', 5: '88.49%'}\n",
      "Val Loss: 1.237385, Val Acc: 79.86%, Val-Class-Acc: {0: '67.39%', 1: '80.60%', 2: '80.56%', 3: '85.66%', 4: '77.50%', 5: '81.74%'}, LR: 0.000729\n",
      "Epoch 69/200, Train Loss: 0.824844, Train-Class-Acc: {0: '77.52%', 1: '77.49%', 2: '81.98%', 3: '85.76%', 4: '75.32%', 5: '87.82%'}\n",
      "Val Loss: 2.287781, Val Acc: 80.56%, Val-Class-Acc: {0: '65.22%', 1: '80.30%', 2: '72.92%', 3: '82.38%', 4: '80.00%', 5: '91.32%'}, LR: 0.000729\n",
      "Epoch 70/200, Train Loss: 0.713077, Train-Class-Acc: {0: '77.25%', 1: '77.34%', 2: '80.42%', 3: '86.17%', 4: '67.72%', 5: '88.79%'}\n",
      "Val Loss: 0.980464, Val Acc: 81.34%, Val-Class-Acc: {0: '76.09%', 1: '77.31%', 2: '81.94%', 3: '87.30%', 4: '77.50%', 5: '84.13%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_61.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_70.pth\n",
      "Epoch 71/200, Train Loss: 0.796317, Train-Class-Acc: {0: '75.61%', 1: '76.96%', 2: '81.98%', 3: '85.14%', 4: '77.22%', 5: '89.31%'}\n",
      "Val Loss: 1.400335, Val Acc: 81.58%, Val-Class-Acc: {0: '66.85%', 1: '79.70%', 2: '79.17%', 3: '84.84%', 4: '80.00%', 5: '90.42%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_54.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_71.pth\n",
      "Epoch 72/200, Train Loss: 0.685873, Train-Class-Acc: {0: '74.93%', 1: '77.41%', 2: '81.46%', 3: '85.86%', 4: '75.32%', 5: '89.09%'}\n",
      "Val Loss: 1.691109, Val Acc: 81.03%, Val-Class-Acc: {0: '72.83%', 1: '77.61%', 2: '76.39%', 3: '87.30%', 4: '80.00%', 5: '86.53%'}, LR: 0.000729\n",
      "Epoch 73/200, Train Loss: 0.890372, Train-Class-Acc: {0: '76.70%', 1: '77.71%', 2: '83.19%', 3: '86.07%', 4: '73.42%', 5: '88.19%'}\n",
      "Val Loss: 0.756798, Val Acc: 81.11%, Val-Class-Acc: {0: '63.04%', 1: '81.79%', 2: '82.64%', 3: '83.20%', 4: '77.50%', 5: '88.62%'}, LR: 0.000729\n",
      "Epoch 74/200, Train Loss: 0.718128, Train-Class-Acc: {0: '76.70%', 1: '77.04%', 2: '81.28%', 3: '86.07%', 4: '74.05%', 5: '89.09%'}\n",
      "Val Loss: 1.198576, Val Acc: 81.26%, Val-Class-Acc: {0: '64.13%', 1: '79.40%', 2: '77.78%', 3: '84.02%', 4: '80.00%', 5: '92.22%'}, LR: 0.000729\n",
      "Epoch 75/200, Train Loss: 0.683458, Train-Class-Acc: {0: '77.11%', 1: '77.86%', 2: '82.32%', 3: '86.27%', 4: '70.25%', 5: '89.16%'}\n",
      "Val Loss: 3.064061, Val Acc: 81.50%, Val-Class-Acc: {0: '68.48%', 1: '79.10%', 2: '84.03%', 3: '81.56%', 4: '77.50%', 5: '90.42%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_65.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_75.pth\n",
      "Epoch 76/200, Train Loss: 0.785546, Train-Class-Acc: {0: '75.34%', 1: '76.59%', 2: '79.90%', 3: '85.55%', 4: '74.05%', 5: '88.19%'}\n",
      "Val Loss: 1.749553, Val Acc: 80.48%, Val-Class-Acc: {0: '75.00%', 1: '76.42%', 2: '77.78%', 3: '79.92%', 4: '77.50%', 5: '89.52%'}, LR: 0.000729\n",
      "Epoch 77/200, Train Loss: 0.673718, Train-Class-Acc: {0: '76.16%', 1: '77.04%', 2: '81.46%', 3: '85.14%', 4: '75.32%', 5: '88.94%'}\n",
      "Val Loss: 1.093867, Val Acc: 81.11%, Val-Class-Acc: {0: '73.37%', 1: '75.82%', 2: '81.25%', 3: '86.48%', 4: '77.50%', 5: '87.13%'}, LR: 0.000729\n",
      "Epoch 78/200, Train Loss: 0.811026, Train-Class-Acc: {0: '77.38%', 1: '80.03%', 2: '82.67%', 3: '87.19%', 4: '72.15%', 5: '89.91%'}\n",
      "Val Loss: 0.921772, Val Acc: 80.48%, Val-Class-Acc: {0: '63.59%', 1: '80.60%', 2: '79.17%', 3: '84.02%', 4: '77.50%', 5: '88.02%'}, LR: 0.000729\n",
      "Epoch 79/200, Train Loss: 0.860074, Train-Class-Acc: {0: '77.38%', 1: '76.59%', 2: '83.54%', 3: '85.66%', 4: '77.22%', 5: '89.24%'}\n",
      "Val Loss: 1.249571, Val Acc: 81.73%, Val-Class-Acc: {0: '69.02%', 1: '81.19%', 2: '79.17%', 3: '84.43%', 4: '77.50%', 5: '88.92%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_70.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_79.pth\n",
      "Epoch 80/200, Train Loss: 0.727597, Train-Class-Acc: {0: '78.20%', 1: '79.66%', 2: '81.63%', 3: '87.70%', 4: '75.32%', 5: '88.94%'}\n",
      "Val Loss: 3.443291, Val Acc: 79.86%, Val-Class-Acc: {0: '69.57%', 1: '77.01%', 2: '72.92%', 3: '76.64%', 4: '80.00%', 5: '93.71%'}, LR: 0.000729\n",
      "Epoch 81/200, Train Loss: 0.677128, Train-Class-Acc: {0: '78.61%', 1: '77.94%', 2: '82.84%', 3: '86.89%', 4: '81.65%', 5: '90.58%'}\n",
      "Val Loss: 1.836085, Val Acc: 81.11%, Val-Class-Acc: {0: '64.13%', 1: '81.19%', 2: '75.00%', 3: '85.66%', 4: '77.50%', 5: '90.12%'}, LR: 0.000729\n",
      "Epoch 82/200, Train Loss: 0.693604, Train-Class-Acc: {0: '78.07%', 1: '79.51%', 2: '82.84%', 3: '88.32%', 4: '75.32%', 5: '89.24%'}\n",
      "Val Loss: 2.041498, Val Acc: 81.11%, Val-Class-Acc: {0: '69.02%', 1: '80.00%', 2: '77.78%', 3: '81.56%', 4: '77.50%', 5: '90.42%'}, LR: 0.000729\n",
      "Epoch 83/200, Train Loss: 0.750744, Train-Class-Acc: {0: '79.02%', 1: '79.13%', 2: '83.71%', 3: '87.30%', 4: '77.22%', 5: '90.13%'}\n",
      "Val Loss: 1.387328, Val Acc: 81.26%, Val-Class-Acc: {0: '63.04%', 1: '81.79%', 2: '81.94%', 3: '84.84%', 4: '80.00%', 5: '88.02%'}, LR: 0.000729\n",
      "Epoch 84/200, Train Loss: 0.678970, Train-Class-Acc: {0: '78.47%', 1: '78.91%', 2: '83.54%', 3: '86.58%', 4: '75.32%', 5: '90.21%'}\n",
      "Val Loss: 2.763485, Val Acc: 81.89%, Val-Class-Acc: {0: '67.93%', 1: '79.70%', 2: '85.42%', 3: '85.66%', 4: '80.00%', 5: '87.72%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_58.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_84.pth\n",
      "Epoch 85/200, Train Loss: 0.552449, Train-Class-Acc: {0: '77.66%', 1: '80.18%', 2: '83.88%', 3: '87.81%', 4: '77.85%', 5: '89.84%'}\n",
      "Val Loss: 2.261698, Val Acc: 81.34%, Val-Class-Acc: {0: '73.37%', 1: '76.42%', 2: '81.25%', 3: '79.92%', 4: '80.00%', 5: '91.92%'}, LR: 0.000656\n",
      "Epoch 86/200, Train Loss: 0.676281, Train-Class-Acc: {0: '80.11%', 1: '78.76%', 2: '84.75%', 3: '88.01%', 4: '79.11%', 5: '90.43%'}\n",
      "Val Loss: 1.229803, Val Acc: 81.34%, Val-Class-Acc: {0: '67.93%', 1: '79.70%', 2: '79.17%', 3: '84.02%', 4: '80.00%', 5: '89.52%'}, LR: 0.000656\n",
      "Epoch 87/200, Train Loss: 0.618363, Train-Class-Acc: {0: '76.43%', 1: '79.96%', 2: '85.10%', 3: '87.40%', 4: '78.48%', 5: '90.58%'}\n",
      "Val Loss: 1.928442, Val Acc: 80.48%, Val-Class-Acc: {0: '73.37%', 1: '77.01%', 2: '78.47%', 3: '87.70%', 4: '80.00%', 5: '83.53%'}, LR: 0.000656\n",
      "Epoch 88/200, Train Loss: 0.663480, Train-Class-Acc: {0: '78.47%', 1: '80.40%', 2: '83.19%', 3: '89.14%', 4: '77.22%', 5: '89.61%'}\n",
      "Val Loss: 1.113929, Val Acc: 80.95%, Val-Class-Acc: {0: '57.61%', 1: '82.39%', 2: '83.33%', 3: '85.25%', 4: '80.00%', 5: '88.32%'}, LR: 0.000656\n",
      "Epoch 89/200, Train Loss: 0.603179, Train-Class-Acc: {0: '77.93%', 1: '79.58%', 2: '85.79%', 3: '88.22%', 4: '78.48%', 5: '90.88%'}\n",
      "Val Loss: 1.672989, Val Acc: 81.97%, Val-Class-Acc: {0: '75.00%', 1: '75.22%', 2: '84.72%', 3: '86.48%', 4: '80.00%', 5: '88.32%'}, LR: 0.000656\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_62.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_89.pth\n",
      "Epoch 90/200, Train Loss: 0.852387, Train-Class-Acc: {0: '77.79%', 1: '76.51%', 2: '82.50%', 3: '86.37%', 4: '75.32%', 5: '90.06%'}\n",
      "Val Loss: 1.385034, Val Acc: 80.56%, Val-Class-Acc: {0: '73.37%', 1: '75.82%', 2: '77.78%', 3: '84.43%', 4: '82.50%', 5: '87.43%'}, LR: 0.000656\n",
      "Epoch 91/200, Train Loss: 0.707042, Train-Class-Acc: {0: '79.02%', 1: '78.01%', 2: '85.10%', 3: '86.37%', 4: '78.48%', 5: '88.86%'}\n",
      "Val Loss: 1.264512, Val Acc: 81.73%, Val-Class-Acc: {0: '71.20%', 1: '77.61%', 2: '82.64%', 3: '87.30%', 4: '80.00%', 5: '87.43%'}, LR: 0.000656\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_75.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_91.pth\n",
      "Epoch 92/200, Train Loss: 0.580113, Train-Class-Acc: {0: '78.61%', 1: '80.63%', 2: '82.32%', 3: '88.32%', 4: '80.38%', 5: '90.81%'}\n",
      "Val Loss: 1.309104, Val Acc: 81.81%, Val-Class-Acc: {0: '69.57%', 1: '79.40%', 2: '82.64%', 3: '83.20%', 4: '80.00%', 5: '89.82%'}, LR: 0.000656\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_71.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_92.pth\n",
      "Epoch 93/200, Train Loss: 0.521327, Train-Class-Acc: {0: '80.38%', 1: '79.96%', 2: '85.96%', 3: '88.73%', 4: '81.65%', 5: '91.03%'}\n",
      "Val Loss: 1.584979, Val Acc: 81.26%, Val-Class-Acc: {0: '67.39%', 1: '79.40%', 2: '81.25%', 3: '83.61%', 4: '77.50%', 5: '89.52%'}, LR: 0.000656\n",
      "Epoch 94/200, Train Loss: 0.744450, Train-Class-Acc: {0: '79.84%', 1: '80.40%', 2: '84.92%', 3: '88.22%', 4: '80.38%', 5: '90.43%'}\n",
      "Val Loss: 1.713697, Val Acc: 82.12%, Val-Class-Acc: {0: '73.91%', 1: '78.21%', 2: '81.94%', 3: '82.38%', 4: '80.00%', 5: '90.72%'}, LR: 0.000656\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_79.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_94.pth\n",
      "Epoch 95/200, Train Loss: 0.511877, Train-Class-Acc: {0: '80.38%', 1: '80.10%', 2: '85.10%', 3: '89.65%', 4: '79.75%', 5: '89.91%'}\n",
      "Val Loss: 1.505561, Val Acc: 82.28%, Val-Class-Acc: {0: '75.54%', 1: '77.61%', 2: '82.64%', 3: '86.07%', 4: '80.00%', 5: '88.02%'}, LR: 0.000656\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_91.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_95.pth\n",
      "Epoch 96/200, Train Loss: 0.575867, Train-Class-Acc: {0: '78.47%', 1: '81.53%', 2: '86.31%', 3: '89.24%', 4: '82.91%', 5: '91.18%'}\n",
      "Val Loss: 1.849652, Val Acc: 82.75%, Val-Class-Acc: {0: '68.48%', 1: '78.81%', 2: '82.64%', 3: '85.66%', 4: '82.50%', 5: '92.51%'}, LR: 0.000590\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_92.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_96.pth\n",
      "Epoch 97/200, Train Loss: 0.524945, Train-Class-Acc: {0: '81.06%', 1: '80.18%', 2: '85.79%', 3: '89.14%', 4: '79.75%', 5: '91.26%'}\n",
      "Val Loss: 1.675047, Val Acc: 82.44%, Val-Class-Acc: {0: '68.48%', 1: '81.49%', 2: '84.03%', 3: '84.84%', 4: '80.00%', 5: '88.92%'}, LR: 0.000590\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_84.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_97.pth\n",
      "Epoch 98/200, Train Loss: 0.524528, Train-Class-Acc: {0: '82.83%', 1: '82.35%', 2: '85.44%', 3: '89.24%', 4: '81.01%', 5: '91.26%'}\n",
      "Val Loss: 1.539644, Val Acc: 82.05%, Val-Class-Acc: {0: '69.02%', 1: '79.70%', 2: '84.72%', 3: '84.84%', 4: '80.00%', 5: '88.62%'}, LR: 0.000590\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_89.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_98.pth\n",
      "Epoch 99/200, Train Loss: 0.464358, Train-Class-Acc: {0: '78.88%', 1: '79.88%', 2: '86.66%', 3: '89.45%', 4: '82.28%', 5: '91.48%'}\n",
      "Val Loss: 1.375537, Val Acc: 82.36%, Val-Class-Acc: {0: '74.46%', 1: '76.12%', 2: '81.94%', 3: '86.89%', 4: '80.00%', 5: '90.12%'}, LR: 0.000590\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_98.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_99.pth\n",
      "Epoch 100/200, Train Loss: 0.632261, Train-Class-Acc: {0: '81.61%', 1: '81.60%', 2: '87.52%', 3: '89.96%', 4: '74.05%', 5: '91.70%'}\n",
      "Val Loss: 0.901062, Val Acc: 82.12%, Val-Class-Acc: {0: '70.65%', 1: '78.81%', 2: '84.72%', 3: '86.89%', 4: '82.50%', 5: '87.13%'}, LR: 0.000590\n",
      "Epoch 101/200, Train Loss: 0.485428, Train-Class-Acc: {0: '82.29%', 1: '81.82%', 2: '86.48%', 3: '89.24%', 4: '79.75%', 5: '92.38%'}\n",
      "Val Loss: 1.235668, Val Acc: 81.81%, Val-Class-Acc: {0: '72.28%', 1: '77.01%', 2: '85.42%', 3: '84.84%', 4: '80.00%', 5: '88.32%'}, LR: 0.000590\n",
      "Epoch 102/200, Train Loss: 0.448323, Train-Class-Acc: {0: '82.29%', 1: '81.90%', 2: '87.87%', 3: '90.88%', 4: '80.38%', 5: '91.78%'}\n",
      "Val Loss: 1.560225, Val Acc: 81.89%, Val-Class-Acc: {0: '71.20%', 1: '78.81%', 2: '81.25%', 3: '84.02%', 4: '80.00%', 5: '89.82%'}, LR: 0.000590\n",
      "Epoch 103/200, Train Loss: 0.540378, Train-Class-Acc: {0: '81.20%', 1: '82.72%', 2: '84.06%', 3: '88.83%', 4: '81.01%', 5: '90.88%'}\n",
      "Val Loss: 1.702453, Val Acc: 81.42%, Val-Class-Acc: {0: '67.39%', 1: '77.91%', 2: '84.03%', 3: '78.69%', 4: '80.00%', 5: '93.71%'}, LR: 0.000590\n",
      "Epoch 104/200, Train Loss: 0.589104, Train-Class-Acc: {0: '80.65%', 1: '82.35%', 2: '85.79%', 3: '87.91%', 4: '82.28%', 5: '92.08%'}\n",
      "Val Loss: 1.820353, Val Acc: 81.50%, Val-Class-Acc: {0: '70.11%', 1: '78.21%', 2: '82.64%', 3: '79.10%', 4: '82.50%', 5: '92.22%'}, LR: 0.000590\n",
      "Epoch 105/200, Train Loss: 0.474267, Train-Class-Acc: {0: '81.20%', 1: '83.32%', 2: '86.66%', 3: '89.04%', 4: '81.01%', 5: '92.30%'}\n",
      "Val Loss: 1.721732, Val Acc: 82.05%, Val-Class-Acc: {0: '70.65%', 1: '76.72%', 2: '85.42%', 3: '86.07%', 4: '82.50%', 5: '89.22%'}, LR: 0.000590\n",
      "Epoch 106/200, Train Loss: 0.409942, Train-Class-Acc: {0: '81.47%', 1: '81.97%', 2: '87.35%', 3: '89.34%', 4: '84.81%', 5: '92.60%'}\n",
      "Val Loss: 1.722451, Val Acc: 82.05%, Val-Class-Acc: {0: '72.83%', 1: '76.72%', 2: '82.64%', 3: '86.89%', 4: '80.00%', 5: '88.92%'}, LR: 0.000590\n",
      "Epoch 107/200, Train Loss: 0.468937, Train-Class-Acc: {0: '81.06%', 1: '80.78%', 2: '87.35%', 3: '88.83%', 4: '79.75%', 5: '91.11%'}\n",
      "Val Loss: 1.808302, Val Acc: 82.67%, Val-Class-Acc: {0: '72.83%', 1: '80.30%', 2: '81.94%', 3: '83.20%', 4: '80.00%', 5: '90.72%'}, LR: 0.000531\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_94.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_107.pth\n",
      "Epoch 108/200, Train Loss: 0.426769, Train-Class-Acc: {0: '82.15%', 1: '82.65%', 2: '87.52%', 3: '91.09%', 4: '80.38%', 5: '92.23%'}\n",
      "Val Loss: 1.384153, Val Acc: 83.06%, Val-Class-Acc: {0: '77.17%', 1: '78.21%', 2: '81.25%', 3: '85.66%', 4: '82.50%', 5: '90.12%'}, LR: 0.000531\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_95.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_108.pth\n",
      "Epoch 109/200, Train Loss: 0.420611, Train-Class-Acc: {0: '82.56%', 1: '82.57%', 2: '88.04%', 3: '90.16%', 4: '81.65%', 5: '92.08%'}\n",
      "Val Loss: 1.359795, Val Acc: 82.44%, Val-Class-Acc: {0: '70.11%', 1: '79.40%', 2: '82.64%', 3: '87.30%', 4: '80.00%', 5: '88.92%'}, LR: 0.000531\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_99.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_109.pth\n",
      "Epoch 110/200, Train Loss: 0.500027, Train-Class-Acc: {0: '82.70%', 1: '83.40%', 2: '86.31%', 3: '90.27%', 4: '83.54%', 5: '92.60%'}\n",
      "Val Loss: 1.980555, Val Acc: 81.03%, Val-Class-Acc: {0: '63.04%', 1: '81.49%', 2: '79.86%', 3: '84.43%', 4: '80.00%', 5: '88.62%'}, LR: 0.000531\n",
      "Epoch 111/200, Train Loss: 0.461486, Train-Class-Acc: {0: '82.56%', 1: '83.02%', 2: '89.43%', 3: '89.96%', 4: '82.28%', 5: '92.00%'}\n",
      "Val Loss: 1.168724, Val Acc: 82.28%, Val-Class-Acc: {0: '70.11%', 1: '78.21%', 2: '79.17%', 3: '90.57%', 4: '82.50%', 5: '88.32%'}, LR: 0.000531\n",
      "Epoch 112/200, Train Loss: 0.407600, Train-Class-Acc: {0: '82.83%', 1: '83.17%', 2: '87.87%', 3: '91.19%', 4: '83.54%', 5: '92.60%'}\n",
      "Val Loss: 1.762782, Val Acc: 82.90%, Val-Class-Acc: {0: '77.17%', 1: '74.33%', 2: '82.64%', 3: '87.30%', 4: '82.50%', 5: '91.62%'}, LR: 0.000531\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_97.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_112.pth\n",
      "Epoch 113/200, Train Loss: 0.431059, Train-Class-Acc: {0: '82.56%', 1: '82.72%', 2: '89.08%', 3: '89.86%', 4: '83.54%', 5: '91.93%'}\n",
      "Val Loss: 1.157783, Val Acc: 82.51%, Val-Class-Acc: {0: '70.11%', 1: '81.19%', 2: '83.33%', 3: '85.25%', 4: '82.50%', 5: '88.32%'}, LR: 0.000531\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_109.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_113.pth\n",
      "Epoch 114/200, Train Loss: 0.500134, Train-Class-Acc: {0: '83.79%', 1: '83.84%', 2: '85.96%', 3: '90.57%', 4: '81.01%', 5: '92.00%'}\n",
      "Val Loss: 1.737468, Val Acc: 82.51%, Val-Class-Acc: {0: '73.37%', 1: '79.40%', 2: '85.42%', 3: '86.48%', 4: '82.50%', 5: '86.53%'}, LR: 0.000531\n",
      "Epoch 115/200, Train Loss: 0.544094, Train-Class-Acc: {0: '81.74%', 1: '83.25%', 2: '88.39%', 3: '90.98%', 4: '81.65%', 5: '91.78%'}\n",
      "Val Loss: 1.070459, Val Acc: 82.59%, Val-Class-Acc: {0: '76.09%', 1: '77.01%', 2: '80.56%', 3: '88.93%', 4: '82.50%', 5: '88.02%'}, LR: 0.000531\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_113.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_115.pth\n",
      "Epoch 116/200, Train Loss: 0.438060, Train-Class-Acc: {0: '85.56%', 1: '84.82%', 2: '86.66%', 3: '90.78%', 4: '87.34%', 5: '92.30%'}\n",
      "Val Loss: 1.495693, Val Acc: 82.67%, Val-Class-Acc: {0: '69.02%', 1: '79.70%', 2: '85.42%', 3: '87.30%', 4: '82.50%', 5: '88.62%'}, LR: 0.000531\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_115.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_116.pth\n",
      "Epoch 117/200, Train Loss: 0.446037, Train-Class-Acc: {0: '82.15%', 1: '82.12%', 2: '87.87%', 3: '90.57%', 4: '79.75%', 5: '91.33%'}\n",
      "Val Loss: 1.646679, Val Acc: 82.75%, Val-Class-Acc: {0: '78.80%', 1: '77.61%', 2: '81.25%', 3: '85.66%', 4: '82.50%', 5: '88.62%'}, LR: 0.000531\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_107.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_117.pth\n",
      "Epoch 118/200, Train Loss: 0.532610, Train-Class-Acc: {0: '84.33%', 1: '83.55%', 2: '85.96%', 3: '90.27%', 4: '81.01%', 5: '92.08%'}\n",
      "Val Loss: 1.744362, Val Acc: 82.83%, Val-Class-Acc: {0: '71.20%', 1: '80.00%', 2: '86.11%', 3: '83.20%', 4: '82.50%', 5: '90.42%'}, LR: 0.000478\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_116.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_118.pth\n",
      "Epoch 119/200, Train Loss: 0.389856, Train-Class-Acc: {0: '83.79%', 1: '82.72%', 2: '88.73%', 3: '90.98%', 4: '82.91%', 5: '91.41%'}\n",
      "Val Loss: 1.336604, Val Acc: 83.06%, Val-Class-Acc: {0: '73.91%', 1: '77.91%', 2: '82.64%', 3: '88.11%', 4: '82.50%', 5: '89.82%'}, LR: 0.000478\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_96.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_119.pth\n",
      "Epoch 120/200, Train Loss: 0.727561, Train-Class-Acc: {0: '83.38%', 1: '83.77%', 2: '83.88%', 3: '88.01%', 4: '84.81%', 5: '91.55%'}\n",
      "Val Loss: 2.193581, Val Acc: 82.67%, Val-Class-Acc: {0: '73.37%', 1: '77.91%', 2: '81.94%', 3: '86.89%', 4: '80.00%', 5: '90.12%'}, LR: 0.000478\n",
      "Epoch 121/200, Train Loss: 0.561821, Train-Class-Acc: {0: '83.38%', 1: '83.77%', 2: '86.66%', 3: '91.70%', 4: '83.54%', 5: '91.78%'}\n",
      "Val Loss: 2.038575, Val Acc: 82.51%, Val-Class-Acc: {0: '75.00%', 1: '79.10%', 2: '79.86%', 3: '86.07%', 4: '82.50%', 5: '88.62%'}, LR: 0.000478\n",
      "Epoch 122/200, Train Loss: 0.556551, Train-Class-Acc: {0: '85.29%', 1: '83.99%', 2: '90.12%', 3: '90.88%', 4: '86.08%', 5: '93.12%'}\n",
      "Val Loss: 1.409285, Val Acc: 83.29%, Val-Class-Acc: {0: '73.37%', 1: '79.70%', 2: '81.94%', 3: '88.11%', 4: '80.00%', 5: '89.82%'}, LR: 0.000478\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_117.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_122.pth\n",
      "Epoch 123/200, Train Loss: 0.540222, Train-Class-Acc: {0: '83.11%', 1: '82.95%', 2: '86.48%', 3: '90.98%', 4: '81.01%', 5: '92.08%'}\n",
      "Val Loss: 1.653509, Val Acc: 82.90%, Val-Class-Acc: {0: '67.39%', 1: '81.79%', 2: '81.94%', 3: '84.43%', 4: '80.00%', 5: '92.22%'}, LR: 0.000478\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_118.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_123.pth\n",
      "Epoch 124/200, Train Loss: 0.457590, Train-Class-Acc: {0: '83.65%', 1: '83.62%', 2: '88.73%', 3: '90.37%', 4: '84.18%', 5: '93.12%'}\n",
      "Val Loss: 1.562502, Val Acc: 83.22%, Val-Class-Acc: {0: '73.37%', 1: '79.70%', 2: '84.72%', 3: '87.70%', 4: '82.50%', 5: '88.32%'}, LR: 0.000478\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_112.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_124.pth\n",
      "Epoch 125/200, Train Loss: 0.429524, Train-Class-Acc: {0: '83.92%', 1: '82.65%', 2: '87.87%', 3: '91.29%', 4: '85.44%', 5: '91.63%'}\n",
      "Val Loss: 1.291317, Val Acc: 82.90%, Val-Class-Acc: {0: '69.02%', 1: '80.60%', 2: '81.94%', 3: '88.93%', 4: '82.50%', 5: '88.92%'}, LR: 0.000478\n",
      "Epoch 126/200, Train Loss: 0.375787, Train-Class-Acc: {0: '85.97%', 1: '86.16%', 2: '88.04%', 3: '90.88%', 4: '89.24%', 5: '93.27%'}\n",
      "Val Loss: 1.439932, Val Acc: 82.67%, Val-Class-Acc: {0: '79.89%', 1: '75.22%', 2: '82.64%', 3: '87.30%', 4: '80.00%', 5: '88.62%'}, LR: 0.000478\n",
      "Epoch 127/200, Train Loss: 0.392128, Train-Class-Acc: {0: '84.74%', 1: '84.37%', 2: '89.08%', 3: '90.68%', 4: '86.71%', 5: '92.15%'}\n",
      "Val Loss: 1.401035, Val Acc: 83.22%, Val-Class-Acc: {0: '71.20%', 1: '80.90%', 2: '84.03%', 3: '86.89%', 4: '82.50%', 5: '89.22%'}, LR: 0.000478\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_123.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_127.pth\n",
      "Epoch 128/200, Train Loss: 0.323474, Train-Class-Acc: {0: '87.06%', 1: '86.69%', 2: '89.08%', 3: '92.01%', 4: '83.54%', 5: '93.72%'}\n",
      "Val Loss: 0.987490, Val Acc: 82.28%, Val-Class-Acc: {0: '77.72%', 1: '78.21%', 2: '84.72%', 3: '87.30%', 4: '82.50%', 5: '84.13%'}, LR: 0.000478\n",
      "Epoch 129/200, Train Loss: 0.440994, Train-Class-Acc: {0: '85.15%', 1: '84.89%', 2: '88.04%', 3: '91.50%', 4: '84.18%', 5: '92.68%'}\n",
      "Val Loss: 1.292913, Val Acc: 83.53%, Val-Class-Acc: {0: '74.46%', 1: '80.30%', 2: '82.64%', 3: '86.48%', 4: '82.50%', 5: '90.12%'}, LR: 0.000430\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_108.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_129.pth\n",
      "Epoch 130/200, Train Loss: 0.381843, Train-Class-Acc: {0: '86.10%', 1: '85.64%', 2: '87.69%', 3: '91.91%', 4: '86.71%', 5: '93.12%'}\n",
      "Val Loss: 2.019626, Val Acc: 82.59%, Val-Class-Acc: {0: '67.93%', 1: '81.79%', 2: '84.03%', 3: '86.07%', 4: '82.50%', 5: '88.32%'}, LR: 0.000430\n",
      "Epoch 131/200, Train Loss: 0.409399, Train-Class-Acc: {0: '85.83%', 1: '84.37%', 2: '89.60%', 3: '91.60%', 4: '86.08%', 5: '93.57%'}\n",
      "Val Loss: 1.334420, Val Acc: 83.22%, Val-Class-Acc: {0: '76.09%', 1: '78.21%', 2: '81.25%', 3: '86.89%', 4: '82.50%', 5: '90.42%'}, LR: 0.000430\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_119.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_131.pth\n",
      "Epoch 132/200, Train Loss: 0.448324, Train-Class-Acc: {0: '85.56%', 1: '85.04%', 2: '87.18%', 3: '91.19%', 4: '82.28%', 5: '92.15%'}\n",
      "Val Loss: 1.614449, Val Acc: 82.90%, Val-Class-Acc: {0: '72.83%', 1: '80.60%', 2: '81.25%', 3: '86.89%', 4: '80.00%', 5: '88.92%'}, LR: 0.000430\n",
      "Epoch 133/200, Train Loss: 0.373939, Train-Class-Acc: {0: '84.60%', 1: '84.74%', 2: '89.08%', 3: '91.60%', 4: '86.08%', 5: '93.12%'}\n",
      "Val Loss: 2.023801, Val Acc: 83.45%, Val-Class-Acc: {0: '76.63%', 1: '78.51%', 2: '80.56%', 3: '84.84%', 4: '82.50%', 5: '92.51%'}, LR: 0.000430\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_124.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_133.pth\n",
      "Epoch 134/200, Train Loss: 0.331437, Train-Class-Acc: {0: '86.38%', 1: '86.61%', 2: '87.35%', 3: '92.11%', 4: '88.61%', 5: '93.72%'}\n",
      "Val Loss: 2.303766, Val Acc: 83.29%, Val-Class-Acc: {0: '79.35%', 1: '75.82%', 2: '81.25%', 3: '87.30%', 4: '82.50%', 5: '91.02%'}, LR: 0.000430\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_127.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_134.pth\n",
      "Epoch 135/200, Train Loss: 0.377748, Train-Class-Acc: {0: '85.01%', 1: '86.69%', 2: '88.91%', 3: '92.21%', 4: '86.71%', 5: '94.10%'}\n",
      "Val Loss: 1.699140, Val Acc: 82.98%, Val-Class-Acc: {0: '79.35%', 1: '74.63%', 2: '83.33%', 3: '87.70%', 4: '82.50%', 5: '89.82%'}, LR: 0.000430\n",
      "Epoch 136/200, Train Loss: 0.321479, Train-Class-Acc: {0: '86.78%', 1: '86.61%', 2: '90.29%', 3: '92.93%', 4: '86.71%', 5: '93.57%'}\n",
      "Val Loss: 1.043791, Val Acc: 83.22%, Val-Class-Acc: {0: '76.09%', 1: '80.00%', 2: '81.94%', 3: '87.30%', 4: '82.50%', 5: '88.02%'}, LR: 0.000430\n",
      "Epoch 137/200, Train Loss: 0.565397, Train-Class-Acc: {0: '86.65%', 1: '84.89%', 2: '88.39%', 3: '92.01%', 4: '81.65%', 5: '92.90%'}\n",
      "Val Loss: 0.905868, Val Acc: 83.45%, Val-Class-Acc: {0: '78.80%', 1: '78.51%', 2: '84.03%', 3: '84.84%', 4: '82.50%', 5: '89.82%'}, LR: 0.000430\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_131.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_137.pth\n",
      "Epoch 138/200, Train Loss: 0.353406, Train-Class-Acc: {0: '83.65%', 1: '86.54%', 2: '89.25%', 3: '91.50%', 4: '84.81%', 5: '93.72%'}\n",
      "Val Loss: 1.806729, Val Acc: 83.14%, Val-Class-Acc: {0: '72.83%', 1: '82.09%', 2: '81.94%', 3: '84.84%', 4: '82.50%', 5: '89.22%'}, LR: 0.000430\n",
      "Epoch 139/200, Train Loss: 0.365098, Train-Class-Acc: {0: '86.51%', 1: '86.46%', 2: '88.21%', 3: '91.39%', 4: '88.61%', 5: '92.75%'}\n",
      "Val Loss: 1.378952, Val Acc: 83.61%, Val-Class-Acc: {0: '76.09%', 1: '77.91%', 2: '83.33%', 3: '87.70%', 4: '82.50%', 5: '90.72%'}, LR: 0.000430\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_122.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_139.pth\n",
      "Epoch 140/200, Train Loss: 0.318152, Train-Class-Acc: {0: '84.74%', 1: '85.86%', 2: '89.08%', 3: '93.34%', 4: '84.81%', 5: '93.05%'}\n",
      "Val Loss: 1.419910, Val Acc: 83.45%, Val-Class-Acc: {0: '73.91%', 1: '79.40%', 2: '82.64%', 3: '87.30%', 4: '82.50%', 5: '90.42%'}, LR: 0.000387\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_134.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_140.pth\n",
      "Epoch 141/200, Train Loss: 0.365578, Train-Class-Acc: {0: '87.06%', 1: '84.67%', 2: '91.33%', 3: '93.03%', 4: '85.44%', 5: '94.47%'}\n",
      "Val Loss: 0.996276, Val Acc: 82.90%, Val-Class-Acc: {0: '75.00%', 1: '79.40%', 2: '80.56%', 3: '88.52%', 4: '82.50%', 5: '87.72%'}, LR: 0.000387\n",
      "Epoch 142/200, Train Loss: 0.355655, Train-Class-Acc: {0: '87.33%', 1: '86.09%', 2: '89.60%', 3: '91.60%', 4: '85.44%', 5: '93.95%'}\n",
      "Val Loss: 1.689267, Val Acc: 82.83%, Val-Class-Acc: {0: '76.63%', 1: '78.21%', 2: '82.64%', 3: '88.11%', 4: '80.00%', 5: '87.43%'}, LR: 0.000387\n",
      "Epoch 143/200, Train Loss: 0.317337, Train-Class-Acc: {0: '87.60%', 1: '87.06%', 2: '90.81%', 3: '92.62%', 4: '88.61%', 5: '93.57%'}\n",
      "Val Loss: 1.538045, Val Acc: 83.84%, Val-Class-Acc: {0: '78.26%', 1: '79.40%', 2: '81.25%', 3: '86.89%', 4: '82.50%', 5: '90.42%'}, LR: 0.000387\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_133.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_143.pth\n",
      "Epoch 144/200, Train Loss: 0.341843, Train-Class-Acc: {0: '85.97%', 1: '85.34%', 2: '90.29%', 3: '92.83%', 4: '87.97%', 5: '93.80%'}\n",
      "Val Loss: 1.465673, Val Acc: 83.84%, Val-Class-Acc: {0: '77.17%', 1: '79.70%', 2: '81.25%', 3: '86.48%', 4: '80.00%', 5: '91.32%'}, LR: 0.000387\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_137.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_144.pth\n",
      "Epoch 145/200, Train Loss: 0.415251, Train-Class-Acc: {0: '86.65%', 1: '86.91%', 2: '90.99%', 3: '93.24%', 4: '86.71%', 5: '93.72%'}\n",
      "Val Loss: 1.547390, Val Acc: 83.29%, Val-Class-Acc: {0: '77.17%', 1: '77.01%', 2: '81.25%', 3: '85.25%', 4: '82.50%', 5: '92.51%'}, LR: 0.000387\n",
      "Epoch 146/200, Train Loss: 0.307701, Train-Class-Acc: {0: '85.97%', 1: '84.67%', 2: '89.08%', 3: '93.03%', 4: '82.91%', 5: '94.17%'}\n",
      "Val Loss: 1.607599, Val Acc: 83.53%, Val-Class-Acc: {0: '72.28%', 1: '80.00%', 2: '86.11%', 3: '87.70%', 4: '82.50%', 5: '89.22%'}, LR: 0.000387\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_140.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_146.pth\n",
      "Epoch 147/200, Train Loss: 0.323002, Train-Class-Acc: {0: '86.65%', 1: '86.16%', 2: '89.25%', 3: '92.62%', 4: '86.08%', 5: '93.42%'}\n",
      "Val Loss: 1.425327, Val Acc: 82.51%, Val-Class-Acc: {0: '72.83%', 1: '80.00%', 2: '80.56%', 3: '85.25%', 4: '80.00%', 5: '89.52%'}, LR: 0.000387\n",
      "Epoch 148/200, Train Loss: 0.319216, Train-Class-Acc: {0: '85.69%', 1: '86.16%', 2: '89.25%', 3: '93.34%', 4: '86.71%', 5: '92.97%'}\n",
      "Val Loss: 1.154468, Val Acc: 83.37%, Val-Class-Acc: {0: '73.37%', 1: '80.00%', 2: '83.33%', 3: '86.89%', 4: '80.00%', 5: '90.12%'}, LR: 0.000387\n",
      "Epoch 149/200, Train Loss: 0.284926, Train-Class-Acc: {0: '87.87%', 1: '86.61%', 2: '89.95%', 3: '93.34%', 4: '87.97%', 5: '94.02%'}\n",
      "Val Loss: 1.381032, Val Acc: 82.90%, Val-Class-Acc: {0: '76.09%', 1: '78.51%', 2: '81.94%', 3: '85.25%', 4: '80.00%', 5: '90.12%'}, LR: 0.000387\n",
      "Epoch 150/200, Train Loss: 0.285791, Train-Class-Acc: {0: '86.24%', 1: '86.69%', 2: '89.60%', 3: '93.03%', 4: '87.34%', 5: '94.02%'}\n",
      "Val Loss: 1.198921, Val Acc: 83.14%, Val-Class-Acc: {0: '70.65%', 1: '82.39%', 2: '81.25%', 3: '87.30%', 4: '80.00%', 5: '88.92%'}, LR: 0.000387\n",
      "Epoch 151/200, Train Loss: 0.307788, Train-Class-Acc: {0: '87.47%', 1: '86.61%', 2: '90.29%', 3: '92.73%', 4: '84.18%', 5: '93.80%'}\n",
      "Val Loss: 2.007876, Val Acc: 83.37%, Val-Class-Acc: {0: '72.28%', 1: '80.00%', 2: '81.25%', 3: '87.70%', 4: '82.50%', 5: '90.72%'}, LR: 0.000349\n",
      "Epoch 152/200, Train Loss: 0.288469, Train-Class-Acc: {0: '88.01%', 1: '88.48%', 2: '90.47%', 3: '94.98%', 4: '86.08%', 5: '94.47%'}\n",
      "Val Loss: 1.330650, Val Acc: 83.14%, Val-Class-Acc: {0: '70.65%', 1: '81.19%', 2: '81.94%', 3: '87.30%', 4: '82.50%', 5: '89.52%'}, LR: 0.000349\n",
      "Epoch 153/200, Train Loss: 0.310155, Train-Class-Acc: {0: '88.83%', 1: '85.86%', 2: '89.95%', 3: '93.03%', 4: '87.34%', 5: '93.80%'}\n",
      "Val Loss: 1.452150, Val Acc: 83.84%, Val-Class-Acc: {0: '75.00%', 1: '78.21%', 2: '84.03%', 3: '87.30%', 4: '82.50%', 5: '91.92%'}, LR: 0.000349\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_129.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_153.pth\n",
      "Epoch 154/200, Train Loss: 0.380042, Train-Class-Acc: {0: '87.60%', 1: '86.99%', 2: '90.29%', 3: '92.83%', 4: '89.87%', 5: '94.10%'}\n",
      "Val Loss: 1.357855, Val Acc: 83.22%, Val-Class-Acc: {0: '71.74%', 1: '80.30%', 2: '81.94%', 3: '86.07%', 4: '80.00%', 5: '91.32%'}, LR: 0.000349\n",
      "Epoch 155/200, Train Loss: 0.280786, Train-Class-Acc: {0: '88.42%', 1: '88.56%', 2: '89.95%', 3: '93.75%', 4: '82.91%', 5: '94.54%'}\n",
      "Val Loss: 1.864345, Val Acc: 83.29%, Val-Class-Acc: {0: '72.83%', 1: '79.40%', 2: '84.03%', 3: '87.30%', 4: '80.00%', 5: '90.12%'}, LR: 0.000349\n",
      "Epoch 156/200, Train Loss: 0.324105, Train-Class-Acc: {0: '89.10%', 1: '86.09%', 2: '90.64%', 3: '93.03%', 4: '86.71%', 5: '94.32%'}\n",
      "Val Loss: 2.880504, Val Acc: 82.28%, Val-Class-Acc: {0: '72.83%', 1: '78.21%', 2: '81.94%', 3: '83.61%', 4: '77.50%', 5: '91.32%'}, LR: 0.000349\n",
      "Epoch 157/200, Train Loss: 0.299430, Train-Class-Acc: {0: '87.19%', 1: '87.88%', 2: '92.03%', 3: '92.73%', 4: '84.18%', 5: '94.69%'}\n",
      "Val Loss: 1.634818, Val Acc: 83.76%, Val-Class-Acc: {0: '70.11%', 1: '82.39%', 2: '81.94%', 3: '86.89%', 4: '82.50%', 5: '91.32%'}, LR: 0.000349\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_146.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_157.pth\n",
      "Epoch 158/200, Train Loss: 0.330841, Train-Class-Acc: {0: '87.74%', 1: '86.54%', 2: '89.08%', 3: '92.52%', 4: '86.71%', 5: '94.32%'}\n",
      "Val Loss: 1.985818, Val Acc: 83.06%, Val-Class-Acc: {0: '72.28%', 1: '80.00%', 2: '82.64%', 3: '85.66%', 4: '80.00%', 5: '90.72%'}, LR: 0.000349\n",
      "Epoch 159/200, Train Loss: 0.378726, Train-Class-Acc: {0: '86.51%', 1: '87.51%', 2: '90.81%', 3: '93.03%', 4: '87.97%', 5: '93.57%'}\n",
      "Val Loss: 1.601371, Val Acc: 83.68%, Val-Class-Acc: {0: '72.28%', 1: '80.30%', 2: '83.33%', 3: '85.25%', 4: '82.50%', 5: '92.51%'}, LR: 0.000349\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_139.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_159.pth\n",
      "Epoch 160/200, Train Loss: 0.398128, Train-Class-Acc: {0: '88.28%', 1: '87.73%', 2: '91.51%', 3: '93.75%', 4: '86.71%', 5: '93.95%'}\n",
      "Val Loss: 1.307771, Val Acc: 83.29%, Val-Class-Acc: {0: '72.83%', 1: '78.81%', 2: '84.72%', 3: '87.30%', 4: '82.50%', 5: '90.12%'}, LR: 0.000349\n",
      "Epoch 161/200, Train Loss: 0.295777, Train-Class-Acc: {0: '89.24%', 1: '87.73%', 2: '92.55%', 3: '93.85%', 4: '86.08%', 5: '94.25%'}\n",
      "Val Loss: 1.280707, Val Acc: 83.68%, Val-Class-Acc: {0: '70.11%', 1: '82.09%', 2: '81.94%', 3: '87.30%', 4: '80.00%', 5: '91.32%'}, LR: 0.000349\n",
      "Epoch 162/200, Train Loss: 0.280557, Train-Class-Acc: {0: '87.87%', 1: '87.66%', 2: '90.29%', 3: '93.95%', 4: '85.44%', 5: '94.39%'}\n",
      "Val Loss: 1.293959, Val Acc: 83.29%, Val-Class-Acc: {0: '68.48%', 1: '80.60%', 2: '84.72%', 3: '88.11%', 4: '82.50%', 5: '90.12%'}, LR: 0.000314\n",
      "Epoch 163/200, Train Loss: 0.273038, Train-Class-Acc: {0: '89.78%', 1: '87.58%', 2: '90.99%', 3: '93.03%', 4: '88.61%', 5: '94.39%'}\n",
      "Val Loss: 1.435197, Val Acc: 83.61%, Val-Class-Acc: {0: '66.30%', 1: '83.88%', 2: '84.72%', 3: '88.11%', 4: '80.00%', 5: '89.52%'}, LR: 0.000314\n",
      "Epoch 164/200, Train Loss: 0.264439, Train-Class-Acc: {0: '88.96%', 1: '88.03%', 2: '90.29%', 3: '93.85%', 4: '83.54%', 5: '93.57%'}\n",
      "Val Loss: 1.492123, Val Acc: 83.76%, Val-Class-Acc: {0: '69.57%', 1: '81.19%', 2: '85.42%', 3: '87.30%', 4: '82.50%', 5: '91.02%'}, LR: 0.000314\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_159.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_164.pth\n",
      "Epoch 165/200, Train Loss: 0.247393, Train-Class-Acc: {0: '88.96%', 1: '87.81%', 2: '90.29%', 3: '94.06%', 4: '87.97%', 5: '94.99%'}\n",
      "Val Loss: 1.500439, Val Acc: 83.84%, Val-Class-Acc: {0: '69.02%', 1: '82.39%', 2: '84.03%', 3: '87.70%', 4: '82.50%', 5: '90.72%'}, LR: 0.000314\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_157.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_165.pth\n",
      "Epoch 166/200, Train Loss: 0.256987, Train-Class-Acc: {0: '88.01%', 1: '87.66%', 2: '92.37%', 3: '94.36%', 4: '91.77%', 5: '94.32%'}\n",
      "Val Loss: 1.238881, Val Acc: 84.07%, Val-Class-Acc: {0: '73.37%', 1: '80.30%', 2: '82.64%', 3: '87.70%', 4: '82.50%', 5: '91.92%'}, LR: 0.000314\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_164.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_166.pth\n",
      "Epoch 167/200, Train Loss: 0.384672, Train-Class-Acc: {0: '88.56%', 1: '87.81%', 2: '92.03%', 3: '94.36%', 4: '87.34%', 5: '94.32%'}\n",
      "Val Loss: 1.748274, Val Acc: 84.07%, Val-Class-Acc: {0: '74.46%', 1: '77.91%', 2: '86.11%', 3: '88.52%', 4: '82.50%', 5: '91.62%'}, LR: 0.000314\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_143.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_167.pth\n",
      "Epoch 168/200, Train Loss: 0.251419, Train-Class-Acc: {0: '89.37%', 1: '88.63%', 2: '90.64%', 3: '93.75%', 4: '87.34%', 5: '94.47%'}\n",
      "Val Loss: 1.815723, Val Acc: 83.53%, Val-Class-Acc: {0: '75.54%', 1: '77.31%', 2: '80.56%', 3: '88.11%', 4: '82.50%', 5: '92.22%'}, LR: 0.000314\n",
      "Epoch 169/200, Train Loss: 0.453946, Train-Class-Acc: {0: '87.60%', 1: '87.51%', 2: '89.95%', 3: '92.93%', 4: '89.24%', 5: '93.50%'}\n",
      "Val Loss: 1.539186, Val Acc: 83.29%, Val-Class-Acc: {0: '70.65%', 1: '80.90%', 2: '85.42%', 3: '86.48%', 4: '82.50%', 5: '89.52%'}, LR: 0.000314\n",
      "Epoch 170/200, Train Loss: 0.248947, Train-Class-Acc: {0: '91.69%', 1: '88.48%', 2: '90.81%', 3: '93.03%', 4: '87.97%', 5: '94.77%'}\n",
      "Val Loss: 1.605994, Val Acc: 83.45%, Val-Class-Acc: {0: '75.54%', 1: '77.61%', 2: '83.33%', 3: '87.70%', 4: '80.00%', 5: '91.02%'}, LR: 0.000314\n",
      "Epoch 171/200, Train Loss: 0.264785, Train-Class-Acc: {0: '89.37%', 1: '86.69%', 2: '91.85%', 3: '93.95%', 4: '89.24%', 5: '94.99%'}\n",
      "Val Loss: 1.646580, Val Acc: 83.29%, Val-Class-Acc: {0: '73.91%', 1: '79.10%', 2: '84.03%', 3: '88.11%', 4: '80.00%', 5: '89.22%'}, LR: 0.000314\n",
      "Epoch 172/200, Train Loss: 0.368322, Train-Class-Acc: {0: '88.69%', 1: '88.86%', 2: '90.64%', 3: '93.85%', 4: '89.24%', 5: '93.87%'}\n",
      "Val Loss: 2.080050, Val Acc: 83.68%, Val-Class-Acc: {0: '76.09%', 1: '79.40%', 2: '80.56%', 3: '86.89%', 4: '80.00%', 5: '91.62%'}, LR: 0.000314\n",
      "Epoch 173/200, Train Loss: 0.230474, Train-Class-Acc: {0: '88.28%', 1: '89.30%', 2: '92.03%', 3: '94.67%', 4: '90.51%', 5: '95.14%'}\n",
      "Val Loss: 1.626923, Val Acc: 83.06%, Val-Class-Acc: {0: '75.00%', 1: '78.51%', 2: '81.94%', 3: '87.70%', 4: '80.00%', 5: '89.52%'}, LR: 0.000282\n",
      "Epoch 174/200, Train Loss: 0.305422, Train-Class-Acc: {0: '89.65%', 1: '87.73%', 2: '91.16%', 3: '94.16%', 4: '87.34%', 5: '94.54%'}\n",
      "Val Loss: 1.451796, Val Acc: 83.29%, Val-Class-Acc: {0: '75.00%', 1: '78.51%', 2: '81.94%', 3: '87.30%', 4: '82.50%', 5: '90.42%'}, LR: 0.000282\n",
      "Epoch 175/200, Train Loss: 0.252559, Train-Class-Acc: {0: '87.33%', 1: '87.66%', 2: '93.24%', 3: '94.77%', 4: '87.34%', 5: '94.77%'}\n",
      "Val Loss: 1.297791, Val Acc: 82.67%, Val-Class-Acc: {0: '70.65%', 1: '80.30%', 2: '81.94%', 3: '86.89%', 4: '80.00%', 5: '89.22%'}, LR: 0.000282\n",
      "Epoch 176/200, Train Loss: 0.259325, Train-Class-Acc: {0: '88.83%', 1: '87.51%', 2: '91.85%', 3: '93.24%', 4: '89.87%', 5: '94.54%'}\n",
      "Val Loss: 1.373594, Val Acc: 83.14%, Val-Class-Acc: {0: '72.28%', 1: '79.40%', 2: '80.56%', 3: '87.70%', 4: '82.50%', 5: '90.72%'}, LR: 0.000282\n",
      "Epoch 177/200, Train Loss: 0.243412, Train-Class-Acc: {0: '90.05%', 1: '88.78%', 2: '90.47%', 3: '94.88%', 4: '88.61%', 5: '95.52%'}\n",
      "Val Loss: 2.346352, Val Acc: 83.37%, Val-Class-Acc: {0: '66.30%', 1: '82.09%', 2: '83.33%', 3: '86.48%', 4: '82.50%', 5: '91.92%'}, LR: 0.000282\n",
      "Epoch 178/200, Train Loss: 0.287976, Train-Class-Acc: {0: '87.87%', 1: '88.11%', 2: '91.85%', 3: '93.95%', 4: '90.51%', 5: '94.84%'}\n",
      "Val Loss: 1.316413, Val Acc: 82.44%, Val-Class-Acc: {0: '71.20%', 1: '79.40%', 2: '83.33%', 3: '88.11%', 4: '80.00%', 5: '87.43%'}, LR: 0.000282\n",
      "Epoch 179/200, Train Loss: 0.229053, Train-Class-Acc: {0: '88.83%', 1: '90.05%', 2: '91.51%', 3: '95.39%', 4: '86.71%', 5: '95.22%'}\n",
      "Val Loss: 2.361806, Val Acc: 83.45%, Val-Class-Acc: {0: '69.57%', 1: '80.90%', 2: '83.33%', 3: '86.89%', 4: '82.50%', 5: '91.32%'}, LR: 0.000282\n",
      "Epoch 180/200, Train Loss: 0.294614, Train-Class-Acc: {0: '89.24%', 1: '87.88%', 2: '91.85%', 3: '94.16%', 4: '86.08%', 5: '95.22%'}\n",
      "Val Loss: 1.498051, Val Acc: 83.53%, Val-Class-Acc: {0: '66.85%', 1: '83.28%', 2: '81.94%', 3: '86.89%', 4: '82.50%', 5: '91.32%'}, LR: 0.000282\n",
      "Epoch 181/200, Train Loss: 0.255421, Train-Class-Acc: {0: '89.37%', 1: '90.35%', 2: '90.81%', 3: '94.36%', 4: '87.97%', 5: '94.92%'}\n",
      "Val Loss: 1.853429, Val Acc: 82.51%, Val-Class-Acc: {0: '69.02%', 1: '78.81%', 2: '82.64%', 3: '87.30%', 4: '82.50%', 5: '90.12%'}, LR: 0.000282\n",
      "Epoch 182/200, Train Loss: 0.294714, Train-Class-Acc: {0: '89.10%', 1: '89.08%', 2: '91.16%', 3: '94.16%', 4: '88.61%', 5: '94.25%'}\n",
      "Val Loss: 1.745687, Val Acc: 82.90%, Val-Class-Acc: {0: '75.54%', 1: '78.51%', 2: '84.03%', 3: '88.11%', 4: '82.50%', 5: '87.13%'}, LR: 0.000282\n",
      "Epoch 183/200, Train Loss: 0.263120, Train-Class-Acc: {0: '88.56%', 1: '87.81%', 2: '90.29%', 3: '93.24%', 4: '85.44%', 5: '94.84%'}\n",
      "Val Loss: 2.044984, Val Acc: 83.29%, Val-Class-Acc: {0: '71.20%', 1: '78.21%', 2: '82.64%', 3: '87.70%', 4: '82.50%', 5: '92.22%'}, LR: 0.000282\n",
      "Epoch 184/200, Train Loss: 0.262677, Train-Class-Acc: {0: '90.05%', 1: '88.56%', 2: '92.03%', 3: '93.55%', 4: '90.51%', 5: '94.77%'}\n",
      "Val Loss: 1.794832, Val Acc: 82.83%, Val-Class-Acc: {0: '77.72%', 1: '76.12%', 2: '82.64%', 3: '86.07%', 4: '80.00%', 5: '90.42%'}, LR: 0.000254\n",
      "Epoch 185/200, Train Loss: 0.230844, Train-Class-Acc: {0: '89.10%', 1: '87.81%', 2: '92.20%', 3: '94.47%', 4: '92.41%', 5: '94.77%'}\n",
      "Val Loss: 1.574369, Val Acc: 82.44%, Val-Class-Acc: {0: '67.93%', 1: '81.79%', 2: '80.56%', 3: '86.07%', 4: '80.00%', 5: '89.52%'}, LR: 0.000254\n",
      "Epoch 186/200, Train Loss: 0.201505, Train-Class-Acc: {0: '91.01%', 1: '89.75%', 2: '91.51%', 3: '93.95%', 4: '91.77%', 5: '95.89%'}\n",
      "Val Loss: 1.522617, Val Acc: 82.59%, Val-Class-Acc: {0: '71.20%', 1: '78.81%', 2: '84.03%', 3: '87.30%', 4: '82.50%', 5: '88.62%'}, LR: 0.000254\n",
      "Epoch 187/200, Train Loss: 0.229536, Train-Class-Acc: {0: '91.01%', 1: '89.68%', 2: '92.20%', 3: '94.88%', 4: '89.87%', 5: '94.77%'}\n",
      "Val Loss: 1.681787, Val Acc: 83.14%, Val-Class-Acc: {0: '73.37%', 1: '77.01%', 2: '82.64%', 3: '87.30%', 4: '82.50%', 5: '91.92%'}, LR: 0.000254\n",
      "Epoch 188/200, Train Loss: 0.215111, Train-Class-Acc: {0: '90.05%', 1: '89.38%', 2: '93.24%', 3: '95.39%', 4: '91.77%', 5: '95.22%'}\n",
      "Val Loss: 1.861265, Val Acc: 82.83%, Val-Class-Acc: {0: '70.65%', 1: '79.70%', 2: '83.33%', 3: '86.89%', 4: '82.50%', 5: '89.52%'}, LR: 0.000254\n",
      "Epoch 189/200, Train Loss: 0.310992, Train-Class-Acc: {0: '90.60%', 1: '89.75%', 2: '92.55%', 3: '94.36%', 4: '91.77%', 5: '94.54%'}\n",
      "Val Loss: 1.500311, Val Acc: 83.22%, Val-Class-Acc: {0: '73.91%', 1: '79.40%', 2: '81.94%', 3: '86.07%', 4: '82.50%', 5: '90.72%'}, LR: 0.000254\n",
      "Epoch 190/200, Train Loss: 0.258789, Train-Class-Acc: {0: '89.78%', 1: '90.65%', 2: '91.33%', 3: '93.75%', 4: '93.67%', 5: '95.52%'}\n",
      "Val Loss: 1.556339, Val Acc: 83.14%, Val-Class-Acc: {0: '72.83%', 1: '79.10%', 2: '82.64%', 3: '87.70%', 4: '80.00%', 5: '90.12%'}, LR: 0.000254\n",
      "Epoch 191/200, Train Loss: 0.226531, Train-Class-Acc: {0: '91.69%', 1: '88.93%', 2: '91.85%', 3: '94.47%', 4: '86.71%', 5: '95.29%'}\n",
      "Val Loss: 1.198244, Val Acc: 83.14%, Val-Class-Acc: {0: '70.65%', 1: '79.40%', 2: '84.72%', 3: '87.30%', 4: '82.50%', 5: '90.12%'}, LR: 0.000254\n",
      "Epoch 192/200, Train Loss: 0.249846, Train-Class-Acc: {0: '90.87%', 1: '89.38%', 2: '90.99%', 3: '93.95%', 4: '89.87%', 5: '95.37%'}\n",
      "Val Loss: 2.392498, Val Acc: 83.22%, Val-Class-Acc: {0: '74.46%', 1: '78.81%', 2: '81.94%', 3: '85.66%', 4: '82.50%', 5: '91.32%'}, LR: 0.000254\n",
      "Epoch 193/200, Train Loss: 0.301268, Train-Class-Acc: {0: '88.56%', 1: '88.71%', 2: '92.20%', 3: '94.67%', 4: '86.71%', 5: '95.37%'}\n",
      "Val Loss: 1.537892, Val Acc: 82.98%, Val-Class-Acc: {0: '69.02%', 1: '79.70%', 2: '84.72%', 3: '85.25%', 4: '82.50%', 5: '91.62%'}, LR: 0.000254\n",
      "Epoch 194/200, Train Loss: 0.293260, Train-Class-Acc: {0: '91.69%', 1: '89.45%', 2: '92.37%', 3: '94.67%', 4: '91.14%', 5: '95.22%'}\n",
      "Val Loss: 1.691560, Val Acc: 82.83%, Val-Class-Acc: {0: '75.00%', 1: '78.51%', 2: '83.33%', 3: '86.07%', 4: '80.00%', 5: '89.22%'}, LR: 0.000254\n",
      "Epoch 195/200, Train Loss: 0.280834, Train-Class-Acc: {0: '89.92%', 1: '90.50%', 2: '91.51%', 3: '94.47%', 4: '88.61%', 5: '94.62%'}\n",
      "Val Loss: 2.044714, Val Acc: 83.29%, Val-Class-Acc: {0: '70.11%', 1: '78.81%', 2: '84.72%', 3: '88.52%', 4: '82.50%', 5: '90.72%'}, LR: 0.000229\n",
      "Epoch 196/200, Train Loss: 0.221122, Train-Class-Acc: {0: '90.74%', 1: '89.38%', 2: '91.68%', 3: '94.57%', 4: '89.24%', 5: '94.77%'}\n",
      "Val Loss: 1.682318, Val Acc: 83.45%, Val-Class-Acc: {0: '72.28%', 1: '80.00%', 2: '83.33%', 3: '86.89%', 4: '80.00%', 5: '91.02%'}, LR: 0.000229\n",
      "Epoch 197/200, Train Loss: 0.423795, Train-Class-Acc: {0: '91.14%', 1: '89.15%', 2: '92.20%', 3: '93.55%', 4: '86.71%', 5: '95.37%'}\n",
      "Val Loss: 1.620210, Val Acc: 83.22%, Val-Class-Acc: {0: '67.39%', 1: '80.60%', 2: '85.42%', 3: '88.11%', 4: '82.50%', 5: '90.12%'}, LR: 0.000229\n",
      "Epoch 198/200, Train Loss: 0.245776, Train-Class-Acc: {0: '89.78%', 1: '89.30%', 2: '92.55%', 3: '94.77%', 4: '87.97%', 5: '95.22%'}\n",
      "Val Loss: 1.521012, Val Acc: 82.83%, Val-Class-Acc: {0: '70.11%', 1: '80.60%', 2: '80.56%', 3: '87.30%', 4: '80.00%', 5: '90.12%'}, LR: 0.000229\n",
      "Epoch 199/200, Train Loss: 0.202096, Train-Class-Acc: {0: '91.55%', 1: '90.80%', 2: '94.11%', 3: '95.08%', 4: '91.77%', 5: '94.99%'}\n",
      "Val Loss: 1.699713, Val Acc: 82.83%, Val-Class-Acc: {0: '66.30%', 1: '81.19%', 2: '81.25%', 3: '88.11%', 4: '80.00%', 5: '90.72%'}, LR: 0.000229\n",
      "Epoch 200/200, Train Loss: 0.206385, Train-Class-Acc: {0: '89.37%', 1: '91.02%', 2: '93.59%', 3: '94.77%', 4: '86.08%', 5: '95.59%'}\n",
      "Val Loss: 2.450465, Val Acc: 83.29%, Val-Class-Acc: {0: '73.37%', 1: '79.10%', 2: '81.25%', 3: '88.52%', 4: '80.00%', 5: '90.42%'}, LR: 0.000229\n",
      "\n",
      "üèÜ Best model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_best.pth (Val Accuracy: 84.07%)\n",
      "\n",
      "üìå Final model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_final.pth\n",
      "\n",
      "üéØ Top 5 Best Models:\n",
      "Epoch 167, Train Loss: 0.384672, Train-Acc: {0: '88.56%', 1: '87.81%', 2: '92.03%', 3: '94.36%', 4: '87.34%', 5: '94.32%'},\n",
      "Val Loss: 1.748274, Val Acc: 84.07%, Val-Acc: {0: '74.46%', 1: '77.91%', 2: '86.11%', 3: '88.52%', 4: '82.50%', 5: '91.62%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_167.pth\n",
      "Epoch 166, Train Loss: 0.256987, Train-Acc: {0: '88.01%', 1: '87.66%', 2: '92.37%', 3: '94.36%', 4: '91.77%', 5: '94.32%'},\n",
      "Val Loss: 1.238881, Val Acc: 84.07%, Val-Acc: {0: '73.37%', 1: '80.30%', 2: '82.64%', 3: '87.70%', 4: '82.50%', 5: '91.92%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_166.pth\n",
      "Epoch 165, Train Loss: 0.247393, Train-Acc: {0: '88.96%', 1: '87.81%', 2: '90.29%', 3: '94.06%', 4: '87.97%', 5: '94.99%'},\n",
      "Val Loss: 1.500439, Val Acc: 83.84%, Val-Acc: {0: '69.02%', 1: '82.39%', 2: '84.03%', 3: '87.70%', 4: '82.50%', 5: '90.72%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_165.pth\n",
      "Epoch 153, Train Loss: 0.310155, Train-Acc: {0: '88.83%', 1: '85.86%', 2: '89.95%', 3: '93.03%', 4: '87.34%', 5: '93.80%'},\n",
      "Val Loss: 1.452150, Val Acc: 83.84%, Val-Acc: {0: '75.00%', 1: '78.21%', 2: '84.03%', 3: '87.30%', 4: '82.50%', 5: '91.92%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_153.pth\n",
      "Epoch 144, Train Loss: 0.341843, Train-Acc: {0: '85.97%', 1: '85.34%', 2: '90.29%', 3: '92.83%', 4: '87.97%', 5: '93.80%'},\n",
      "Val Loss: 1.465673, Val Acc: 83.84%, Val-Acc: {0: '77.17%', 1: '79.70%', 2: '81.25%', 3: '86.48%', 4: '80.00%', 5: '91.32%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3/ResNet18_1D_LoRA_epoch_144.pth\n",
      "---\n",
      "### Period 3\n",
      "+ ##### Total training time: 469.49 seconds\n",
      "+ ##### Model: ResNet18_1D_LoRA\n",
      "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_3'*\n",
      "+ ##### Best Epoch: 167\n",
      "#### __Val Accuracy: 84.07%__\n",
      "#### __Val-Class-Acc: {0: '74.46%', 1: '77.91%', 2: '86.11%', 3: '88.52%', 4: '82.50%', 5: '91.62%'}__\n",
      "#### __Total Parameters: 3,891,846__\n",
      "#### __Model Size (float32): 14.85 MB__\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå Period 3: Standard LoRA Training (ECG)\n",
    "# ================================\n",
    "period = 3\n",
    "\n",
    "# ==== Paths ====\n",
    "stop_signal_file = os.path.join(BASE_DIR, \"stop_training.txt\")\n",
    "model_saving_folder = os.path.join(BASE_DIR, \"Trained_models\", \"Standard_LoRA_CIL_v3\", f\"Period_{period}\")\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# ==== Load Period 3 Data ====\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{period}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{period}.npy\"))\n",
    "X_val   = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val   = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "# ==== Device ====\n",
    "device = auto_select_cuda_device()\n",
    "\n",
    "# ==== Model Configuration ====\n",
    "input_channels = X_train.shape[2]  # ECG 12-lead\n",
    "output_size = len(np.unique(y_train))  # Êñ∞Â¢ûÈ°ûÂà•Êï∏\n",
    "model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size, lora_rank=4).to(device)\n",
    "\n",
    "# ==== Initialize LoRA Adapters FIRST ====\n",
    "model.init_lora()\n",
    "\n",
    "# ==== Load Period 2 Best Model Weights (excluding FC & LoRA) ====\n",
    "prev_model_path = os.path.join(BASE_DIR, \"Trained_models\", \"Standard_LoRA_CIL_v2\", f\"Period_{period - 1}\", \"ResNet18_1D_LoRA_best.pth\")\n",
    "prev_checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "prev_state_dict = prev_checkpoint[\"model_state_dict\"]\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "# filtered_state_dict = {\n",
    "#     k: v for k, v in prev_state_dict.items()\n",
    "#     if k in model_dict and model_dict[k].shape == v.shape and not (k.startswith(\"fc\") or \"lora_adapter\" in k)\n",
    "# }\n",
    "filtered_state_dict = {\n",
    "    k: v for k, v in prev_state_dict.items()\n",
    "    if k in model_dict and model_dict[k].shape == v.shape and not (\n",
    "        k.startswith(\"fc\") or \"lora_adapter.lora_A\" in k or \"lora_adapter.lora_B\" in k\n",
    "    )\n",
    "}\n",
    "model.load_state_dict(filtered_state_dict, strict=False)\n",
    "for k in model_dict:\n",
    "    if k not in filtered_state_dict:\n",
    "        print(f\"üîç Not loaded: {k}, shape={model_dict[k].shape}\")\n",
    "print(\"‚úÖ Loaded Period 2 weights (excluding FC & LoRA)\")\n",
    "\n",
    "# ==== Optimizer / Scheduler ====\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.get_trainable_parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# ==== Start Training ====\n",
    "train_with_lora_ecg(\n",
    "    model=model,\n",
    "    output_size=output_size,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    model_saving_folder=model_saving_folder,\n",
    "    model_name=\"ResNet18_1D_LoRA\",\n",
    "    stop_signal_file=stop_signal_file,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# ‚úÖ Cleanup\n",
    "# ================================\n",
    "del X_train, y_train, X_val, y_val\n",
    "del prev_model_path, prev_checkpoint, prev_state_dict, filtered_state_dict\n",
    "del model, criterion, optimizer, scheduler\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df753cf",
   "metadata": {},
   "source": [
    "### Period 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8d60731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 2\n",
      "    - Memory Used    : 1607 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ LoRA adapters initialized for 8 conv2 layers\n",
      "üîç Not loaded: layer1.0.lora_adapter.lora_A, shape=torch.Size([64, 4])\n",
      "üîç Not loaded: layer1.0.lora_adapter.lora_B, shape=torch.Size([4, 192])\n",
      "üîç Not loaded: layer1.1.lora_adapter.lora_A, shape=torch.Size([64, 4])\n",
      "üîç Not loaded: layer1.1.lora_adapter.lora_B, shape=torch.Size([4, 192])\n",
      "üîç Not loaded: layer2.0.lora_adapter.lora_A, shape=torch.Size([128, 4])\n",
      "üîç Not loaded: layer2.0.lora_adapter.lora_B, shape=torch.Size([4, 384])\n",
      "üîç Not loaded: layer2.1.lora_adapter.lora_A, shape=torch.Size([128, 4])\n",
      "üîç Not loaded: layer2.1.lora_adapter.lora_B, shape=torch.Size([4, 384])\n",
      "üîç Not loaded: layer3.0.lora_adapter.lora_A, shape=torch.Size([256, 4])\n",
      "üîç Not loaded: layer3.0.lora_adapter.lora_B, shape=torch.Size([4, 768])\n",
      "üîç Not loaded: layer3.1.lora_adapter.lora_A, shape=torch.Size([256, 4])\n",
      "üîç Not loaded: layer3.1.lora_adapter.lora_B, shape=torch.Size([4, 768])\n",
      "üîç Not loaded: layer4.0.lora_adapter.lora_A, shape=torch.Size([512, 4])\n",
      "üîç Not loaded: layer4.0.lora_adapter.lora_B, shape=torch.Size([4, 1536])\n",
      "üîç Not loaded: layer4.1.lora_adapter.lora_A, shape=torch.Size([512, 4])\n",
      "üîç Not loaded: layer4.1.lora_adapter.lora_B, shape=torch.Size([4, 1536])\n",
      "üîç Not loaded: fc.weight, shape=torch.Size([10, 1024])\n",
      "üîç Not loaded: fc.bias, shape=torch.Size([10])\n",
      "‚úÖ Loaded Period 3 weights (excluding FC & LoRA A/B)\n",
      "üìä Parameter Statistics:\n",
      "  - Total parameters: 3,895,946\n",
      "  - Trainable parameters: 40,970 (1.05%)\n",
      "    - LoRA parameters: 30,720 (0.79%)\n",
      "    - FC parameters: 10,250 (0.26%)\n",
      "  - Frozen parameters: 3,854,976 (98.95%)\n",
      "üß† Trainable parameter names:\n",
      "  ‚úÖ layer1.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer1.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer1.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer1.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer2.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer2.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer2.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer2.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer3.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer3.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer3.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer3.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer4.0.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer4.0.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ layer4.1.lora_adapter.lora_A (LoRA)\n",
      "  ‚úÖ layer4.1.lora_adapter.lora_B (LoRA)\n",
      "  ‚úÖ fc.weight (FC)\n",
      "  ‚úÖ fc.bias (FC)\n",
      "\n",
      "üöÄ 'train_with_lora_ecg' started.\n",
      "‚úÖ Removed existing folder: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_763947/3463635584.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  prev_checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Data Overview:\n",
      "X_train: torch.Size([5493, 5000, 12]), y_train: torch.Size([5493])\n",
      "X_val: torch.Size([1374, 5000, 12]), y_val: torch.Size([1374])\n",
      "Epoch 1/200, Train Loss: 33.972773, Train-Class-Acc: {0: '38.42%', 2: '10.40%', 3: '25.72%', 4: '5.06%', 5: '56.77%', 6: '10.37%', 7: '20.56%', 8: '24.52%', 9: '6.08%'}\n",
      "Val Loss: 11.989978, Val Acc: 53.42%, Val-Class-Acc: {0: '73.37%', 2: '6.94%', 3: '40.57%', 4: '7.50%', 5: '88.06%', 6: '8.33%', 7: '69.60%', 8: '61.15%', 9: '0.00%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_1.pth\n",
      "Epoch 2/200, Train Loss: 13.480584, Train-Class-Acc: {0: '57.63%', 2: '23.57%', 3: '37.19%', 4: '23.42%', 5: '70.91%', 6: '10.14%', 7: '35.93%', 8: '32.48%', 9: '8.11%'}\n",
      "Val Loss: 6.176110, Val Acc: 57.35%, Val-Class-Acc: {0: '72.28%', 2: '39.58%', 3: '40.98%', 4: '25.00%', 5: '85.67%', 6: '11.11%', 7: '64.80%', 8: '68.15%', 9: '2.70%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_2.pth\n",
      "Epoch 3/200, Train Loss: 9.746113, Train-Class-Acc: {0: '57.08%', 2: '24.61%', 3: '38.01%', 4: '25.32%', 5: '71.35%', 6: '14.75%', 7: '41.32%', 8: '37.10%', 9: '6.76%'}\n",
      "Val Loss: 4.068825, Val Acc: 60.99%, Val-Class-Acc: {0: '80.98%', 2: '33.33%', 3: '70.49%', 4: '32.50%', 5: '79.70%', 6: '5.56%', 7: '64.80%', 8: '63.06%', 9: '8.11%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_3.pth\n",
      "Epoch 4/200, Train Loss: 7.790527, Train-Class-Acc: {0: '56.27%', 2: '30.50%', 3: '40.06%', 4: '22.15%', 5: '69.26%', 6: '11.29%', 7: '40.32%', 8: '36.78%', 9: '10.81%'}\n",
      "Val Loss: 3.051830, Val Acc: 64.19%, Val-Class-Acc: {0: '69.02%', 2: '50.00%', 3: '69.26%', 4: '45.00%', 5: '81.79%', 6: '9.26%', 7: '68.00%', 8: '79.62%', 9: '5.41%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_4.pth\n",
      "Epoch 5/200, Train Loss: 6.455739, Train-Class-Acc: {0: '56.27%', 2: '31.89%', 3: '42.11%', 4: '29.75%', 5: '68.29%', 6: '13.59%', 7: '39.32%', 8: '35.99%', 9: '8.78%'}\n",
      "Val Loss: 4.343505, Val Acc: 64.63%, Val-Class-Acc: {0: '70.65%', 2: '59.03%', 3: '69.26%', 4: '35.00%', 5: '85.97%', 6: '3.70%', 7: '56.80%', 8: '78.98%', 9: '8.11%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_5.pth\n",
      "Epoch 6/200, Train Loss: 5.289514, Train-Class-Acc: {0: '54.36%', 2: '39.34%', 3: '45.08%', 4: '22.15%', 5: '68.21%', 6: '14.06%', 7: '42.91%', 8: '37.90%', 9: '8.78%'}\n",
      "Val Loss: 3.241648, Val Acc: 67.69%, Val-Class-Acc: {0: '77.72%', 2: '73.61%', 3: '67.62%', 4: '60.00%', 5: '83.58%', 6: '12.04%', 7: '66.40%', 8: '73.89%', 9: '0.00%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_1.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_6.pth\n",
      "Epoch 7/200, Train Loss: 4.645557, Train-Class-Acc: {0: '55.72%', 2: '42.11%', 3: '48.87%', 4: '31.65%', 5: '69.11%', 6: '14.29%', 7: '42.51%', 8: '41.72%', 9: '12.84%'}\n",
      "Val Loss: 2.496480, Val Acc: 67.90%, Val-Class-Acc: {0: '81.52%', 2: '65.28%', 3: '71.31%', 4: '50.00%', 5: '85.67%', 6: '2.78%', 7: '67.20%', 8: '75.16%', 9: '8.11%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_2.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_7.pth\n",
      "Epoch 8/200, Train Loss: 4.465737, Train-Class-Acc: {0: '56.13%', 2: '47.66%', 3: '50.20%', 4: '29.11%', 5: '71.13%', 6: '13.59%', 7: '45.71%', 8: '42.04%', 9: '7.43%'}\n",
      "Val Loss: 1.882134, Val Acc: 67.47%, Val-Class-Acc: {0: '84.78%', 2: '65.97%', 3: '84.84%', 4: '47.50%', 5: '76.12%', 6: '11.11%', 7: '57.60%', 8: '70.70%', 9: '0.00%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_3.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_8.pth\n",
      "Epoch 9/200, Train Loss: 3.710065, Train-Class-Acc: {0: '56.95%', 2: '49.74%', 3: '53.59%', 4: '37.97%', 5: '71.35%', 6: '16.82%', 7: '46.31%', 8: '42.99%', 9: '11.49%'}\n",
      "Val Loss: 2.427530, Val Acc: 68.49%, Val-Class-Acc: {0: '81.52%', 2: '72.22%', 3: '70.08%', 4: '52.50%', 5: '86.57%', 6: '6.48%', 7: '65.60%', 8: '73.89%', 9: '0.00%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_4.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_9.pth\n",
      "Epoch 10/200, Train Loss: 3.756392, Train-Class-Acc: {0: '56.54%', 2: '53.55%', 3: '54.51%', 4: '39.87%', 5: '73.60%', 6: '17.74%', 7: '47.31%', 8: '43.79%', 9: '12.16%'}\n",
      "Val Loss: 2.066405, Val Acc: 68.34%, Val-Class-Acc: {0: '85.87%', 2: '59.72%', 3: '89.75%', 4: '50.00%', 5: '75.52%', 6: '7.41%', 7: '66.40%', 8: '70.70%', 9: '2.70%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_5.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_10.pth\n",
      "Epoch 11/200, Train Loss: 3.488448, Train-Class-Acc: {0: '58.86%', 2: '54.59%', 3: '56.76%', 4: '43.04%', 5: '71.05%', 6: '17.05%', 7: '46.31%', 8: '46.02%', 9: '12.16%'}\n",
      "Val Loss: 1.779129, Val Acc: 71.32%, Val-Class-Acc: {0: '82.07%', 2: '67.36%', 3: '84.02%', 4: '60.00%', 5: '85.67%', 6: '9.26%', 7: '64.80%', 8: '79.62%', 9: '0.00%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_8.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_11.pth\n",
      "Epoch 12/200, Train Loss: 3.476064, Train-Class-Acc: {0: '60.90%', 2: '51.82%', 3: '56.76%', 4: '40.51%', 5: '72.03%', 6: '17.05%', 7: '42.71%', 8: '46.66%', 9: '13.51%'}\n",
      "Val Loss: 2.379724, Val Acc: 70.96%, Val-Class-Acc: {0: '78.80%', 2: '63.19%', 3: '80.74%', 4: '45.00%', 5: '87.16%', 6: '8.33%', 7: '76.00%', 8: '81.53%', 9: '0.00%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_6.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_12.pth\n",
      "Epoch 13/200, Train Loss: 3.303161, Train-Class-Acc: {0: '58.86%', 2: '52.69%', 3: '57.27%', 4: '34.81%', 5: '74.50%', 6: '14.06%', 7: '44.91%', 8: '49.68%', 9: '13.51%'}\n",
      "Val Loss: 1.504133, Val Acc: 70.74%, Val-Class-Acc: {0: '82.61%', 2: '67.36%', 3: '90.98%', 4: '45.00%', 5: '84.48%', 6: '9.26%', 7: '56.80%', 8: '75.80%', 9: '0.00%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_7.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_13.pth\n",
      "Epoch 14/200, Train Loss: 3.067778, Train-Class-Acc: {0: '60.63%', 2: '57.71%', 3: '60.35%', 4: '39.87%', 5: '74.79%', 6: '15.21%', 7: '45.51%', 8: '48.57%', 9: '7.43%'}\n",
      "Val Loss: 1.587202, Val Acc: 70.74%, Val-Class-Acc: {0: '71.20%', 2: '80.56%', 3: '77.87%', 4: '50.00%', 5: '85.37%', 6: '8.33%', 7: '68.80%', 8: '85.35%', 9: '0.00%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_10.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_14.pth\n",
      "Epoch 15/200, Train Loss: 2.719974, Train-Class-Acc: {0: '59.54%', 2: '58.75%', 3: '61.48%', 4: '46.84%', 5: '75.69%', 6: '16.82%', 7: '46.71%', 8: '53.98%', 9: '15.54%'}\n",
      "Val Loss: 1.785797, Val Acc: 72.34%, Val-Class-Acc: {0: '73.91%', 2: '76.39%', 3: '83.20%', 4: '57.50%', 5: '87.16%', 6: '13.89%', 7: '70.40%', 8: '80.89%', 9: '0.00%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_9.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_15.pth\n",
      "Epoch 16/200, Train Loss: 2.521545, Train-Class-Acc: {0: '61.17%', 2: '59.62%', 3: '63.63%', 4: '50.63%', 5: '76.07%', 6: '15.90%', 7: '49.90%', 8: '49.84%', 9: '15.54%'}\n",
      "Val Loss: 1.862285, Val Acc: 71.98%, Val-Class-Acc: {0: '72.83%', 2: '77.08%', 3: '79.92%', 4: '52.50%', 5: '87.76%', 6: '14.81%', 7: '72.80%', 8: '80.25%', 9: '2.70%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_13.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_16.pth\n",
      "Epoch 17/200, Train Loss: 2.523198, Train-Class-Acc: {0: '60.90%', 2: '58.58%', 3: '64.45%', 4: '46.84%', 5: '76.07%', 6: '15.90%', 7: '51.30%', 8: '53.66%', 9: '16.22%'}\n",
      "Val Loss: 1.563879, Val Acc: 72.34%, Val-Class-Acc: {0: '80.43%', 2: '75.69%', 3: '90.57%', 4: '62.50%', 5: '82.09%', 6: '14.81%', 7: '62.40%', 8: '76.43%', 9: '5.41%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_14.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_17.pth\n",
      "Epoch 18/200, Train Loss: 2.715898, Train-Class-Acc: {0: '65.12%', 2: '60.83%', 3: '64.34%', 4: '55.06%', 5: '74.87%', 6: '17.97%', 7: '47.90%', 8: '53.66%', 9: '14.19%'}\n",
      "Val Loss: 1.280793, Val Acc: 72.34%, Val-Class-Acc: {0: '76.63%', 2: '76.39%', 3: '84.02%', 4: '55.00%', 5: '87.76%', 6: '12.96%', 7: '60.80%', 8: '83.44%', 9: '2.70%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_12.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_18.pth\n",
      "Epoch 19/200, Train Loss: 2.478674, Train-Class-Acc: {0: '62.94%', 2: '62.22%', 3: '64.45%', 4: '46.84%', 5: '77.11%', 6: '17.97%', 7: '49.30%', 8: '54.62%', 9: '18.24%'}\n",
      "Val Loss: 1.676684, Val Acc: 74.53%, Val-Class-Acc: {0: '81.52%', 2: '72.22%', 3: '88.11%', 4: '52.50%', 5: '87.76%', 6: '20.37%', 7: '76.00%', 8: '78.34%', 9: '0.00%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_11.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_19.pth\n",
      "Epoch 20/200, Train Loss: 2.450928, Train-Class-Acc: {0: '64.71%', 2: '63.78%', 3: '65.27%', 4: '51.90%', 5: '78.46%', 6: '17.97%', 7: '49.10%', 8: '58.12%', 9: '18.92%'}\n",
      "Val Loss: 1.179987, Val Acc: 74.89%, Val-Class-Acc: {0: '82.07%', 2: '78.47%', 3: '88.93%', 4: '55.00%', 5: '83.88%', 6: '23.15%', 7: '79.20%', 8: '75.80%', 9: '5.41%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_16.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_20.pth\n",
      "Epoch 21/200, Train Loss: 2.315375, Train-Class-Acc: {0: '64.17%', 2: '62.22%', 3: '65.98%', 4: '54.43%', 5: '77.19%', 6: '15.90%', 7: '53.49%', 8: '58.92%', 9: '21.62%'}\n",
      "Val Loss: 1.461882, Val Acc: 74.02%, Val-Class-Acc: {0: '78.26%', 2: '74.31%', 3: '90.98%', 4: '50.00%', 5: '89.25%', 6: '10.19%', 7: '68.00%', 8: '82.17%', 9: '0.00%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_15.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_21.pth\n",
      "Epoch 22/200, Train Loss: 2.159061, Train-Class-Acc: {0: '66.35%', 2: '65.86%', 3: '70.29%', 4: '58.86%', 5: '77.49%', 6: '18.89%', 7: '51.10%', 8: '60.35%', 9: '14.19%'}\n",
      "Val Loss: 1.439478, Val Acc: 74.53%, Val-Class-Acc: {0: '85.87%', 2: '78.47%', 3: '88.11%', 4: '47.50%', 5: '88.06%', 6: '12.96%', 7: '72.80%', 8: '75.16%', 9: '2.70%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_17.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_22.pth\n",
      "Epoch 23/200, Train Loss: 2.267047, Train-Class-Acc: {0: '62.94%', 2: '66.20%', 3: '70.08%', 4: '61.39%', 5: '78.68%', 6: '20.74%', 7: '52.69%', 8: '57.96%', 9: '22.30%'}\n",
      "Val Loss: 1.066419, Val Acc: 74.82%, Val-Class-Acc: {0: '77.72%', 2: '80.56%', 3: '91.80%', 4: '55.00%', 5: '84.48%', 6: '20.37%', 7: '73.60%', 8: '78.98%', 9: '5.41%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_18.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_23.pth\n",
      "Epoch 24/200, Train Loss: 2.068454, Train-Class-Acc: {0: '67.03%', 2: '65.86%', 3: '71.11%', 4: '59.49%', 5: '79.06%', 6: '22.12%', 7: '51.70%', 8: '57.32%', 9: '19.59%'}\n",
      "Val Loss: 1.374435, Val Acc: 75.40%, Val-Class-Acc: {0: '78.80%', 2: '80.56%', 3: '88.11%', 4: '52.50%', 5: '86.87%', 6: '21.30%', 7: '77.60%', 8: '80.25%', 9: '5.41%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_21.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_24.pth\n",
      "Epoch 25/200, Train Loss: 1.830179, Train-Class-Acc: {0: '65.80%', 2: '67.42%', 3: '71.31%', 4: '55.70%', 5: '78.76%', 6: '16.36%', 7: '52.89%', 8: '59.39%', 9: '15.54%'}\n",
      "Val Loss: 1.327083, Val Acc: 75.62%, Val-Class-Acc: {0: '81.52%', 2: '81.94%', 3: '87.70%', 4: '55.00%', 5: '88.96%', 6: '22.22%', 7: '73.60%', 8: '75.80%', 9: '5.41%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_19.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_25.pth\n",
      "Epoch 26/200, Train Loss: 1.922144, Train-Class-Acc: {0: '67.03%', 2: '68.46%', 3: '72.54%', 4: '53.80%', 5: '80.18%', 6: '23.27%', 7: '52.30%', 8: '60.19%', 9: '18.92%'}\n",
      "Val Loss: 6.717623, Val Acc: 69.58%, Val-Class-Acc: {0: '76.09%', 2: '76.39%', 3: '68.03%', 4: '32.50%', 5: '93.43%', 6: '11.11%', 7: '63.20%', 8: '74.52%', 9: '16.22%'}, LR: 0.001000\n",
      "Epoch 27/200, Train Loss: 2.198401, Train-Class-Acc: {0: '63.90%', 2: '63.78%', 3: '68.34%', 4: '59.49%', 5: '77.94%', 6: '19.12%', 7: '50.70%', 8: '59.87%', 9: '14.86%'}\n",
      "Val Loss: 1.461729, Val Acc: 74.02%, Val-Class-Acc: {0: '80.43%', 2: '74.31%', 3: '84.02%', 4: '52.50%', 5: '90.45%', 6: '12.04%', 7: '71.20%', 8: '82.80%', 9: '2.70%'}, LR: 0.001000\n",
      "Epoch 28/200, Train Loss: 1.768451, Train-Class-Acc: {0: '66.89%', 2: '70.54%', 3: '74.59%', 4: '58.23%', 5: '82.35%', 6: '22.35%', 7: '51.30%', 8: '64.33%', 9: '19.59%'}\n",
      "Val Loss: 1.327233, Val Acc: 75.76%, Val-Class-Acc: {0: '80.43%', 2: '79.86%', 3: '87.30%', 4: '62.50%', 5: '90.15%', 6: '20.37%', 7: '70.40%', 8: '80.25%', 9: '5.41%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_22.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_28.pth\n",
      "Epoch 29/200, Train Loss: 1.499223, Train-Class-Acc: {0: '68.26%', 2: '70.19%', 3: '71.82%', 4: '57.59%', 5: '82.27%', 6: '23.50%', 7: '55.09%', 8: '61.46%', 9: '23.65%'}\n",
      "Val Loss: 1.300732, Val Acc: 74.89%, Val-Class-Acc: {0: '79.35%', 2: '79.86%', 3: '94.67%', 4: '52.50%', 5: '85.67%', 6: '11.11%', 7: '63.20%', 8: '84.71%', 9: '13.51%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_23.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_29.pth\n",
      "Epoch 30/200, Train Loss: 1.820159, Train-Class-Acc: {0: '67.44%', 2: '70.71%', 3: '74.90%', 4: '58.86%', 5: '80.85%', 6: '19.12%', 7: '51.90%', 8: '64.97%', 9: '25.68%'}\n",
      "Val Loss: 1.647810, Val Acc: 75.62%, Val-Class-Acc: {0: '82.61%', 2: '79.17%', 3: '93.44%', 4: '62.50%', 5: '87.46%', 6: '24.07%', 7: '66.40%', 8: '71.34%', 9: '16.22%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_20.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_30.pth\n",
      "Epoch 31/200, Train Loss: 1.674737, Train-Class-Acc: {0: '67.98%', 2: '68.98%', 3: '74.59%', 4: '55.70%', 5: '81.68%', 6: '19.59%', 7: '56.09%', 8: '63.38%', 9: '25.00%'}\n",
      "Val Loss: 2.056373, Val Acc: 75.84%, Val-Class-Acc: {0: '82.61%', 2: '81.94%', 3: '90.98%', 4: '47.50%', 5: '86.57%', 6: '25.93%', 7: '71.20%', 8: '77.07%', 9: '8.11%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_29.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_31.pth\n",
      "Epoch 32/200, Train Loss: 1.574681, Train-Class-Acc: {0: '68.53%', 2: '68.98%', 3: '73.87%', 4: '60.76%', 5: '81.38%', 6: '25.12%', 7: '54.49%', 8: '63.54%', 9: '19.59%'}\n",
      "Val Loss: 1.420418, Val Acc: 75.76%, Val-Class-Acc: {0: '80.43%', 2: '84.72%', 3: '86.48%', 4: '57.50%', 5: '89.55%', 6: '11.11%', 7: '71.20%', 8: '81.53%', 9: '21.62%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_24.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_32.pth\n",
      "Epoch 33/200, Train Loss: 1.395099, Train-Class-Acc: {0: '68.66%', 2: '69.84%', 3: '75.20%', 4: '60.13%', 5: '83.02%', 6: '20.05%', 7: '57.09%', 8: '65.61%', 9: '24.32%'}\n",
      "Val Loss: 2.208449, Val Acc: 76.56%, Val-Class-Acc: {0: '76.09%', 2: '80.56%', 3: '88.11%', 4: '60.00%', 5: '90.75%', 6: '25.93%', 7: '70.40%', 8: '82.17%', 9: '21.62%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_25.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_33.pth\n",
      "Epoch 34/200, Train Loss: 1.850524, Train-Class-Acc: {0: '66.62%', 2: '71.40%', 3: '71.93%', 4: '58.86%', 5: '79.66%', 6: '24.42%', 7: '54.29%', 8: '65.92%', 9: '20.95%'}\n",
      "Val Loss: 1.573394, Val Acc: 77.73%, Val-Class-Acc: {0: '82.61%', 2: '84.72%', 3: '90.98%', 4: '60.00%', 5: '87.76%', 6: '27.78%', 7: '68.80%', 8: '84.71%', 9: '13.51%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_30.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_34.pth\n",
      "Epoch 35/200, Train Loss: 1.622793, Train-Class-Acc: {0: '69.48%', 2: '70.71%', 3: '76.95%', 4: '56.33%', 5: '81.82%', 6: '29.03%', 7: '56.29%', 8: '65.29%', 9: '23.65%'}\n",
      "Val Loss: 1.502217, Val Acc: 76.64%, Val-Class-Acc: {0: '81.52%', 2: '79.86%', 3: '88.11%', 4: '62.50%', 5: '90.75%', 6: '21.30%', 7: '73.60%', 8: '80.25%', 9: '8.11%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_28.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_35.pth\n",
      "Epoch 36/200, Train Loss: 1.389268, Train-Class-Acc: {0: '69.75%', 2: '70.71%', 3: '77.05%', 4: '63.92%', 5: '83.92%', 6: '23.27%', 7: '56.69%', 8: '64.65%', 9: '27.70%'}\n",
      "Val Loss: 1.880452, Val Acc: 76.93%, Val-Class-Acc: {0: '83.70%', 2: '82.64%', 3: '87.70%', 4: '62.50%', 5: '88.66%', 6: '23.15%', 7: '76.80%', 8: '77.07%', 9: '16.22%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_32.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_36.pth\n",
      "Epoch 37/200, Train Loss: 1.525802, Train-Class-Acc: {0: '71.53%', 2: '74.00%', 3: '76.74%', 4: '63.29%', 5: '83.17%', 6: '24.42%', 7: '59.88%', 8: '66.72%', 9: '27.03%'}\n",
      "Val Loss: 1.215377, Val Acc: 76.93%, Val-Class-Acc: {0: '78.26%', 2: '84.72%', 3: '89.34%', 4: '65.00%', 5: '85.97%', 6: '29.63%', 7: '71.20%', 8: '84.71%', 9: '13.51%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_31.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_37.pth\n",
      "Epoch 38/200, Train Loss: 1.405550, Train-Class-Acc: {0: '71.12%', 2: '76.26%', 3: '77.25%', 4: '63.29%', 5: '83.55%', 6: '26.04%', 7: '56.09%', 8: '68.47%', 9: '29.73%'}\n",
      "Val Loss: 1.719635, Val Acc: 77.29%, Val-Class-Acc: {0: '83.70%', 2: '80.56%', 3: '94.67%', 4: '65.00%', 5: '89.55%', 6: '23.15%', 7: '68.00%', 8: '77.71%', 9: '8.11%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_33.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_38.pth\n",
      "Epoch 39/200, Train Loss: 1.460478, Train-Class-Acc: {0: '71.66%', 2: '73.48%', 3: '78.48%', 4: '59.49%', 5: '83.17%', 6: '22.12%', 7: '56.89%', 8: '68.63%', 9: '22.97%'}\n",
      "Val Loss: 1.344549, Val Acc: 77.58%, Val-Class-Acc: {0: '85.33%', 2: '83.33%', 3: '90.57%', 4: '65.00%', 5: '89.85%', 6: '14.81%', 7: '70.40%', 8: '83.44%', 9: '16.22%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_35.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_39.pth\n",
      "Epoch 40/200, Train Loss: 1.468917, Train-Class-Acc: {0: '70.71%', 2: '73.31%', 3: '76.54%', 4: '60.13%', 5: '83.47%', 6: '24.19%', 7: '55.89%', 8: '65.61%', 9: '29.73%'}\n",
      "Val Loss: 1.779460, Val Acc: 77.66%, Val-Class-Acc: {0: '80.98%', 2: '81.94%', 3: '89.34%', 4: '55.00%', 5: '90.75%', 6: '25.00%', 7: '67.20%', 8: '87.90%', 9: '18.92%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_36.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_40.pth\n",
      "Epoch 41/200, Train Loss: 1.255157, Train-Class-Acc: {0: '72.62%', 2: '74.18%', 3: '79.20%', 4: '62.66%', 5: '84.07%', 6: '25.12%', 7: '59.08%', 8: '70.22%', 9: '27.03%'}\n",
      "Val Loss: 1.465093, Val Acc: 78.17%, Val-Class-Acc: {0: '82.07%', 2: '84.03%', 3: '92.21%', 4: '70.00%', 5: '91.04%', 6: '18.52%', 7: '71.20%', 8: '82.80%', 9: '13.51%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_37.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_41.pth\n",
      "Epoch 42/200, Train Loss: 1.279383, Train-Class-Acc: {0: '72.75%', 2: '76.95%', 3: '78.48%', 4: '65.82%', 5: '85.04%', 6: '26.73%', 7: '58.48%', 8: '66.56%', 9: '28.38%'}\n",
      "Val Loss: 1.296097, Val Acc: 77.95%, Val-Class-Acc: {0: '82.61%', 2: '79.86%', 3: '93.03%', 4: '62.50%', 5: '88.96%', 6: '24.07%', 7: '74.40%', 8: '82.17%', 9: '16.22%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_38.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_42.pth\n",
      "Epoch 43/200, Train Loss: 1.443009, Train-Class-Acc: {0: '69.35%', 2: '72.27%', 3: '77.25%', 4: '58.86%', 5: '84.67%', 6: '26.73%', 7: '57.29%', 8: '68.79%', 9: '33.11%'}\n",
      "Val Loss: 2.159114, Val Acc: 76.93%, Val-Class-Acc: {0: '80.98%', 2: '79.17%', 3: '79.10%', 4: '60.00%', 5: '93.73%', 6: '17.59%', 7: '80.80%', 8: '86.62%', 9: '18.92%'}, LR: 0.000900\n",
      "Epoch 44/200, Train Loss: 1.256835, Train-Class-Acc: {0: '72.48%', 2: '74.87%', 3: '77.46%', 4: '65.19%', 5: '84.44%', 6: '28.57%', 7: '58.88%', 8: '69.43%', 9: '27.70%'}\n",
      "Val Loss: 1.819015, Val Acc: 78.89%, Val-Class-Acc: {0: '83.70%', 2: '82.64%', 3: '89.34%', 4: '65.00%', 5: '91.04%', 6: '19.44%', 7: '79.20%', 8: '85.35%', 9: '21.62%'}, LR: 0.000900\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_39.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_44.pth\n",
      "Epoch 45/200, Train Loss: 1.199514, Train-Class-Acc: {0: '74.80%', 2: '75.74%', 3: '80.23%', 4: '62.03%', 5: '84.22%', 6: '23.73%', 7: '60.88%', 8: '70.22%', 9: '26.35%'}\n",
      "Val Loss: 1.469430, Val Acc: 77.66%, Val-Class-Acc: {0: '80.98%', 2: '83.33%', 3: '89.75%', 4: '62.50%', 5: '90.45%', 6: '21.30%', 7: '71.20%', 8: '82.80%', 9: '24.32%'}, LR: 0.000900\n",
      "Epoch 46/200, Train Loss: 1.255385, Train-Class-Acc: {0: '71.80%', 2: '76.60%', 3: '80.64%', 4: '69.62%', 5: '85.49%', 6: '26.73%', 7: '59.28%', 8: '66.24%', 9: '33.11%'}\n",
      "Val Loss: 1.531915, Val Acc: 78.53%, Val-Class-Acc: {0: '83.15%', 2: '83.33%', 3: '89.75%', 4: '65.00%', 5: '88.36%', 6: '20.37%', 7: '78.40%', 8: '88.54%', 9: '16.22%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_40.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_46.pth\n",
      "Epoch 47/200, Train Loss: 1.056133, Train-Class-Acc: {0: '75.61%', 2: '74.52%', 3: '80.84%', 4: '63.92%', 5: '86.84%', 6: '31.34%', 7: '59.48%', 8: '71.02%', 9: '30.41%'}\n",
      "Val Loss: 1.225640, Val Acc: 78.31%, Val-Class-Acc: {0: '82.61%', 2: '81.94%', 3: '94.67%', 4: '67.50%', 5: '88.36%', 6: '27.78%', 7: '71.20%', 8: '80.89%', 9: '16.22%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_34.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_47.pth\n",
      "Epoch 48/200, Train Loss: 1.203566, Train-Class-Acc: {0: '72.48%', 2: '78.16%', 3: '81.15%', 4: '68.35%', 5: '84.59%', 6: '27.19%', 7: '62.67%', 8: '71.66%', 9: '30.41%'}\n",
      "Val Loss: 1.198574, Val Acc: 78.46%, Val-Class-Acc: {0: '80.43%', 2: '81.25%', 3: '88.93%', 4: '62.50%', 5: '91.64%', 6: '29.63%', 7: '74.40%', 8: '84.08%', 9: '18.92%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_42.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_48.pth\n",
      "Epoch 49/200, Train Loss: 1.250776, Train-Class-Acc: {0: '72.34%', 2: '76.08%', 3: '79.41%', 4: '65.82%', 5: '84.07%', 6: '29.26%', 7: '60.48%', 8: '68.79%', 9: '34.46%'}\n",
      "Val Loss: 1.776002, Val Acc: 78.17%, Val-Class-Acc: {0: '88.04%', 2: '78.47%', 3: '89.34%', 4: '67.50%', 5: '91.64%', 6: '17.59%', 7: '78.40%', 8: '79.62%', 9: '13.51%'}, LR: 0.000810\n",
      "Epoch 50/200, Train Loss: 1.532740, Train-Class-Acc: {0: '74.66%', 2: '77.64%', 3: '79.41%', 4: '66.46%', 5: '85.42%', 6: '29.26%', 7: '60.08%', 8: '73.57%', 9: '27.03%'}\n",
      "Val Loss: 1.166066, Val Acc: 77.58%, Val-Class-Acc: {0: '75.54%', 2: '81.25%', 3: '95.90%', 4: '62.50%', 5: '89.25%', 6: '19.44%', 7: '71.20%', 8: '85.35%', 9: '21.62%'}, LR: 0.000810\n",
      "Epoch 51/200, Train Loss: 1.117262, Train-Class-Acc: {0: '75.20%', 2: '78.34%', 3: '80.94%', 4: '69.62%', 5: '85.71%', 6: '28.57%', 7: '59.88%', 8: '74.20%', 9: '31.08%'}\n",
      "Val Loss: 1.427134, Val Acc: 79.69%, Val-Class-Acc: {0: '80.98%', 2: '86.11%', 3: '93.44%', 4: '70.00%', 5: '88.66%', 6: '34.26%', 7: '75.20%', 8: '83.44%', 9: '18.92%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_41.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_51.pth\n",
      "Epoch 52/200, Train Loss: 1.497804, Train-Class-Acc: {0: '72.21%', 2: '72.79%', 3: '80.84%', 4: '61.39%', 5: '85.86%', 6: '27.42%', 7: '59.88%', 8: '70.38%', 9: '27.03%'}\n",
      "Val Loss: 1.368139, Val Acc: 78.02%, Val-Class-Acc: {0: '79.35%', 2: '79.86%', 3: '90.16%', 4: '62.50%', 5: '89.85%', 6: '37.96%', 7: '72.80%', 8: '80.89%', 9: '16.22%'}, LR: 0.000810\n",
      "Epoch 53/200, Train Loss: 1.132706, Train-Class-Acc: {0: '75.34%', 2: '77.99%', 3: '81.56%', 4: '67.09%', 5: '87.66%', 6: '32.03%', 7: '61.08%', 8: '71.82%', 9: '28.38%'}\n",
      "Val Loss: 1.576841, Val Acc: 78.68%, Val-Class-Acc: {0: '77.72%', 2: '84.03%', 3: '90.16%', 4: '65.00%', 5: '89.55%', 6: '31.48%', 7: '77.60%', 8: '84.71%', 9: '18.92%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_47.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_53.pth\n",
      "Epoch 54/200, Train Loss: 1.134470, Train-Class-Acc: {0: '74.52%', 2: '77.30%', 3: '81.86%', 4: '69.62%', 5: '86.99%', 6: '30.88%', 7: '62.48%', 8: '72.29%', 9: '33.78%'}\n",
      "Val Loss: 1.059685, Val Acc: 79.26%, Val-Class-Acc: {0: '80.98%', 2: '82.64%', 3: '94.26%', 4: '62.50%', 5: '89.25%', 6: '31.48%', 7: '70.40%', 8: '85.35%', 9: '29.73%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_48.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_54.pth\n",
      "Epoch 55/200, Train Loss: 1.005599, Train-Class-Acc: {0: '76.02%', 2: '80.07%', 3: '82.89%', 4: '72.15%', 5: '86.31%', 6: '29.95%', 7: '63.67%', 8: '73.73%', 9: '29.05%'}\n",
      "Val Loss: 1.203513, Val Acc: 79.11%, Val-Class-Acc: {0: '79.35%', 2: '82.64%', 3: '93.44%', 4: '65.00%', 5: '91.64%', 6: '28.70%', 7: '68.80%', 8: '84.08%', 9: '32.43%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_46.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_55.pth\n",
      "Epoch 56/200, Train Loss: 0.996041, Train-Class-Acc: {0: '76.29%', 2: '79.20%', 3: '83.71%', 4: '69.62%', 5: '87.73%', 6: '33.41%', 7: '63.67%', 8: '74.20%', 9: '33.78%'}\n",
      "Val Loss: 2.185601, Val Acc: 79.69%, Val-Class-Acc: {0: '83.70%', 2: '83.33%', 3: '93.85%', 4: '65.00%', 5: '90.45%', 6: '28.70%', 7: '79.20%', 8: '80.89%', 9: '16.22%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_53.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_56.pth\n",
      "Epoch 57/200, Train Loss: 0.953808, Train-Class-Acc: {0: '77.11%', 2: '79.03%', 3: '83.30%', 4: '67.09%', 5: '86.39%', 6: '31.57%', 7: '64.27%', 8: '73.73%', 9: '29.73%'}\n",
      "Val Loss: 1.008484, Val Acc: 80.42%, Val-Class-Acc: {0: '80.98%', 2: '86.81%', 3: '94.67%', 4: '67.50%', 5: '88.96%', 6: '34.26%', 7: '76.00%', 8: '86.62%', 9: '18.92%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_44.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_57.pth\n",
      "Epoch 58/200, Train Loss: 1.025995, Train-Class-Acc: {0: '76.16%', 2: '79.55%', 3: '83.71%', 4: '74.68%', 5: '87.28%', 6: '33.41%', 7: '63.67%', 8: '74.36%', 9: '43.24%'}\n",
      "Val Loss: 1.280954, Val Acc: 79.55%, Val-Class-Acc: {0: '83.15%', 2: '84.03%', 3: '86.07%', 4: '70.00%', 5: '92.54%', 6: '29.63%', 7: '76.00%', 8: '87.26%', 9: '18.92%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_55.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_58.pth\n",
      "Epoch 59/200, Train Loss: 1.083297, Train-Class-Acc: {0: '79.16%', 2: '81.11%', 3: '82.99%', 4: '69.62%', 5: '87.36%', 6: '35.48%', 7: '61.68%', 8: '75.64%', 9: '33.78%'}\n",
      "Val Loss: 1.282823, Val Acc: 79.62%, Val-Class-Acc: {0: '84.78%', 2: '79.17%', 3: '93.44%', 4: '67.50%', 5: '90.15%', 6: '28.70%', 7: '73.60%', 8: '86.62%', 9: '21.62%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_54.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_59.pth\n",
      "Epoch 60/200, Train Loss: 1.078600, Train-Class-Acc: {0: '75.34%', 2: '79.38%', 3: '83.09%', 4: '75.95%', 5: '86.46%', 6: '35.48%', 7: '62.48%', 8: '73.73%', 9: '35.81%'}\n",
      "Val Loss: 1.014341, Val Acc: 79.99%, Val-Class-Acc: {0: '76.63%', 2: '86.11%', 3: '94.26%', 4: '67.50%', 5: '89.25%', 6: '28.70%', 7: '74.40%', 8: '89.81%', 9: '35.14%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_58.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_60.pth\n",
      "Epoch 61/200, Train Loss: 0.906459, Train-Class-Acc: {0: '76.57%', 2: '78.68%', 3: '81.76%', 4: '74.68%', 5: '88.03%', 6: '34.79%', 7: '64.47%', 8: '75.80%', 9: '37.84%'}\n",
      "Val Loss: 1.064455, Val Acc: 79.91%, Val-Class-Acc: {0: '78.80%', 2: '84.03%', 3: '95.08%', 4: '65.00%', 5: '89.85%', 6: '34.26%', 7: '72.00%', 8: '84.71%', 9: '35.14%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_59.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_61.pth\n",
      "Epoch 62/200, Train Loss: 0.980128, Train-Class-Acc: {0: '78.88%', 2: '82.15%', 3: '83.30%', 4: '70.25%', 5: '87.21%', 6: '35.48%', 7: '63.27%', 8: '74.68%', 9: '32.43%'}\n",
      "Val Loss: 1.214988, Val Acc: 80.49%, Val-Class-Acc: {0: '81.52%', 2: '85.42%', 3: '93.85%', 4: '67.50%', 5: '90.45%', 6: '25.00%', 7: '80.00%', 8: '88.54%', 9: '21.62%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_51.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_62.pth\n",
      "Epoch 63/200, Train Loss: 0.971461, Train-Class-Acc: {0: '77.25%', 2: '80.24%', 3: '84.63%', 4: '70.89%', 5: '87.73%', 6: '32.03%', 7: '64.67%', 8: '75.32%', 9: '35.81%'}\n",
      "Val Loss: 1.104219, Val Acc: 79.62%, Val-Class-Acc: {0: '79.35%', 2: '81.25%', 3: '93.03%', 4: '65.00%', 5: '91.34%', 6: '24.07%', 7: '81.60%', 8: '85.35%', 9: '27.03%'}, LR: 0.000810\n",
      "Epoch 64/200, Train Loss: 1.033967, Train-Class-Acc: {0: '76.57%', 2: '80.07%', 3: '84.22%', 4: '74.68%', 5: '88.33%', 6: '33.87%', 7: '65.47%', 8: '77.71%', 9: '37.84%'}\n",
      "Val Loss: 1.009306, Val Acc: 80.35%, Val-Class-Acc: {0: '80.43%', 2: '84.03%', 3: '92.21%', 4: '60.00%', 5: '89.55%', 6: '39.81%', 7: '83.20%', 8: '84.71%', 9: '16.22%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_56.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_64.pth\n",
      "Epoch 65/200, Train Loss: 0.953694, Train-Class-Acc: {0: '78.07%', 2: '78.68%', 3: '83.30%', 4: '74.68%', 5: '87.28%', 6: '38.48%', 7: '64.07%', 8: '75.32%', 9: '37.16%'}\n",
      "Val Loss: 1.483974, Val Acc: 79.48%, Val-Class-Acc: {0: '83.15%', 2: '79.86%', 3: '95.08%', 4: '60.00%', 5: '90.75%', 6: '26.85%', 7: '76.80%', 8: '84.71%', 9: '16.22%'}, LR: 0.000810\n",
      "Epoch 66/200, Train Loss: 0.910127, Train-Class-Acc: {0: '78.07%', 2: '80.94%', 3: '85.14%', 4: '76.58%', 5: '87.88%', 6: '34.33%', 7: '65.07%', 8: '78.50%', 9: '39.86%'}\n",
      "Val Loss: 1.135647, Val Acc: 80.20%, Val-Class-Acc: {0: '81.52%', 2: '84.72%', 3: '93.03%', 4: '65.00%', 5: '90.15%', 6: '29.63%', 7: '78.40%', 8: '87.26%', 9: '21.62%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_61.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_66.pth\n",
      "Epoch 67/200, Train Loss: 1.018494, Train-Class-Acc: {0: '77.25%', 2: '80.76%', 3: '84.22%', 4: '67.72%', 5: '87.14%', 6: '32.95%', 7: '63.87%', 8: '78.03%', 9: '35.81%'}\n",
      "Val Loss: 1.039841, Val Acc: 79.55%, Val-Class-Acc: {0: '82.61%', 2: '82.64%', 3: '93.44%', 4: '60.00%', 5: '89.25%', 6: '31.48%', 7: '75.20%', 8: '85.35%', 9: '24.32%'}, LR: 0.000810\n",
      "Epoch 68/200, Train Loss: 0.880760, Train-Class-Acc: {0: '79.16%', 2: '80.94%', 3: '85.04%', 4: '74.68%', 5: '88.03%', 6: '33.64%', 7: '67.47%', 8: '76.11%', 9: '33.78%'}\n",
      "Val Loss: 1.228538, Val Acc: 80.57%, Val-Class-Acc: {0: '83.70%', 2: '86.81%', 3: '92.62%', 4: '67.50%', 5: '90.75%', 6: '30.56%', 7: '76.80%', 8: '86.62%', 9: '16.22%'}, LR: 0.000810\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_60.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_68.pth\n",
      "Epoch 69/200, Train Loss: 0.870102, Train-Class-Acc: {0: '80.25%', 2: '80.59%', 3: '85.96%', 4: '73.42%', 5: '89.01%', 6: '35.25%', 7: '64.87%', 8: '79.62%', 9: '40.54%'}\n",
      "Val Loss: 1.017533, Val Acc: 80.28%, Val-Class-Acc: {0: '82.07%', 2: '85.42%', 3: '94.67%', 4: '72.50%', 5: '89.55%', 6: '31.48%', 7: '72.80%', 8: '87.26%', 9: '18.92%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_66.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_69.pth\n",
      "Epoch 70/200, Train Loss: 0.921609, Train-Class-Acc: {0: '78.61%', 2: '81.80%', 3: '84.53%', 4: '76.58%', 5: '89.01%', 6: '37.56%', 7: '64.47%', 8: '75.48%', 9: '43.92%'}\n",
      "Val Loss: 1.357993, Val Acc: 80.93%, Val-Class-Acc: {0: '80.98%', 2: '86.11%', 3: '91.80%', 4: '67.50%', 5: '91.04%', 6: '37.96%', 7: '77.60%', 8: '85.99%', 9: '27.03%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_69.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_70.pth\n",
      "Epoch 71/200, Train Loss: 0.836833, Train-Class-Acc: {0: '77.52%', 2: '83.19%', 3: '84.63%', 4: '77.22%', 5: '88.41%', 6: '40.09%', 7: '64.27%', 8: '78.50%', 9: '39.86%'}\n",
      "Val Loss: 1.563578, Val Acc: 80.64%, Val-Class-Acc: {0: '82.07%', 2: '82.64%', 3: '94.67%', 4: '67.50%', 5: '89.55%', 6: '36.11%', 7: '76.80%', 8: '85.99%', 9: '27.03%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_64.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_71.pth\n",
      "Epoch 72/200, Train Loss: 1.055556, Train-Class-Acc: {0: '78.20%', 2: '82.15%', 3: '85.55%', 4: '77.85%', 5: '88.86%', 6: '37.79%', 7: '65.47%', 8: '75.48%', 9: '36.49%'}\n",
      "Val Loss: 1.757292, Val Acc: 79.40%, Val-Class-Acc: {0: '79.89%', 2: '84.72%', 3: '93.85%', 4: '67.50%', 5: '86.57%', 6: '36.11%', 7: '80.80%', 8: '80.89%', 9: '24.32%'}, LR: 0.000729\n",
      "Epoch 73/200, Train Loss: 1.020596, Train-Class-Acc: {0: '78.07%', 2: '81.80%', 3: '84.12%', 4: '74.68%', 5: '88.18%', 6: '43.09%', 7: '64.27%', 8: '77.71%', 9: '40.54%'}\n",
      "Val Loss: 0.947281, Val Acc: 79.69%, Val-Class-Acc: {0: '80.98%', 2: '86.11%', 3: '94.67%', 4: '67.50%', 5: '86.57%', 6: '24.07%', 7: '79.20%', 8: '89.81%', 9: '21.62%'}, LR: 0.000729\n",
      "Epoch 74/200, Train Loss: 1.047467, Train-Class-Acc: {0: '77.25%', 2: '81.63%', 3: '85.04%', 4: '74.68%', 5: '87.14%', 6: '39.86%', 7: '65.67%', 8: '78.50%', 9: '40.54%'}\n",
      "Val Loss: 1.066083, Val Acc: 79.04%, Val-Class-Acc: {0: '77.72%', 2: '84.03%', 3: '95.90%', 4: '67.50%', 5: '89.55%', 6: '25.93%', 7: '76.00%', 8: '82.17%', 9: '24.32%'}, LR: 0.000729\n",
      "Epoch 75/200, Train Loss: 0.845636, Train-Class-Acc: {0: '78.61%', 2: '81.28%', 3: '86.27%', 4: '75.32%', 5: '88.71%', 6: '37.10%', 7: '64.07%', 8: '76.75%', 9: '42.57%'}\n",
      "Val Loss: 1.293649, Val Acc: 80.57%, Val-Class-Acc: {0: '80.43%', 2: '82.64%', 3: '95.08%', 4: '70.00%', 5: '90.45%', 6: '25.93%', 7: '79.20%', 8: '89.17%', 9: '27.03%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_57.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_75.pth\n",
      "Epoch 76/200, Train Loss: 0.877554, Train-Class-Acc: {0: '79.84%', 2: '82.67%', 3: '85.35%', 4: '78.48%', 5: '89.38%', 6: '37.33%', 7: '66.87%', 8: '80.41%', 9: '36.49%'}\n",
      "Val Loss: 0.988275, Val Acc: 80.49%, Val-Class-Acc: {0: '82.07%', 2: '83.33%', 3: '95.90%', 4: '67.50%', 5: '89.25%', 6: '29.63%', 7: '76.00%', 8: '89.17%', 9: '21.62%'}, LR: 0.000729\n",
      "Epoch 77/200, Train Loss: 0.928445, Train-Class-Acc: {0: '79.97%', 2: '83.36%', 3: '87.30%', 4: '74.68%', 5: '89.98%', 6: '37.10%', 7: '66.07%', 8: '77.23%', 9: '40.54%'}\n",
      "Val Loss: 0.967112, Val Acc: 80.57%, Val-Class-Acc: {0: '78.26%', 2: '86.81%', 3: '94.26%', 4: '67.50%', 5: '88.66%', 6: '33.33%', 7: '77.60%', 8: '87.90%', 9: '35.14%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_62.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_77.pth\n",
      "Epoch 78/200, Train Loss: 0.986143, Train-Class-Acc: {0: '79.56%', 2: '84.92%', 3: '86.48%', 4: '77.85%', 5: '88.93%', 6: '40.09%', 7: '67.66%', 8: '77.23%', 9: '43.24%'}\n",
      "Val Loss: 1.204317, Val Acc: 80.93%, Val-Class-Acc: {0: '82.07%', 2: '85.42%', 3: '94.26%', 4: '65.00%', 5: '89.25%', 6: '31.48%', 7: '84.80%', 8: '85.99%', 9: '21.62%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_68.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_78.pth\n",
      "Epoch 79/200, Train Loss: 0.912443, Train-Class-Acc: {0: '77.66%', 2: '82.84%', 3: '86.27%', 4: '77.85%', 5: '89.45%', 6: '40.09%', 7: '66.87%', 8: '79.14%', 9: '36.49%'}\n",
      "Val Loss: 1.723124, Val Acc: 80.86%, Val-Class-Acc: {0: '78.80%', 2: '84.72%', 3: '94.26%', 4: '67.50%', 5: '89.85%', 6: '34.26%', 7: '79.20%', 8: '87.90%', 9: '32.43%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_75.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_79.pth\n",
      "Epoch 80/200, Train Loss: 0.895342, Train-Class-Acc: {0: '78.47%', 2: '84.58%', 3: '86.99%', 4: '77.85%', 5: '88.48%', 6: '44.24%', 7: '66.47%', 8: '79.14%', 9: '37.84%'}\n",
      "Val Loss: 0.873886, Val Acc: 79.91%, Val-Class-Acc: {0: '76.63%', 2: '83.33%', 3: '95.08%', 4: '67.50%', 5: '87.46%', 6: '30.56%', 7: '83.20%', 8: '86.62%', 9: '32.43%'}, LR: 0.000729\n",
      "Epoch 81/200, Train Loss: 0.917916, Train-Class-Acc: {0: '79.29%', 2: '85.27%', 3: '85.96%', 4: '78.48%', 5: '89.83%', 6: '38.25%', 7: '69.86%', 8: '80.57%', 9: '44.59%'}\n",
      "Val Loss: 1.004005, Val Acc: 81.00%, Val-Class-Acc: {0: '80.98%', 2: '85.42%', 3: '95.08%', 4: '67.50%', 5: '89.25%', 6: '30.56%', 7: '80.80%', 8: '87.90%', 9: '29.73%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_77.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_81.pth\n",
      "Epoch 82/200, Train Loss: 1.020026, Train-Class-Acc: {0: '79.56%', 2: '82.32%', 3: '86.27%', 4: '79.11%', 5: '89.75%', 6: '40.09%', 7: '67.66%', 8: '79.30%', 9: '41.22%'}\n",
      "Val Loss: 1.766193, Val Acc: 80.42%, Val-Class-Acc: {0: '75.00%', 2: '82.64%', 3: '95.08%', 4: '72.50%', 5: '90.15%', 6: '31.48%', 7: '80.00%', 8: '87.90%', 9: '35.14%'}, LR: 0.000729\n",
      "Epoch 83/200, Train Loss: 0.877223, Train-Class-Acc: {0: '80.25%', 2: '82.32%', 3: '86.17%', 4: '72.78%', 5: '87.96%', 6: '44.01%', 7: '67.86%', 8: '80.41%', 9: '48.65%'}\n",
      "Val Loss: 1.311189, Val Acc: 80.49%, Val-Class-Acc: {0: '80.98%', 2: '84.72%', 3: '94.67%', 4: '72.50%', 5: '88.96%', 6: '30.56%', 7: '80.00%', 8: '85.99%', 9: '24.32%'}, LR: 0.000729\n",
      "Epoch 84/200, Train Loss: 0.820050, Train-Class-Acc: {0: '77.66%', 2: '83.71%', 3: '87.50%', 4: '75.95%', 5: '89.30%', 6: '40.55%', 7: '68.86%', 8: '78.50%', 9: '47.97%'}\n",
      "Val Loss: 1.830518, Val Acc: 80.64%, Val-Class-Acc: {0: '82.61%', 2: '86.81%', 3: '91.80%', 4: '75.00%', 5: '89.25%', 6: '25.93%', 7: '84.00%', 8: '87.26%', 9: '21.62%'}, LR: 0.000729\n",
      "Epoch 85/200, Train Loss: 0.908506, Train-Class-Acc: {0: '79.84%', 2: '84.23%', 3: '86.78%', 4: '83.54%', 5: '89.30%', 6: '43.32%', 7: '69.26%', 8: '80.41%', 9: '39.86%'}\n",
      "Val Loss: 0.952101, Val Acc: 79.11%, Val-Class-Acc: {0: '75.54%', 2: '86.11%', 3: '95.49%', 4: '65.00%', 5: '85.67%', 6: '35.19%', 7: '75.20%', 8: '86.62%', 9: '27.03%'}, LR: 0.000729\n",
      "Epoch 86/200, Train Loss: 0.937582, Train-Class-Acc: {0: '80.52%', 2: '82.32%', 3: '87.19%', 4: '80.38%', 5: '89.08%', 6: '42.63%', 7: '69.26%', 8: '82.01%', 9: '43.92%'}\n",
      "Val Loss: 1.034414, Val Acc: 79.77%, Val-Class-Acc: {0: '80.98%', 2: '86.11%', 3: '93.85%', 4: '60.00%', 5: '87.46%', 6: '27.78%', 7: '80.00%', 8: '87.26%', 9: '27.03%'}, LR: 0.000729\n",
      "Epoch 87/200, Train Loss: 0.964462, Train-Class-Acc: {0: '80.65%', 2: '83.88%', 3: '87.91%', 4: '75.95%', 5: '89.45%', 6: '42.63%', 7: '68.06%', 8: '77.87%', 9: '47.97%'}\n",
      "Val Loss: 1.570996, Val Acc: 81.00%, Val-Class-Acc: {0: '80.43%', 2: '84.72%', 3: '89.75%', 4: '75.00%', 5: '92.84%', 6: '37.04%', 7: '80.80%', 8: '84.71%', 9: '24.32%'}, LR: 0.000729\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_71.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_87.pth\n",
      "Epoch 88/200, Train Loss: 0.913944, Train-Class-Acc: {0: '79.16%', 2: '82.67%', 3: '86.48%', 4: '76.58%', 5: '90.28%', 6: '41.94%', 7: '68.46%', 8: '79.14%', 9: '47.30%'}\n",
      "Val Loss: 1.318606, Val Acc: 79.84%, Val-Class-Acc: {0: '79.35%', 2: '85.42%', 3: '95.49%', 4: '65.00%', 5: '91.04%', 6: '31.48%', 7: '72.80%', 8: '80.25%', 9: '35.14%'}, LR: 0.000729\n",
      "Epoch 89/200, Train Loss: 0.874200, Train-Class-Acc: {0: '79.84%', 2: '83.02%', 3: '85.76%', 4: '78.48%', 5: '88.93%', 6: '40.32%', 7: '66.27%', 8: '79.46%', 9: '46.62%'}\n",
      "Val Loss: 1.285833, Val Acc: 80.13%, Val-Class-Acc: {0: '78.26%', 2: '85.42%', 3: '94.67%', 4: '70.00%', 5: '89.85%', 6: '29.63%', 7: '76.00%', 8: '85.99%', 9: '32.43%'}, LR: 0.000729\n",
      "Epoch 90/200, Train Loss: 0.922813, Train-Class-Acc: {0: '80.11%', 2: '83.02%', 3: '87.19%', 4: '75.32%', 5: '89.68%', 6: '40.78%', 7: '67.07%', 8: '82.17%', 9: '50.68%'}\n",
      "Val Loss: 1.274313, Val Acc: 79.55%, Val-Class-Acc: {0: '80.98%', 2: '84.03%', 3: '86.07%', 4: '72.50%', 5: '93.73%', 6: '32.41%', 7: '74.40%', 8: '84.08%', 9: '27.03%'}, LR: 0.000729\n",
      "Epoch 91/200, Train Loss: 0.806404, Train-Class-Acc: {0: '80.65%', 2: '83.54%', 3: '86.89%', 4: '77.85%', 5: '89.90%', 6: '45.16%', 7: '68.26%', 8: '80.10%', 9: '54.05%'}\n",
      "Val Loss: 2.299969, Val Acc: 79.55%, Val-Class-Acc: {0: '82.07%', 2: '84.72%', 3: '85.25%', 4: '67.50%', 5: '94.33%', 6: '27.78%', 7: '74.40%', 8: '86.62%', 9: '27.03%'}, LR: 0.000729\n",
      "Epoch 92/200, Train Loss: 0.826140, Train-Class-Acc: {0: '80.11%', 2: '83.71%', 3: '88.11%', 4: '77.85%', 5: '89.98%', 6: '45.62%', 7: '69.46%', 8: '80.73%', 9: '47.97%'}\n",
      "Val Loss: 1.151470, Val Acc: 80.86%, Val-Class-Acc: {0: '78.26%', 2: '87.50%', 3: '92.62%', 4: '67.50%', 5: '89.85%', 6: '32.41%', 7: '80.80%', 8: '89.81%', 9: '27.03%'}, LR: 0.000656\n",
      "Epoch 93/200, Train Loss: 0.732829, Train-Class-Acc: {0: '82.56%', 2: '84.92%', 3: '86.58%', 4: '76.58%', 5: '89.53%', 6: '46.77%', 7: '70.06%', 8: '81.05%', 9: '45.95%'}\n",
      "Val Loss: 1.607376, Val Acc: 79.91%, Val-Class-Acc: {0: '80.98%', 2: '82.64%', 3: '90.57%', 4: '65.00%', 5: '91.04%', 6: '34.26%', 7: '78.40%', 8: '85.99%', 9: '21.62%'}, LR: 0.000656\n",
      "Epoch 94/200, Train Loss: 0.752460, Train-Class-Acc: {0: '80.25%', 2: '83.71%', 3: '86.68%', 4: '76.58%', 5: '89.90%', 6: '40.55%', 7: '69.26%', 8: '80.57%', 9: '40.54%'}\n",
      "Val Loss: 0.777386, Val Acc: 80.35%, Val-Class-Acc: {0: '80.98%', 2: '82.64%', 3: '94.26%', 4: '75.00%', 5: '88.36%', 6: '27.78%', 7: '80.80%', 8: '88.54%', 9: '27.03%'}, LR: 0.000656\n",
      "Epoch 95/200, Train Loss: 0.780252, Train-Class-Acc: {0: '78.20%', 2: '82.50%', 3: '86.37%', 4: '75.32%', 5: '89.53%', 6: '40.55%', 7: '66.87%', 8: '78.98%', 9: '47.97%'}\n",
      "Val Loss: 1.226380, Val Acc: 80.20%, Val-Class-Acc: {0: '84.24%', 2: '87.50%', 3: '91.80%', 4: '72.50%', 5: '89.25%', 6: '27.78%', 7: '80.80%', 8: '80.89%', 9: '29.73%'}, LR: 0.000656\n",
      "Epoch 96/200, Train Loss: 0.835785, Train-Class-Acc: {0: '79.84%', 2: '84.75%', 3: '86.58%', 4: '75.95%', 5: '89.98%', 6: '42.86%', 7: '67.66%', 8: '81.05%', 9: '43.24%'}\n",
      "Val Loss: 1.007947, Val Acc: 79.62%, Val-Class-Acc: {0: '80.98%', 2: '87.50%', 3: '95.90%', 4: '72.50%', 5: '84.48%', 6: '29.63%', 7: '75.20%', 8: '86.62%', 9: '29.73%'}, LR: 0.000656\n",
      "Epoch 97/200, Train Loss: 0.659653, Train-Class-Acc: {0: '81.06%', 2: '84.75%', 3: '88.93%', 4: '81.01%', 5: '90.88%', 6: '44.47%', 7: '69.06%', 8: '81.69%', 9: '45.27%'}\n",
      "Val Loss: 0.998459, Val Acc: 80.71%, Val-Class-Acc: {0: '76.63%', 2: '86.81%', 3: '93.85%', 4: '72.50%', 5: '90.15%', 6: '34.26%', 7: '76.00%', 8: '89.17%', 9: '29.73%'}, LR: 0.000656\n",
      "Epoch 98/200, Train Loss: 0.678494, Train-Class-Acc: {0: '81.61%', 2: '86.31%', 3: '88.11%', 4: '79.11%', 5: '91.55%', 6: '46.08%', 7: '69.66%', 8: '81.37%', 9: '45.95%'}\n",
      "Val Loss: 1.101457, Val Acc: 81.08%, Val-Class-Acc: {0: '76.09%', 2: '87.50%', 3: '92.21%', 4: '72.50%', 5: '91.64%', 6: '34.26%', 7: '81.60%', 8: '87.26%', 9: '29.73%'}, LR: 0.000656\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_79.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_98.pth\n",
      "Epoch 99/200, Train Loss: 0.716725, Train-Class-Acc: {0: '82.56%', 2: '85.79%', 3: '89.86%', 4: '80.38%', 5: '91.17%', 6: '45.85%', 7: '68.26%', 8: '83.76%', 9: '47.97%'}\n",
      "Val Loss: 0.859372, Val Acc: 80.64%, Val-Class-Acc: {0: '78.26%', 2: '84.72%', 3: '93.44%', 4: '70.00%', 5: '88.36%', 6: '35.19%', 7: '81.60%', 8: '87.90%', 9: '32.43%'}, LR: 0.000656\n",
      "Epoch 100/200, Train Loss: 0.659373, Train-Class-Acc: {0: '80.79%', 2: '83.71%', 3: '88.22%', 4: '81.01%', 5: '90.58%', 6: '46.31%', 7: '71.06%', 8: '82.01%', 9: '48.65%'}\n",
      "Val Loss: 1.293299, Val Acc: 80.57%, Val-Class-Acc: {0: '79.89%', 2: '85.42%', 3: '95.08%', 4: '75.00%', 5: '90.75%', 6: '33.33%', 7: '77.60%', 8: '81.53%', 9: '27.03%'}, LR: 0.000656\n",
      "Epoch 101/200, Train Loss: 0.666123, Train-Class-Acc: {0: '82.97%', 2: '85.44%', 3: '88.11%', 4: '79.75%', 5: '89.45%', 6: '47.00%', 7: '70.26%', 8: '82.64%', 9: '48.65%'}\n",
      "Val Loss: 1.029545, Val Acc: 80.20%, Val-Class-Acc: {0: '78.80%', 2: '83.33%', 3: '94.67%', 4: '72.50%', 5: '89.85%', 6: '35.19%', 7: '76.80%', 8: '83.44%', 9: '29.73%'}, LR: 0.000656\n",
      "Epoch 102/200, Train Loss: 0.737875, Train-Class-Acc: {0: '80.93%', 2: '84.23%', 3: '88.22%', 4: '79.11%', 5: '90.28%', 6: '47.24%', 7: '68.06%', 8: '82.01%', 9: '52.03%'}\n",
      "Val Loss: 1.831940, Val Acc: 79.77%, Val-Class-Acc: {0: '78.80%', 2: '84.72%', 3: '93.03%', 4: '77.50%', 5: '91.64%', 6: '34.26%', 7: '70.40%', 8: '83.44%', 9: '21.62%'}, LR: 0.000656\n",
      "Epoch 103/200, Train Loss: 0.999975, Train-Class-Acc: {0: '80.52%', 2: '85.10%', 3: '85.45%', 4: '79.75%', 5: '89.08%', 6: '44.24%', 7: '67.66%', 8: '81.37%', 9: '37.84%'}\n",
      "Val Loss: 1.260349, Val Acc: 80.42%, Val-Class-Acc: {0: '79.35%', 2: '84.72%', 3: '94.67%', 4: '75.00%', 5: '91.64%', 6: '26.85%', 7: '76.80%', 8: '84.08%', 9: '32.43%'}, LR: 0.000656\n",
      "Epoch 104/200, Train Loss: 0.728609, Train-Class-Acc: {0: '83.65%', 2: '85.79%', 3: '89.24%', 4: '81.65%', 5: '90.43%', 6: '43.55%', 7: '69.46%', 8: '81.21%', 9: '47.30%'}\n",
      "Val Loss: 1.124957, Val Acc: 79.84%, Val-Class-Acc: {0: '76.63%', 2: '84.72%', 3: '94.67%', 4: '70.00%', 5: '90.45%', 6: '34.26%', 7: '77.60%', 8: '82.17%', 9: '24.32%'}, LR: 0.000656\n",
      "Epoch 105/200, Train Loss: 0.718505, Train-Class-Acc: {0: '82.56%', 2: '86.14%', 3: '88.73%', 4: '81.01%', 5: '91.02%', 6: '48.62%', 7: '71.06%', 8: '82.96%', 9: '50.00%'}\n",
      "Val Loss: 0.952817, Val Acc: 80.71%, Val-Class-Acc: {0: '76.63%', 2: '84.72%', 3: '91.39%', 4: '77.50%', 5: '92.84%', 6: '36.11%', 7: '78.40%', 8: '84.71%', 9: '29.73%'}, LR: 0.000656\n",
      "Epoch 106/200, Train Loss: 0.806710, Train-Class-Acc: {0: '83.51%', 2: '84.75%', 3: '89.45%', 4: '80.38%', 5: '90.95%', 6: '45.85%', 7: '70.86%', 8: '83.12%', 9: '48.65%'}\n",
      "Val Loss: 1.269907, Val Acc: 80.13%, Val-Class-Acc: {0: '80.98%', 2: '86.11%', 3: '94.67%', 4: '77.50%', 5: '87.16%', 6: '30.56%', 7: '80.00%', 8: '84.08%', 9: '24.32%'}, LR: 0.000590\n",
      "Epoch 107/200, Train Loss: 0.721965, Train-Class-Acc: {0: '81.20%', 2: '86.48%', 3: '90.06%', 4: '82.91%', 5: '91.40%', 6: '47.93%', 7: '69.66%', 8: '84.24%', 9: '51.35%'}\n",
      "Val Loss: 0.991938, Val Acc: 79.91%, Val-Class-Acc: {0: '79.89%', 2: '86.11%', 3: '93.85%', 4: '77.50%', 5: '86.57%', 6: '31.48%', 7: '80.80%', 8: '85.99%', 9: '18.92%'}, LR: 0.000590\n",
      "Epoch 108/200, Train Loss: 0.719652, Train-Class-Acc: {0: '83.11%', 2: '82.50%', 3: '89.24%', 4: '81.65%', 5: '90.58%', 6: '47.93%', 7: '69.66%', 8: '82.80%', 9: '45.27%'}\n",
      "Val Loss: 1.355805, Val Acc: 80.86%, Val-Class-Acc: {0: '79.35%', 2: '85.42%', 3: '94.26%', 4: '77.50%', 5: '90.45%', 6: '32.41%', 7: '77.60%', 8: '85.35%', 9: '32.43%'}, LR: 0.000590\n",
      "Epoch 109/200, Train Loss: 0.678752, Train-Class-Acc: {0: '82.70%', 2: '85.44%', 3: '88.01%', 4: '80.38%', 5: '90.80%', 6: '49.31%', 7: '71.26%', 8: '82.17%', 9: '50.00%'}\n",
      "Val Loss: 0.959077, Val Acc: 80.86%, Val-Class-Acc: {0: '79.89%', 2: '83.33%', 3: '93.85%', 4: '82.50%', 5: '91.34%', 6: '30.56%', 7: '79.20%', 8: '84.71%', 9: '29.73%'}, LR: 0.000590\n",
      "Epoch 110/200, Train Loss: 0.726321, Train-Class-Acc: {0: '81.88%', 2: '85.27%', 3: '88.83%', 4: '81.65%', 5: '90.80%', 6: '43.78%', 7: '67.66%', 8: '82.17%', 9: '56.08%'}\n",
      "Val Loss: 1.218543, Val Acc: 81.08%, Val-Class-Acc: {0: '83.70%', 2: '87.50%', 3: '90.57%', 4: '72.50%', 5: '92.84%', 6: '31.48%', 7: '80.80%', 8: '81.53%', 9: '27.03%'}, LR: 0.000590\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_70.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_110.pth\n",
      "Epoch 111/200, Train Loss: 0.693459, Train-Class-Acc: {0: '83.11%', 2: '84.92%', 3: '89.55%', 4: '85.44%', 5: '91.55%', 6: '48.62%', 7: '72.85%', 8: '82.80%', 9: '54.05%'}\n",
      "Val Loss: 1.173796, Val Acc: 80.79%, Val-Class-Acc: {0: '78.26%', 2: '85.42%', 3: '93.03%', 4: '75.00%', 5: '91.04%', 6: '34.26%', 7: '79.20%', 8: '85.99%', 9: '27.03%'}, LR: 0.000590\n",
      "Epoch 112/200, Train Loss: 0.810861, Train-Class-Acc: {0: '83.92%', 2: '86.31%', 3: '88.83%', 4: '79.11%', 5: '91.25%', 6: '51.38%', 7: '70.46%', 8: '83.44%', 9: '52.70%'}\n",
      "Val Loss: 1.271081, Val Acc: 80.79%, Val-Class-Acc: {0: '80.43%', 2: '85.42%', 3: '95.08%', 4: '77.50%', 5: '90.45%', 6: '26.85%', 7: '81.60%', 8: '84.08%', 9: '27.03%'}, LR: 0.000590\n",
      "Epoch 113/200, Train Loss: 0.700152, Train-Class-Acc: {0: '83.79%', 2: '85.79%', 3: '90.06%', 4: '79.75%', 5: '91.40%', 6: '47.93%', 7: '71.66%', 8: '84.71%', 9: '53.38%'}\n",
      "Val Loss: 1.313995, Val Acc: 80.06%, Val-Class-Acc: {0: '79.89%', 2: '85.42%', 3: '93.03%', 4: '77.50%', 5: '88.36%', 6: '32.41%', 7: '78.40%', 8: '84.71%', 9: '27.03%'}, LR: 0.000590\n",
      "Epoch 114/200, Train Loss: 0.625312, Train-Class-Acc: {0: '84.74%', 2: '87.18%', 3: '89.86%', 4: '81.65%', 5: '92.52%', 6: '50.46%', 7: '72.65%', 8: '86.31%', 9: '49.32%'}\n",
      "Val Loss: 0.978534, Val Acc: 79.99%, Val-Class-Acc: {0: '72.83%', 2: '86.11%', 3: '95.49%', 4: '77.50%', 5: '87.76%', 6: '35.19%', 7: '80.00%', 8: '84.08%', 9: '35.14%'}, LR: 0.000590\n",
      "Epoch 115/200, Train Loss: 0.724797, Train-Class-Acc: {0: '83.79%', 2: '86.66%', 3: '89.14%', 4: '84.81%', 5: '91.70%', 6: '52.30%', 7: '72.85%', 8: '83.92%', 9: '52.03%'}\n",
      "Val Loss: 1.302090, Val Acc: 80.35%, Val-Class-Acc: {0: '74.46%', 2: '86.81%', 3: '92.62%', 4: '77.50%', 5: '89.85%', 6: '31.48%', 7: '79.20%', 8: '85.99%', 9: '43.24%'}, LR: 0.000590\n",
      "Epoch 116/200, Train Loss: 0.762186, Train-Class-Acc: {0: '82.70%', 2: '86.14%', 3: '89.86%', 4: '79.11%', 5: '91.25%', 6: '48.16%', 7: '70.66%', 8: '83.28%', 9: '58.11%'}\n",
      "Val Loss: 1.153407, Val Acc: 80.71%, Val-Class-Acc: {0: '85.33%', 2: '82.64%', 3: '91.80%', 4: '75.00%', 5: '91.04%', 6: '34.26%', 7: '79.20%', 8: '82.80%', 9: '21.62%'}, LR: 0.000590\n",
      "Epoch 117/200, Train Loss: 0.884476, Train-Class-Acc: {0: '83.11%', 2: '88.04%', 3: '89.65%', 4: '80.38%', 5: '90.65%', 6: '53.23%', 7: '72.26%', 8: '84.08%', 9: '49.32%'}\n",
      "Val Loss: 2.629139, Val Acc: 80.20%, Val-Class-Acc: {0: '78.26%', 2: '84.72%', 3: '91.80%', 4: '75.00%', 5: '90.15%', 6: '35.19%', 7: '80.00%', 8: '83.44%', 9: '29.73%'}, LR: 0.000531\n",
      "Epoch 118/200, Train Loss: 0.833472, Train-Class-Acc: {0: '83.92%', 2: '85.27%', 3: '87.09%', 4: '79.75%', 5: '90.05%', 6: '46.08%', 7: '70.86%', 8: '83.92%', 9: '52.03%'}\n",
      "Val Loss: 1.208586, Val Acc: 80.79%, Val-Class-Acc: {0: '76.63%', 2: '86.11%', 3: '85.66%', 4: '77.50%', 5: '93.43%', 6: '39.81%', 7: '80.80%', 8: '85.35%', 9: '37.84%'}, LR: 0.000531\n",
      "Epoch 119/200, Train Loss: 0.675856, Train-Class-Acc: {0: '82.97%', 2: '87.52%', 3: '89.04%', 4: '79.11%', 5: '92.45%', 6: '53.23%', 7: '73.45%', 8: '85.35%', 9: '45.95%'}\n",
      "Val Loss: 0.957422, Val Acc: 80.49%, Val-Class-Acc: {0: '78.80%', 2: '87.50%', 3: '93.44%', 4: '77.50%', 5: '87.16%', 6: '33.33%', 7: '83.20%', 8: '84.08%', 9: '32.43%'}, LR: 0.000531\n",
      "Epoch 120/200, Train Loss: 0.665506, Train-Class-Acc: {0: '82.70%', 2: '87.18%', 3: '89.86%', 4: '84.81%', 5: '91.10%', 6: '51.15%', 7: '72.46%', 8: '81.85%', 9: '51.35%'}\n",
      "Val Loss: 1.055079, Val Acc: 80.13%, Val-Class-Acc: {0: '78.80%', 2: '85.42%', 3: '93.03%', 4: '72.50%', 5: '89.55%', 6: '34.26%', 7: '80.00%', 8: '82.17%', 9: '29.73%'}, LR: 0.000531\n",
      "Epoch 121/200, Train Loss: 0.643114, Train-Class-Acc: {0: '83.92%', 2: '86.14%', 3: '90.57%', 4: '82.91%', 5: '91.77%', 6: '48.62%', 7: '72.06%', 8: '83.76%', 9: '50.68%'}\n",
      "Val Loss: 1.494007, Val Acc: 80.06%, Val-Class-Acc: {0: '80.98%', 2: '85.42%', 3: '89.75%', 4: '72.50%', 5: '90.75%', 6: '34.26%', 7: '79.20%', 8: '82.80%', 9: '27.03%'}, LR: 0.000531\n",
      "Epoch 122/200, Train Loss: 0.865666, Train-Class-Acc: {0: '82.56%', 2: '85.27%', 3: '87.70%', 4: '76.58%', 5: '91.25%', 6: '55.76%', 7: '72.85%', 8: '85.67%', 9: '53.38%'}\n",
      "Val Loss: 1.357941, Val Acc: 80.28%, Val-Class-Acc: {0: '80.98%', 2: '84.72%', 3: '93.44%', 4: '70.00%', 5: '88.66%', 6: '32.41%', 7: '80.00%', 8: '85.99%', 9: '24.32%'}, LR: 0.000531\n",
      "Epoch 123/200, Train Loss: 0.637788, Train-Class-Acc: {0: '84.60%', 2: '87.87%', 3: '89.65%', 4: '81.01%', 5: '90.88%', 6: '53.23%', 7: '71.06%', 8: '85.03%', 9: '57.43%'}\n",
      "Val Loss: 1.162907, Val Acc: 80.20%, Val-Class-Acc: {0: '79.89%', 2: '84.03%', 3: '94.26%', 4: '70.00%', 5: '89.25%', 6: '34.26%', 7: '80.00%', 8: '82.17%', 9: '29.73%'}, LR: 0.000531\n",
      "Epoch 124/200, Train Loss: 0.584750, Train-Class-Acc: {0: '85.15%', 2: '87.52%', 3: '89.75%', 4: '82.91%', 5: '91.10%', 6: '52.07%', 7: '73.05%', 8: '83.60%', 9: '54.05%'}\n",
      "Val Loss: 1.274814, Val Acc: 79.84%, Val-Class-Acc: {0: '79.35%', 2: '84.03%', 3: '93.03%', 4: '72.50%', 5: '89.85%', 6: '30.56%', 7: '76.80%', 8: '85.35%', 9: '27.03%'}, LR: 0.000531\n",
      "Epoch 125/200, Train Loss: 0.560434, Train-Class-Acc: {0: '84.33%', 2: '87.18%', 3: '90.88%', 4: '82.28%', 5: '91.32%', 6: '51.38%', 7: '71.46%', 8: '85.19%', 9: '49.32%'}\n",
      "Val Loss: 1.081452, Val Acc: 80.35%, Val-Class-Acc: {0: '77.72%', 2: '85.42%', 3: '94.26%', 4: '77.50%', 5: '88.96%', 6: '33.33%', 7: '79.20%', 8: '84.08%', 9: '32.43%'}, LR: 0.000531\n",
      "Epoch 126/200, Train Loss: 0.540867, Train-Class-Acc: {0: '82.70%', 2: '88.73%', 3: '91.80%', 4: '82.28%', 5: '92.52%', 6: '49.08%', 7: '72.06%', 8: '84.24%', 9: '59.46%'}\n",
      "Val Loss: 0.934721, Val Acc: 79.69%, Val-Class-Acc: {0: '77.72%', 2: '85.42%', 3: '95.49%', 4: '75.00%', 5: '88.36%', 6: '32.41%', 7: '73.60%', 8: '84.71%', 9: '27.03%'}, LR: 0.000531\n",
      "Epoch 127/200, Train Loss: 0.631164, Train-Class-Acc: {0: '84.74%', 2: '87.69%', 3: '89.55%', 4: '82.28%', 5: '92.45%', 6: '54.84%', 7: '71.06%', 8: '84.39%', 9: '58.11%'}\n",
      "Val Loss: 2.086039, Val Acc: 80.49%, Val-Class-Acc: {0: '76.09%', 2: '84.03%', 3: '95.08%', 4: '77.50%', 5: '90.15%', 6: '34.26%', 7: '78.40%', 8: '84.71%', 9: '32.43%'}, LR: 0.000531\n",
      "Epoch 128/200, Train Loss: 0.613450, Train-Class-Acc: {0: '85.01%', 2: '88.04%', 3: '91.09%', 4: '80.38%', 5: '92.52%', 6: '51.84%', 7: '72.26%', 8: '86.15%', 9: '55.41%'}\n",
      "Val Loss: 0.999948, Val Acc: 80.57%, Val-Class-Acc: {0: '76.09%', 2: '84.03%', 3: '93.44%', 4: '72.50%', 5: '90.15%', 6: '41.67%', 7: '80.80%', 8: '84.08%', 9: '24.32%'}, LR: 0.000478\n",
      "Epoch 129/200, Train Loss: 0.559911, Train-Class-Acc: {0: '84.47%', 2: '90.29%', 3: '90.68%', 4: '82.91%', 5: '91.70%', 6: '52.76%', 7: '72.06%', 8: '86.62%', 9: '58.78%'}\n",
      "Val Loss: 1.152713, Val Acc: 80.71%, Val-Class-Acc: {0: '79.89%', 2: '84.72%', 3: '93.03%', 4: '77.50%', 5: '91.34%', 6: '34.26%', 7: '78.40%', 8: '83.44%', 9: '27.03%'}, LR: 0.000478\n",
      "Epoch 130/200, Train Loss: 0.736344, Train-Class-Acc: {0: '85.42%', 2: '89.60%', 3: '90.88%', 4: '86.08%', 5: '91.70%', 6: '49.77%', 7: '72.65%', 8: '86.31%', 9: '56.76%'}\n",
      "Val Loss: 1.035194, Val Acc: 80.06%, Val-Class-Acc: {0: '78.26%', 2: '85.42%', 3: '93.85%', 4: '77.50%', 5: '87.76%', 6: '36.11%', 7: '78.40%', 8: '83.44%', 9: '29.73%'}, LR: 0.000478\n",
      "Epoch 131/200, Train Loss: 0.626926, Train-Class-Acc: {0: '83.92%', 2: '87.69%', 3: '90.57%', 4: '84.18%', 5: '91.70%', 6: '54.15%', 7: '73.05%', 8: '85.67%', 9: '54.05%'}\n",
      "Val Loss: 1.220665, Val Acc: 80.06%, Val-Class-Acc: {0: '79.35%', 2: '84.72%', 3: '90.57%', 4: '75.00%', 5: '91.64%', 6: '31.48%', 7: '79.20%', 8: '82.80%', 9: '29.73%'}, LR: 0.000478\n",
      "Epoch 132/200, Train Loss: 0.563770, Train-Class-Acc: {0: '84.60%', 2: '87.35%', 3: '91.70%', 4: '82.28%', 5: '92.30%', 6: '53.46%', 7: '72.26%', 8: '85.35%', 9: '54.73%'}\n",
      "Val Loss: 1.161385, Val Acc: 80.49%, Val-Class-Acc: {0: '79.35%', 2: '85.42%', 3: '93.85%', 4: '75.00%', 5: '88.96%', 6: '35.19%', 7: '79.20%', 8: '84.71%', 9: '27.03%'}, LR: 0.000478\n",
      "Epoch 133/200, Train Loss: 0.505833, Train-Class-Acc: {0: '84.33%', 2: '89.08%', 3: '91.39%', 4: '84.18%', 5: '92.30%', 6: '55.76%', 7: '75.25%', 8: '86.31%', 9: '54.73%'}\n",
      "Val Loss: 1.055123, Val Acc: 80.13%, Val-Class-Acc: {0: '76.09%', 2: '84.03%', 3: '94.67%', 4: '72.50%', 5: '90.15%', 6: '38.89%', 7: '78.40%', 8: '82.17%', 9: '24.32%'}, LR: 0.000478\n",
      "Epoch 134/200, Train Loss: 0.611659, Train-Class-Acc: {0: '87.74%', 2: '87.18%', 3: '90.57%', 4: '82.91%', 5: '91.92%', 6: '55.76%', 7: '73.25%', 8: '85.51%', 9: '55.41%'}\n",
      "Val Loss: 0.959053, Val Acc: 79.26%, Val-Class-Acc: {0: '77.17%', 2: '85.42%', 3: '89.75%', 4: '72.50%', 5: '88.66%', 6: '34.26%', 7: '78.40%', 8: '85.35%', 9: '27.03%'}, LR: 0.000478\n",
      "Epoch 135/200, Train Loss: 0.539829, Train-Class-Acc: {0: '85.29%', 2: '90.12%', 3: '90.37%', 4: '84.81%', 5: '91.62%', 6: '51.61%', 7: '72.46%', 8: '86.94%', 9: '56.08%'}\n",
      "Val Loss: 1.102690, Val Acc: 79.99%, Val-Class-Acc: {0: '78.80%', 2: '84.03%', 3: '92.21%', 4: '72.50%', 5: '90.75%', 6: '35.19%', 7: '78.40%', 8: '82.80%', 9: '24.32%'}, LR: 0.000478\n",
      "Epoch 136/200, Train Loss: 0.705158, Train-Class-Acc: {0: '85.01%', 2: '86.14%', 3: '89.55%', 4: '84.81%', 5: '92.22%', 6: '50.69%', 7: '71.86%', 8: '85.35%', 9: '52.70%'}\n",
      "Val Loss: 1.321528, Val Acc: 79.48%, Val-Class-Acc: {0: '80.43%', 2: '85.42%', 3: '91.80%', 4: '75.00%', 5: '88.36%', 6: '35.19%', 7: '73.60%', 8: '84.08%', 9: '24.32%'}, LR: 0.000478\n",
      "Epoch 137/200, Train Loss: 0.579040, Train-Class-Acc: {0: '84.74%', 2: '88.56%', 3: '91.19%', 4: '80.38%', 5: '91.62%', 6: '54.61%', 7: '74.65%', 8: '85.83%', 9: '58.11%'}\n",
      "Val Loss: 1.467536, Val Acc: 80.49%, Val-Class-Acc: {0: '80.43%', 2: '84.03%', 3: '89.75%', 4: '75.00%', 5: '94.03%', 6: '30.56%', 7: '77.60%', 8: '82.80%', 9: '35.14%'}, LR: 0.000478\n",
      "Epoch 138/200, Train Loss: 0.687977, Train-Class-Acc: {0: '85.69%', 2: '88.91%', 3: '90.16%', 4: '82.28%', 5: '92.82%', 6: '57.37%', 7: '76.25%', 8: '84.55%', 9: '55.41%'}\n",
      "Val Loss: 1.610820, Val Acc: 79.84%, Val-Class-Acc: {0: '80.43%', 2: '84.72%', 3: '88.93%', 4: '80.00%', 5: '90.15%', 6: '24.07%', 7: '82.40%', 8: '84.08%', 9: '40.54%'}, LR: 0.000478\n",
      "Epoch 139/200, Train Loss: 0.697097, Train-Class-Acc: {0: '83.51%', 2: '87.18%', 3: '89.86%', 4: '83.54%', 5: '91.85%', 6: '54.84%', 7: '71.46%', 8: '85.35%', 9: '54.73%'}\n",
      "Val Loss: 1.523680, Val Acc: 80.71%, Val-Class-Acc: {0: '79.35%', 2: '85.42%', 3: '92.62%', 4: '72.50%', 5: '91.64%', 6: '38.89%', 7: '78.40%', 8: '80.89%', 9: '29.73%'}, LR: 0.000430\n",
      "Epoch 140/200, Train Loss: 0.578213, Train-Class-Acc: {0: '85.56%', 2: '87.87%', 3: '89.55%', 4: '81.01%', 5: '91.70%', 6: '53.69%', 7: '71.46%', 8: '86.78%', 9: '57.43%'}\n",
      "Val Loss: 1.111199, Val Acc: 79.62%, Val-Class-Acc: {0: '75.00%', 2: '80.56%', 3: '95.49%', 4: '75.00%', 5: '90.15%', 6: '34.26%', 7: '72.80%', 8: '85.35%', 9: '35.14%'}, LR: 0.000430\n",
      "Epoch 141/200, Train Loss: 0.561009, Train-Class-Acc: {0: '85.69%', 2: '89.08%', 3: '91.50%', 4: '84.18%', 5: '92.89%', 6: '55.99%', 7: '73.25%', 8: '87.58%', 9: '56.08%'}\n",
      "Val Loss: 0.996685, Val Acc: 80.13%, Val-Class-Acc: {0: '75.00%', 2: '84.03%', 3: '94.67%', 4: '77.50%', 5: '89.55%', 6: '37.96%', 7: '79.20%', 8: '82.80%', 9: '27.03%'}, LR: 0.000430\n",
      "Epoch 142/200, Train Loss: 0.514953, Train-Class-Acc: {0: '85.01%', 2: '89.60%', 3: '91.29%', 4: '87.97%', 5: '92.97%', 6: '56.45%', 7: '74.85%', 8: '87.58%', 9: '56.08%'}\n",
      "Val Loss: 0.932192, Val Acc: 79.77%, Val-Class-Acc: {0: '75.54%', 2: '86.81%', 3: '91.39%', 4: '75.00%', 5: '89.25%', 6: '34.26%', 7: '77.60%', 8: '85.35%', 9: '32.43%'}, LR: 0.000430\n",
      "Epoch 143/200, Train Loss: 0.614462, Train-Class-Acc: {0: '85.69%', 2: '89.77%', 3: '90.68%', 4: '82.28%', 5: '92.74%', 6: '54.38%', 7: '75.45%', 8: '86.15%', 9: '60.81%'}\n",
      "Val Loss: 1.317962, Val Acc: 80.35%, Val-Class-Acc: {0: '78.80%', 2: '83.33%', 3: '94.67%', 4: '75.00%', 5: '89.55%', 6: '38.89%', 7: '77.60%', 8: '82.80%', 9: '24.32%'}, LR: 0.000430\n",
      "Epoch 144/200, Train Loss: 0.608102, Train-Class-Acc: {0: '86.65%', 2: '88.56%', 3: '91.91%', 4: '87.34%', 5: '93.04%', 6: '57.14%', 7: '74.25%', 8: '86.62%', 9: '52.03%'}\n",
      "Val Loss: 1.300695, Val Acc: 80.35%, Val-Class-Acc: {0: '75.00%', 2: '85.42%', 3: '89.34%', 4: '72.50%', 5: '94.03%', 6: '35.19%', 7: '79.20%', 8: '82.17%', 9: '40.54%'}, LR: 0.000430\n",
      "Epoch 145/200, Train Loss: 0.520056, Train-Class-Acc: {0: '85.29%', 2: '90.47%', 3: '92.11%', 4: '80.38%', 5: '92.97%', 6: '54.84%', 7: '72.85%', 8: '85.99%', 9: '56.08%'}\n",
      "Val Loss: 1.120372, Val Acc: 80.71%, Val-Class-Acc: {0: '79.35%', 2: '83.33%', 3: '93.85%', 4: '75.00%', 5: '90.45%', 6: '39.81%', 7: '76.80%', 8: '83.44%', 9: '29.73%'}, LR: 0.000430\n",
      "Epoch 146/200, Train Loss: 0.539784, Train-Class-Acc: {0: '88.69%', 2: '89.25%', 3: '91.80%', 4: '85.44%', 5: '93.57%', 6: '55.99%', 7: '74.85%', 8: '85.67%', 9: '57.43%'}\n",
      "Val Loss: 1.094538, Val Acc: 79.55%, Val-Class-Acc: {0: '78.26%', 2: '81.94%', 3: '95.08%', 4: '75.00%', 5: '88.36%', 6: '32.41%', 7: '76.80%', 8: '84.08%', 9: '27.03%'}, LR: 0.000430\n",
      "Epoch 147/200, Train Loss: 0.465198, Train-Class-Acc: {0: '87.74%', 2: '88.56%', 3: '91.19%', 4: '83.54%', 5: '93.04%', 6: '57.60%', 7: '75.85%', 8: '89.33%', 9: '54.73%'}\n",
      "Val Loss: 1.238865, Val Acc: 80.79%, Val-Class-Acc: {0: '79.89%', 2: '84.03%', 3: '94.26%', 4: '77.50%', 5: '90.45%', 6: '35.19%', 7: '79.20%', 8: '81.53%', 9: '35.14%'}, LR: 0.000430\n",
      "Epoch 148/200, Train Loss: 0.471594, Train-Class-Acc: {0: '85.97%', 2: '89.43%', 3: '92.42%', 4: '86.08%', 5: '93.42%', 6: '58.76%', 7: '77.05%', 8: '88.22%', 9: '62.16%'}\n",
      "Val Loss: 1.119034, Val Acc: 80.28%, Val-Class-Acc: {0: '77.17%', 2: '84.72%', 3: '94.26%', 4: '72.50%', 5: '89.55%', 6: '38.89%', 7: '76.80%', 8: '82.17%', 9: '35.14%'}, LR: 0.000430\n",
      "Epoch 149/200, Train Loss: 0.506377, Train-Class-Acc: {0: '85.69%', 2: '88.56%', 3: '92.11%', 4: '85.44%', 5: '92.82%', 6: '59.22%', 7: '76.05%', 8: '87.42%', 9: '58.11%'}\n",
      "Val Loss: 1.254988, Val Acc: 80.28%, Val-Class-Acc: {0: '75.00%', 2: '85.42%', 3: '92.62%', 4: '75.00%', 5: '91.64%', 6: '37.96%', 7: '76.00%', 8: '82.80%', 9: '35.14%'}, LR: 0.000430\n",
      "Epoch 150/200, Train Loss: 0.523210, Train-Class-Acc: {0: '88.28%', 2: '89.95%', 3: '91.29%', 4: '84.18%', 5: '93.04%', 6: '58.29%', 7: '73.65%', 8: '87.26%', 9: '60.14%'}\n",
      "Val Loss: 1.407138, Val Acc: 79.18%, Val-Class-Acc: {0: '74.46%', 2: '82.64%', 3: '93.85%', 4: '72.50%', 5: '88.96%', 6: '37.96%', 7: '76.80%', 8: '82.17%', 9: '27.03%'}, LR: 0.000387\n",
      "Epoch 151/200, Train Loss: 0.597040, Train-Class-Acc: {0: '86.92%', 2: '88.91%', 3: '92.01%', 4: '81.65%', 5: '91.85%', 6: '60.14%', 7: '74.85%', 8: '86.94%', 9: '63.51%'}\n",
      "Val Loss: 0.832868, Val Acc: 79.69%, Val-Class-Acc: {0: '78.26%', 2: '84.03%', 3: '95.49%', 4: '77.50%', 5: '87.16%', 6: '31.48%', 7: '77.60%', 8: '84.08%', 9: '29.73%'}, LR: 0.000387\n",
      "Epoch 152/200, Train Loss: 0.573575, Train-Class-Acc: {0: '85.29%', 2: '89.43%', 3: '92.21%', 4: '86.08%', 5: '93.64%', 6: '54.15%', 7: '74.05%', 8: '88.85%', 9: '58.78%'}\n",
      "Val Loss: 1.296184, Val Acc: 80.06%, Val-Class-Acc: {0: '77.17%', 2: '82.64%', 3: '93.44%', 4: '72.50%', 5: '90.15%', 6: '33.33%', 7: '80.80%', 8: '84.08%', 9: '29.73%'}, LR: 0.000387\n",
      "Epoch 153/200, Train Loss: 0.611582, Train-Class-Acc: {0: '87.47%', 2: '89.77%', 3: '91.09%', 4: '84.81%', 5: '92.07%', 6: '56.22%', 7: '72.85%', 8: '87.42%', 9: '62.16%'}\n",
      "Val Loss: 1.009289, Val Acc: 80.42%, Val-Class-Acc: {0: '78.80%', 2: '85.42%', 3: '90.16%', 4: '77.50%', 5: '91.64%', 6: '31.48%', 7: '81.60%', 8: '84.08%', 9: '29.73%'}, LR: 0.000387\n",
      "Epoch 154/200, Train Loss: 0.466066, Train-Class-Acc: {0: '86.51%', 2: '88.73%', 3: '92.52%', 4: '82.91%', 5: '93.42%', 6: '60.14%', 7: '76.05%', 8: '88.69%', 9: '57.43%'}\n",
      "Val Loss: 1.148592, Val Acc: 80.64%, Val-Class-Acc: {0: '73.91%', 2: '84.03%', 3: '94.67%', 4: '77.50%', 5: '90.75%', 6: '41.67%', 7: '78.40%', 8: '82.80%', 9: '32.43%'}, LR: 0.000387\n",
      "Epoch 155/200, Train Loss: 0.507560, Train-Class-Acc: {0: '86.24%', 2: '89.25%', 3: '92.01%', 4: '84.18%', 5: '92.74%', 6: '58.53%', 7: '75.85%', 8: '88.54%', 9: '62.16%'}\n",
      "Val Loss: 1.103274, Val Acc: 80.93%, Val-Class-Acc: {0: '78.26%', 2: '84.03%', 3: '95.49%', 4: '75.00%', 5: '90.45%', 6: '36.11%', 7: '79.20%', 8: '84.08%', 9: '29.73%'}, LR: 0.000387\n",
      "Epoch 156/200, Train Loss: 0.493003, Train-Class-Acc: {0: '86.38%', 2: '90.29%', 3: '92.11%', 4: '84.18%', 5: '94.32%', 6: '56.45%', 7: '74.85%', 8: '88.06%', 9: '61.49%'}\n",
      "Val Loss: 1.274197, Val Acc: 80.35%, Val-Class-Acc: {0: '76.63%', 2: '85.42%', 3: '93.85%', 4: '72.50%', 5: '89.85%', 6: '39.81%', 7: '77.60%', 8: '82.80%', 9: '29.73%'}, LR: 0.000387\n",
      "Epoch 157/200, Train Loss: 0.462841, Train-Class-Acc: {0: '88.42%', 2: '90.29%', 3: '93.03%', 4: '85.44%', 5: '93.79%', 6: '61.06%', 7: '75.65%', 8: '86.94%', 9: '63.51%'}\n",
      "Val Loss: 1.207037, Val Acc: 80.71%, Val-Class-Acc: {0: '79.89%', 2: '85.42%', 3: '90.98%', 4: '75.00%', 5: '91.94%', 6: '37.04%', 7: '76.80%', 8: '82.17%', 9: '37.84%'}, LR: 0.000387\n",
      "Epoch 158/200, Train Loss: 0.516703, Train-Class-Acc: {0: '86.51%', 2: '89.60%', 3: '93.44%', 4: '82.28%', 5: '93.42%', 6: '62.44%', 7: '74.65%', 8: '85.51%', 9: '58.78%'}\n",
      "Val Loss: 1.348907, Val Acc: 80.28%, Val-Class-Acc: {0: '77.17%', 2: '81.94%', 3: '90.98%', 4: '72.50%', 5: '91.64%', 6: '35.19%', 7: '79.20%', 8: '85.99%', 9: '35.14%'}, LR: 0.000387\n",
      "Epoch 159/200, Train Loss: 0.468122, Train-Class-Acc: {0: '85.69%', 2: '88.39%', 3: '92.01%', 4: '86.71%', 5: '93.27%', 6: '59.68%', 7: '76.45%', 8: '87.42%', 9: '60.14%'}\n",
      "Val Loss: 1.655832, Val Acc: 80.86%, Val-Class-Acc: {0: '79.35%', 2: '86.81%', 3: '91.39%', 4: '72.50%', 5: '91.34%', 6: '37.96%', 7: '76.80%', 8: '82.80%', 9: '40.54%'}, LR: 0.000387\n",
      "Epoch 160/200, Train Loss: 0.565936, Train-Class-Acc: {0: '87.60%', 2: '91.33%', 3: '92.21%', 4: '85.44%', 5: '94.02%', 6: '60.60%', 7: '74.45%', 8: '87.90%', 9: '58.11%'}\n",
      "Val Loss: 1.203411, Val Acc: 80.42%, Val-Class-Acc: {0: '78.80%', 2: '82.64%', 3: '92.21%', 4: '72.50%', 5: '91.64%', 6: '35.19%', 7: '76.00%', 8: '85.35%', 9: '35.14%'}, LR: 0.000387\n",
      "Epoch 161/200, Train Loss: 0.540122, Train-Class-Acc: {0: '85.69%', 2: '90.99%', 3: '91.39%', 4: '84.18%', 5: '93.57%', 6: '58.06%', 7: '75.25%', 8: '87.42%', 9: '48.65%'}\n",
      "Val Loss: 1.066684, Val Acc: 80.79%, Val-Class-Acc: {0: '79.89%', 2: '84.72%', 3: '93.44%', 4: '75.00%', 5: '90.75%', 6: '34.26%', 7: '76.00%', 8: '85.35%', 9: '35.14%'}, LR: 0.000349\n",
      "Epoch 162/200, Train Loss: 0.453050, Train-Class-Acc: {0: '87.33%', 2: '89.95%', 3: '93.34%', 4: '89.24%', 5: '93.19%', 6: '60.83%', 7: '76.45%', 8: '88.54%', 9: '58.78%'}\n",
      "Val Loss: 1.162775, Val Acc: 80.35%, Val-Class-Acc: {0: '76.63%', 2: '82.64%', 3: '95.49%', 4: '75.00%', 5: '88.66%', 6: '38.89%', 7: '76.80%', 8: '83.44%', 9: '40.54%'}, LR: 0.000349\n",
      "Epoch 163/200, Train Loss: 0.506112, Train-Class-Acc: {0: '87.74%', 2: '91.51%', 3: '92.62%', 4: '82.91%', 5: '93.57%', 6: '60.60%', 7: '76.05%', 8: '87.42%', 9: '65.54%'}\n",
      "Val Loss: 1.306138, Val Acc: 80.42%, Val-Class-Acc: {0: '76.09%', 2: '84.72%', 3: '93.03%', 4: '75.00%', 5: '89.55%', 6: '37.96%', 7: '78.40%', 8: '85.35%', 9: '35.14%'}, LR: 0.000349\n",
      "Epoch 164/200, Train Loss: 0.470239, Train-Class-Acc: {0: '87.60%', 2: '91.16%', 3: '92.01%', 4: '87.34%', 5: '93.12%', 6: '59.45%', 7: '74.45%', 8: '86.78%', 9: '62.84%'}\n",
      "Val Loss: 1.448354, Val Acc: 81.08%, Val-Class-Acc: {0: '78.26%', 2: '82.64%', 3: '93.44%', 4: '75.00%', 5: '92.54%', 6: '37.04%', 7: '79.20%', 8: '82.80%', 9: '37.84%'}, LR: 0.000349\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_78.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_164.pth\n",
      "Epoch 165/200, Train Loss: 0.475311, Train-Class-Acc: {0: '86.78%', 2: '89.08%', 3: '92.73%', 4: '82.91%', 5: '93.64%', 6: '57.60%', 7: '77.25%', 8: '85.99%', 9: '55.41%'}\n",
      "Val Loss: 1.098525, Val Acc: 80.57%, Val-Class-Acc: {0: '81.52%', 2: '81.94%', 3: '93.03%', 4: '75.00%', 5: '91.04%', 6: '36.11%', 7: '78.40%', 8: '82.17%', 9: '29.73%'}, LR: 0.000349\n",
      "Epoch 166/200, Train Loss: 0.605014, Train-Class-Acc: {0: '86.78%', 2: '90.12%', 3: '91.91%', 4: '87.34%', 5: '92.82%', 6: '59.45%', 7: '76.45%', 8: '88.85%', 9: '61.49%'}\n",
      "Val Loss: 0.971936, Val Acc: 80.42%, Val-Class-Acc: {0: '79.89%', 2: '84.03%', 3: '89.34%', 4: '75.00%', 5: '92.54%', 6: '37.04%', 7: '78.40%', 8: '83.44%', 9: '27.03%'}, LR: 0.000349\n",
      "Epoch 167/200, Train Loss: 0.445051, Train-Class-Acc: {0: '88.56%', 2: '90.99%', 3: '92.62%', 4: '86.71%', 5: '93.49%', 6: '63.13%', 7: '74.45%', 8: '88.54%', 9: '60.14%'}\n",
      "Val Loss: 1.306608, Val Acc: 80.49%, Val-Class-Acc: {0: '76.09%', 2: '84.72%', 3: '93.44%', 4: '72.50%', 5: '90.45%', 6: '39.81%', 7: '79.20%', 8: '84.08%', 9: '27.03%'}, LR: 0.000349\n",
      "Epoch 168/200, Train Loss: 0.480362, Train-Class-Acc: {0: '88.01%', 2: '89.77%', 3: '92.21%', 4: '87.97%', 5: '93.79%', 6: '65.90%', 7: '78.24%', 8: '89.17%', 9: '62.16%'}\n",
      "Val Loss: 1.064057, Val Acc: 80.64%, Val-Class-Acc: {0: '78.26%', 2: '81.25%', 3: '93.44%', 4: '75.00%', 5: '91.94%', 6: '38.89%', 7: '78.40%', 8: '82.80%', 9: '29.73%'}, LR: 0.000349\n",
      "Epoch 169/200, Train Loss: 0.531698, Train-Class-Acc: {0: '86.51%', 2: '89.08%', 3: '92.93%', 4: '83.54%', 5: '93.19%', 6: '61.06%', 7: '75.25%', 8: '88.54%', 9: '61.49%'}\n",
      "Val Loss: 1.200617, Val Acc: 80.35%, Val-Class-Acc: {0: '79.35%', 2: '81.94%', 3: '93.85%', 4: '75.00%', 5: '91.34%', 6: '33.33%', 7: '79.20%', 8: '82.17%', 9: '29.73%'}, LR: 0.000349\n",
      "Epoch 170/200, Train Loss: 0.567357, Train-Class-Acc: {0: '87.33%', 2: '90.47%', 3: '92.42%', 4: '83.54%', 5: '93.27%', 6: '60.60%', 7: '76.25%', 8: '89.81%', 9: '63.51%'}\n",
      "Val Loss: 0.994705, Val Acc: 80.49%, Val-Class-Acc: {0: '80.98%', 2: '81.25%', 3: '94.26%', 4: '77.50%', 5: '91.34%', 6: '30.56%', 7: '80.00%', 8: '82.17%', 9: '29.73%'}, LR: 0.000349\n",
      "Epoch 171/200, Train Loss: 0.486588, Train-Class-Acc: {0: '89.65%', 2: '92.20%', 3: '91.39%', 4: '87.34%', 5: '94.17%', 6: '59.91%', 7: '77.45%', 8: '88.85%', 9: '57.43%'}\n",
      "Val Loss: 1.008706, Val Acc: 80.35%, Val-Class-Acc: {0: '77.72%', 2: '84.03%', 3: '94.67%', 4: '77.50%', 5: '89.85%', 6: '38.89%', 7: '77.60%', 8: '80.89%', 9: '29.73%'}, LR: 0.000349\n",
      "Epoch 172/200, Train Loss: 0.548205, Train-Class-Acc: {0: '90.05%', 2: '88.56%', 3: '91.60%', 4: '84.81%', 5: '93.57%', 6: '60.60%', 7: '79.64%', 8: '87.42%', 9: '67.57%'}\n",
      "Val Loss: 1.085030, Val Acc: 80.35%, Val-Class-Acc: {0: '73.37%', 2: '84.03%', 3: '93.44%', 4: '75.00%', 5: '89.85%', 6: '38.89%', 7: '80.80%', 8: '84.08%', 9: '37.84%'}, LR: 0.000314\n",
      "Epoch 173/200, Train Loss: 0.481761, Train-Class-Acc: {0: '89.24%', 2: '92.03%', 3: '93.55%', 4: '87.34%', 5: '93.19%', 6: '64.98%', 7: '76.05%', 8: '88.22%', 9: '63.51%'}\n",
      "Val Loss: 1.107891, Val Acc: 79.84%, Val-Class-Acc: {0: '75.54%', 2: '82.64%', 3: '93.03%', 4: '77.50%', 5: '89.55%', 6: '36.11%', 7: '77.60%', 8: '83.44%', 9: '37.84%'}, LR: 0.000314\n",
      "Epoch 174/200, Train Loss: 0.483062, Train-Class-Acc: {0: '86.78%', 2: '92.20%', 3: '90.27%', 4: '86.71%', 5: '93.49%', 6: '59.45%', 7: '75.05%', 8: '88.54%', 9: '66.22%'}\n",
      "Val Loss: 1.316856, Val Acc: 80.64%, Val-Class-Acc: {0: '78.80%', 2: '84.03%', 3: '93.44%', 4: '77.50%', 5: '90.15%', 6: '35.19%', 7: '79.20%', 8: '82.17%', 9: '40.54%'}, LR: 0.000314\n",
      "Epoch 175/200, Train Loss: 0.457860, Train-Class-Acc: {0: '88.15%', 2: '90.29%', 3: '92.73%', 4: '86.08%', 5: '94.24%', 6: '63.59%', 7: '75.65%', 8: '89.17%', 9: '60.14%'}\n",
      "Val Loss: 1.337173, Val Acc: 80.35%, Val-Class-Acc: {0: '73.91%', 2: '82.64%', 3: '93.85%', 4: '77.50%', 5: '89.85%', 6: '43.52%', 7: '76.80%', 8: '85.35%', 9: '29.73%'}, LR: 0.000314\n",
      "Epoch 176/200, Train Loss: 0.445593, Train-Class-Acc: {0: '89.65%', 2: '90.47%', 3: '93.44%', 4: '86.71%', 5: '94.09%', 6: '62.44%', 7: '75.65%', 8: '88.22%', 9: '63.51%'}\n",
      "Val Loss: 0.979832, Val Acc: 80.35%, Val-Class-Acc: {0: '75.54%', 2: '81.25%', 3: '95.08%', 4: '77.50%', 5: '91.04%', 6: '38.89%', 7: '76.00%', 8: '82.80%', 9: '35.14%'}, LR: 0.000314\n",
      "Epoch 177/200, Train Loss: 0.426321, Train-Class-Acc: {0: '87.74%', 2: '92.89%', 3: '93.24%', 4: '86.08%', 5: '93.87%', 6: '61.06%', 7: '77.05%', 8: '87.74%', 9: '62.16%'}\n",
      "Val Loss: 1.242376, Val Acc: 80.64%, Val-Class-Acc: {0: '81.52%', 2: '78.47%', 3: '93.03%', 4: '77.50%', 5: '92.24%', 6: '35.19%', 7: '78.40%', 8: '83.44%', 9: '29.73%'}, LR: 0.000314\n",
      "Epoch 178/200, Train Loss: 0.454740, Train-Class-Acc: {0: '85.97%', 2: '89.95%', 3: '93.14%', 4: '91.77%', 5: '94.09%', 6: '62.90%', 7: '77.84%', 8: '87.74%', 9: '65.54%'}\n",
      "Val Loss: 1.255443, Val Acc: 80.35%, Val-Class-Acc: {0: '77.17%', 2: '83.33%', 3: '91.39%', 4: '77.50%', 5: '91.94%', 6: '38.89%', 7: '77.60%', 8: '83.44%', 9: '27.03%'}, LR: 0.000314\n",
      "Epoch 179/200, Train Loss: 0.522373, Train-Class-Acc: {0: '90.87%', 2: '91.16%', 3: '92.21%', 4: '85.44%', 5: '93.19%', 6: '62.21%', 7: '75.45%', 8: '88.22%', 9: '66.22%'}\n",
      "Val Loss: 1.368955, Val Acc: 80.79%, Val-Class-Acc: {0: '79.35%', 2: '80.56%', 3: '91.80%', 4: '77.50%', 5: '92.84%', 6: '36.11%', 7: '80.80%', 8: '82.17%', 9: '35.14%'}, LR: 0.000314\n",
      "Epoch 180/200, Train Loss: 0.479474, Train-Class-Acc: {0: '88.15%', 2: '89.77%', 3: '92.52%', 4: '86.71%', 5: '93.72%', 6: '62.44%', 7: '74.65%', 8: '89.81%', 9: '62.16%'}\n",
      "Val Loss: 1.062960, Val Acc: 80.13%, Val-Class-Acc: {0: '78.26%', 2: '85.42%', 3: '92.62%', 4: '77.50%', 5: '90.75%', 6: '32.41%', 7: '78.40%', 8: '81.53%', 9: '32.43%'}, LR: 0.000314\n",
      "Epoch 181/200, Train Loss: 0.401087, Train-Class-Acc: {0: '89.37%', 2: '92.03%', 3: '92.21%', 4: '84.81%', 5: '94.39%', 6: '64.98%', 7: '76.45%', 8: '88.06%', 9: '67.57%'}\n",
      "Val Loss: 1.157735, Val Acc: 80.28%, Val-Class-Acc: {0: '75.54%', 2: '82.64%', 3: '94.26%', 4: '77.50%', 5: '90.45%', 6: '37.04%', 7: '77.60%', 8: '83.44%', 9: '35.14%'}, LR: 0.000314\n",
      "Epoch 182/200, Train Loss: 0.389945, Train-Class-Acc: {0: '88.15%', 2: '92.37%', 3: '92.73%', 4: '85.44%', 5: '94.84%', 6: '62.67%', 7: '75.85%', 8: '88.69%', 9: '66.89%'}\n",
      "Val Loss: 1.154555, Val Acc: 79.62%, Val-Class-Acc: {0: '75.54%', 2: '80.56%', 3: '93.85%', 4: '77.50%', 5: '89.55%', 6: '36.11%', 7: '78.40%', 8: '83.44%', 9: '29.73%'}, LR: 0.000314\n",
      "Epoch 183/200, Train Loss: 0.407727, Train-Class-Acc: {0: '88.96%', 2: '90.81%', 3: '94.16%', 4: '84.18%', 5: '93.34%', 6: '65.67%', 7: '77.45%', 8: '88.85%', 9: '62.84%'}\n",
      "Val Loss: 1.396439, Val Acc: 80.42%, Val-Class-Acc: {0: '80.43%', 2: '81.94%', 3: '91.80%', 4: '77.50%', 5: '91.04%', 6: '35.19%', 7: '78.40%', 8: '84.08%', 9: '29.73%'}, LR: 0.000282\n",
      "Epoch 184/200, Train Loss: 0.456941, Train-Class-Acc: {0: '89.10%', 2: '90.29%', 3: '92.11%', 4: '85.44%', 5: '92.82%', 6: '62.67%', 7: '76.85%', 8: '87.90%', 9: '64.19%'}\n",
      "Val Loss: 1.219516, Val Acc: 79.84%, Val-Class-Acc: {0: '73.91%', 2: '82.64%', 3: '92.62%', 4: '72.50%', 5: '89.25%', 6: '39.81%', 7: '80.80%', 8: '84.71%', 9: '29.73%'}, LR: 0.000282\n",
      "Epoch 185/200, Train Loss: 0.389074, Train-Class-Acc: {0: '88.96%', 2: '91.51%', 3: '93.65%', 4: '84.18%', 5: '94.54%', 6: '64.29%', 7: '76.25%', 8: '89.97%', 9: '67.57%'}\n",
      "Val Loss: 1.132312, Val Acc: 80.20%, Val-Class-Acc: {0: '76.63%', 2: '81.94%', 3: '91.80%', 4: '77.50%', 5: '90.15%', 6: '41.67%', 7: '77.60%', 8: '84.08%', 9: '32.43%'}, LR: 0.000282\n",
      "Epoch 186/200, Train Loss: 0.508743, Train-Class-Acc: {0: '88.28%', 2: '91.85%', 3: '93.65%', 4: '89.24%', 5: '93.19%', 6: '61.06%', 7: '74.65%', 8: '88.85%', 9: '60.81%'}\n",
      "Val Loss: 1.298580, Val Acc: 79.84%, Val-Class-Acc: {0: '74.46%', 2: '81.94%', 3: '90.57%', 4: '72.50%', 5: '90.75%', 6: '38.89%', 7: '82.40%', 8: '84.71%', 9: '27.03%'}, LR: 0.000282\n",
      "Epoch 187/200, Train Loss: 0.474908, Train-Class-Acc: {0: '87.74%', 2: '92.03%', 3: '92.93%', 4: '89.24%', 5: '94.91%', 6: '60.83%', 7: '78.44%', 8: '88.85%', 9: '65.54%'}\n",
      "Val Loss: 1.050257, Val Acc: 80.13%, Val-Class-Acc: {0: '76.09%', 2: '80.56%', 3: '93.85%', 4: '77.50%', 5: '90.75%', 6: '37.96%', 7: '77.60%', 8: '83.44%', 9: '32.43%'}, LR: 0.000282\n",
      "Epoch 188/200, Train Loss: 0.469203, Train-Class-Acc: {0: '88.56%', 2: '91.85%', 3: '93.34%', 4: '87.34%', 5: '94.32%', 6: '62.44%', 7: '78.84%', 8: '90.92%', 9: '66.89%'}\n",
      "Val Loss: 1.042113, Val Acc: 80.20%, Val-Class-Acc: {0: '73.37%', 2: '81.25%', 3: '92.21%', 4: '77.50%', 5: '91.94%', 6: '43.52%', 7: '76.80%', 8: '82.80%', 9: '35.14%'}, LR: 0.000282\n",
      "Epoch 189/200, Train Loss: 0.431724, Train-Class-Acc: {0: '89.65%', 2: '91.33%', 3: '93.65%', 4: '86.71%', 5: '94.32%', 6: '66.82%', 7: '76.05%', 8: '89.33%', 9: '64.19%'}\n",
      "Val Loss: 1.005232, Val Acc: 80.06%, Val-Class-Acc: {0: '74.46%', 2: '81.25%', 3: '90.98%', 4: '77.50%', 5: '91.04%', 6: '37.04%', 7: '80.00%', 8: '85.35%', 9: '37.84%'}, LR: 0.000282\n",
      "Epoch 190/200, Train Loss: 0.413617, Train-Class-Acc: {0: '89.65%', 2: '92.20%', 3: '93.55%', 4: '86.08%', 5: '95.14%', 6: '60.83%', 7: '76.85%', 8: '88.54%', 9: '70.95%'}\n",
      "Val Loss: 1.012936, Val Acc: 79.84%, Val-Class-Acc: {0: '75.00%', 2: '84.03%', 3: '93.03%', 4: '77.50%', 5: '89.25%', 6: '34.26%', 7: '80.00%', 8: '84.08%', 9: '32.43%'}, LR: 0.000282\n",
      "Epoch 191/200, Train Loss: 0.409185, Train-Class-Acc: {0: '87.33%', 2: '91.16%', 3: '94.47%', 4: '86.71%', 5: '94.17%', 6: '65.44%', 7: '78.64%', 8: '87.10%', 9: '66.22%'}\n",
      "Val Loss: 1.257033, Val Acc: 80.35%, Val-Class-Acc: {0: '77.17%', 2: '81.25%', 3: '92.21%', 4: '75.00%', 5: '92.24%', 6: '39.81%', 7: '76.80%', 8: '83.44%', 9: '29.73%'}, LR: 0.000282\n",
      "Epoch 192/200, Train Loss: 0.381456, Train-Class-Acc: {0: '88.69%', 2: '92.20%', 3: '93.03%', 4: '86.08%', 5: '95.59%', 6: '66.36%', 7: '77.45%', 8: '89.17%', 9: '68.24%'}\n",
      "Val Loss: 1.382242, Val Acc: 80.64%, Val-Class-Acc: {0: '78.80%', 2: '82.64%', 3: '92.21%', 4: '75.00%', 5: '92.24%', 6: '34.26%', 7: '81.60%', 8: '82.80%', 9: '29.73%'}, LR: 0.000282\n",
      "Epoch 193/200, Train Loss: 0.489424, Train-Class-Acc: {0: '87.60%', 2: '89.77%', 3: '92.93%', 4: '86.71%', 5: '94.09%', 6: '65.90%', 7: '76.45%', 8: '90.92%', 9: '62.84%'}\n",
      "Val Loss: 1.263283, Val Acc: 80.42%, Val-Class-Acc: {0: '71.74%', 2: '80.56%', 3: '94.26%', 4: '75.00%', 5: '92.24%', 6: '37.96%', 7: '78.40%', 8: '83.44%', 9: '48.65%'}, LR: 0.000282\n",
      "Epoch 194/200, Train Loss: 0.430701, Train-Class-Acc: {0: '89.78%', 2: '91.68%', 3: '94.06%', 4: '87.34%', 5: '95.21%', 6: '64.98%', 7: '77.45%', 8: '90.29%', 9: '72.30%'}\n",
      "Val Loss: 1.036701, Val Acc: 79.84%, Val-Class-Acc: {0: '74.46%', 2: '83.33%', 3: '92.62%', 4: '75.00%', 5: '89.55%', 6: '37.96%', 7: '80.00%', 8: '84.08%', 9: '29.73%'}, LR: 0.000254\n",
      "Epoch 195/200, Train Loss: 0.659925, Train-Class-Acc: {0: '89.51%', 2: '91.51%', 3: '93.03%', 4: '88.61%', 5: '93.34%', 6: '62.21%', 7: '78.44%', 8: '88.22%', 9: '56.08%'}\n",
      "Val Loss: 1.068254, Val Acc: 78.75%, Val-Class-Acc: {0: '76.63%', 2: '79.86%', 3: '92.62%', 4: '72.50%', 5: '88.06%', 6: '30.56%', 7: '80.80%', 8: '83.44%', 9: '29.73%'}, LR: 0.000254\n",
      "Epoch 196/200, Train Loss: 0.551131, Train-Class-Acc: {0: '88.42%', 2: '91.68%', 3: '92.52%', 4: '85.44%', 5: '94.54%', 6: '60.60%', 7: '75.65%', 8: '89.01%', 9: '65.54%'}\n",
      "Val Loss: 1.503501, Val Acc: 80.28%, Val-Class-Acc: {0: '77.17%', 2: '82.64%', 3: '92.21%', 4: '77.50%', 5: '91.04%', 6: '36.11%', 7: '79.20%', 8: '84.08%', 9: '29.73%'}, LR: 0.000254\n",
      "Epoch 197/200, Train Loss: 0.401853, Train-Class-Acc: {0: '89.24%', 2: '92.37%', 3: '92.52%', 4: '88.61%', 5: '94.84%', 6: '62.67%', 7: '77.25%', 8: '90.61%', 9: '64.19%'}\n",
      "Val Loss: 1.140209, Val Acc: 80.28%, Val-Class-Acc: {0: '76.09%', 2: '81.94%', 3: '93.44%', 4: '77.50%', 5: '91.64%', 6: '38.89%', 7: '76.00%', 8: '83.44%', 9: '29.73%'}, LR: 0.000254\n",
      "Epoch 198/200, Train Loss: 0.512992, Train-Class-Acc: {0: '87.60%', 2: '92.37%', 3: '91.80%', 4: '83.54%', 5: '93.87%', 6: '66.59%', 7: '78.64%', 8: '86.94%', 9: '58.78%'}\n",
      "Val Loss: 1.337562, Val Acc: 80.06%, Val-Class-Acc: {0: '75.00%', 2: '81.25%', 3: '93.85%', 4: '77.50%', 5: '89.25%', 6: '37.96%', 7: '78.40%', 8: '83.44%', 9: '43.24%'}, LR: 0.000254\n",
      "Epoch 199/200, Train Loss: 0.412740, Train-Class-Acc: {0: '90.87%', 2: '92.37%', 3: '93.03%', 4: '87.34%', 5: '94.24%', 6: '65.44%', 7: '78.04%', 8: '89.81%', 9: '64.86%'}\n",
      "Val Loss: 1.149431, Val Acc: 79.84%, Val-Class-Acc: {0: '75.54%', 2: '82.64%', 3: '93.03%', 4: '77.50%', 5: '89.25%', 6: '34.26%', 7: '78.40%', 8: '84.08%', 9: '40.54%'}, LR: 0.000254\n",
      "Epoch 200/200, Train Loss: 0.403386, Train-Class-Acc: {0: '89.24%', 2: '91.68%', 3: '92.42%', 4: '87.34%', 5: '94.32%', 6: '64.06%', 7: '78.64%', 8: '91.40%', 9: '65.54%'}\n",
      "Val Loss: 1.262926, Val Acc: 80.20%, Val-Class-Acc: {0: '76.09%', 2: '81.94%', 3: '93.44%', 4: '77.50%', 5: '91.64%', 6: '37.04%', 7: '76.80%', 8: '82.17%', 9: '35.14%'}, LR: 0.000254\n",
      "\n",
      "üèÜ Best model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_best.pth (Val Accuracy: 81.08%)\n",
      "\n",
      "üìå Final model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_final.pth\n",
      "\n",
      "üéØ Top 5 Best Models:\n",
      "Epoch 164, Train Loss: 0.470239, Train-Acc: {0: '87.60%', 2: '91.16%', 3: '92.01%', 4: '87.34%', 5: '93.12%', 6: '59.45%', 7: '74.45%', 8: '86.78%', 9: '62.84%'},\n",
      "Val Loss: 1.448354, Val Acc: 81.08%, Val-Acc: {0: '78.26%', 2: '82.64%', 3: '93.44%', 4: '75.00%', 5: '92.54%', 6: '37.04%', 7: '79.20%', 8: '82.80%', 9: '37.84%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_164.pth\n",
      "Epoch 110, Train Loss: 0.726321, Train-Acc: {0: '81.88%', 2: '85.27%', 3: '88.83%', 4: '81.65%', 5: '90.80%', 6: '43.78%', 7: '67.66%', 8: '82.17%', 9: '56.08%'},\n",
      "Val Loss: 1.218543, Val Acc: 81.08%, Val-Acc: {0: '83.70%', 2: '87.50%', 3: '90.57%', 4: '72.50%', 5: '92.84%', 6: '31.48%', 7: '80.80%', 8: '81.53%', 9: '27.03%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_110.pth\n",
      "Epoch 98, Train Loss: 0.678494, Train-Acc: {0: '81.61%', 2: '86.31%', 3: '88.11%', 4: '79.11%', 5: '91.55%', 6: '46.08%', 7: '69.66%', 8: '81.37%', 9: '45.95%'},\n",
      "Val Loss: 1.101457, Val Acc: 81.08%, Val-Acc: {0: '76.09%', 2: '87.50%', 3: '92.21%', 4: '72.50%', 5: '91.64%', 6: '34.26%', 7: '81.60%', 8: '87.26%', 9: '29.73%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_98.pth\n",
      "Epoch 87, Train Loss: 0.964462, Train-Acc: {0: '80.65%', 2: '83.88%', 3: '87.91%', 4: '75.95%', 5: '89.45%', 6: '42.63%', 7: '68.06%', 8: '77.87%', 9: '47.97%'},\n",
      "Val Loss: 1.570996, Val Acc: 81.00%, Val-Acc: {0: '80.43%', 2: '84.72%', 3: '89.75%', 4: '75.00%', 5: '92.84%', 6: '37.04%', 7: '80.80%', 8: '84.71%', 9: '24.32%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_87.pth\n",
      "Epoch 81, Train Loss: 0.917916, Train-Acc: {0: '79.29%', 2: '85.27%', 3: '85.96%', 4: '78.48%', 5: '89.83%', 6: '38.25%', 7: '69.86%', 8: '80.57%', 9: '44.59%'},\n",
      "Val Loss: 1.004005, Val Acc: 81.00%, Val-Acc: {0: '80.98%', 2: '85.42%', 3: '95.08%', 4: '67.50%', 5: '89.25%', 6: '30.56%', 7: '80.80%', 8: '87.90%', 9: '29.73%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4/ResNet18_1D_LoRA_epoch_81.pth\n",
      "---\n",
      "### Period 4\n",
      "+ ##### Total training time: 506.13 seconds\n",
      "+ ##### Model: ResNet18_1D_LoRA\n",
      "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/Standard_LoRA_CIL_v3/Period_4'*\n",
      "+ ##### Best Epoch: 164\n",
      "#### __Val Accuracy: 81.08%__\n",
      "#### __Val-Class-Acc: {0: '78.26%', 2: '82.64%', 3: '93.44%', 4: '75.00%', 5: '92.54%', 6: '37.04%', 7: '79.20%', 8: '82.80%', 9: '37.84%'}__\n",
      "#### __Total Parameters: 3,895,946__\n",
      "#### __Model Size (float32): 14.86 MB__\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå Period 4: Standard LoRA Training (ECG)\n",
    "# ================================\n",
    "period = 4\n",
    "\n",
    "# ==== Paths ====\n",
    "stop_signal_file = os.path.join(BASE_DIR, \"stop_training.txt\")\n",
    "model_saving_folder = os.path.join(BASE_DIR, \"Trained_models\", \"Standard_LoRA_CIL_v3\", f\"Period_{period}\")\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# ==== Load Period 4 Data ====\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{period}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{period}.npy\"))\n",
    "X_val   = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val   = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "# ==== Device ====\n",
    "device = auto_select_cuda_device()\n",
    "\n",
    "# ==== Model Configuration ====\n",
    "input_channels = X_train.shape[2]  # ECG 12-lead\n",
    "output_size = int(np.max(y_train)) + 1  # e.g., max=9 ‚Üí output_size=10\n",
    "model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size, lora_rank=4).to(device)\n",
    "\n",
    "# ==== Initialize LoRA Adapters FIRST ====\n",
    "model.init_lora()\n",
    "\n",
    "# ==== Load Period 3 Best Model Weights (excluding FC & LoRA A/B) ====\n",
    "prev_model_path = os.path.join(BASE_DIR, \"Trained_models\", \"Standard_LoRA_CIL_v3\", f\"Period_{period - 1}\", \"ResNet18_1D_LoRA_best.pth\")\n",
    "prev_checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "prev_state_dict = prev_checkpoint[\"model_state_dict\"]\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "filtered_state_dict = {\n",
    "    k: v for k, v in prev_state_dict.items()\n",
    "    if k in model_dict and model_dict[k].shape == v.shape and not (\n",
    "        k.startswith(\"fc\") or \"lora_adapter.lora_A\" in k or \"lora_adapter.lora_B\" in k\n",
    "    )\n",
    "}\n",
    "model.load_state_dict(filtered_state_dict, strict=False)\n",
    "for k in model_dict:\n",
    "    if k not in filtered_state_dict:\n",
    "        print(f\"üîç Not loaded: {k}, shape={model_dict[k].shape}\")\n",
    "print(\"‚úÖ Loaded Period 3 weights (excluding FC & LoRA A/B)\")\n",
    "\n",
    "# ==== Optimizer / Scheduler ====\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.get_trainable_parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# ==== Start Training ====\n",
    "train_with_lora_ecg(\n",
    "    model=model,\n",
    "    output_size=output_size,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    model_saving_folder=model_saving_folder,\n",
    "    model_name=\"ResNet18_1D_LoRA\",\n",
    "    stop_signal_file=stop_signal_file,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# ‚úÖ Cleanup\n",
    "# ================================\n",
    "del X_train, y_train, X_val, y_val\n",
    "del prev_model_path, prev_checkpoint, prev_state_dict, filtered_state_dict\n",
    "del model, criterion, optimizer, scheduler\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e39306",
   "metadata": {},
   "source": [
    "##  Compute FWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b5bdc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fwt_ecg(previous_model, init_model, X_val, y_val, known_classes, batch_size=64):\n",
    "    \"\"\"\n",
    "    FWT computation for ECG-style inputs with 1D CNN (e.g., ResNet18_1D).\n",
    "    X_val: shape [B, T, C]  (e.g., [N, 5000, 12])\n",
    "    y_val: shape [B]        (e.g., [N])\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    previous_model.to(device).eval()\n",
    "    init_model.to(device).eval()\n",
    "\n",
    "    # Âè™ÈÅ∏Âèñ known classes\n",
    "    mask = np.isin(y_val, known_classes)\n",
    "    X_known = X_val[mask]\n",
    "    y_known = y_val[mask]\n",
    "\n",
    "    if len(y_known) == 0:\n",
    "        print(f\"‚ö†Ô∏è No validation samples for known classes {known_classes}.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(f\"üìã Total samples for known classes {known_classes}: {len(y_known)}\")\n",
    "\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(X_known, dtype=torch.float32),\n",
    "        torch.tensor(y_known, dtype=torch.long)\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    correct_prev, correct_init, total = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            out_prev = previous_model(xb)  # [B, C]\n",
    "            out_init = init_model(xb)\n",
    "\n",
    "            preds_prev = torch.argmax(out_prev, dim=-1)\n",
    "            preds_init = torch.argmax(out_init, dim=-1)\n",
    "\n",
    "            correct_prev += (preds_prev == yb).sum().item()\n",
    "            correct_init += (preds_init == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "    acc_prev = 100 * correct_prev / total\n",
    "    acc_init = 100 * correct_init / total\n",
    "    fwt_value = acc_prev - acc_init\n",
    "\n",
    "    print(f\"\\n### üîç FWT Debug Info:\")\n",
    "    print(f\"- Total evaluated samples: {total}\")\n",
    "    print(f\"- Accuracy by previous model: {acc_prev:.2f}%\")\n",
    "    print(f\"- Accuracy by init model:     {acc_init:.2f}%\")\n",
    "    print(f\"- FWT = Acc_prev - Acc_init = {fwt_value:.2f}%\")\n",
    "\n",
    "    return fwt_value, acc_prev, acc_init\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd09a7a",
   "metadata": {},
   "source": [
    "### Period 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d203479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 0\n",
      "    - Memory Used    : 3281 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_149835/2406496834.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Total samples for known classes [0, 1]: 428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_149835/1383618855.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_known, dtype=torch.float32),\n",
      "/tmp/ipykernel_149835/1383618855.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_known, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### üîç FWT Debug Info:\n",
      "- Total evaluated samples: 428\n",
      "- Accuracy by previous model: 89.25%\n",
      "- Accuracy by init model:     49.53%\n",
      "- FWT = Acc_prev - Acc_init = 39.72%\n",
      "\n",
      "### Period 2:\n",
      "- FWT (Standard LoRA ECG Period 2, old classes [0, 1]): 39.72%\n",
      "- Accuracy by previous model: 89.25%\n",
      "- Accuracy by init model:     49.53%\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå FWT - Period 2 (Standard LoRA - ECG)\n",
    "# ================================\n",
    "period = 2\n",
    "\n",
    "# === Load Validation Data ===\n",
    "X_val = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "device = auto_select_cuda_device()\n",
    "input_channels = X_val.shape[2]\n",
    "output_size_prev = 2  # Period 1 output classes\n",
    "known_classes = [0, 1]  # Period 2 ‰∏≠ÁöÑÁ©©ÂÆöËàäÈ°ûÂà•\n",
    "\n",
    "# === Load Period 1 Baseline Model ===\n",
    "prev_model_path = os.path.join(BASE_DIR, \"ResNet18_Selection\", \"ResNet18_big_inplane_v1\", \"ResNet18_big_inplane_1D_best.pth\")\n",
    "checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "\n",
    "frozen_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size_prev, lora_rank=4).to(device)\n",
    "frozen_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "frozen_model.output_size = output_size_prev\n",
    "frozen_model.eval()\n",
    "del checkpoint\n",
    "\n",
    "# === Init Model ===\n",
    "init_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size_prev, lora_rank=4).to(device)\n",
    "init_model.output_size = output_size_prev\n",
    "\n",
    "# === Tensor Conversion ===\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# === Run FWT Evaluation ===\n",
    "fwt, acc_prev, acc_init = compute_fwt_ecg(frozen_model, init_model, X_val_tensor, y_val_tensor, known_classes)\n",
    "\n",
    "print(f\"\\n### Period 2:\")\n",
    "print(f\"- FWT (Standard LoRA ECG Period 2, old classes {known_classes}): {fwt:.2f}%\")\n",
    "print(f\"- Accuracy by previous model: {acc_prev:.2f}%\")\n",
    "print(f\"- Accuracy by init model:     {acc_init:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe386bfa",
   "metadata": {},
   "source": [
    "### Period 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75581787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 0\n",
      "    - Memory Used    : 3848 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ LoRA adapters initialized for 8 conv2 layers\n",
      "‚úÖ LoRA adapters initialized for 8 conv2 layers\n",
      "üìã Total samples for known classes [0, 1, 2, 3]: 907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_149835/3956697940.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_model_path, map_location=device)\n",
      "/tmp/ipykernel_149835/1383618855.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_known, dtype=torch.float32),\n",
      "/tmp/ipykernel_149835/1383618855.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_known, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### üîç FWT Debug Info:\n",
      "- Total evaluated samples: 907\n",
      "- Accuracy by previous model: 87.32%\n",
      "- Accuracy by init model:     32.30%\n",
      "- FWT = Acc_prev - Acc_init = 55.02%\n",
      "\n",
      "### Period 3:\n",
      "- FWT (Standard LoRA ECG Period 3, old classes [0, 1, 2, 3]): 55.02%\n",
      "- Accuracy by previous model: 87.32%\n",
      "- Accuracy by init model:     32.30%\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå FWT - Period 3 (Standard LoRA - ECG)\n",
    "# ================================\n",
    "period = 3\n",
    "\n",
    "# === Load Validation Data ===\n",
    "X_val = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "device = auto_select_cuda_device()\n",
    "input_channels = X_val.shape[2]\n",
    "output_size_prev = 4\n",
    "known_classes = [0, 1, 2, 3]\n",
    "\n",
    "# === Load Period 2 Best Model ===\n",
    "prev_model_path = os.path.join(BASE_DIR, \"Trained_models\", \"Standard_LoRA_CIL_v2\", \"Period_2\", \"ResNet18_1D_LoRA_best.pth\")\n",
    "checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "state_dict = checkpoint[\"model_state_dict\"]\n",
    "del checkpoint\n",
    "\n",
    "# === Rebuild previous model and init LoRA ===\n",
    "frozen_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size_prev, lora_rank=4).to(device)\n",
    "frozen_model.init_lora()\n",
    "frozen_model.load_state_dict(state_dict, strict=False)\n",
    "frozen_model.output_size = output_size_prev\n",
    "frozen_model.eval()\n",
    "\n",
    "# === Init model (untrained) ===\n",
    "init_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size_prev, lora_rank=4).to(device)\n",
    "init_model.init_lora()\n",
    "init_model.output_size = output_size_prev\n",
    "\n",
    "# === Tensor Conversion ===\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# === Run FWT Evaluation ===\n",
    "fwt, acc_prev, acc_init = compute_fwt_ecg(frozen_model, init_model, X_val_tensor, y_val_tensor, known_classes)\n",
    "\n",
    "print(f\"\\n### Period 3:\")\n",
    "print(f\"- FWT (Standard LoRA ECG Period 3, old classes {known_classes}): {fwt:.2f}%\")\n",
    "print(f\"- Accuracy by previous model: {acc_prev:.2f}%\")\n",
    "print(f\"- Accuracy by init model:     {acc_init:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26e038a",
   "metadata": {},
   "source": [
    "### Period 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33541a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 0\n",
      "    - Memory Used    : 20473 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ LoRA adapters initialized for 8 conv2 layers\n",
      "‚úÖ LoRA adapters initialized for 8 conv2 layers\n",
      "üìã Total samples for known classes [0, 1, 2, 3, 4, 5]: 947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_149835/3776746818.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_model_path, map_location=device)\n",
      "/tmp/ipykernel_149835/1383618855.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_known, dtype=torch.float32),\n",
      "/tmp/ipykernel_149835/1383618855.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_known, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### üîç FWT Debug Info:\n",
      "- Total evaluated samples: 947\n",
      "- Accuracy by previous model: 94.93%\n",
      "- Accuracy by init model:     25.77%\n",
      "- FWT = Acc_prev - Acc_init = 69.17%\n",
      "\n",
      "### Period 4:\n",
      "- FWT (Standard LoRA ECG Period 4, old classes [0, 1, 2, 3, 4, 5]): 69.17%\n",
      "- Accuracy by previous model: 94.93%\n",
      "- Accuracy by init model:     25.77%\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå FWT - Period 4 (Standard LoRA - ECG)\n",
    "# ================================\n",
    "period = 4\n",
    "\n",
    "# === Load Validation Data ===\n",
    "X_val = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "device = auto_select_cuda_device()\n",
    "input_channels = X_val.shape[2]\n",
    "output_size_prev = 6\n",
    "known_classes = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# === Load Period 2 Best Model ===\n",
    "prev_model_path = os.path.join(BASE_DIR, \"Trained_models\", \"Standard_LoRA_CIL_v3\", \"Period_3\", \"ResNet18_1D_LoRA_best.pth\")\n",
    "checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "state_dict = checkpoint[\"model_state_dict\"]\n",
    "del checkpoint\n",
    "\n",
    "# === Rebuild previous model and init LoRA ===\n",
    "frozen_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size_prev, lora_rank=4).to(device)\n",
    "frozen_model.init_lora()\n",
    "frozen_model.load_state_dict(state_dict, strict=False)\n",
    "frozen_model.output_size = output_size_prev\n",
    "frozen_model.eval()\n",
    "\n",
    "# === Init model (untrained) ===\n",
    "init_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size_prev, lora_rank=4).to(device)\n",
    "init_model.init_lora()\n",
    "init_model.output_size = output_size_prev\n",
    "\n",
    "# === Tensor Conversion ===\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# === Run FWT Evaluation ===\n",
    "fwt, acc_prev, acc_init = compute_fwt_ecg(frozen_model, init_model, X_val_tensor, y_val_tensor, known_classes)\n",
    "\n",
    "print(f\"\\n### Period 4:\")\n",
    "print(f\"- FWT (Standard LoRA ECG Period 4, old classes {known_classes}): {fwt:.2f}%\")\n",
    "print(f\"- Accuracy by previous model: {acc_prev:.2f}%\")\n",
    "print(f\"- Accuracy by init model:     {acc_init:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1667cd49",
   "metadata": {},
   "source": [
    "## üìä Summary: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba405ae6",
   "metadata": {},
   "source": [
    "### ‚úîÔ∏è CPSC - Standard LoRA: Validation Summary\n",
    "\n",
    "| Period | Training Time (s) | Validation Accuracy | Class-wise Accuracy                                                                 |\n",
    "|--------|-------------------|---------------------|--------------------------------------------------------------------------------------|\n",
    "| 1      | 134.66            | **88.86%**          | {0: 91.85%, 1: 85.87%}                                                              |\n",
    "| 2      | 298.35            | **87.25%**          | {0: 85.33%, 1: 80.33%, 2: 85.42%, 3: 96.72%}                                        |\n",
    "| 3      | 469.49            | **84.07%**          | {0: 74.46%, 1: 77.91%, 2: 86.11%, 3: 88.52%, 4: 82.50%, 5: 91.62%}                  |\n",
    "| 4      | 506.13            | **81.08%**          | {0: 78.26%, 2: 82.64%, 3: 93.44%, 4: 75.00%, 5: 92.54%, 6: 37.04%, 7: 79.20%, 8: 82.80%, 9: 37.84%} |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0802bf5",
   "metadata": {},
   "source": [
    "### üß† Continual Learning Metrics\n",
    "\n",
    "| Period | AA_old (%) | AA_new (%) | BWT (%) | FWT (%) | FWT Classes        | Prev. Model Acc | Init Model Acc |\n",
    "|--------|------------|------------|---------|---------|---------------------|------------------|-----------------|\n",
    "| 2      | 82.83%     | 91.07%     | -6.03%  | 39.72%  | [0, 1]              | 89.25%           | 49.53%          |\n",
    "| 3      | 81.75%     | 87.06%     | -5.20%  | 55.02%  | [0, 1, 2, 3]        | 87.32%           | 32.30%          |\n",
    "| 4      | 84.38%     | 59.22%     | -0.27%  | 69.17%  | [0, 1, 2, 3, 4, 5]  | 94.93%           | 25.77%          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ef7ee",
   "metadata": {},
   "source": [
    "### üì¶ Model Size per Period\n",
    "\n",
    "| Period | Output Size | LoRA Added? | Total Params | Œî Params vs Prev | Œî % vs Prev | Model Size (float32) |\n",
    "|--------|-------------|-------------|--------------|------------------|-------------|-----------------------|\n",
    "| 1      | 2           | ‚úò No        | 3,857,026    | ‚Äî                | ‚Äî           | 14.71 MB              |\n",
    "| 2      | 4           | ‚úî Yes       | 3,889,796    | +32,770          | +0.85%      | 14.84 MB              |\n",
    "| 3      | 6           | ‚úò No        | 3,891,846    | +2,050           | +0.05%      | 14.85 MB              |\n",
    "| 4      | 10          | ‚úò No        | 3,895,946    | +4,100           | +0.11%      | 14.86 MB              |\n",
    "\n",
    "**üìà Model Growth Rate (MGR) = (3,895,946 - 3,857,026) / (3,857,026 √ó 3) ‚âà +0.34%**\n",
    "\n",
    "**üìà Max trainable ratio ‚âà 1.05%**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
