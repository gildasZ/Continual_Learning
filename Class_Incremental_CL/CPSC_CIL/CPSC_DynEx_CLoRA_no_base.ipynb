{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07af288c",
   "metadata": {},
   "source": [
    "## __Check first before starting__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f00b1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /mnt/mydisk/Continual_Learning_JL/Continual_Learning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "Working_directory = os.path.normpath(\"/mnt/mydisk/Continual_Learning_JL/Continual_Learning/\")\n",
    "os.chdir(Working_directory)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97bdf56",
   "metadata": {},
   "source": [
    "## __All imports__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83b85d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating system and file management\n",
    "import os\n",
    "import shutil\n",
    "import contextlib\n",
    "import traceback\n",
    "import gc\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import subprocess\n",
    "import time\n",
    "import re, pickle\n",
    "import scipy.io\n",
    "from scipy.io import loadmat\n",
    "from glob import glob\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Jupyter notebook widgets and display\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_interactions import zoom_factory, panhandler\n",
    "\n",
    "# Machine learning and preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from ta import trend, momentum, volatility, volume\n",
    "\n",
    "# Mathematical and scientific computing\n",
    "import math\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Type hinting\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "# Deep learning with PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a496fe70",
   "metadata": {},
   "source": [
    "## __üìÅ Path Settings and Constants__\n",
    "This cell defines essential paths and constants for the CPSC2018 ECG dataset processing:\n",
    "- `BASE_DIR`: Root directory of the project.\n",
    "- `save_dir`: Path to the preprocessed `.npy` files (one for each continual learning period).\n",
    "- `ECG_PATH`: Directory containing original `.mat` and `.hea` files.\n",
    "- `MAX_LEN`: Length of each ECG sample, fixed to 5000 time steps (i.e., 10 seconds at 500Hz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7748e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL\"\n",
    "save_dir = os.path.join(BASE_DIR, \"processed\")\n",
    "ECG_PATH = os.path.join(BASE_DIR, \"datas\")\n",
    "MAX_LEN = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba7249f",
   "metadata": {},
   "source": [
    "## __üè∑Ô∏è Label Mapping and Period Configuration__\n",
    "\n",
    "This section defines:\n",
    "- `snomed_map`: Mapping from SNOMED CT codes to readable class names for 9 major ECG conditions.\n",
    "- `period_label_map`: Incremental learning task structure across four periods.  \n",
    "  Class `1` is reserved for \"OTHER\" abnormalities until Period 4 when all 9 classes are explicitly categorized.\n",
    "- `print_class_distribution()`: Helper function to show class-wise data distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "103ca271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNOMED CT to readable names\n",
    "snomed_map = {\n",
    "    \"426783006\": \"NSR\",    # Ê≠£Â∏∏Á´áÊÄßÂøÉÂæã\n",
    "    \"270492004\": \"I-AVB\",  # ‰∏ÄÂ∫¶ÊàøÂÆ§ÂÇ≥Â∞éÈòªÊªØ\n",
    "    \"164889003\": \"AF\",     # ÂøÉÊàøÁ∫ñÁ∂≠È°´Âãï\n",
    "    \"164909002\": \"LBBB\",   # Â∑¶ÊùüÊîØÂÇ≥Â∞éÈòªÊªØ\n",
    "    \"59118001\":  \"RBBB\",   # Âè≥ÊùüÊîØÂÇ≥Â∞éÈòªÊªØ\n",
    "    \"284470004\": \"PAC\",    # ÂøÉÊàøÊó©ÊúüÊêèÂãï\n",
    "    \"164884008\": \"PVC\",    # ÂÆ§ÊÄßÊó©ÊúüÊêèÂãï\n",
    "    \"429622005\": \"STD\",    # ST ÊÆµÂ£ì‰Ωé\n",
    "    \"164931005\": \"STE\"     # ST ÊÆµÊä¨È´ò\n",
    "}\n",
    "\n",
    "# Period class mapping (Âõ∫ÂÆö class 1 ÊòØ„ÄåÂÖ∂‰ªñÁï∞Â∏∏„ÄçÁõ¥Âà∞ P4 ÁßªÈô§)\n",
    "period_label_map = {\n",
    "    1: {\"NSR\": 0, \"OTHER\": 1},\n",
    "    2: {\"NSR\": 0, \"I-AVB\": 2, \"AF\": 3, \"OTHER\": 1},\n",
    "    3: {\"NSR\": 0, \"I-AVB\": 2, \"AF\": 3, \"LBBB\": 4, \"RBBB\": 5, \"OTHER\": 1},\n",
    "    4: {\"NSR\": 0, \"I-AVB\": 2, \"AF\": 3, \"LBBB\": 4, \"RBBB\": 5, \"PAC\": 6, \"PVC\": 7, \"STD\": 8, \"STE\": 9}\n",
    "}\n",
    "\n",
    "def print_class_distribution(y, label_map):\n",
    "    y = np.array(y).flatten()\n",
    "    total = len(y)\n",
    "    all_labels = sorted(label_map.values())\n",
    "    print(\"\\nüìä Class Distribution\")\n",
    "    for lbl in all_labels:\n",
    "        count = np.sum(y == lbl)\n",
    "        label = [k for k, v in label_map.items() if v == lbl]\n",
    "        name = label[0] if label else str(lbl)\n",
    "        print(f\"  ‚îú‚îÄ Label {lbl:<2} ({name:<10}) ‚Üí {count:>5} samples ({(count/total)*100:5.2f}%)\")\n",
    "\n",
    "def ensure_folder(folder_path: str) -> None:\n",
    "    \"\"\"Ensure the given folder exists, create it if not.\"\"\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f95af3",
   "metadata": {},
   "source": [
    "## üì¶ EX. Load Example (Period 4) Data and View Format\n",
    "\n",
    "This example demonstrates how to load preprocessed `.npy` data for **Period 4**, and inspect the dataset shapes and label distribution.  \n",
    "Use this format as a reference when loading data in other methods (e.g., EWC, PNN, DynEx-CLoRA).\n",
    "\n",
    "Each ECG sample:\n",
    "- Has shape `(5000, 12)` ‚Üí represents 10 seconds (at 500Hz) across 12-lead channels.\n",
    "- Corresponding label is an integer ID (e.g., 0‚Äì9) defined by `period_label_map[4]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a401dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ÁØÑ‰æã:ËºâÂÖ• period 4\n",
    "# save_dir = os.path.join(BASE_DIR, \"processed\")\n",
    "# X_train = np.load(os.path.join(save_dir, \"X_train_p4.npy\"))\n",
    "# y_train = np.load(os.path.join(save_dir, \"y_train_p4.npy\"))\n",
    "# X_test = np.load(os.path.join(save_dir, \"X_test_p4.npy\"))\n",
    "# y_test = np.load(os.path.join(save_dir, \"y_test_p4.npy\"))\n",
    "\n",
    "# print(\"‚úÖ Loaded\")\n",
    "# print(\"X_train shape:\", X_train.shape)\n",
    "# print(\"y_train shape:\", y_train.shape)\n",
    "# print(\"X_test shape:\", X_test.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)\n",
    "# print_class_distribution(y_train, period_label_map[4])\n",
    "# print_class_distribution(y_test, period_label_map[4])\n",
    "\n",
    "# del X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d4b54b",
   "metadata": {},
   "source": [
    "## __Check GPU, CUDA, Pytorch__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50a987a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  8 21:01:35 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               Off |   00000000:2A:00.0 Off |                  Off |\n",
      "| 46%   62C    P2             88W /  300W |    2876MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               Off |   00000000:3D:00.0 Off |                  Off |\n",
      "| 30%   35C    P3             44W /  300W |     283MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000               Off |   00000000:AB:00.0 Off |                  Off |\n",
      "| 33%   66C    P2            296W /  300W |    9977MiB /  49140MiB |     99%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2845      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    0   N/A  N/A         1775968      C   ...onda3/envs/CIL_env/bin/python        516MiB |\n",
      "|    0   N/A  N/A         1777853      C   python3                                1196MiB |\n",
      "|    0   N/A  N/A         1785939      C   python3                                1130MiB |\n",
      "|    1   N/A  N/A            2845      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    1   N/A  N/A         1775968      C   ...onda3/envs/CIL_env/bin/python        260MiB |\n",
      "|    2   N/A  N/A            2845      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    2   N/A  N/A         1784514      C   python                                 9954MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f78325",
   "metadata": {},
   "source": [
    "### CUDA Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00875f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "             GPU Configuration Check              \n",
      "==================================================\n",
      "PyTorch Version          : 2.5.1\n",
      "GPU Available            : Yes\n",
      "--------------------------------------------------\n",
      "                   GPU Details                    \n",
      "--------------------------------------------------\n",
      "Device Name              : NVIDIA RTX A6000\n",
      "Number of GPUs           : 3\n",
      "Current Device Index     : 0\n",
      "Compute Capability       : 8.6\n",
      "Total CUDA Cores         : 10752\n",
      "Total Memory (GB)        : 47.41\n",
      "Allocated Memory (GB)    : 0.00\n",
      "Reserved Memory (GB)     : 0.00\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def check_gpu_config():\n",
    "    \"\"\"\n",
    "    Check GPU availability and display detailed configuration information.\n",
    "    \"\"\"\n",
    "    # Check if GPU is available\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    \n",
    "    # Print header\n",
    "    print(\"=\" * 50)\n",
    "    print(\"GPU Configuration Check\".center(50))\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic GPU availability\n",
    "    print(f\"{'PyTorch Version':<25}: {torch.__version__}\")\n",
    "    print(f\"{'GPU Available':<25}: {'Yes' if gpu_available else 'No'}\")\n",
    "    \n",
    "    # If GPU is available, print detailed info\n",
    "    if gpu_available:\n",
    "        print(\"-\" * 50)\n",
    "        print(\"GPU Details\".center(50))\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Device info\n",
    "        print(f\"{'Device Name':<25}: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"{'Number of GPUs':<25}: {torch.cuda.device_count()}\")\n",
    "        print(f\"{'Current Device Index':<25}: {torch.cuda.current_device()}\")\n",
    "        \n",
    "        # Compute capability and CUDA cores\n",
    "        props = torch.cuda.get_device_properties(0)\n",
    "        print(f\"{'Compute Capability':<25}: {props.major}.{props.minor}\")\n",
    "        print(f\"{'Total CUDA Cores':<25}: {props.multi_processor_count * 128}\")  # Approx. 128 cores per SM\n",
    "        \n",
    "        # Memory info\n",
    "        total_memory = props.total_memory / (1024 ** 3)  # Convert to GB\n",
    "        memory_allocated = torch.cuda.memory_allocated(0) / (1024 ** 3)\n",
    "        memory_reserved = torch.cuda.memory_reserved(0) / (1024 ** 3)\n",
    "        print(f\"{'Total Memory (GB)':<25}: {total_memory:.2f}\")\n",
    "        print(f\"{'Allocated Memory (GB)':<25}: {memory_allocated:.2f}\")\n",
    "        print(f\"{'Reserved Memory (GB)':<25}: {memory_reserved:.2f}\")\n",
    "    else:\n",
    "        print(\"-\" * 50)\n",
    "        print(\"No GPU detected. Running on CPU.\".center(50))\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_gpu_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a868ff5",
   "metadata": {},
   "source": [
    "### PyTorch Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d153a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "              PyTorch Configuration               \n",
      "==================================================\n",
      "PyTorch Version          : 2.5.1\n",
      "CUDA Compiled Version    : 12.1\n",
      "CUDA Available           : Yes\n",
      "Number of GPUs           : 3\n",
      "GPU Name                 : NVIDIA RTX A6000\n",
      "--------------------------------------------------\n",
      "Random Seed              : 42 (Seeding successful!)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def print_torch_config():\n",
    "    \"\"\"Print PyTorch and CUDA configuration in a formatted manner.\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"PyTorch Configuration\".center(50))\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic PyTorch and CUDA info\n",
    "    print(f\"{'PyTorch Version':<25}: {torch.__version__}\")\n",
    "    print(f\"{'CUDA Compiled Version':<25}: {torch.version.cuda}\")\n",
    "    print(f\"{'CUDA Available':<25}: {'Yes' if torch.cuda.is_available() else 'No'}\")\n",
    "    print(f\"{'Number of GPUs':<25}: {torch.cuda.device_count()}\")\n",
    "\n",
    "    # GPU details if available\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"{'GPU Name':<25}: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Seed setting\n",
    "    torch.manual_seed(42)\n",
    "    print(f\"{'Random Seed':<25}: 42 (Seeding successful!)\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print_torch_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbab549",
   "metadata": {},
   "source": [
    "## __‚öôÔ∏è GPU Selection ‚Äî Auto-select the least loaded GPU__\n",
    "This code automatically scans available GPUs and selects the one with the lowest current memory usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4ffdd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 1\n",
      "    - Memory Used    : 283 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "def auto_select_cuda_device(verbose=True):\n",
    "    \"\"\"\n",
    "    Automatically selects the CUDA GPU with the least memory usage.\n",
    "    Falls back to CPU if no GPU is available.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"üö´ No CUDA GPU available. Using CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "    try:\n",
    "        # Run nvidia-smi to get memory usage of each GPU\n",
    "        smi_output = subprocess.check_output(\n",
    "            ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'],\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        memory_used = [int(x) for x in smi_output.strip().split('\\n')]\n",
    "        best_gpu = int(np.argmin(memory_used))\n",
    "\n",
    "        if verbose:\n",
    "            print(\"üéØ Automatically selected GPU:\")\n",
    "            print(f\"    - CUDA Device ID : {best_gpu}\")\n",
    "            print(f\"    - Memory Used    : {memory_used[best_gpu]} MiB\")\n",
    "            print(f\"    - Device Name    : {torch.cuda.get_device_name(best_gpu)}\")\n",
    "        return torch.device(f\"cuda:{best_gpu}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to auto-detect GPU. Falling back to cuda:0. ({e})\")\n",
    "        return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Execute and assign\n",
    "device = auto_select_cuda_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae7c5a",
   "metadata": {},
   "source": [
    "## __Model__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b4707e",
   "metadata": {},
   "source": [
    "### ResNet 18 - 1D (ResNet18_1D_big_inplane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cab8843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAConv1d(nn.Module):\n",
    "    def __init__(self, conv_layer: nn.Conv1d, rank: int):\n",
    "        super(LoRAConv1d, self).__init__()\n",
    "        self.conv = conv_layer\n",
    "        self.rank = rank\n",
    "        \n",
    "        # ÁÇ∫ÈÅ©ÊáâLoRA‰ΩéÁß©ÂàÜËß£ÂâµÂª∫AÂíåBÁü©Èô£\n",
    "        self.lora_A = nn.Parameter(torch.zeros(conv_layer.out_channels, rank))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, conv_layer.in_channels * conv_layer.kernel_size[0]))\n",
    "        \n",
    "        # ÂàùÂßãÂåñÊ¨äÈáçÔºöAÁî®Ê≠£ÊÖãÂàÜ‰ΩàÔºåBÁî®Èõ∂ÂàùÂßãÂåñ‰ª•Á¢∫‰øùË®ìÁ∑¥ÈñãÂßãÊôÇLoRAÁÑ°ÂΩ±Èüø\n",
    "        nn.init.normal_(self.lora_A, mean=0.0, std=0.01)\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ‰ΩøÁî®ÂéüÂßãÂç∑Á©çÂ±§ÁöÑÊ¨äÈáçÂíåÂèÉÊï∏ÈÄ≤Ë°åÂç∑Á©ç\n",
    "        return self.conv(x)\n",
    "        \n",
    "    def get_delta(self):\n",
    "        # Ë®àÁÆóLoRAÊ¨äÈáç‰∏¶ÈáçÂ°ëÁÇ∫Âç∑Á©çÊ†∏ÂΩ¢ÁãÄ\n",
    "        lora_weight = torch.matmul(self.lora_A, self.lora_B).view(\n",
    "            self.conv.out_channels, self.conv.in_channels, self.conv.kernel_size[0]\n",
    "        )\n",
    "        return lora_weight\n",
    "        \n",
    "    def parameters(self, recurse=True):\n",
    "        # Âè™ËøîÂõûLoRAÂèÉÊï∏Ôºå‰∏çÂåÖÊã¨ÂéüÂßãÂç∑Á©çÂ±§ÂèÉÊï∏\n",
    "        return [self.lora_A, self.lora_B]\n",
    "\n",
    "class BasicBlock1d_LoRA(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, lora_rank=None):\n",
    "        super(BasicBlock1d_LoRA, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "        self.lora_rank = lora_rank\n",
    "        self.lora_adapters = nn.ModuleList()  # ‰ΩøÁî®ModuleListÂ≠òÂÑ≤Â§öÂÄãLoRAÈÅ©ÈÖçÂô®\n",
    "\n",
    "    def add_lora_adapter(self):\n",
    "        \"\"\"Ê∑ªÂä†‰∏ÄÂÄãÊñ∞ÁöÑLoRAÈÅ©ÈÖçÂô®Âà∞conv2Â±§\"\"\"\n",
    "        new_lora = LoRAConv1d(self.conv2, self.lora_rank)\n",
    "        device = next(self.parameters()).device\n",
    "        new_lora.to(device)\n",
    "        self.lora_adapters.append(new_lora)\n",
    "        return new_lora\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        if len(self.lora_adapters) > 0:\n",
    "            # ‰ΩøÁî®ÂéüÂßãconv2ÈÄ≤Ë°åÂü∫Êú¨Âç∑Á©ç\n",
    "            base_out = self.conv2(out)\n",
    "            \n",
    "            # Â¶ÇÊûúÊúâLoRAÈÅ©ÈÖçÂô®ÔºåË®àÁÆóÊâÄÊúâLoRAÁöÑÊ¨äÈáçÂ¢ûÈáè‰∏¶ÊáâÁî®\n",
    "            if self.lora_adapters:\n",
    "                # Ë®àÁÆóÊâÄÊúâLoRAÈÅ©ÈÖçÂô®ÁöÑÊ¨äÈáçÁ∏ΩÂíå\n",
    "                lora_weight_delta = sum(adapter.get_delta() for adapter in self.lora_adapters)\n",
    "                # Ë™øÊï¥ÂæåÁöÑÊ¨äÈáç = ÂéüÂßãÊ¨äÈáç + LoRAÊ¨äÈáçÂ¢ûÈáè\n",
    "                adapted_weight = self.conv2.weight + lora_weight_delta\n",
    "                # ‰ΩøÁî®‰øÆÊîπÂæåÁöÑÊ¨äÈáçÂü∑Ë°åÂç∑Á©ç\n",
    "                out = F.conv1d(out, adapted_weight, bias=self.conv2.bias,\n",
    "                              stride=self.conv2.stride, padding=self.conv2.padding,\n",
    "                              dilation=self.conv2.dilation, groups=self.conv2.groups)\n",
    "            else:\n",
    "                out = base_out\n",
    "        else:\n",
    "            # Â¶ÇÊûúÊ≤íÊúâLoRAÈÅ©ÈÖçÂô®Ôºå‰ΩøÁî®ÂéüÂßãconv2\n",
    "            out = self.conv2(out)\n",
    "\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18_1D_LoRA(nn.Module):\n",
    "    def __init__(self, input_channels=12, output_size=9, inplanes=64, lora_rank=4):\n",
    "        super(ResNet18_1D_LoRA, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.lora_rank = lora_rank\n",
    "\n",
    "        # ÂàùÂßãÂç∑Á©çÂ±§\n",
    "        self.conv1 = nn.Conv1d(input_channels, inplanes, kernel_size=15, stride=2, padding=7, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ÊÆòÂ∑ÆÂ±§\n",
    "        self.layer1 = self._make_layer(BasicBlock1d_LoRA, 64, 2)\n",
    "        self.layer2 = self._make_layer(BasicBlock1d_LoRA, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock1d_LoRA, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlock1d_LoRA, 512, 2, stride=2)\n",
    "\n",
    "        # Ëá™ÈÅ©ÊáâÊ±†ÂåñÔºàÂπ≥ÂùáÂíåÊúÄÂ§ßÔºâ\n",
    "        self.adaptiveavgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.adaptivemaxpool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # ÂÖ®ÈÄ£Êé•Â±§Ëàádropout\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(512 * 2, output_size)  # *2Âõ†ÁÇ∫concat‰∫ÜavgÂíåmaxÊ±†Âåñ\n",
    "\n",
    "        # ÂàùÂßãÂåñÊ¨äÈáç\n",
    "        self.init_weights()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.lora_rank))\n",
    "        self.inplanes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, lora_rank=self.lora_rank))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # È†êÊúüËº∏ÂÖ•ÂΩ¢ÁãÄ: (batch_size, time_steps, channels)\n",
    "        x = x.permute(0, 2, 1)  # ‚Üí (batch_size, channels, time_steps)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        # ÊáâÁî®Âπ≥ÂùáÂíåÊúÄÂ§ßÊ±†Âåñ\n",
    "        x1 = self.adaptiveavgpool(x)\n",
    "        x2 = self.adaptivemaxpool(x)\n",
    "        \n",
    "        # ÈÄ£Êé•Ê±†ÂåñÁµêÊûú\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        \n",
    "        # Â±ïÂπ≥\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # ÊáâÁî®dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # ÊúÄÁµÇÂàÜÈ°û\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"ÂàùÂßãÂåñÁ∂≤Áµ°Ê¨äÈáç\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def add_lora_adapter(self):\n",
    "        \"\"\"ÁÇ∫ÊâÄÊúâBasicBlockÁöÑconv2Â±§Ê∑ªÂä†‰∏ÄÂÄãÊñ∞ÁöÑLoRAÈÅ©ÈÖçÂô®\"\"\"\n",
    "        lora_count = 0\n",
    "        added_loras = []\n",
    "        \n",
    "        for module in self.modules():\n",
    "            if isinstance(module, BasicBlock1d_LoRA):\n",
    "                new_lora = module.add_lora_adapter()\n",
    "                added_loras.append(new_lora)\n",
    "                lora_count += 1\n",
    "                \n",
    "        print(f\"‚úÖ Added new LoRA adapters to {lora_count} BasicBlocks\")\n",
    "        return added_loras\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        \"\"\"ËøîÂõûÂèØË®ìÁ∑¥ÂèÉÊï∏ÂàóË°®ÔºàÁî®ÊñºÂÑ™ÂåñÂô®Ôºâ‰∏¶Êèê‰æõÂèÉÊï∏Áµ±Ë®à\"\"\"\n",
    "        lora_params = []\n",
    "        lora_names = []\n",
    "        fc_params = []\n",
    "        fc_names = []\n",
    "        \n",
    "        # Ë®àÁÆóÁ∏ΩÂèÉÊï∏Êï∏Èáè\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        \n",
    "        # Êî∂ÈõÜÊâÄÊúâLoRAÂèÉÊï∏\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, LoRAConv1d):\n",
    "                lora_params.append(module.lora_A)\n",
    "                lora_names.append(f\"{name}.lora_A\")\n",
    "                lora_params.append(module.lora_B)\n",
    "                lora_names.append(f\"{name}.lora_B\")\n",
    "\n",
    "        # Ê∑ªÂä†fcÂ±§ÂèÉÊï∏\n",
    "        for name, param in self.fc.named_parameters():\n",
    "            fc_params.append(param)\n",
    "            fc_names.append(f\"fc.{name}\")\n",
    "        \n",
    "        # Ë®àÁÆóÁµ±Ë®àÊï∏Êìö\n",
    "        trainable_params = lora_params + fc_params\n",
    "        frozen_params = total_params - sum(p.numel() for p in trainable_params)\n",
    "        lora_param_count = sum(p.numel() for p in lora_params)\n",
    "        fc_param_count = sum(p.numel() for p in fc_params)\n",
    "        trainable_param_count = lora_param_count + fc_param_count\n",
    "        \n",
    "        # ÊâìÂç∞Áµ±Ë®à‰ø°ÊÅØ\n",
    "        print(f\"üìä Parameter Statistics:\")\n",
    "        print(f\"  - Total parameters: {total_params:,}\")\n",
    "        print(f\"  - Trainable parameters: {trainable_param_count:,} ({trainable_param_count/total_params*100:.2f}%)\")\n",
    "        print(f\"    - LoRA parameters: {lora_param_count:,} ({lora_param_count/total_params*100:.2f}%)\")\n",
    "        print(f\"    - FC parameters: {fc_param_count:,} ({fc_param_count/total_params*100:.2f}%)\")\n",
    "        print(f\"  - Frozen parameters: {frozen_params:,} ({frozen_params/total_params*100:.2f}%)\")\n",
    "        \n",
    "        print(f\"üß† Trainable parameter names:\")\n",
    "        for name in lora_names:\n",
    "            print(f\"  ‚úÖ {name} (LoRA)\")\n",
    "        for name in fc_names:\n",
    "            print(f\"  ‚úÖ {name} (FC)\")\n",
    "        \n",
    "        return trainable_params\n",
    "    \n",
    "    def count_lora_adapters(self):\n",
    "        \"\"\"Ë®àÁÆóÁ∂≤Áµ°‰∏≠ÊâÄÊúâLoRAÈÅ©ÈÖçÂô®ÁöÑÊï∏Èáè\"\"\"\n",
    "        total_adapters = 0\n",
    "        blocks_with_lora = 0\n",
    "        \n",
    "        for module in self.modules():\n",
    "            if isinstance(module, BasicBlock1d_LoRA):\n",
    "                if len(module.lora_adapters) > 0:\n",
    "                    blocks_with_lora += 1\n",
    "                    total_adapters += len(module.lora_adapters)\n",
    "        \n",
    "        print(f\"üìà LoRA Adapter Statistics:\")\n",
    "        print(f\"  - Total LoRA adapters: {total_adapters}\")\n",
    "        print(f\"  - BasicBlocks with adapters: {blocks_with_lora}\")\n",
    "        \n",
    "        return total_adapters\n",
    "    \n",
    "    def count_lora_groups(self):\n",
    "        blocks = [m for m in self.modules() if isinstance(m, BasicBlock1d_LoRA)]\n",
    "        if not blocks:\n",
    "            return 0\n",
    "        return len(blocks[0].lora_adapters)  # ÊâÄÊúâ block ÁöÑ group Êï∏ÊáâË©≤‰∏ÄËá¥\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ebc8c6",
   "metadata": {},
   "source": [
    "## __Training and validation function__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2616acb4",
   "metadata": {},
   "source": [
    "### Extra Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3ce00ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classwise_accuracy(student_logits_flat, y_batch, class_correct, class_total):\n",
    "    \"\"\"\n",
    "    Computes per-class accuracy by accumulating correct and total samples for each class using vectorized operations.\n",
    "    \n",
    "    Args:\n",
    "        student_logits_flat (torch.Tensor): Model predictions (logits) in shape [batch_size * seq_len, output_size]\n",
    "        y_batch (torch.Tensor): True labels in shape [batch_size * seq_len]\n",
    "        class_correct (dict): Dictionary to store correct predictions per class\n",
    "        class_total (dict): Dictionary to store total samples per class\n",
    "    \"\"\"\n",
    "    # Ensure inputs are on the same device\n",
    "    if student_logits_flat.device != y_batch.device:\n",
    "        raise ValueError(\"student_logits_flat and y_batch must be on the same device\")\n",
    "\n",
    "    # Convert logits to predicted class indices\n",
    "    predictions = torch.argmax(student_logits_flat, dim=-1)  # Shape: [batch_size * seq_len]\n",
    "\n",
    "    # Compute correct predictions mask\n",
    "    correct_mask = (predictions == y_batch)  # Shape: [batch_size * seq_len], boolean\n",
    "\n",
    "    # Get unique labels in this batch\n",
    "    unique_labels = torch.unique(y_batch)\n",
    "\n",
    "    # Update class_total and class_correct using vectorized operations\n",
    "    for label in unique_labels:\n",
    "        label = label.item()  # Convert tensor to scalar\n",
    "        if label not in class_total:\n",
    "            class_total[label] = 0\n",
    "            class_correct[label] = 0\n",
    "        \n",
    "        # Count total samples for this label\n",
    "        label_mask = (y_batch == label)\n",
    "        class_total[label] += label_mask.sum().item()\n",
    "        \n",
    "        # Count correct predictions for this label\n",
    "        class_correct[label] += (label_mask & correct_mask).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "724647c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_parameter_info(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    param_size_bytes = total_params * 4\n",
    "    param_size_MB = param_size_bytes / (1024**2)\n",
    "    return total_params, param_size_MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b7f0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(y: np.ndarray, num_classes: int, exclude_classes: list = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Ë®àÁÆó class weightsÔºàinverse frequencyÔºâÈÅøÂÖç class imbalance„ÄÇ\n",
    "    ÂèØÊéíÈô§Êüê‰∫õÈ°ûÂà•ÔºàÂ¶Ç‰∏çÂ≠òÂú®ÁöÑÈ°ûÂà•ÔºâÔºåÈÄô‰∫õÈ°ûÂà•ÁöÑÊ¨äÈáçÂ∞áË®≠ÁÇ∫ 0„ÄÇ\n",
    "    \"\"\"\n",
    "    exclude_classes = set(exclude_classes or [])\n",
    "    class_sample_counts = np.bincount(y, minlength=num_classes)\n",
    "    total_samples = len(y)\n",
    "\n",
    "    weights = np.zeros(num_classes, dtype=np.float32)\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        if cls in exclude_classes:\n",
    "            weights[cls] = 0.0\n",
    "        else:\n",
    "            count = class_sample_counts[cls]\n",
    "            weights[cls] = total_samples / (count + 1e-6)\n",
    "\n",
    "    # Normalize only non-excluded weights\n",
    "    valid_mask = np.array([cls not in exclude_classes for cls in range(num_classes)])\n",
    "    norm_sum = weights[valid_mask].sum()\n",
    "    if norm_sum > 0:\n",
    "        weights[valid_mask] /= norm_sum\n",
    "\n",
    "    print(\"\\nüìä Class Weights (normalized):\")\n",
    "    for i, w in enumerate(weights):\n",
    "        status = \" (excluded)\" if i in exclude_classes else \"\"\n",
    "        print(f\"  - Class {i}: {w:.4f}{status}\")\n",
    "    \n",
    "    return torch.tensor(weights, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abe5f03",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e14ba",
   "metadata": {},
   "source": [
    "#### v10 pure optimizer, all freeze, no consider base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7051b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_dynex_clora_ecg(model, teacher_model, output_size, criterion, optimizer,\n",
    "                           X_train, y_train, X_val, y_val,\n",
    "                           num_epochs, batch_size, alpha,\n",
    "                           model_saving_folder, model_name,\n",
    "                           stop_signal_file=None, scheduler=None,\n",
    "                           period=None, stable_classes=None,\n",
    "                           similarity_threshold=0.0,\n",
    "                           class_features_dict=None, related_labels=None, device=None):\n",
    "    \n",
    "    print(f\"\\nüöÄ 'train_with_dynex_clora_ecg' started for Period {period}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_name = model_name or 'dynex_clora_model'\n",
    "    model_saving_folder = model_saving_folder or './saved_models'\n",
    "    \n",
    "    if os.path.exists(model_saving_folder):\n",
    "        shutil.rmtree(model_saving_folder)\n",
    "        print(f\"‚úÖ Removed existing folder: {model_saving_folder}\")\n",
    "    os.makedirs(model_saving_folder, exist_ok=True)\n",
    "    \n",
    "    device = device or auto_select_cuda_device()\n",
    "    model.to(device)\n",
    "    \n",
    "    if teacher_model:\n",
    "        teacher_model.to(device)\n",
    "        teacher_model.eval()\n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(\"\\n‚úÖ Data Overview:\")\n",
    "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "    \n",
    "    best_results = []\n",
    "    \n",
    "    # === Class Feature Extraction ===\n",
    "    model.eval()\n",
    "    new_class_features = {}\n",
    "    \n",
    "    # Extract features for current classes\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in train_loader:\n",
    "            # Get feature representations\n",
    "            features = extract_features(model, xb)\n",
    "            for cls in torch.unique(yb):\n",
    "                cls_mask = (yb == cls)\n",
    "                cls_feat = features[cls_mask]\n",
    "                if cls.item() not in new_class_features:\n",
    "                    new_class_features[cls.item()] = []\n",
    "                new_class_features[cls.item()].append(cls_feat)\n",
    "    \n",
    "    # Average features per class\n",
    "    for cls in new_class_features:\n",
    "        new_class_features[cls] = torch.cat(new_class_features[cls], dim=0).mean(dim=0)\n",
    "    \n",
    "    # Initialize related_labels if not provided\n",
    "    if related_labels is None:\n",
    "        related_labels = {}\n",
    "    \n",
    "    # === Similarity Computation (only for Period > 1) ===\n",
    "    if period > 1 and class_features_dict:\n",
    "        cosine_sim = torch.nn.CosineSimilarity(dim=0)\n",
    "        similarity_scores = {}\n",
    "        \n",
    "        # Calculate similarities between new and old classes\n",
    "        for new_label, new_feat in new_class_features.items():\n",
    "            similarity_scores[new_label] = {}\n",
    "            for old_label, old_feat in class_features_dict.items():\n",
    "                sim = cosine_sim(new_feat.to(device), old_feat.to(device)).item()\n",
    "                similarity_scores[new_label][old_label] = sim\n",
    "        \n",
    "        # Print similarity information\n",
    "        print(\"\\nüîé Similarity Analysis:\")\n",
    "        print(f\"  Similarity threshold: {similarity_threshold:.4f}\")\n",
    "        print(f\"  Existing classes: {sorted(list(class_features_dict.keys()))}\")\n",
    "        print(f\"  Current classes: {sorted(list(new_class_features.keys()))}\")\n",
    "        \n",
    "        # Calculate new classes (classes not in previous periods)\n",
    "        existing_classes = set(class_features_dict.keys())\n",
    "        current_classes = set(new_class_features.keys())\n",
    "        new_classes = current_classes - existing_classes\n",
    "        print(f\"  New classes: {sorted(list(new_classes))}\")\n",
    "        \n",
    "        # Print similarity scores\n",
    "        print(\"\\nüìä Similarity Scores:\")\n",
    "        for new_label, scores in similarity_scores.items():\n",
    "            if new_label in new_classes:  # Only show for new classes\n",
    "                print(f\"  New Class {new_label}:\")\n",
    "                if scores:\n",
    "                    for old_label, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
    "                        print(f\"    - Existing Class {old_label}: {score:.4f}\")\n",
    "                else:\n",
    "                    print(\"    - No existing classes to compare\")\n",
    "        \n",
    "        # Calculate average similarity statistics\n",
    "        all_similarities = [s for scores in similarity_scores.values() for s in scores.values()]\n",
    "        if all_similarities:\n",
    "            avg_similarity = np.mean(all_similarities)\n",
    "            std_similarity = np.std(all_similarities)\n",
    "            print(f\"\\n  Average similarity: {avg_similarity:.4f}, Std: {std_similarity:.4f}\")\n",
    "    \n",
    "    # === Period-specific LoRA Management Logic ===\n",
    "    to_unfreeze = set()\n",
    "    \n",
    "    # Special handling for period 1 - no LoRA adapters yet\n",
    "    if period == 1:\n",
    "        if not related_labels:\n",
    "            # Initialize related_labels for first period\n",
    "            # The base conv layers (index 'base') are associated with initial classes\n",
    "            initial_classes = list(new_class_features.keys())\n",
    "            related_labels['base'] = initial_classes\n",
    "            print(f\"\\nüîÑ Initializing related_labels for first period: {related_labels}\")\n",
    "        \n",
    "        # For first period, all base model parameters are trainable\n",
    "        print(\"\\nüîì First period: All model parameters are trainable\")\n",
    "    \n",
    "    # For periods > 1, manage LoRA adapters\n",
    "    elif period > 1 and class_features_dict:\n",
    "        new_lora_indices = []\n",
    "        cosine_sim = torch.nn.CosineSimilarity(dim=0)\n",
    "        \n",
    "        existing_classes = set(class_features_dict.keys())\n",
    "        current_classes = set(new_class_features.keys())\n",
    "        new_classes = current_classes - existing_classes\n",
    "        \n",
    "        print(f\"\\nüß© Managing LoRA adapters for {len(new_classes)} new classes...\")\n",
    "        \n",
    "        # Process each new class\n",
    "        for new_cls in new_classes:\n",
    "            new_feat = new_class_features[new_cls]\n",
    "            \n",
    "            # Calculate similarities to all existing classes\n",
    "            sims = [(old_cls, cosine_sim(new_feat.to(device), class_features_dict[old_cls].to(device)).item())\n",
    "                   for old_cls in class_features_dict]\n",
    "            \n",
    "            # Sort by similarity (highest first)\n",
    "            sims.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Check if new class is similar to any existing class\n",
    "            matched = False\n",
    "            for old_cls, sim in sims:\n",
    "                if sim >= similarity_threshold:\n",
    "                    matched = True\n",
    "                    # Find which adapter/network is associated with this old class\n",
    "                    for adapter_idx, related_cls_list in related_labels.items():\n",
    "                        if old_cls in related_cls_list:\n",
    "                            # Add this new class to the same adapter's related classes\n",
    "                            if new_cls not in related_labels[adapter_idx]:\n",
    "                                related_labels[adapter_idx].append(new_cls)\n",
    "                                print(f\"üîÑ New Class {new_cls} is similar to Class {old_cls} (sim={sim:.4f}) ‚Üí Added to adapter '{adapter_idx}'\")\n",
    "                            \n",
    "                            # Mark this adapter for unfreezing during training\n",
    "                            to_unfreeze.add(adapter_idx)\n",
    "                            break\n",
    "            \n",
    "            # If no match found, create new LoRA adapter\n",
    "            if not matched:\n",
    "                # === Âä†ÂÖ•‰∏ÄÊï¥ÁµÑ LoRA adaptersÔºàÊØèÂ±§‰∏ÄÂÄãÔºâ ===\n",
    "                model.add_lora_adapter()\n",
    "\n",
    "                # Ë®òÈåÑÈÄôÊòØÁ¨¨ÂπæÁµÑÔºàÁî® group indexÔºâ\n",
    "                group_idx = max([k for k in related_labels.keys() if isinstance(k, int)], default=-1) + 1\n",
    "\n",
    "                related_labels[group_idx] = [new_cls]\n",
    "                new_lora_indices.append(group_idx)\n",
    "                print(f\"‚ûï New Class {new_cls} is not similar to any existing class ‚Üí Created new adapter group #{group_idx}\")\n",
    "        \n",
    "        # Check stability of existing classes\n",
    "        print(\"\\nüîç Checking stability of existing classes...\")\n",
    "        for old_cls in existing_classes & current_classes:  # Intersection - classes that exist in both periods\n",
    "            if old_cls in new_class_features:\n",
    "                sim_self = cosine_sim(new_class_features[old_cls].to(device), \n",
    "                                      class_features_dict[old_cls].to(device)).item()\n",
    "                print(f\"  Class {old_cls} similarity with itself: {sim_self:.4f}\")\n",
    "                \n",
    "                # If class representation has drifted too much\n",
    "                if sim_self < similarity_threshold:\n",
    "                    for adapter_idx, related_cls_list in related_labels.items():\n",
    "                        if old_cls in related_cls_list:\n",
    "                            to_unfreeze.add(adapter_idx)\n",
    "                            print(f\"‚ö†Ô∏è Class {old_cls} has drifted (self-sim={sim_self:.4f}) ‚Üí Unfreezing adapter '{adapter_idx}'\")\n",
    "        \n",
    "        # # Freeze all LoRA adapters and conv2 weights\n",
    "        # print(\"\\nüîí Default: Freezing all LoRA adapters and base conv2 weights\")\n",
    "        # for module in model.modules():\n",
    "        #     if isinstance(module, BasicBlock1d_LoRA):\n",
    "        #         # freeze all conv2\n",
    "        #         for param in module.conv2.parameters():\n",
    "        #             param.requires_grad = False\n",
    "        #         # freeze all LoRA inside\n",
    "        #         for adapter in module.lora_adapters:\n",
    "        #             for param in adapter.parameters():\n",
    "        #                 param.requires_grad = False\n",
    "\n",
    "        # üîí Freeze ALL model parameters first\n",
    "        print(\"\\nüîí Default: Freezing ALL model parameters\")\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        ####### Check frozen parameters #######\n",
    "        print(\"\\nüîç Frozen Parameters:\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "        ####### Check frozen parameters #######\n",
    "\n",
    "        # üîì Unfreeze specific adapter groups\n",
    "        print(\"\\nüîì Unfreezing selected adapters (by group):\")\n",
    "        for adapter_group_idx in to_unfreeze:\n",
    "            if isinstance(adapter_group_idx, int):\n",
    "                for module in model.modules():\n",
    "                    if isinstance(module, BasicBlock1d_LoRA):\n",
    "                        if adapter_group_idx < len(module.lora_adapters):\n",
    "                            for param in module.lora_adapters[adapter_group_idx].parameters():\n",
    "                                param.requires_grad = True\n",
    "                print(f\"  - Adapter Group #{adapter_group_idx} (all blocks) (classes: {related_labels.get(adapter_group_idx, [])})\")\n",
    "            elif adapter_group_idx == 'base':\n",
    "                print(f\"  ‚õî Base layers (classes: {related_labels.get('base', [])}) are frozen and will NOT be updated.\")\n",
    "                # for module in model.modules():\n",
    "                #     if isinstance(module, BasicBlock1d_LoRA):\n",
    "                #         for p in module.conv2.parameters():\n",
    "                #             p.requires_grad = True\n",
    "                # print(f\"  - Base layers (classes: {related_labels.get('base', [])})\")\n",
    "\n",
    "        ####### Check frozen parameters #######\n",
    "        print(\"\\nüîç Frozen Parameters: (After Unfreeze specific adapter groups)\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "        ####### Check frozen parameters #######\n",
    "\n",
    "        # üîì Unfreeze newly added adapter groups\n",
    "        for group_idx in new_lora_indices:\n",
    "            block_counter = 0\n",
    "            for module in model.modules():\n",
    "                if isinstance(module, BasicBlock1d_LoRA):\n",
    "                    if group_idx < len(module.lora_adapters):\n",
    "                        adapter = module.lora_adapters[group_idx]\n",
    "                        for param in adapter.parameters():\n",
    "                            param.requires_grad = True\n",
    "                    block_counter += 1\n",
    "            print(f\"  - New adapter group #{group_idx} (classes: {related_labels[group_idx]})\")\n",
    "            \n",
    "        ####### Check frozen parameters #######\n",
    "        print(\"\\nüîç Frozen Parameters (After Unfreeze newly added adapter groups):\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "        ####### Check frozen parameters #######\n",
    "\n",
    "        # Always unfreeze the final layer for all periods\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        ####### Check frozen parameters #######\n",
    "        print(\"\\nüîç Frozen Parameters (After unfreeze the final layer):\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "        ####### Check frozen parameters #######\n",
    "\n",
    "    # Print summary of related_labels\n",
    "    print(f\"\\nüìã Related Labels Summary:\")\n",
    "    for adapter_idx, classes in related_labels.items():\n",
    "        print(f\"  - {'Base network' if adapter_idx == 'base' else f'Adapter #{adapter_idx}'}: Classes {classes}\")\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    print(\"\\nüîß Trainable Parameter Status:\")\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    trainable_count = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            trainable_count += 1\n",
    "            print(f\"  ‚úÖ {name:<50} | shape={list(param.shape)}\")\n",
    "    print(f\"trainable_count: {trainable_count}\")\n",
    "    \n",
    "    frozen_params = total_params - trainable_params\n",
    "    print(f\"\\nüìä Parameter Statistics:\")\n",
    "    print(f\"  - Total parameters: {total_params:,}\")\n",
    "    print(f\"  - Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
    "    print(f\"  - Frozen parameters: {frozen_params:,} ({frozen_params/total_params*100:.2f}%)\")\n",
    "    \n",
    "    # Update optimizer with only trainable parameters\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=optimizer.param_groups[0]['lr'],\n",
    "        weight_decay=optimizer.param_groups[0].get('weight_decay', 0)\n",
    "    )\n",
    "\n",
    "    # ÈáçË®≠ÊâÄÊúâÂèÉÊï∏ÁöÑ requires_grad = True\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    ####### Check frozen parameters #######\n",
    "    print(\"\\nüîç Frozen Parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            print(f\"  ‚ùå {name:<50} | shape={list(param.shape)}\")\n",
    "    ####### Check frozen parameters #######\n",
    "\n",
    "    # Ê™¢Êü•ÊúÄÁµÇ optimizer ÊéßÂà∂‰∫ÜÂì™‰∫õÂèÉÊï∏\n",
    "    optimizer_param_ids = set(id(p) for group in optimizer.param_groups for p in group['params'])\n",
    "\n",
    "    named_params = list(model.named_parameters())\n",
    "    print(f\"\\nüß† Parameters currently controlled by the optimizer: ({len(named_params)})\")\n",
    "    for name, param in named_params:\n",
    "        if id(param) in optimizer_param_ids:\n",
    "            print(f\"  ‚úÖ {name}\")\n",
    "        else:\n",
    "            print(f\"  ‚õî {name} (NOT included in optimizer)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Starting training...\\n\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        if stop_signal_file and os.path.exists(stop_signal_file):\n",
    "            print(\"\\nüõë Stop signal detected. Exiting training loop.\")\n",
    "            break\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        class_correct, class_total = {}, {}\n",
    "        \n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            ce_loss = criterion(logits, yb)\n",
    "            \n",
    "            # Apply distillation if teacher model is provided\n",
    "            if teacher_model and stable_classes:\n",
    "                with torch.no_grad():\n",
    "                    teacher_logits = teacher_model(xb)\n",
    "                student_stable = logits[:, stable_classes]\n",
    "                teacher_stable = teacher_logits[:, stable_classes]\n",
    "                distill_loss = F.mse_loss(student_stable, teacher_stable)\n",
    "                total_loss = alpha * distill_loss + (1 - alpha) * ce_loss\n",
    "            else:\n",
    "                total_loss = ce_loss\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += total_loss.item() * xb.size(0)\n",
    "            compute_classwise_accuracy(logits, yb, class_correct, class_total)\n",
    "        \n",
    "        train_loss = epoch_loss / len(train_loader.dataset)\n",
    "        train_acc = {int(k): f\"{(class_correct[k] / class_total[k]) * 100:.2f}%\" \n",
    "                    if class_total[k] > 0 else \"0.00%\" for k in class_total}\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        val_class_correct, val_class_total = {}, {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                outputs = model(xb)\n",
    "                val_loss += criterion(outputs, yb).item() * xb.size(0)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_correct += (preds == yb).sum().item()\n",
    "                val_total += yb.size(0)\n",
    "                compute_classwise_accuracy(outputs, yb, val_class_correct, val_class_total)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct / val_total\n",
    "        val_acc_cls = {int(k): f\"{(val_class_correct[k]/val_class_total[k])*100:.2f}%\" \n",
    "                      if val_class_total[k] > 0 else \"0.00%\" for k in val_class_total}\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Train-Class-Acc: {train_acc}\")\n",
    "        print(f\"Val Loss: {val_loss:.6f}, Val Acc: {val_acc*100:.2f}%, Val-Class-Acc: {val_acc_cls}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Save model checkpoint\n",
    "        model_path = os.path.join(model_saving_folder, f\"{model_name}_epoch_{epoch+1}.pth\")\n",
    "        current = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_acc,\n",
    "            'train_classwise_accuracy': train_acc,\n",
    "            'val_classwise_accuracy': val_acc_cls,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "            'model_path': model_path,\n",
    "            'num_lora_groups': model.count_lora_groups(),\n",
    "            'related_labels': related_labels\n",
    "        }\n",
    "        \n",
    "        # Keep top 5 best models\n",
    "        if len(best_results) < 5 or val_acc > best_results[-1]['val_accuracy']:\n",
    "            if len(best_results) == 5:\n",
    "                to_remove = best_results.pop()\n",
    "                if os.path.exists(to_remove['model_path']):\n",
    "                    os.remove(to_remove['model_path'])\n",
    "                    print(f\"üóë Removed: {to_remove['model_path']}\")\n",
    "            best_results.append(current)\n",
    "            best_results.sort(key=lambda x: (x['val_accuracy'], x['epoch']), reverse=True)\n",
    "            torch.save(current, model_path)\n",
    "            print(f\"‚úÖ Saved model: {model_path}\")\n",
    "        \n",
    "        # Update learning rate scheduler if provided\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "    \n",
    "    # End of training\n",
    "    elapsed_time = time.time() - start_time\n",
    "    total_params, param_size_MB = get_model_parameter_info(model)\n",
    "    \n",
    "    # Save best model\n",
    "    if best_results:\n",
    "        best = best_results[0]\n",
    "        best_model_path = os.path.join(model_saving_folder, f\"{model_name}_best.pth\")\n",
    "        torch.save(best, best_model_path)\n",
    "        print(f\"\\nüèÜ Best model saved as: {best_model_path} (Val Accuracy: {best['val_accuracy'] * 100:.2f}%)\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = os.path.join(model_saving_folder, f\"{model_name}_final.pth\")\n",
    "    torch.save(current, final_model_path)\n",
    "    print(f\"\\nüìå Final model saved as: {final_model_path}\")\n",
    "    \n",
    "    # Print top 5 models\n",
    "    print(\"\\nüéØ Top 5 Best Models:\")\n",
    "    for res in best_results:\n",
    "        print(f\"Epoch {res['epoch']}, Train Loss: {res['train_loss']:.6f}, Train-Acc: {res['train_classwise_accuracy']},\\n\"\n",
    "              f\"Val Loss: {res['val_loss']:.6f}, Val Acc: {res['val_accuracy']*100:.2f}%, Val-Acc: {res['val_classwise_accuracy']}, \"\n",
    "              f\"Model Path: {res['model_path']}\")\n",
    "    \n",
    "    # Print model summary\n",
    "    print(f\"\\nüß† Model Summary:\")\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    print(f\"Model Size (float32): {param_size_MB:.2f} MB\")\n",
    "    print(f\"Number of LoRA adapters: {model.count_lora_adapters()}\")\n",
    "    print(f\"Number of LoRA groups: {model.count_lora_groups()}\")\n",
    "    print(f\"Total Training Time: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    # Extract period number from folder name\n",
    "    match = re.search(r'Period_(\\d+)', model_saving_folder)\n",
    "    period_label = match.group(1) if match else str(period)\n",
    "    model_name_str = model.__class__.__name__\n",
    "    \n",
    "    # Print markdown summary\n",
    "    best_model = max(best_results, key=lambda x: x['val_accuracy'])\n",
    "    print(f\"\"\"\n",
    "---\n",
    "### Period {period_label} (alpha = {alpha}, similarity_threshold = {similarity_threshold})\n",
    "+ ##### Total training time: {elapsed_time:.2f} seconds\n",
    "+ ##### Model: {model_name_str}\n",
    "+ ##### Training and saving in *'{model_saving_folder}'*\n",
    "+ ##### Best Epoch: {best_model['epoch']}\n",
    "#### __Val Accuracy: {best_model['val_accuracy'] * 100:.2f}%__\n",
    "#### __Val-Class-Acc: {best_model['val_classwise_accuracy']}__\n",
    "#### __Total Parameters: {total_params:,}__\n",
    "#### __Model Size (float32): {param_size_MB:.2f} MB__\n",
    "#### __Number of LoRA adapters: {model.count_lora_adapters()}__\n",
    "#### __Number of LoRA groups: {model.count_lora_groups()}__\n",
    "\"\"\".strip())\n",
    "    \n",
    "    # Save class features for next period\n",
    "    if class_features_dict is None:\n",
    "        class_features_dict = {}\n",
    "    class_features_dict.update(new_class_features)\n",
    "    with open(os.path.join(model_saving_folder, \"class_features.pkl\"), 'wb') as f:\n",
    "        pickle.dump(class_features_dict, f)\n",
    "    print(f\"\\nSaved class features to: {os.path.join(model_saving_folder, 'class_features.pkl')}\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def extract_features(model, x):\n",
    "    \"\"\"Helper function to extract features from the model for similarity calculation\"\"\"\n",
    "    # This is a placeholder - you'll need to adapt this based on your actual model architecture\n",
    "    # The goal is to extract meaningful features before the classification layer\n",
    "    # For ResNet18_1D_LoRA model, this would typically be the features right before the fc layer\n",
    "    \n",
    "    # Example (pseudo-code - adapt to your actual model):\n",
    "    x = x.permute(0, 2, 1)  # Convert to (batch_size, channels, time_steps)\n",
    "    \n",
    "    # Feed through the network up to the point before classification\n",
    "    with torch.no_grad():\n",
    "        x = model.conv1(x)\n",
    "        x = model.bn1(x)\n",
    "        x = model.relu(x)\n",
    "        x = model.maxpool(x)\n",
    "        \n",
    "        x = model.layer1(x)\n",
    "        x = model.layer2(x)\n",
    "        x = model.layer3(x)\n",
    "        x = model.layer4(x)\n",
    "        \n",
    "        # Apply pooling\n",
    "        x1 = model.adaptiveavgpool(x)\n",
    "        x2 = model.adaptivemaxpool(x)\n",
    "        \n",
    "        # Concatenate pooling results\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        \n",
    "        # Flatten\n",
    "        features = x.view(x.size(0), -1)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1085b2",
   "metadata": {},
   "source": [
    "## __Training__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8ee5f8",
   "metadata": {},
   "source": [
    "### Period 1 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c948b89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n",
      "ResNet18_1D_LoRA(\n",
      "  (conv1): Conv1d(12, 64, kernel_size=(15,), stride=(2,), padding=(7,), bias=False)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (lora_adapters): ModuleList()\n",
      "    )\n",
      "    (1): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (lora_adapters): ModuleList()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (lora_adapters): ModuleList()\n",
      "    )\n",
      "    (1): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (lora_adapters): ModuleList()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (lora_adapters): ModuleList()\n",
      "    )\n",
      "    (1): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (lora_adapters): ModuleList()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (lora_adapters): ModuleList()\n",
      "    )\n",
      "    (1): BasicBlock1d_LoRA(\n",
      "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (lora_adapters): ModuleList()\n",
      "    )\n",
      "  )\n",
      "  (adaptiveavgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (adaptivemaxpool): AdaptiveMaxPool1d(output_size=1)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "üì¶ Model Summary from: Class_Incremental_CL/CPSC_CIL/ResNet18_Selection/ResNet18_big_inplane_v1/ResNet18_big_inplane_1D_best.pth\n",
      "üìå Epoch: 63\n",
      "üßÆ Train Loss: 0.007664\n",
      "üéØ Val Loss: 0.800983\n",
      "‚úÖ Val Accuracy: 88.86%\n",
      "üìé Learning Rate: 0.0006561000000000001\n",
      "üìÅ Stored Model Path: Class_Incremental_CL/CPSC_CIL/ResNet18_Selection/ResNet18_big_inplane_v1/ResNet18_big_inplane_1D_epoch_63.pth\n",
      "üß† Total Parameters: 3,857,026\n",
      "üìè Model Size (float32): 14.71 MB\n",
      "\n",
      "üìä Train Class-wise Accuracy:\n",
      "  ‚îî‚îÄ Class 0 : 99.86%\n",
      "  ‚îî‚îÄ Class 1 : 99.59%\n",
      "\n",
      "üìä Val Class-wise Accuracy:\n",
      "  ‚îî‚îÄ Class 0 : 91.85%\n",
      "  ‚îî‚îÄ Class 1 : 85.87%\n",
      "\n",
      "---\n",
      "### Period 1 Summary (Markdown Format)\n",
      "+ **Epoch:** 63\n",
      "+ **Train Loss:** 0.0076635455248283595\n",
      "+ **Val Loss:** 0.800983331773592\n",
      "+ **Val Accuracy:** 88.86%\n",
      "+ **Learning Rate:** 0.0006561000000000001\n",
      "+ **Stored Model Path:** `Class_Incremental_CL/CPSC_CIL/ResNet18_Selection/ResNet18_big_inplane_v1/ResNet18_big_inplane_1D_epoch_63.pth`\n",
      "+ **Total Parameters:** 3,857,026\n",
      "+ **Model Size (float32):** 14.71 MB\n",
      "+ **Train-Class-Acc:** {0: '99.86%', 1: '99.59%'}\n",
      "+ **Val-Class-Acc:** {0: '91.85%', 1: '85.87%'}\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1514991/2299765635.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "def display_model_summary_with_params(model_folder, model_filename=\"ResNet18_big_inplane_1D_best.pth\", input_channels=12, output_size=10):\n",
    "    model_path = os.path.join(model_folder, model_filename)\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"‚ùå File not found: {model_path}\")\n",
    "        return\n",
    "\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "    # === ÈÇÑÂéüÊ®°Âûã‰∏¶ËºâÂÖ•ÂèÉÊï∏ ===\n",
    "    model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    total_params, param_size_MB = get_model_parameter_info(model)\n",
    "\n",
    "    # === È°ØÁ§∫ÊëòË¶Å ===\n",
    "    epoch = checkpoint.get(\"epoch\", \"?\")\n",
    "    train_loss = checkpoint.get(\"train_loss\", \"?\")\n",
    "    val_loss = checkpoint.get(\"val_loss\", \"?\")\n",
    "    val_acc = checkpoint.get(\"val_accuracy\", \"?\")\n",
    "    train_acc_dict = checkpoint.get(\"train_classwise_accuracy\", {})\n",
    "    val_acc_dict = checkpoint.get(\"val_classwise_accuracy\", {})\n",
    "    lr = checkpoint.get(\"learning_rate\", \"?\")\n",
    "    stored_path = checkpoint.get(\"model_path\", \"N/A\")\n",
    "    print(f\"Model Architecture:\")\n",
    "    print(model)\n",
    "    print(f\"\\nüì¶ Model Summary from: {model_path}\")\n",
    "    print(f\"üìå Epoch: {epoch}\")\n",
    "    print(f\"üßÆ Train Loss: {train_loss:.6f}\" if isinstance(train_loss, float) else f\"üßÆ Train Loss: {train_loss}\")\n",
    "    print(f\"üéØ Val Loss: {val_loss:.6f}\" if isinstance(val_loss, float) else f\"üéØ Val Loss: {val_loss}\")\n",
    "    print(f\"‚úÖ Val Accuracy: {val_acc*100:.2f}%\" if isinstance(val_acc, float) else f\"‚úÖ Val Accuracy: {val_acc}\")\n",
    "    print(f\"üìé Learning Rate: {lr}\")\n",
    "    print(f\"üìÅ Stored Model Path: {stored_path}\")\n",
    "    print(f\"üß† Total Parameters: {total_params:,}\")\n",
    "    print(f\"üìè Model Size (float32): {param_size_MB:.2f} MB\")\n",
    "\n",
    "    print(\"\\nüìä Train Class-wise Accuracy:\")\n",
    "    for c, acc in train_acc_dict.items():\n",
    "        print(f\"  ‚îî‚îÄ Class {c:<2}: {acc}\")\n",
    "\n",
    "    print(\"\\nüìä Val Class-wise Accuracy:\")\n",
    "    for c, acc in val_acc_dict.items():\n",
    "        print(f\"  ‚îî‚îÄ Class {c:<2}: {acc}\")\n",
    "\n",
    "    print(\"\\n---\\n### Period 1 Summary (Markdown Format)\")\n",
    "    print(f\"+ **Epoch:** {epoch}\")\n",
    "    print(f\"+ **Train Loss:** {train_loss}\")\n",
    "    print(f\"+ **Val Loss:** {val_loss}\")\n",
    "    print(f\"+ **Val Accuracy:** {val_acc*100:.2f}%\" if isinstance(val_acc, float) else f\"+ **Val Accuracy:** {val_acc}\")\n",
    "    print(f\"+ **Learning Rate:** {lr}\")\n",
    "    print(f\"+ **Stored Model Path:** `{stored_path}`\")\n",
    "    print(f\"+ **Total Parameters:** {total_params:,}\")\n",
    "    print(f\"+ **Model Size (float32):** {param_size_MB:.2f} MB\")\n",
    "    print(f\"+ **Train-Class-Acc:** {train_acc_dict}\")\n",
    "    print(f\"+ **Val-Class-Acc:** {val_acc_dict}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# Example call:\n",
    "display_model_summary_with_params(\n",
    "    model_folder=os.path.join(\"Class_Incremental_CL\", \"CPSC_CIL\", \"ResNet18_Selection\", \"ResNet18_big_inplane_v1\"),\n",
    "    input_channels=12,\n",
    "    output_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a690ca",
   "metadata": {},
   "source": [
    "### Generate class features (period1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bf72fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 1\n",
      "    - Memory Used    : 18 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1514991/3142115414.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location='cpu')\n",
      "Extracting Features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [00:00<00:00, 59.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved class_features_dict to: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/ResNet18_Selection/ResNet18_big_inplane_v1/class_features.pkl\n"
     ]
    }
   ],
   "source": [
    "def generate_class_features_period1(\n",
    "    model_path: str,\n",
    "    save_path: str,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    input_channels: int = 12,\n",
    "    output_size: int = 2,\n",
    "    batch_size: int = 64\n",
    "):\n",
    "    # === ËºâÂÖ•Ê®°Âûã ===\n",
    "    device = auto_select_cuda_device()\n",
    "    model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size)\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # === Ë£Ω‰Ωú Dataloader ===\n",
    "    X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    dataloader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # === ÈñãÂßãËêÉÂèñÁâπÂæµ ===\n",
    "    class_features_dict = {}\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            features = extract_features(model, xb)  # shape: [B, F]\n",
    "            for cls in torch.unique(yb):\n",
    "                cls_mask = (yb == cls)\n",
    "                cls_feat = features[cls_mask]\n",
    "                cls_id = cls.item()\n",
    "                if cls_id not in class_features_dict:\n",
    "                    class_features_dict[cls_id] = []\n",
    "                class_features_dict[cls_id].append(cls_feat.cpu())\n",
    "\n",
    "    # === Âπ≥ÂùáÊØèÂÄã class ÁöÑ feature ÂêëÈáè ===\n",
    "    for cls in class_features_dict:\n",
    "        class_features_dict[cls] = torch.cat(class_features_dict[cls], dim=0).mean(dim=0)\n",
    "\n",
    "    # === ÂÑ≤Â≠òÁÇ∫ .pkl Êñπ‰æø Period 2 ËºâÂÖ• ===\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(class_features_dict, f)\n",
    "    \n",
    "    print(f\"‚úÖ Saved class_features_dict to: {save_path}\")\n",
    "    return class_features_dict\n",
    "\n",
    "\n",
    "model_path = os.path.join(BASE_DIR, \"ResNet18_Selection\", \"ResNet18_big_inplane_v1\", \"ResNet18_big_inplane_1D_best.pth\")\n",
    "save_path = os.path.join(BASE_DIR, \"ResNet18_Selection\", \"ResNet18_big_inplane_v1\", \"class_features.pkl\")\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{1}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{1}.npy\"))\n",
    "\n",
    "class_features_dict = generate_class_features_period1(\n",
    "    model_path=model_path,\n",
    "    save_path=save_path,\n",
    "    X_train=X_train,  # ‰Ω†Âæû npy ËºâÂÖ•ÁöÑ Period 1 Ë≥áÊñô\n",
    "    y_train=y_train,\n",
    "    input_channels=12,\n",
    "    output_size=2,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e2f418",
   "metadata": {},
   "source": [
    "### Period 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f49a82f",
   "metadata": {},
   "source": [
    "#### v10 no distillation, all freeze, th=0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cfc7958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 1\n",
      "    - Memory Used    : 283 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ Loaded class features from: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/ResNet18_Selection/ResNet18_big_inplane_v1/class_features.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1777561/3830121183.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Number of LoRA groups: 0\n",
      "üîç Not loaded: fc.weight, shape=torch.Size([4, 1024])\n",
      "üîç Not loaded: fc.bias, shape=torch.Size([4])\n",
      "‚úÖ Loaded shared weights from Period 1 (excluding FC only)\n",
      "\n",
      "üöÄ 'train_with_dynex_clora_ecg' started for Period 2\n",
      "\n",
      "‚úÖ Removed existing folder: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2\n",
      "\n",
      "‚úÖ Data Overview:\n",
      "X_train: torch.Size([3263, 5000, 12]), y_train: torch.Size([3263])\n",
      "X_val: torch.Size([816, 5000, 12]), y_val: torch.Size([816])\n",
      "\n",
      "üîé Similarity Analysis:\n",
      "  Similarity threshold: 0.9900\n",
      "  Existing classes: [0, 1]\n",
      "  Current classes: [0, 1, 2, 3]\n",
      "  New classes: [2, 3]\n",
      "\n",
      "üìä Similarity Scores:\n",
      "  New Class 2:\n",
      "    - Existing Class 1: 0.9665\n",
      "    - Existing Class 0: 0.8415\n",
      "  New Class 3:\n",
      "    - Existing Class 1: 0.9912\n",
      "    - Existing Class 0: 0.8097\n",
      "\n",
      "  Average similarity: 0.9055, Std: 0.0837\n",
      "\n",
      "üß© Managing LoRA adapters for 2 new classes...\n",
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n",
      "‚ûï New Class 2 is not similar to any existing class ‚Üí Created new adapter group #0\n",
      "üîÑ New Class 3 is similar to Class 1 (sim=0.9912) ‚Üí Added to adapter 'base'\n",
      "\n",
      "üîç Checking stability of existing classes...\n",
      "  Class 0 similarity with itself: 0.9998\n",
      "  Class 1 similarity with itself: 0.9948\n",
      "\n",
      "üîí Default: Freezing ALL model parameters\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå fc.weight                                          | shape=[4, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[4]\n",
      "\n",
      "üîì Unfreezing selected adapters (by group):\n",
      "  ‚õî Base layers (classes: [0, 1, 3]) are frozen and will NOT be updated.\n",
      "\n",
      "üîç Frozen Parameters: (After Unfreeze specific adapter groups)\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå fc.weight                                          | shape=[4, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[4]\n",
      "  - New adapter group #0 (classes: [2])\n",
      "\n",
      "üîç Frozen Parameters (After Unfreeze newly added adapter groups):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå fc.weight                                          | shape=[4, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[4]\n",
      "\n",
      "üîç Frozen Parameters (After unfreeze the final layer):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "\n",
      "üìã Related Labels Summary:\n",
      "  - Base network: Classes [0, 1, 3]\n",
      "  - Adapter #0: Classes [2]\n",
      "\n",
      "üîß Trainable Parameter Status:\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ fc.weight                                          | shape=[4, 1024]\n",
      "  ‚úÖ fc.bias                                            | shape=[4]\n",
      "trainable_count: 18\n",
      "\n",
      "üìä Parameter Statistics:\n",
      "  - Total parameters: 3,889,796\n",
      "  - Trainable parameters: 34,820 (0.90%)\n",
      "  - Frozen parameters: 3,854,976 (99.10%)\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "\n",
      "üß† Parameters currently controlled by the optimizer: (78)\n",
      "  ‚õî conv1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer1.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer1.0.conv2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer1.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer1.1.conv2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer2.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer2.0.conv2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer2.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer2.1.conv2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer3.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer3.0.conv2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer3.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer3.1.conv2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer4.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer4.0.conv2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer4.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer4.1.conv2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ fc.weight\n",
      "  ‚úÖ fc.bias\n",
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/200, Train Loss: 43.262860, Train-Class-Acc: {0: '51.50%', 1: '35.35%', 2: '20.62%', 3: '35.35%'}\n",
      "Val Loss: 24.254611, Val Acc: 58.46%, Val-Class-Acc: {0: '88.59%', 1: '49.59%', 2: '27.78%', 3: '62.70%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_1.pth\n",
      "Epoch 2/200, Train Loss: 21.625523, Train-Class-Acc: {0: '72.89%', 1: '50.31%', 2: '37.78%', 3: '54.61%'}\n",
      "Val Loss: 10.591681, Val Acc: 69.12%, Val-Class-Acc: {0: '92.39%', 1: '58.20%', 2: '54.17%', 3: '71.31%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_2.pth\n",
      "Epoch 3/200, Train Loss: 17.576098, Train-Class-Acc: {0: '75.75%', 1: '51.54%', 2: '46.62%', 3: '57.17%'}\n",
      "Val Loss: 7.244172, Val Acc: 74.51%, Val-Class-Acc: {0: '89.13%', 1: '64.75%', 2: '59.72%', 3: '81.97%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_3.pth\n",
      "Epoch 4/200, Train Loss: 14.039087, Train-Class-Acc: {0: '78.34%', 1: '59.94%', 2: '55.81%', 3: '65.27%'}\n",
      "Val Loss: 5.804420, Val Acc: 75.12%, Val-Class-Acc: {0: '90.76%', 1: '64.34%', 2: '72.92%', 3: '75.41%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_4.pth\n",
      "Epoch 5/200, Train Loss: 12.286078, Train-Class-Acc: {0: '77.11%', 1: '58.91%', 2: '58.93%', 3: '66.39%'}\n",
      "Val Loss: 6.135186, Val Acc: 78.31%, Val-Class-Acc: {0: '91.85%', 1: '58.20%', 2: '74.31%', 3: '90.57%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_5.pth\n",
      "Epoch 6/200, Train Loss: 10.234196, Train-Class-Acc: {0: '78.47%', 1: '63.22%', 2: '65.16%', 3: '69.77%'}\n",
      "Val Loss: 5.182424, Val Acc: 80.88%, Val-Class-Acc: {0: '89.67%', 1: '75.00%', 2: '69.44%', 3: '86.89%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_1.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_6.pth\n",
      "Epoch 7/200, Train Loss: 9.031730, Train-Class-Acc: {0: '78.47%', 1: '66.19%', 2: '71.06%', 3: '73.77%'}\n",
      "Val Loss: 5.700192, Val Acc: 80.15%, Val-Class-Acc: {0: '84.78%', 1: '79.10%', 2: '76.39%', 3: '79.92%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_2.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_7.pth\n",
      "Epoch 8/200, Train Loss: 8.675445, Train-Class-Acc: {0: '78.75%', 1: '65.57%', 2: '71.92%', 3: '73.36%'}\n",
      "Val Loss: 5.712218, Val Acc: 80.27%, Val-Class-Acc: {0: '91.85%', 1: '60.25%', 2: '73.61%', 3: '95.49%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_3.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_8.pth\n",
      "Epoch 9/200, Train Loss: 7.644866, Train-Class-Acc: {0: '80.11%', 1: '69.16%', 2: '71.75%', 3: '77.25%'}\n",
      "Val Loss: 4.411407, Val Acc: 81.99%, Val-Class-Acc: {0: '92.93%', 1: '71.31%', 2: '79.86%', 3: '85.66%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_4.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_9.pth\n",
      "Epoch 10/200, Train Loss: 7.808428, Train-Class-Acc: {0: '78.75%', 1: '66.19%', 2: '75.74%', 3: '77.36%'}\n",
      "Val Loss: 4.347906, Val Acc: 81.50%, Val-Class-Acc: {0: '89.67%', 1: '75.00%', 2: '82.64%', 3: '81.15%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_5.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_10.pth\n",
      "Epoch 11/200, Train Loss: 7.364798, Train-Class-Acc: {0: '78.20%', 1: '65.68%', 2: '74.70%', 3: '78.59%'}\n",
      "Val Loss: 4.352304, Val Acc: 83.33%, Val-Class-Acc: {0: '91.85%', 1: '71.72%', 2: '75.00%', 3: '93.44%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_7.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_11.pth\n",
      "Epoch 12/200, Train Loss: 6.391832, Train-Class-Acc: {0: '80.25%', 1: '72.64%', 2: '77.47%', 3: '79.71%'}\n",
      "Val Loss: 3.769439, Val Acc: 82.23%, Val-Class-Acc: {0: '85.87%', 1: '69.67%', 2: '77.08%', 3: '95.08%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_8.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_12.pth\n",
      "Epoch 13/200, Train Loss: 6.261508, Train-Class-Acc: {0: '80.25%', 1: '70.08%', 2: '75.91%', 3: '80.12%'}\n",
      "Val Loss: 3.580310, Val Acc: 84.19%, Val-Class-Acc: {0: '88.59%', 1: '74.59%', 2: '79.86%', 3: '93.03%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_6.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_13.pth\n",
      "Epoch 14/200, Train Loss: 5.304131, Train-Class-Acc: {0: '81.74%', 1: '71.11%', 2: '78.68%', 3: '81.76%'}\n",
      "Val Loss: 4.631302, Val Acc: 82.60%, Val-Class-Acc: {0: '87.50%', 1: '66.80%', 2: '78.47%', 3: '97.13%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_10.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_14.pth\n",
      "Epoch 15/200, Train Loss: 5.792166, Train-Class-Acc: {0: '79.84%', 1: '70.80%', 2: '80.42%', 3: '81.86%'}\n",
      "Val Loss: 3.265090, Val Acc: 83.95%, Val-Class-Acc: {0: '86.41%', 1: '72.95%', 2: '81.94%', 3: '94.26%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_9.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_15.pth\n",
      "Epoch 16/200, Train Loss: 5.417987, Train-Class-Acc: {0: '82.70%', 1: '70.90%', 2: '77.82%', 3: '81.45%'}\n",
      "Val Loss: 3.481208, Val Acc: 84.31%, Val-Class-Acc: {0: '94.57%', 1: '70.49%', 2: '80.56%', 3: '92.62%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_12.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_16.pth\n",
      "Epoch 17/200, Train Loss: 5.075339, Train-Class-Acc: {0: '80.79%', 1: '72.03%', 2: '80.59%', 3: '81.86%'}\n",
      "Val Loss: 3.493519, Val Acc: 83.82%, Val-Class-Acc: {0: '90.22%', 1: '76.64%', 2: '78.47%', 3: '89.34%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_14.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_17.pth\n",
      "Epoch 18/200, Train Loss: 5.302387, Train-Class-Acc: {0: '81.74%', 1: '73.16%', 2: '77.82%', 3: '83.40%'}\n",
      "Val Loss: 3.881246, Val Acc: 84.44%, Val-Class-Acc: {0: '88.04%', 1: '73.77%', 2: '77.78%', 3: '96.31%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_11.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_18.pth\n",
      "Epoch 19/200, Train Loss: 5.318535, Train-Class-Acc: {0: '79.70%', 1: '74.08%', 2: '79.20%', 3: '81.66%'}\n",
      "Val Loss: 3.553623, Val Acc: 84.56%, Val-Class-Acc: {0: '89.13%', 1: '77.05%', 2: '86.11%', 3: '87.70%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_17.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_19.pth\n",
      "Epoch 20/200, Train Loss: 4.766455, Train-Class-Acc: {0: '82.15%', 1: '73.57%', 2: '80.59%', 3: '83.09%'}\n",
      "Val Loss: 4.606769, Val Acc: 83.21%, Val-Class-Acc: {0: '92.39%', 1: '77.05%', 2: '78.47%', 3: '85.25%'}, LR: 0.001000\n",
      "Epoch 21/200, Train Loss: 4.551414, Train-Class-Acc: {0: '82.29%', 1: '75.72%', 2: '81.63%', 3: '84.73%'}\n",
      "Val Loss: 3.112842, Val Acc: 84.80%, Val-Class-Acc: {0: '91.30%', 1: '69.26%', 2: '83.33%', 3: '96.31%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_15.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_21.pth\n",
      "Epoch 22/200, Train Loss: 4.524184, Train-Class-Acc: {0: '81.47%', 1: '72.64%', 2: '83.02%', 3: '84.02%'}\n",
      "Val Loss: 3.088880, Val Acc: 84.68%, Val-Class-Acc: {0: '93.48%', 1: '70.49%', 2: '86.81%', 3: '90.98%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_13.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_22.pth\n",
      "Epoch 23/200, Train Loss: 4.149067, Train-Class-Acc: {0: '82.83%', 1: '74.28%', 2: '81.28%', 3: '84.12%'}\n",
      "Val Loss: 3.283041, Val Acc: 84.93%, Val-Class-Acc: {0: '90.76%', 1: '79.10%', 2: '78.47%', 3: '90.16%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_16.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_23.pth\n",
      "Epoch 24/200, Train Loss: 4.291702, Train-Class-Acc: {0: '80.25%', 1: '72.64%', 2: '79.03%', 3: '83.40%'}\n",
      "Val Loss: 3.102961, Val Acc: 85.05%, Val-Class-Acc: {0: '86.41%', 1: '79.92%', 2: '83.33%', 3: '90.16%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_18.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_24.pth\n",
      "Epoch 25/200, Train Loss: 4.090457, Train-Class-Acc: {0: '82.83%', 1: '76.54%', 2: '83.36%', 3: '84.12%'}\n",
      "Val Loss: 5.006185, Val Acc: 81.62%, Val-Class-Acc: {0: '81.52%', 1: '83.20%', 2: '84.03%', 3: '78.69%'}, LR: 0.001000\n",
      "Epoch 26/200, Train Loss: 3.831455, Train-Class-Acc: {0: '83.11%', 1: '75.31%', 2: '81.98%', 3: '84.22%'}\n",
      "Val Loss: 2.760238, Val Acc: 85.78%, Val-Class-Acc: {0: '94.02%', 1: '71.72%', 2: '82.64%', 3: '95.49%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_19.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_26.pth\n",
      "Epoch 27/200, Train Loss: 4.007866, Train-Class-Acc: {0: '81.06%', 1: '75.20%', 2: '81.80%', 3: '86.37%'}\n",
      "Val Loss: 3.271405, Val Acc: 85.05%, Val-Class-Acc: {0: '94.02%', 1: '70.08%', 2: '79.86%', 3: '96.31%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_22.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_27.pth\n",
      "Epoch 28/200, Train Loss: 3.544822, Train-Class-Acc: {0: '82.15%', 1: '75.92%', 2: '85.27%', 3: '85.55%'}\n",
      "Val Loss: 2.777153, Val Acc: 85.05%, Val-Class-Acc: {0: '86.96%', 1: '79.51%', 2: '81.94%', 3: '90.98%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_21.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_28.pth\n",
      "Epoch 29/200, Train Loss: 3.196493, Train-Class-Acc: {0: '82.70%', 1: '76.64%', 2: '83.02%', 3: '87.50%'}\n",
      "Val Loss: 2.596130, Val Acc: 84.80%, Val-Class-Acc: {0: '86.41%', 1: '74.18%', 2: '82.64%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 30/200, Train Loss: 3.559220, Train-Class-Acc: {0: '82.43%', 1: '75.92%', 2: '83.19%', 3: '86.99%'}\n",
      "Val Loss: 3.081075, Val Acc: 85.54%, Val-Class-Acc: {0: '95.65%', 1: '68.85%', 2: '80.56%', 3: '97.54%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_23.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_30.pth\n",
      "Epoch 31/200, Train Loss: 3.186367, Train-Class-Acc: {0: '82.83%', 1: '73.98%', 2: '85.27%', 3: '84.43%'}\n",
      "Val Loss: 2.538649, Val Acc: 84.93%, Val-Class-Acc: {0: '94.57%', 1: '70.49%', 2: '80.56%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 32/200, Train Loss: 3.075509, Train-Class-Acc: {0: '83.38%', 1: '78.07%', 2: '84.40%', 3: '86.58%'}\n",
      "Val Loss: 3.174027, Val Acc: 84.93%, Val-Class-Acc: {0: '83.15%', 1: '84.43%', 2: '84.03%', 3: '87.30%'}, LR: 0.001000\n",
      "Epoch 33/200, Train Loss: 3.048260, Train-Class-Acc: {0: '83.51%', 1: '76.43%', 2: '84.40%', 3: '87.60%'}\n",
      "Val Loss: 2.406104, Val Acc: 84.93%, Val-Class-Acc: {0: '92.39%', 1: '71.72%', 2: '81.94%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 34/200, Train Loss: 3.115120, Train-Class-Acc: {0: '83.65%', 1: '76.95%', 2: '83.71%', 3: '87.19%'}\n",
      "Val Loss: 2.357680, Val Acc: 84.80%, Val-Class-Acc: {0: '84.78%', 1: '82.38%', 2: '80.56%', 3: '89.75%'}, LR: 0.001000\n",
      "Epoch 35/200, Train Loss: 3.124739, Train-Class-Acc: {0: '84.60%', 1: '77.97%', 2: '84.58%', 3: '86.78%'}\n",
      "Val Loss: 3.050826, Val Acc: 85.42%, Val-Class-Acc: {0: '92.93%', 1: '78.28%', 2: '81.25%', 3: '89.34%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_24.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_35.pth\n",
      "Epoch 36/200, Train Loss: 2.906201, Train-Class-Acc: {0: '82.29%', 1: '76.74%', 2: '83.02%', 3: '88.73%'}\n",
      "Val Loss: 2.206272, Val Acc: 86.15%, Val-Class-Acc: {0: '90.76%', 1: '79.10%', 2: '81.94%', 3: '92.21%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_27.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_36.pth\n",
      "Epoch 37/200, Train Loss: 2.867986, Train-Class-Acc: {0: '83.65%', 1: '77.66%', 2: '83.71%', 3: '88.32%'}\n",
      "Val Loss: 3.203440, Val Acc: 83.33%, Val-Class-Acc: {0: '82.61%', 1: '76.23%', 2: '73.61%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 38/200, Train Loss: 2.646685, Train-Class-Acc: {0: '84.33%', 1: '78.28%', 2: '86.31%', 3: '87.30%'}\n",
      "Val Loss: 2.386226, Val Acc: 86.15%, Val-Class-Acc: {0: '88.04%', 1: '78.69%', 2: '83.33%', 3: '93.85%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_28.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_38.pth\n",
      "Epoch 39/200, Train Loss: 2.839871, Train-Class-Acc: {0: '84.20%', 1: '78.38%', 2: '84.92%', 3: '86.27%'}\n",
      "Val Loss: 2.581416, Val Acc: 85.54%, Val-Class-Acc: {0: '89.13%', 1: '78.28%', 2: '84.03%', 3: '90.98%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_35.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_39.pth\n",
      "Epoch 40/200, Train Loss: 2.483645, Train-Class-Acc: {0: '82.70%', 1: '79.51%', 2: '85.79%', 3: '88.42%'}\n",
      "Val Loss: 2.580412, Val Acc: 84.93%, Val-Class-Acc: {0: '93.48%', 1: '66.80%', 2: '86.81%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 41/200, Train Loss: 2.634136, Train-Class-Acc: {0: '86.24%', 1: '78.18%', 2: '84.58%', 3: '88.73%'}\n",
      "Val Loss: 2.519287, Val Acc: 85.17%, Val-Class-Acc: {0: '94.57%', 1: '69.26%', 2: '80.56%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 42/200, Train Loss: 2.219107, Train-Class-Acc: {0: '84.74%', 1: '79.51%', 2: '86.66%', 3: '88.93%'}\n",
      "Val Loss: 2.269026, Val Acc: 86.27%, Val-Class-Acc: {0: '92.39%', 1: '76.64%', 2: '82.64%', 3: '93.44%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_30.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_42.pth\n",
      "Epoch 43/200, Train Loss: 2.557190, Train-Class-Acc: {0: '86.65%', 1: '79.51%', 2: '85.79%', 3: '86.78%'}\n",
      "Val Loss: 2.380737, Val Acc: 85.54%, Val-Class-Acc: {0: '86.96%', 1: '78.69%', 2: '84.03%', 3: '92.21%'}, LR: 0.001000\n",
      "Epoch 44/200, Train Loss: 2.268556, Train-Class-Acc: {0: '84.88%', 1: '81.76%', 2: '86.66%', 3: '89.45%'}\n",
      "Val Loss: 2.140730, Val Acc: 85.05%, Val-Class-Acc: {0: '91.85%', 1: '69.67%', 2: '82.64%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 45/200, Train Loss: 2.291061, Train-Class-Acc: {0: '85.15%', 1: '78.59%', 2: '88.04%', 3: '89.45%'}\n",
      "Val Loss: 2.354011, Val Acc: 85.42%, Val-Class-Acc: {0: '94.57%', 1: '73.77%', 2: '77.78%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 46/200, Train Loss: 1.992552, Train-Class-Acc: {0: '85.42%', 1: '80.64%', 2: '87.35%', 3: '90.37%'}\n",
      "Val Loss: 2.182073, Val Acc: 86.15%, Val-Class-Acc: {0: '90.76%', 1: '75.41%', 2: '85.42%', 3: '93.85%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_39.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_46.pth\n",
      "Epoch 47/200, Train Loss: 2.006216, Train-Class-Acc: {0: '86.24%', 1: '80.84%', 2: '86.14%', 3: '89.45%'}\n",
      "Val Loss: 2.398864, Val Acc: 86.15%, Val-Class-Acc: {0: '90.22%', 1: '75.82%', 2: '85.42%', 3: '93.85%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_26.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_47.pth\n",
      "Epoch 48/200, Train Loss: 2.200016, Train-Class-Acc: {0: '86.51%', 1: '80.43%', 2: '85.96%', 3: '89.86%'}\n",
      "Val Loss: 2.428687, Val Acc: 85.54%, Val-Class-Acc: {0: '89.13%', 1: '82.38%', 2: '82.64%', 3: '87.70%'}, LR: 0.001000\n",
      "Epoch 49/200, Train Loss: 2.028754, Train-Class-Acc: {0: '85.29%', 1: '79.92%', 2: '87.00%', 3: '88.42%'}\n",
      "Val Loss: 2.619475, Val Acc: 85.66%, Val-Class-Acc: {0: '94.02%', 1: '76.23%', 2: '85.42%', 3: '88.93%'}, LR: 0.001000\n",
      "Epoch 50/200, Train Loss: 2.037412, Train-Class-Acc: {0: '86.65%', 1: '80.33%', 2: '86.48%', 3: '89.45%'}\n",
      "Val Loss: 2.288972, Val Acc: 84.68%, Val-Class-Acc: {0: '83.15%', 1: '77.46%', 2: '84.72%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 51/200, Train Loss: 1.972792, Train-Class-Acc: {0: '86.78%', 1: '82.68%', 2: '87.87%', 3: '89.34%'}\n",
      "Val Loss: 2.224729, Val Acc: 86.03%, Val-Class-Acc: {0: '91.85%', 1: '72.54%', 2: '81.25%', 3: '97.95%'}, LR: 0.001000\n",
      "Epoch 52/200, Train Loss: 1.939430, Train-Class-Acc: {0: '85.69%', 1: '80.84%', 2: '86.31%', 3: '89.75%'}\n",
      "Val Loss: 2.609268, Val Acc: 85.78%, Val-Class-Acc: {0: '93.48%', 1: '77.46%', 2: '83.33%', 3: '89.75%'}, LR: 0.001000\n",
      "Epoch 53/200, Train Loss: 2.055363, Train-Class-Acc: {0: '86.51%', 1: '80.94%', 2: '87.00%', 3: '89.34%'}\n",
      "Val Loss: 2.297493, Val Acc: 86.64%, Val-Class-Acc: {0: '91.30%', 1: '75.00%', 2: '84.03%', 3: '96.31%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_36.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_53.pth\n",
      "Epoch 54/200, Train Loss: 1.985254, Train-Class-Acc: {0: '86.92%', 1: '82.58%', 2: '87.52%', 3: '90.16%'}\n",
      "Val Loss: 2.122310, Val Acc: 85.29%, Val-Class-Acc: {0: '89.67%', 1: '71.31%', 2: '85.42%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 55/200, Train Loss: 2.013678, Train-Class-Acc: {0: '87.19%', 1: '80.84%', 2: '87.35%', 3: '90.16%'}\n",
      "Val Loss: 2.281786, Val Acc: 86.52%, Val-Class-Acc: {0: '82.61%', 1: '81.15%', 2: '82.64%', 3: '97.13%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_38.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_55.pth\n",
      "Epoch 56/200, Train Loss: 1.835482, Train-Class-Acc: {0: '85.97%', 1: '82.07%', 2: '87.69%', 3: '89.75%'}\n",
      "Val Loss: 2.190802, Val Acc: 85.91%, Val-Class-Acc: {0: '90.76%', 1: '75.82%', 2: '78.47%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 57/200, Train Loss: 2.113619, Train-Class-Acc: {0: '86.51%', 1: '82.58%', 2: '87.69%', 3: '88.73%'}\n",
      "Val Loss: 2.435656, Val Acc: 85.05%, Val-Class-Acc: {0: '94.02%', 1: '68.03%', 2: '83.33%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 58/200, Train Loss: 1.609839, Train-Class-Acc: {0: '89.37%', 1: '81.97%', 2: '87.87%', 3: '92.11%'}\n",
      "Val Loss: 2.013492, Val Acc: 86.27%, Val-Class-Acc: {0: '86.96%', 1: '81.97%', 2: '84.03%', 3: '91.39%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_46.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_58.pth\n",
      "Epoch 59/200, Train Loss: 1.677127, Train-Class-Acc: {0: '84.60%', 1: '81.66%', 2: '89.43%', 3: '90.27%'}\n",
      "Val Loss: 2.185891, Val Acc: 85.17%, Val-Class-Acc: {0: '93.48%', 1: '73.77%', 2: '85.42%', 3: '90.16%'}, LR: 0.001000\n",
      "Epoch 60/200, Train Loss: 1.682413, Train-Class-Acc: {0: '90.46%', 1: '83.30%', 2: '87.87%', 3: '91.09%'}\n",
      "Val Loss: 2.021093, Val Acc: 86.40%, Val-Class-Acc: {0: '89.13%', 1: '80.33%', 2: '83.33%', 3: '92.21%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_47.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_60.pth\n",
      "Epoch 61/200, Train Loss: 1.624422, Train-Class-Acc: {0: '89.92%', 1: '82.68%', 2: '89.95%', 3: '91.09%'}\n",
      "Val Loss: 2.852414, Val Acc: 85.78%, Val-Class-Acc: {0: '82.61%', 1: '85.66%', 2: '77.78%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 62/200, Train Loss: 1.754959, Train-Class-Acc: {0: '88.28%', 1: '84.02%', 2: '89.08%', 3: '91.09%'}\n",
      "Val Loss: 2.125139, Val Acc: 87.01%, Val-Class-Acc: {0: '92.39%', 1: '78.28%', 2: '85.42%', 3: '92.62%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_42.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_62.pth\n",
      "Epoch 63/200, Train Loss: 1.535729, Train-Class-Acc: {0: '88.28%', 1: '82.58%', 2: '89.60%', 3: '90.78%'}\n",
      "Val Loss: 2.096243, Val Acc: 87.13%, Val-Class-Acc: {0: '90.22%', 1: '81.15%', 2: '84.72%', 3: '92.21%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_58.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_63.pth\n",
      "Epoch 64/200, Train Loss: 1.457337, Train-Class-Acc: {0: '88.96%', 1: '85.04%', 2: '89.08%', 3: '91.70%'}\n",
      "Val Loss: 2.303931, Val Acc: 86.76%, Val-Class-Acc: {0: '88.04%', 1: '78.69%', 2: '84.03%', 3: '95.49%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_60.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_64.pth\n",
      "Epoch 65/200, Train Loss: 1.584602, Train-Class-Acc: {0: '88.83%', 1: '84.22%', 2: '89.08%', 3: '90.68%'}\n",
      "Val Loss: 2.181992, Val Acc: 85.29%, Val-Class-Acc: {0: '85.87%', 1: '78.28%', 2: '80.56%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 66/200, Train Loss: 1.669162, Train-Class-Acc: {0: '90.19%', 1: '84.63%', 2: '87.52%', 3: '90.68%'}\n",
      "Val Loss: 2.177840, Val Acc: 85.91%, Val-Class-Acc: {0: '88.04%', 1: '78.28%', 2: '84.03%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 67/200, Train Loss: 1.323925, Train-Class-Acc: {0: '89.51%', 1: '84.12%', 2: '90.99%', 3: '92.42%'}\n",
      "Val Loss: 1.922006, Val Acc: 86.64%, Val-Class-Acc: {0: '87.50%', 1: '80.74%', 2: '87.50%', 3: '91.39%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_55.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_67.pth\n",
      "Epoch 68/200, Train Loss: 1.515544, Train-Class-Acc: {0: '88.15%', 1: '83.81%', 2: '88.39%', 3: '92.11%'}\n",
      "Val Loss: 2.171187, Val Acc: 86.03%, Val-Class-Acc: {0: '90.76%', 1: '75.82%', 2: '79.17%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 69/200, Train Loss: 1.380943, Train-Class-Acc: {0: '89.78%', 1: '85.96%', 2: '88.39%', 3: '92.21%'}\n",
      "Val Loss: 2.156918, Val Acc: 87.01%, Val-Class-Acc: {0: '91.85%', 1: '80.33%', 2: '83.33%', 3: '92.21%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_53.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_69.pth\n",
      "Epoch 70/200, Train Loss: 1.422227, Train-Class-Acc: {0: '87.60%', 1: '83.71%', 2: '89.95%', 3: '91.60%'}\n",
      "Val Loss: 1.914232, Val Acc: 88.11%, Val-Class-Acc: {0: '85.87%', 1: '84.43%', 2: '86.81%', 3: '94.26%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_67.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_70.pth\n",
      "Epoch 71/200, Train Loss: 1.619088, Train-Class-Acc: {0: '88.83%', 1: '84.63%', 2: '88.04%', 3: '91.09%'}\n",
      "Val Loss: 2.145569, Val Acc: 86.03%, Val-Class-Acc: {0: '88.04%', 1: '78.69%', 2: '79.86%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 72/200, Train Loss: 1.171234, Train-Class-Acc: {0: '88.69%', 1: '85.14%', 2: '89.60%', 3: '92.21%'}\n",
      "Val Loss: 1.868927, Val Acc: 86.27%, Val-Class-Acc: {0: '89.13%', 1: '76.64%', 2: '84.03%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 73/200, Train Loss: 1.333781, Train-Class-Acc: {0: '90.05%', 1: '85.45%', 2: '91.16%', 3: '92.42%'}\n",
      "Val Loss: 2.327987, Val Acc: 86.15%, Val-Class-Acc: {0: '86.41%', 1: '79.92%', 2: '83.33%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 74/200, Train Loss: 1.514394, Train-Class-Acc: {0: '89.92%', 1: '85.14%', 2: '89.77%', 3: '91.09%'}\n",
      "Val Loss: 2.120083, Val Acc: 86.03%, Val-Class-Acc: {0: '83.15%', 1: '81.97%', 2: '82.64%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 75/200, Train Loss: 1.332051, Train-Class-Acc: {0: '89.51%', 1: '84.53%', 2: '90.64%', 3: '93.65%'}\n",
      "Val Loss: 1.967157, Val Acc: 87.13%, Val-Class-Acc: {0: '87.50%', 1: '80.74%', 2: '84.72%', 3: '94.67%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_64.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_75.pth\n",
      "Epoch 76/200, Train Loss: 1.207291, Train-Class-Acc: {0: '90.60%', 1: '85.96%', 2: '89.43%', 3: '91.60%'}\n",
      "Val Loss: 2.102077, Val Acc: 86.15%, Val-Class-Acc: {0: '94.02%', 1: '73.36%', 2: '84.03%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 77/200, Train Loss: 1.162298, Train-Class-Acc: {0: '91.55%', 1: '87.40%', 2: '91.16%', 3: '92.73%'}\n",
      "Val Loss: 2.121795, Val Acc: 87.01%, Val-Class-Acc: {0: '88.04%', 1: '81.97%', 2: '84.03%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 78/200, Train Loss: 1.294726, Train-Class-Acc: {0: '89.78%', 1: '85.96%', 2: '89.25%', 3: '92.93%'}\n",
      "Val Loss: 2.281698, Val Acc: 86.15%, Val-Class-Acc: {0: '83.70%', 1: '80.74%', 2: '83.33%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 79/200, Train Loss: 1.235546, Train-Class-Acc: {0: '90.74%', 1: '86.89%', 2: '90.12%', 3: '92.62%'}\n",
      "Val Loss: 2.698446, Val Acc: 84.80%, Val-Class-Acc: {0: '91.85%', 1: '79.10%', 2: '85.42%', 3: '84.84%'}, LR: 0.001000\n",
      "Epoch 80/200, Train Loss: 1.365400, Train-Class-Acc: {0: '89.78%', 1: '85.86%', 2: '92.03%', 3: '92.42%'}\n",
      "Val Loss: 3.401439, Val Acc: 83.21%, Val-Class-Acc: {0: '84.24%', 1: '67.62%', 2: '82.64%', 3: '98.36%'}, LR: 0.001000\n",
      "Epoch 81/200, Train Loss: 1.389130, Train-Class-Acc: {0: '90.60%', 1: '86.48%', 2: '90.64%', 3: '93.24%'}\n",
      "Val Loss: 2.382089, Val Acc: 84.93%, Val-Class-Acc: {0: '87.50%', 1: '74.18%', 2: '86.11%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 82/200, Train Loss: 1.225734, Train-Class-Acc: {0: '90.33%', 1: '86.48%', 2: '92.03%', 3: '92.01%'}\n",
      "Val Loss: 2.277206, Val Acc: 85.91%, Val-Class-Acc: {0: '89.13%', 1: '74.18%', 2: '84.03%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 83/200, Train Loss: 0.940910, Train-Class-Acc: {0: '91.69%', 1: '88.52%', 2: '92.20%', 3: '93.44%'}\n",
      "Val Loss: 2.231899, Val Acc: 85.78%, Val-Class-Acc: {0: '88.59%', 1: '81.15%', 2: '82.64%', 3: '90.16%'}, LR: 0.001000\n",
      "Epoch 84/200, Train Loss: 0.950820, Train-Class-Acc: {0: '91.28%', 1: '85.96%', 2: '90.81%', 3: '93.85%'}\n",
      "Val Loss: 2.452324, Val Acc: 85.66%, Val-Class-Acc: {0: '86.41%', 1: '84.02%', 2: '80.56%', 3: '89.75%'}, LR: 0.001000\n",
      "Epoch 85/200, Train Loss: 0.890807, Train-Class-Acc: {0: '90.87%', 1: '89.04%', 2: '92.20%', 3: '93.24%'}\n",
      "Val Loss: 2.112921, Val Acc: 87.50%, Val-Class-Acc: {0: '89.13%', 1: '81.15%', 2: '85.42%', 3: '93.85%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_62.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_85.pth\n",
      "Epoch 86/200, Train Loss: 1.068957, Train-Class-Acc: {0: '90.87%', 1: '86.27%', 2: '91.51%', 3: '93.44%'}\n",
      "Val Loss: 2.698904, Val Acc: 85.91%, Val-Class-Acc: {0: '84.78%', 1: '84.43%', 2: '88.19%', 3: '86.89%'}, LR: 0.001000\n",
      "Epoch 87/200, Train Loss: 1.188021, Train-Class-Acc: {0: '91.83%', 1: '88.01%', 2: '90.64%', 3: '92.11%'}\n",
      "Val Loss: 2.444401, Val Acc: 84.80%, Val-Class-Acc: {0: '90.76%', 1: '68.85%', 2: '84.72%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 88/200, Train Loss: 1.137762, Train-Class-Acc: {0: '91.42%', 1: '86.89%', 2: '91.16%', 3: '93.24%'}\n",
      "Val Loss: 2.260262, Val Acc: 86.89%, Val-Class-Acc: {0: '86.41%', 1: '79.92%', 2: '84.72%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 89/200, Train Loss: 0.907763, Train-Class-Acc: {0: '91.69%', 1: '87.09%', 2: '92.03%', 3: '92.83%'}\n",
      "Val Loss: 2.239556, Val Acc: 85.17%, Val-Class-Acc: {0: '86.41%', 1: '77.46%', 2: '86.81%', 3: '90.98%'}, LR: 0.001000\n",
      "Epoch 90/200, Train Loss: 0.967132, Train-Class-Acc: {0: '92.37%', 1: '89.86%', 2: '92.55%', 3: '93.03%'}\n",
      "Val Loss: 2.292446, Val Acc: 87.50%, Val-Class-Acc: {0: '86.96%', 1: '79.51%', 2: '86.11%', 3: '96.72%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_69.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_90.pth\n",
      "Epoch 91/200, Train Loss: 0.836144, Train-Class-Acc: {0: '91.01%', 1: '88.22%', 2: '92.55%', 3: '93.95%'}\n",
      "Val Loss: 2.576162, Val Acc: 85.42%, Val-Class-Acc: {0: '89.67%', 1: '78.69%', 2: '79.17%', 3: '92.62%'}, LR: 0.001000\n",
      "Epoch 92/200, Train Loss: 1.118484, Train-Class-Acc: {0: '92.78%', 1: '89.34%', 2: '90.29%', 3: '94.26%'}\n",
      "Val Loss: 2.363850, Val Acc: 84.68%, Val-Class-Acc: {0: '89.13%', 1: '78.28%', 2: '79.17%', 3: '90.98%'}, LR: 0.001000\n",
      "Epoch 93/200, Train Loss: 1.036912, Train-Class-Acc: {0: '91.96%', 1: '87.30%', 2: '92.37%', 3: '93.03%'}\n",
      "Val Loss: 2.281546, Val Acc: 85.42%, Val-Class-Acc: {0: '88.04%', 1: '81.97%', 2: '84.72%', 3: '87.30%'}, LR: 0.001000\n",
      "Epoch 94/200, Train Loss: 0.910386, Train-Class-Acc: {0: '91.42%', 1: '88.73%', 2: '92.37%', 3: '94.16%'}\n",
      "Val Loss: 2.429079, Val Acc: 84.68%, Val-Class-Acc: {0: '86.41%', 1: '76.23%', 2: '82.64%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 95/200, Train Loss: 0.714279, Train-Class-Acc: {0: '93.05%', 1: '89.65%', 2: '91.85%', 3: '94.36%'}\n",
      "Val Loss: 2.384505, Val Acc: 86.27%, Val-Class-Acc: {0: '86.41%', 1: '81.97%', 2: '83.33%', 3: '92.21%'}, LR: 0.001000\n",
      "Epoch 96/200, Train Loss: 0.995600, Train-Class-Acc: {0: '92.10%', 1: '87.30%', 2: '91.33%', 3: '93.75%'}\n",
      "Val Loss: 2.624744, Val Acc: 85.42%, Val-Class-Acc: {0: '89.13%', 1: '78.28%', 2: '80.56%', 3: '92.62%'}, LR: 0.001000\n",
      "Epoch 97/200, Train Loss: 0.818502, Train-Class-Acc: {0: '92.92%', 1: '90.37%', 2: '93.07%', 3: '93.95%'}\n",
      "Val Loss: 3.159935, Val Acc: 82.97%, Val-Class-Acc: {0: '85.33%', 1: '68.44%', 2: '79.86%', 3: '97.54%'}, LR: 0.001000\n",
      "Epoch 98/200, Train Loss: 1.041349, Train-Class-Acc: {0: '91.55%', 1: '88.22%', 2: '91.68%', 3: '94.06%'}\n",
      "Val Loss: 2.432442, Val Acc: 84.93%, Val-Class-Acc: {0: '83.70%', 1: '82.79%', 2: '85.42%', 3: '87.70%'}, LR: 0.001000\n",
      "Epoch 99/200, Train Loss: 0.816061, Train-Class-Acc: {0: '93.46%', 1: '89.14%', 2: '93.59%', 3: '94.57%'}\n",
      "Val Loss: 2.177634, Val Acc: 86.40%, Val-Class-Acc: {0: '85.87%', 1: '81.56%', 2: '82.64%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 100/200, Train Loss: 1.005390, Train-Class-Acc: {0: '91.55%', 1: '87.70%', 2: '92.37%', 3: '94.47%'}\n",
      "Val Loss: 2.758951, Val Acc: 85.54%, Val-Class-Acc: {0: '85.87%', 1: '76.23%', 2: '83.33%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 101/200, Train Loss: 1.055083, Train-Class-Acc: {0: '93.73%', 1: '90.37%', 2: '92.37%', 3: '94.06%'}\n",
      "Val Loss: 2.266406, Val Acc: 86.15%, Val-Class-Acc: {0: '86.41%', 1: '79.51%', 2: '81.25%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 102/200, Train Loss: 0.855312, Train-Class-Acc: {0: '93.60%', 1: '88.52%', 2: '92.89%', 3: '94.26%'}\n",
      "Val Loss: 2.315455, Val Acc: 86.15%, Val-Class-Acc: {0: '88.59%', 1: '78.69%', 2: '80.56%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 103/200, Train Loss: 0.834678, Train-Class-Acc: {0: '93.32%', 1: '90.68%', 2: '91.85%', 3: '95.29%'}\n",
      "Val Loss: 2.539140, Val Acc: 85.66%, Val-Class-Acc: {0: '86.96%', 1: '77.87%', 2: '82.64%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 104/200, Train Loss: 0.740479, Train-Class-Acc: {0: '93.73%', 1: '91.39%', 2: '92.72%', 3: '94.88%'}\n",
      "Val Loss: 2.477910, Val Acc: 85.91%, Val-Class-Acc: {0: '86.96%', 1: '76.64%', 2: '86.81%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 105/200, Train Loss: 0.698149, Train-Class-Acc: {0: '94.96%', 1: '91.39%', 2: '91.51%', 3: '95.08%'}\n",
      "Val Loss: 2.544096, Val Acc: 84.80%, Val-Class-Acc: {0: '83.15%', 1: '81.15%', 2: '83.33%', 3: '90.57%'}, LR: 0.001000\n",
      "Epoch 106/200, Train Loss: 0.787564, Train-Class-Acc: {0: '94.28%', 1: '90.06%', 2: '94.63%', 3: '94.98%'}\n",
      "Val Loss: 2.322234, Val Acc: 86.15%, Val-Class-Acc: {0: '85.33%', 1: '78.28%', 2: '86.11%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 107/200, Train Loss: 0.819530, Train-Class-Acc: {0: '93.60%', 1: '90.78%', 2: '93.41%', 3: '95.08%'}\n",
      "Val Loss: 3.060580, Val Acc: 83.95%, Val-Class-Acc: {0: '88.59%', 1: '67.21%', 2: '83.33%', 3: '97.54%'}, LR: 0.001000\n",
      "Epoch 108/200, Train Loss: 0.646071, Train-Class-Acc: {0: '94.01%', 1: '89.96%', 2: '93.24%', 3: '95.08%'}\n",
      "Val Loss: 2.652871, Val Acc: 85.54%, Val-Class-Acc: {0: '85.33%', 1: '83.61%', 2: '81.94%', 3: '89.75%'}, LR: 0.001000\n",
      "Epoch 109/200, Train Loss: 0.800207, Train-Class-Acc: {0: '92.64%', 1: '91.29%', 2: '92.72%', 3: '94.88%'}\n",
      "Val Loss: 2.691904, Val Acc: 85.05%, Val-Class-Acc: {0: '89.67%', 1: '72.95%', 2: '85.42%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 110/200, Train Loss: 0.784224, Train-Class-Acc: {0: '94.14%', 1: '90.78%', 2: '92.89%', 3: '94.88%'}\n",
      "Val Loss: 2.674102, Val Acc: 84.07%, Val-Class-Acc: {0: '86.41%', 1: '75.41%', 2: '84.03%', 3: '90.98%'}, LR: 0.001000\n",
      "Epoch 111/200, Train Loss: 0.684054, Train-Class-Acc: {0: '94.55%', 1: '91.39%', 2: '93.41%', 3: '95.39%'}\n",
      "Val Loss: 2.749973, Val Acc: 84.68%, Val-Class-Acc: {0: '84.24%', 1: '74.59%', 2: '84.03%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 112/200, Train Loss: 0.736274, Train-Class-Acc: {0: '95.50%', 1: '91.80%', 2: '94.80%', 3: '95.59%'}\n",
      "Val Loss: 2.939388, Val Acc: 84.31%, Val-Class-Acc: {0: '86.96%', 1: '82.79%', 2: '72.22%', 3: '90.98%'}, LR: 0.001000\n",
      "Epoch 113/200, Train Loss: 0.774738, Train-Class-Acc: {0: '94.28%', 1: '91.60%', 2: '91.85%', 3: '94.26%'}\n",
      "Val Loss: 4.753957, Val Acc: 82.11%, Val-Class-Acc: {0: '82.61%', 1: '86.07%', 2: '79.17%', 3: '79.51%'}, LR: 0.001000\n",
      "Epoch 114/200, Train Loss: 0.638421, Train-Class-Acc: {0: '94.82%', 1: '91.50%', 2: '92.72%', 3: '95.80%'}\n",
      "Val Loss: 2.487984, Val Acc: 85.17%, Val-Class-Acc: {0: '86.96%', 1: '77.46%', 2: '82.64%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 115/200, Train Loss: 0.684121, Train-Class-Acc: {0: '94.01%', 1: '90.78%', 2: '92.89%', 3: '94.98%'}\n",
      "Val Loss: 3.255254, Val Acc: 84.44%, Val-Class-Acc: {0: '85.33%', 1: '79.51%', 2: '80.56%', 3: '90.98%'}, LR: 0.001000\n",
      "Epoch 116/200, Train Loss: 0.645377, Train-Class-Acc: {0: '94.14%', 1: '90.98%', 2: '94.45%', 3: '94.57%'}\n",
      "Val Loss: 2.738960, Val Acc: 84.93%, Val-Class-Acc: {0: '85.33%', 1: '78.28%', 2: '77.08%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 117/200, Train Loss: 0.701212, Train-Class-Acc: {0: '93.87%', 1: '90.57%', 2: '92.37%', 3: '94.98%'}\n",
      "Val Loss: 4.078580, Val Acc: 82.60%, Val-Class-Acc: {0: '83.70%', 1: '83.61%', 2: '82.64%', 3: '80.74%'}, LR: 0.001000\n",
      "Epoch 118/200, Train Loss: 0.943375, Train-Class-Acc: {0: '93.05%', 1: '89.86%', 2: '92.55%', 3: '94.47%'}\n",
      "Val Loss: 2.430944, Val Acc: 85.54%, Val-Class-Acc: {0: '90.22%', 1: '75.00%', 2: '81.94%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 119/200, Train Loss: 0.842631, Train-Class-Acc: {0: '92.92%', 1: '91.80%', 2: '93.07%', 3: '95.18%'}\n",
      "Val Loss: 2.505802, Val Acc: 86.15%, Val-Class-Acc: {0: '83.15%', 1: '82.79%', 2: '81.94%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 120/200, Train Loss: 0.583661, Train-Class-Acc: {0: '94.82%', 1: '92.62%', 2: '93.59%', 3: '95.49%'}\n",
      "Val Loss: 2.543784, Val Acc: 84.93%, Val-Class-Acc: {0: '84.24%', 1: '76.64%', 2: '87.50%', 3: '92.21%'}, LR: 0.001000\n",
      "Epoch 121/200, Train Loss: 0.604225, Train-Class-Acc: {0: '95.10%', 1: '92.32%', 2: '93.59%', 3: '95.29%'}\n",
      "Val Loss: 2.713514, Val Acc: 85.29%, Val-Class-Acc: {0: '85.33%', 1: '76.23%', 2: '83.33%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 122/200, Train Loss: 0.626830, Train-Class-Acc: {0: '94.96%', 1: '92.11%', 2: '94.11%', 3: '96.31%'}\n",
      "Val Loss: 2.353236, Val Acc: 85.17%, Val-Class-Acc: {0: '85.87%', 1: '79.51%', 2: '80.56%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 123/200, Train Loss: 0.610488, Train-Class-Acc: {0: '94.14%', 1: '91.50%', 2: '93.59%', 3: '96.41%'}\n",
      "Val Loss: 2.426396, Val Acc: 84.93%, Val-Class-Acc: {0: '85.33%', 1: '73.77%', 2: '86.81%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 124/200, Train Loss: 0.549549, Train-Class-Acc: {0: '97.00%', 1: '92.73%', 2: '95.15%', 3: '95.08%'}\n",
      "Val Loss: 2.815515, Val Acc: 85.42%, Val-Class-Acc: {0: '86.41%', 1: '80.74%', 2: '82.64%', 3: '90.98%'}, LR: 0.001000\n",
      "Epoch 125/200, Train Loss: 0.594991, Train-Class-Acc: {0: '95.10%', 1: '92.73%', 2: '94.28%', 3: '95.29%'}\n",
      "Val Loss: 2.925355, Val Acc: 84.56%, Val-Class-Acc: {0: '89.67%', 1: '72.95%', 2: '79.17%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 126/200, Train Loss: 0.750034, Train-Class-Acc: {0: '96.46%', 1: '92.52%', 2: '93.93%', 3: '95.18%'}\n",
      "Val Loss: 3.752400, Val Acc: 82.72%, Val-Class-Acc: {0: '83.70%', 1: '82.38%', 2: '86.11%', 3: '80.33%'}, LR: 0.001000\n",
      "Epoch 127/200, Train Loss: 0.664808, Train-Class-Acc: {0: '95.23%', 1: '91.50%', 2: '93.59%', 3: '95.49%'}\n",
      "Val Loss: 3.439238, Val Acc: 82.97%, Val-Class-Acc: {0: '89.13%', 1: '62.30%', 2: '84.03%', 3: '98.36%'}, LR: 0.001000\n",
      "Epoch 128/200, Train Loss: 0.634958, Train-Class-Acc: {0: '95.23%', 1: '92.62%', 2: '93.24%', 3: '94.16%'}\n",
      "Val Loss: 2.943707, Val Acc: 84.93%, Val-Class-Acc: {0: '84.24%', 1: '80.33%', 2: '83.33%', 3: '90.98%'}, LR: 0.001000\n",
      "Epoch 129/200, Train Loss: 0.473564, Train-Class-Acc: {0: '95.10%', 1: '94.16%', 2: '94.63%', 3: '97.03%'}\n",
      "Val Loss: 2.631312, Val Acc: 85.29%, Val-Class-Acc: {0: '88.04%', 1: '79.51%', 2: '83.33%', 3: '90.16%'}, LR: 0.001000\n",
      "Epoch 130/200, Train Loss: 0.561540, Train-Class-Acc: {0: '94.14%', 1: '92.73%', 2: '95.15%', 3: '95.29%'}\n",
      "Val Loss: 2.687561, Val Acc: 84.80%, Val-Class-Acc: {0: '89.67%', 1: '73.36%', 2: '83.33%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 131/200, Train Loss: 0.437758, Train-Class-Acc: {0: '95.78%', 1: '92.11%', 2: '96.71%', 3: '97.13%'}\n",
      "Val Loss: 2.928964, Val Acc: 85.54%, Val-Class-Acc: {0: '85.87%', 1: '80.74%', 2: '80.56%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 132/200, Train Loss: 0.520062, Train-Class-Acc: {0: '95.91%', 1: '93.65%', 2: '94.97%', 3: '96.11%'}\n",
      "Val Loss: 3.625967, Val Acc: 82.48%, Val-Class-Acc: {0: '88.04%', 1: '63.11%', 2: '84.72%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 133/200, Train Loss: 0.434505, Train-Class-Acc: {0: '93.87%', 1: '93.34%', 2: '95.15%', 3: '96.93%'}\n",
      "Val Loss: 2.837179, Val Acc: 83.70%, Val-Class-Acc: {0: '83.70%', 1: '77.46%', 2: '77.08%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 134/200, Train Loss: 0.587060, Train-Class-Acc: {0: '95.78%', 1: '92.62%', 2: '93.07%', 3: '96.21%'}\n",
      "Val Loss: 3.087061, Val Acc: 85.05%, Val-Class-Acc: {0: '90.76%', 1: '71.72%', 2: '81.25%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 135/200, Train Loss: 0.741344, Train-Class-Acc: {0: '95.91%', 1: '91.80%', 2: '93.93%', 3: '96.72%'}\n",
      "Val Loss: 3.670950, Val Acc: 84.19%, Val-Class-Acc: {0: '86.96%', 1: '80.33%', 2: '81.25%', 3: '87.70%'}, LR: 0.001000\n",
      "Epoch 136/200, Train Loss: 0.534913, Train-Class-Acc: {0: '95.37%', 1: '93.75%', 2: '93.93%', 3: '95.49%'}\n",
      "Val Loss: 2.734012, Val Acc: 83.58%, Val-Class-Acc: {0: '84.24%', 1: '74.18%', 2: '83.33%', 3: '92.62%'}, LR: 0.001000\n",
      "Epoch 137/200, Train Loss: 0.456532, Train-Class-Acc: {0: '95.10%', 1: '93.14%', 2: '95.67%', 3: '96.93%'}\n",
      "Val Loss: 3.064242, Val Acc: 85.29%, Val-Class-Acc: {0: '86.41%', 1: '80.33%', 2: '77.08%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 138/200, Train Loss: 0.388240, Train-Class-Acc: {0: '96.05%', 1: '93.44%', 2: '95.15%', 3: '96.62%'}\n",
      "Val Loss: 3.085625, Val Acc: 85.17%, Val-Class-Acc: {0: '86.41%', 1: '78.69%', 2: '86.11%', 3: '90.16%'}, LR: 0.001000\n",
      "Epoch 139/200, Train Loss: 0.419069, Train-Class-Acc: {0: '95.50%', 1: '94.67%', 2: '95.49%', 3: '96.62%'}\n",
      "Val Loss: 2.966357, Val Acc: 84.93%, Val-Class-Acc: {0: '89.67%', 1: '74.18%', 2: '82.64%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 140/200, Train Loss: 0.509586, Train-Class-Acc: {0: '96.46%', 1: '94.77%', 2: '95.32%', 3: '96.82%'}\n",
      "Val Loss: 5.179531, Val Acc: 82.72%, Val-Class-Acc: {0: '85.87%', 1: '75.00%', 2: '74.31%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 141/200, Train Loss: 0.609877, Train-Class-Acc: {0: '95.91%', 1: '92.52%', 2: '94.63%', 3: '95.59%'}\n",
      "Val Loss: 2.910400, Val Acc: 85.05%, Val-Class-Acc: {0: '83.15%', 1: '80.33%', 2: '81.25%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 142/200, Train Loss: 0.590650, Train-Class-Acc: {0: '96.59%', 1: '93.24%', 2: '94.80%', 3: '96.41%'}\n",
      "Val Loss: 2.824106, Val Acc: 84.93%, Val-Class-Acc: {0: '88.04%', 1: '77.46%', 2: '78.47%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 143/200, Train Loss: 0.475718, Train-Class-Acc: {0: '95.50%', 1: '93.65%', 2: '95.32%', 3: '97.75%'}\n",
      "Val Loss: 2.643867, Val Acc: 85.78%, Val-Class-Acc: {0: '88.04%', 1: '80.74%', 2: '83.33%', 3: '90.57%'}, LR: 0.001000\n",
      "Epoch 144/200, Train Loss: 0.526717, Train-Class-Acc: {0: '96.05%', 1: '94.06%', 2: '94.45%', 3: '95.80%'}\n",
      "Val Loss: 3.482809, Val Acc: 83.46%, Val-Class-Acc: {0: '89.67%', 1: '66.80%', 2: '79.17%', 3: '97.95%'}, LR: 0.001000\n",
      "Epoch 145/200, Train Loss: 0.483301, Train-Class-Acc: {0: '96.46%', 1: '94.26%', 2: '95.15%', 3: '96.52%'}\n",
      "Val Loss: 3.127625, Val Acc: 84.80%, Val-Class-Acc: {0: '89.13%', 1: '74.59%', 2: '79.17%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 146/200, Train Loss: 0.537656, Train-Class-Acc: {0: '96.19%', 1: '93.95%', 2: '94.80%', 3: '96.41%'}\n",
      "Val Loss: 3.446122, Val Acc: 85.42%, Val-Class-Acc: {0: '87.50%', 1: '75.41%', 2: '81.25%', 3: '96.31%'}, LR: 0.001000\n",
      "Epoch 147/200, Train Loss: 0.444064, Train-Class-Acc: {0: '95.64%', 1: '94.67%', 2: '96.36%', 3: '96.82%'}\n",
      "Val Loss: 3.237614, Val Acc: 85.54%, Val-Class-Acc: {0: '86.41%', 1: '82.38%', 2: '82.64%', 3: '89.75%'}, LR: 0.001000\n",
      "Epoch 148/200, Train Loss: 0.573582, Train-Class-Acc: {0: '94.96%', 1: '92.42%', 2: '96.53%', 3: '96.82%'}\n",
      "Val Loss: 3.230387, Val Acc: 85.05%, Val-Class-Acc: {0: '85.87%', 1: '81.56%', 2: '79.17%', 3: '91.39%'}, LR: 0.001000\n",
      "Epoch 149/200, Train Loss: 0.595451, Train-Class-Acc: {0: '96.46%', 1: '94.57%', 2: '95.15%', 3: '96.52%'}\n",
      "Val Loss: 4.716870, Val Acc: 83.82%, Val-Class-Acc: {0: '83.70%', 1: '85.66%', 2: '74.31%', 3: '87.70%'}, LR: 0.001000\n",
      "Epoch 150/200, Train Loss: 0.506360, Train-Class-Acc: {0: '95.91%', 1: '93.55%', 2: '94.97%', 3: '96.62%'}\n",
      "Val Loss: 3.201366, Val Acc: 84.68%, Val-Class-Acc: {0: '85.33%', 1: '81.56%', 2: '79.17%', 3: '90.57%'}, LR: 0.001000\n",
      "Epoch 151/200, Train Loss: 0.556051, Train-Class-Acc: {0: '96.32%', 1: '94.06%', 2: '95.84%', 3: '96.82%'}\n",
      "Val Loss: 3.195783, Val Acc: 85.91%, Val-Class-Acc: {0: '89.13%', 1: '80.33%', 2: '84.72%', 3: '89.75%'}, LR: 0.001000\n",
      "Epoch 152/200, Train Loss: 0.406463, Train-Class-Acc: {0: '95.78%', 1: '94.88%', 2: '94.80%', 3: '97.34%'}\n",
      "Val Loss: 3.137441, Val Acc: 85.17%, Val-Class-Acc: {0: '83.70%', 1: '79.10%', 2: '81.94%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 153/200, Train Loss: 0.471732, Train-Class-Acc: {0: '96.73%', 1: '95.49%', 2: '96.01%', 3: '97.13%'}\n",
      "Val Loss: 3.160828, Val Acc: 84.68%, Val-Class-Acc: {0: '84.24%', 1: '77.05%', 2: '79.86%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 154/200, Train Loss: 0.389017, Train-Class-Acc: {0: '95.64%', 1: '94.06%', 2: '95.67%', 3: '97.34%'}\n",
      "Val Loss: 3.119565, Val Acc: 84.44%, Val-Class-Acc: {0: '80.98%', 1: '78.69%', 2: '83.33%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 155/200, Train Loss: 0.300510, Train-Class-Acc: {0: '97.41%', 1: '94.88%', 2: '95.32%', 3: '97.23%'}\n",
      "Val Loss: 3.155701, Val Acc: 85.05%, Val-Class-Acc: {0: '83.70%', 1: '82.38%', 2: '81.94%', 3: '90.57%'}, LR: 0.001000\n",
      "Epoch 156/200, Train Loss: 0.263125, Train-Class-Acc: {0: '97.00%', 1: '96.11%', 2: '96.88%', 3: '97.64%'}\n",
      "Val Loss: 3.394425, Val Acc: 85.91%, Val-Class-Acc: {0: '88.59%', 1: '79.10%', 2: '80.56%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 157/200, Train Loss: 0.292496, Train-Class-Acc: {0: '96.87%', 1: '95.80%', 2: '95.84%', 3: '97.44%'}\n",
      "Val Loss: 3.316698, Val Acc: 85.29%, Val-Class-Acc: {0: '88.04%', 1: '80.33%', 2: '75.00%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 158/200, Train Loss: 0.357023, Train-Class-Acc: {0: '97.28%', 1: '95.39%', 2: '95.84%', 3: '97.23%'}\n",
      "Val Loss: 3.148409, Val Acc: 85.91%, Val-Class-Acc: {0: '80.98%', 1: '84.02%', 2: '84.72%', 3: '92.21%'}, LR: 0.001000\n",
      "Epoch 159/200, Train Loss: 0.423392, Train-Class-Acc: {0: '97.00%', 1: '95.29%', 2: '95.49%', 3: '97.03%'}\n",
      "Val Loss: 3.276937, Val Acc: 84.80%, Val-Class-Acc: {0: '83.70%', 1: '81.56%', 2: '88.19%', 3: '86.89%'}, LR: 0.001000\n",
      "Epoch 160/200, Train Loss: 0.265476, Train-Class-Acc: {0: '97.14%', 1: '95.80%', 2: '97.05%', 3: '97.54%'}\n",
      "Val Loss: 3.071938, Val Acc: 84.80%, Val-Class-Acc: {0: '79.35%', 1: '79.92%', 2: '84.72%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 161/200, Train Loss: 0.288380, Train-Class-Acc: {0: '97.14%', 1: '95.80%', 2: '95.49%', 3: '97.44%'}\n",
      "Val Loss: 3.276882, Val Acc: 85.29%, Val-Class-Acc: {0: '86.41%', 1: '77.46%', 2: '81.25%', 3: '94.67%'}, LR: 0.001000\n",
      "Epoch 162/200, Train Loss: 0.294326, Train-Class-Acc: {0: '96.32%', 1: '95.70%', 2: '96.53%', 3: '97.64%'}\n",
      "Val Loss: 3.404553, Val Acc: 84.93%, Val-Class-Acc: {0: '84.24%', 1: '81.97%', 2: '80.56%', 3: '90.98%'}, LR: 0.001000\n",
      "Epoch 163/200, Train Loss: 0.264016, Train-Class-Acc: {0: '97.14%', 1: '94.77%', 2: '97.23%', 3: '97.85%'}\n",
      "Val Loss: 3.375651, Val Acc: 85.54%, Val-Class-Acc: {0: '83.70%', 1: '79.51%', 2: '84.72%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 164/200, Train Loss: 0.300891, Train-Class-Acc: {0: '97.00%', 1: '95.80%', 2: '96.53%', 3: '97.34%'}\n",
      "Val Loss: 3.500182, Val Acc: 84.56%, Val-Class-Acc: {0: '82.61%', 1: '79.10%', 2: '83.33%', 3: '92.21%'}, LR: 0.001000\n",
      "Epoch 165/200, Train Loss: 0.333206, Train-Class-Acc: {0: '96.87%', 1: '95.18%', 2: '96.71%', 3: '97.85%'}\n",
      "Val Loss: 3.871180, Val Acc: 84.56%, Val-Class-Acc: {0: '85.87%', 1: '81.15%', 2: '87.50%', 3: '85.25%'}, LR: 0.001000\n",
      "Epoch 166/200, Train Loss: 0.351753, Train-Class-Acc: {0: '98.09%', 1: '95.80%', 2: '96.36%', 3: '97.23%'}\n",
      "Val Loss: 3.488379, Val Acc: 83.70%, Val-Class-Acc: {0: '83.15%', 1: '81.97%', 2: '77.78%', 3: '89.34%'}, LR: 0.001000\n",
      "Epoch 167/200, Train Loss: 0.212503, Train-Class-Acc: {0: '97.68%', 1: '96.21%', 2: '96.53%', 3: '98.05%'}\n",
      "Val Loss: 3.380801, Val Acc: 85.17%, Val-Class-Acc: {0: '88.04%', 1: '75.41%', 2: '82.64%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 168/200, Train Loss: 0.188610, Train-Class-Acc: {0: '97.68%', 1: '96.00%', 2: '97.23%', 3: '98.87%'}\n",
      "Val Loss: 3.638842, Val Acc: 85.05%, Val-Class-Acc: {0: '83.70%', 1: '75.00%', 2: '86.11%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 169/200, Train Loss: 0.359235, Train-Class-Acc: {0: '97.28%', 1: '94.98%', 2: '96.36%', 3: '97.03%'}\n",
      "Val Loss: 3.329583, Val Acc: 84.68%, Val-Class-Acc: {0: '84.78%', 1: '78.69%', 2: '79.17%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 170/200, Train Loss: 0.216286, Train-Class-Acc: {0: '97.82%', 1: '97.03%', 2: '97.57%', 3: '97.75%'}\n",
      "Val Loss: 3.409040, Val Acc: 84.68%, Val-Class-Acc: {0: '85.87%', 1: '77.46%', 2: '81.25%', 3: '93.03%'}, LR: 0.001000\n",
      "Epoch 171/200, Train Loss: 0.240542, Train-Class-Acc: {0: '97.68%', 1: '96.21%', 2: '96.01%', 3: '97.64%'}\n",
      "Val Loss: 3.534084, Val Acc: 83.70%, Val-Class-Acc: {0: '78.80%', 1: '79.51%', 2: '82.64%', 3: '92.21%'}, LR: 0.001000\n",
      "Epoch 172/200, Train Loss: 0.555679, Train-Class-Acc: {0: '96.46%', 1: '93.95%', 2: '96.19%', 3: '97.54%'}\n",
      "Val Loss: 5.087365, Val Acc: 82.60%, Val-Class-Acc: {0: '79.89%', 1: '78.69%', 2: '84.03%', 3: '87.70%'}, LR: 0.001000\n",
      "Epoch 173/200, Train Loss: 0.311537, Train-Class-Acc: {0: '97.14%', 1: '96.11%', 2: '96.36%', 3: '96.93%'}\n",
      "Val Loss: 3.598010, Val Acc: 84.56%, Val-Class-Acc: {0: '86.96%', 1: '76.64%', 2: '81.94%', 3: '92.21%'}, LR: 0.001000\n",
      "Epoch 174/200, Train Loss: 0.231693, Train-Class-Acc: {0: '96.32%', 1: '96.11%', 2: '97.40%', 3: '98.05%'}\n",
      "Val Loss: 3.749698, Val Acc: 85.29%, Val-Class-Acc: {0: '81.52%', 1: '84.02%', 2: '81.94%', 3: '91.39%'}, LR: 0.001000\n",
      "Epoch 175/200, Train Loss: 0.251863, Train-Class-Acc: {0: '98.09%', 1: '97.13%', 2: '97.05%', 3: '97.95%'}\n",
      "Val Loss: 3.483954, Val Acc: 85.29%, Val-Class-Acc: {0: '87.50%', 1: '77.46%', 2: '84.03%', 3: '92.21%'}, LR: 0.001000\n",
      "Epoch 176/200, Train Loss: 0.246882, Train-Class-Acc: {0: '97.55%', 1: '96.52%', 2: '97.23%', 3: '98.05%'}\n",
      "Val Loss: 3.863448, Val Acc: 84.44%, Val-Class-Acc: {0: '85.87%', 1: '78.69%', 2: '80.56%', 3: '91.39%'}, LR: 0.001000\n",
      "Epoch 177/200, Train Loss: 0.251276, Train-Class-Acc: {0: '98.64%', 1: '95.80%', 2: '96.53%', 3: '98.46%'}\n",
      "Val Loss: 3.887230, Val Acc: 83.58%, Val-Class-Acc: {0: '83.70%', 1: '75.41%', 2: '79.17%', 3: '94.26%'}, LR: 0.001000\n",
      "Epoch 178/200, Train Loss: 0.159139, Train-Class-Acc: {0: '97.82%', 1: '97.54%', 2: '97.40%', 3: '97.95%'}\n",
      "Val Loss: 4.270509, Val Acc: 84.19%, Val-Class-Acc: {0: '86.41%', 1: '76.64%', 2: '81.94%', 3: '91.39%'}, LR: 0.001000\n",
      "Epoch 179/200, Train Loss: 0.193662, Train-Class-Acc: {0: '97.68%', 1: '97.23%', 2: '98.44%', 3: '98.26%'}\n",
      "Val Loss: 3.763647, Val Acc: 84.44%, Val-Class-Acc: {0: '88.04%', 1: '70.49%', 2: '82.64%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 180/200, Train Loss: 0.304832, Train-Class-Acc: {0: '97.96%', 1: '96.41%', 2: '98.79%', 3: '98.57%'}\n",
      "Val Loss: 3.776724, Val Acc: 85.54%, Val-Class-Acc: {0: '90.76%', 1: '78.28%', 2: '81.94%', 3: '90.98%'}, LR: 0.001000\n",
      "Epoch 181/200, Train Loss: 0.263790, Train-Class-Acc: {0: '97.41%', 1: '95.90%', 2: '97.05%', 3: '97.95%'}\n",
      "Val Loss: 3.622361, Val Acc: 84.68%, Val-Class-Acc: {0: '82.61%', 1: '80.33%', 2: '79.17%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 182/200, Train Loss: 0.191589, Train-Class-Acc: {0: '98.37%', 1: '97.03%', 2: '97.75%', 3: '98.46%'}\n",
      "Val Loss: 3.757592, Val Acc: 84.68%, Val-Class-Acc: {0: '88.04%', 1: '81.56%', 2: '76.39%', 3: '90.16%'}, LR: 0.001000\n",
      "Epoch 183/200, Train Loss: 0.283488, Train-Class-Acc: {0: '97.96%', 1: '95.39%', 2: '96.53%', 3: '98.05%'}\n",
      "Val Loss: 3.393656, Val Acc: 84.68%, Val-Class-Acc: {0: '85.33%', 1: '81.97%', 2: '80.56%', 3: '89.34%'}, LR: 0.001000\n",
      "Epoch 184/200, Train Loss: 0.280476, Train-Class-Acc: {0: '97.55%', 1: '96.41%', 2: '96.19%', 3: '97.85%'}\n",
      "Val Loss: 4.929446, Val Acc: 81.25%, Val-Class-Acc: {0: '81.52%', 1: '65.16%', 2: '81.94%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 185/200, Train Loss: 0.200998, Train-Class-Acc: {0: '98.09%', 1: '96.62%', 2: '96.71%', 3: '97.54%'}\n",
      "Val Loss: 3.937136, Val Acc: 84.44%, Val-Class-Acc: {0: '86.41%', 1: '78.28%', 2: '79.17%', 3: '92.21%'}, LR: 0.001000\n",
      "Epoch 186/200, Train Loss: 0.273304, Train-Class-Acc: {0: '97.28%', 1: '96.62%', 2: '97.40%', 3: '97.95%'}\n",
      "Val Loss: 3.664115, Val Acc: 85.29%, Val-Class-Acc: {0: '87.50%', 1: '74.59%', 2: '84.03%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 187/200, Train Loss: 0.453967, Train-Class-Acc: {0: '98.09%', 1: '95.90%', 2: '97.05%', 3: '97.54%'}\n",
      "Val Loss: 4.232362, Val Acc: 84.93%, Val-Class-Acc: {0: '85.33%', 1: '82.38%', 2: '84.72%', 3: '87.30%'}, LR: 0.001000\n",
      "Epoch 188/200, Train Loss: 0.582962, Train-Class-Acc: {0: '97.68%', 1: '95.49%', 2: '97.05%', 3: '96.52%'}\n",
      "Val Loss: 5.096687, Val Acc: 81.37%, Val-Class-Acc: {0: '85.33%', 1: '63.11%', 2: '81.25%', 3: '96.72%'}, LR: 0.001000\n",
      "Epoch 189/200, Train Loss: 0.344013, Train-Class-Acc: {0: '96.87%', 1: '95.80%', 2: '95.84%', 3: '98.05%'}\n",
      "Val Loss: 4.375478, Val Acc: 85.42%, Val-Class-Acc: {0: '88.59%', 1: '78.69%', 2: '78.47%', 3: '93.85%'}, LR: 0.001000\n",
      "Epoch 190/200, Train Loss: 0.307615, Train-Class-Acc: {0: '97.14%', 1: '95.70%', 2: '97.05%', 3: '97.75%'}\n",
      "Val Loss: 3.689837, Val Acc: 84.44%, Val-Class-Acc: {0: '86.96%', 1: '77.46%', 2: '80.56%', 3: '91.80%'}, LR: 0.001000\n",
      "Epoch 191/200, Train Loss: 0.258990, Train-Class-Acc: {0: '97.96%', 1: '96.52%', 2: '97.23%', 3: '98.05%'}\n",
      "Val Loss: 3.944966, Val Acc: 84.68%, Val-Class-Acc: {0: '85.33%', 1: '79.51%', 2: '82.64%', 3: '90.57%'}, LR: 0.001000\n",
      "Epoch 192/200, Train Loss: 0.788513, Train-Class-Acc: {0: '96.32%', 1: '93.85%', 2: '94.63%', 3: '97.44%'}\n",
      "Val Loss: 4.986976, Val Acc: 83.95%, Val-Class-Acc: {0: '85.87%', 1: '69.67%', 2: '86.81%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 193/200, Train Loss: 0.284953, Train-Class-Acc: {0: '96.59%', 1: '95.80%', 2: '96.88%', 3: '97.95%'}\n",
      "Val Loss: 3.898330, Val Acc: 85.54%, Val-Class-Acc: {0: '84.78%', 1: '76.64%', 2: '84.72%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 194/200, Train Loss: 0.230439, Train-Class-Acc: {0: '97.68%', 1: '97.23%', 2: '97.57%', 3: '98.67%'}\n",
      "Val Loss: 4.373098, Val Acc: 83.09%, Val-Class-Acc: {0: '83.15%', 1: '69.67%', 2: '84.03%', 3: '95.90%'}, LR: 0.001000\n",
      "Epoch 195/200, Train Loss: 0.295412, Train-Class-Acc: {0: '98.64%', 1: '96.21%', 2: '97.57%', 3: '97.64%'}\n",
      "Val Loss: 3.614897, Val Acc: 86.27%, Val-Class-Acc: {0: '86.41%', 1: '80.74%', 2: '79.86%', 3: '95.49%'}, LR: 0.001000\n",
      "Epoch 196/200, Train Loss: 0.207733, Train-Class-Acc: {0: '97.96%', 1: '96.00%', 2: '97.92%', 3: '98.98%'}\n",
      "Val Loss: 3.561341, Val Acc: 84.80%, Val-Class-Acc: {0: '86.96%', 1: '75.41%', 2: '86.11%', 3: '91.80%'}, LR: 0.001000\n",
      "Epoch 197/200, Train Loss: 0.219296, Train-Class-Acc: {0: '97.41%', 1: '96.82%', 2: '97.75%', 3: '98.36%'}\n",
      "Val Loss: 3.781125, Val Acc: 84.56%, Val-Class-Acc: {0: '80.98%', 1: '77.05%', 2: '84.03%', 3: '95.08%'}, LR: 0.001000\n",
      "Epoch 198/200, Train Loss: 0.106677, Train-Class-Acc: {0: '97.96%', 1: '97.34%', 2: '98.27%', 3: '98.98%'}\n",
      "Val Loss: 3.696622, Val Acc: 84.56%, Val-Class-Acc: {0: '84.24%', 1: '77.05%', 2: '82.64%', 3: '93.44%'}, LR: 0.001000\n",
      "Epoch 199/200, Train Loss: 0.189986, Train-Class-Acc: {0: '98.37%', 1: '98.26%', 2: '97.92%', 3: '98.87%'}\n",
      "Val Loss: 4.156312, Val Acc: 83.95%, Val-Class-Acc: {0: '84.24%', 1: '81.97%', 2: '79.17%', 3: '88.52%'}, LR: 0.001000\n",
      "Epoch 200/200, Train Loss: 0.143091, Train-Class-Acc: {0: '98.91%', 1: '97.95%', 2: '98.44%', 3: '99.08%'}\n",
      "Val Loss: 4.018873, Val Acc: 85.54%, Val-Class-Acc: {0: '83.15%', 1: '81.15%', 2: '80.56%', 3: '94.67%'}, LR: 0.001000\n",
      "\n",
      "üèÜ Best model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_best.pth (Val Accuracy: 88.11%)\n",
      "\n",
      "üìå Final model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_final.pth\n",
      "\n",
      "üéØ Top 5 Best Models:\n",
      "Epoch 70, Train Loss: 1.422227, Train-Acc: {0: '87.60%', 1: '83.71%', 2: '89.95%', 3: '91.60%'},\n",
      "Val Loss: 1.914232, Val Acc: 88.11%, Val-Acc: {0: '85.87%', 1: '84.43%', 2: '86.81%', 3: '94.26%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_70.pth\n",
      "Epoch 90, Train Loss: 0.967132, Train-Acc: {0: '92.37%', 1: '89.86%', 2: '92.55%', 3: '93.03%'},\n",
      "Val Loss: 2.292446, Val Acc: 87.50%, Val-Acc: {0: '86.96%', 1: '79.51%', 2: '86.11%', 3: '96.72%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_90.pth\n",
      "Epoch 85, Train Loss: 0.890807, Train-Acc: {0: '90.87%', 1: '89.04%', 2: '92.20%', 3: '93.24%'},\n",
      "Val Loss: 2.112921, Val Acc: 87.50%, Val-Acc: {0: '89.13%', 1: '81.15%', 2: '85.42%', 3: '93.85%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_85.pth\n",
      "Epoch 75, Train Loss: 1.332051, Train-Acc: {0: '89.51%', 1: '84.53%', 2: '90.64%', 3: '93.65%'},\n",
      "Val Loss: 1.967157, Val Acc: 87.13%, Val-Acc: {0: '87.50%', 1: '80.74%', 2: '84.72%', 3: '94.67%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_75.pth\n",
      "Epoch 63, Train Loss: 1.535729, Train-Acc: {0: '88.28%', 1: '82.58%', 2: '89.60%', 3: '90.78%'},\n",
      "Val Loss: 2.096243, Val Acc: 87.13%, Val-Acc: {0: '90.22%', 1: '81.15%', 2: '84.72%', 3: '92.21%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/ResNet18_1D_LoRA_epoch_63.pth\n",
      "\n",
      "üß† Model Summary:\n",
      "Total Parameters: 3,889,796\n",
      "Model Size (float32): 14.84 MB\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "Number of LoRA adapters: 8\n",
      "Number of LoRA groups: 1\n",
      "Total Training Time: 321.20 seconds\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "---\n",
      "### Period 2 (alpha = 0.0, similarity_threshold = 0.99)\n",
      "+ ##### Total training time: 321.20 seconds\n",
      "+ ##### Model: ResNet18_1D_LoRA\n",
      "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2'*\n",
      "+ ##### Best Epoch: 70\n",
      "#### __Val Accuracy: 88.11%__\n",
      "#### __Val-Class-Acc: {0: '85.87%', 1: '84.43%', 2: '86.81%', 3: '94.26%'}__\n",
      "#### __Total Parameters: 3,889,796__\n",
      "#### __Model Size (float32): 14.84 MB__\n",
      "#### __Number of LoRA adapters: 8__\n",
      "#### __Number of LoRA groups: 1__\n",
      "\n",
      "Saved class features to: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/class_features.pkl\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå Period 2: DynEx-CLoRA Training (ECG)\n",
    "# ================================\n",
    "period = 2\n",
    "\n",
    "# ==== Paths ====\n",
    "stop_signal_file = os.path.join(BASE_DIR, \"stop_training.txt\")\n",
    "model_saving_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v10\", f\"Period_{period}\")\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# ==== Load Period 2 Data ====\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{period}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{period}.npy\"))\n",
    "X_val   = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val   = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "# ==== Device ====\n",
    "device = auto_select_cuda_device()\n",
    "\n",
    "# ==== ËºâÂÖ• Period 1 ÁöÑ class features ====\n",
    "prev_folder = os.path.join(BASE_DIR, \"ResNet18_Selection\", \"ResNet18_big_inplane_v1\")\n",
    "class_features_path = os.path.join(prev_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, \"rb\") as f:\n",
    "    class_features_dict = pickle.load(f)\n",
    "print(f\"‚úÖ Loaded class features from: {class_features_path}\")\n",
    "\n",
    "# ==== ËºâÂÖ• Period 1 È†êË®ìÁ∑¥Ê®°Âûã ====\n",
    "prev_model_path = os.path.join(prev_folder, \"ResNet18_big_inplane_1D_best.pth\")\n",
    "checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "\n",
    "input_channels = X_train.shape[2]\n",
    "output_size = len(np.unique(y_train))\n",
    "\n",
    "# ==== Âª∫Á´ã teacher modelÔºàoutput_size Ë¶ÅÊâ£ÊéâÊñ∞È°ûÂà•Êï∏Ôºâ\n",
    "teacher_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size - 2).to(device)\n",
    "teacher_model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "teacher_model.eval()\n",
    "\n",
    "# ==== Âª∫Á´ã student model ====\n",
    "model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size).to(device)\n",
    "\n",
    "# ==== Ê†πÊìö LoRA Êï∏ÈáèÂêåÊ≠• adapter ÁµêÊßã ====\n",
    "num_lora_groups = checkpoint.get(\"num_lora_groups\", 0)\n",
    "print(f\"üîÑ Number of LoRA groups: {num_lora_groups}\")\n",
    "related_labels = checkpoint.get(\"related_labels\", {\"base\": [0, 1]})\n",
    "for _ in range(num_lora_groups):\n",
    "    model.add_lora_adapter()\n",
    "\n",
    "# ==== Ë§áË£Ω shared Ê¨äÈáçÔºàÊéíÈô§ fc / lora_adapterÔºâ====\n",
    "model_dict = model.state_dict()\n",
    "prev_state_dict = checkpoint[\"model_state_dict\"]\n",
    "filtered_dict = {\n",
    "    k: v for k, v in prev_state_dict.items()\n",
    "    if k in model_dict and model_dict[k].shape == v.shape and k not in [\"fc.weight\", \"fc.bias\"]\n",
    "}\n",
    "model.load_state_dict(filtered_dict, strict=False)\n",
    "for k in model_dict:\n",
    "    if k not in filtered_dict:\n",
    "        print(f\"üîç Not loaded: {k}, shape={model_dict[k].shape}\")\n",
    "print(\"‚úÖ Loaded shared weights from Period 1 (excluding FC only)\")\n",
    "\n",
    "# ==== Ë®ìÁ∑¥ÂèÉÊï∏ ====\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "alpha = 0.0 # No distillation\n",
    "similarity_threshold = 0.99\n",
    "stable_classes = [0]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# ==== ÈñãÂßãË®ìÁ∑¥ ====\n",
    "train_with_dynex_clora_ecg(\n",
    "    model=model,\n",
    "    teacher_model=None,\n",
    "    output_size=output_size,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    alpha=alpha,\n",
    "    model_saving_folder=model_saving_folder,\n",
    "    model_name=\"ResNet18_1D_LoRA\",\n",
    "    stop_signal_file=stop_signal_file,\n",
    "    scheduler=scheduler,\n",
    "    period=period,\n",
    "    stable_classes=stable_classes,\n",
    "    similarity_threshold=similarity_threshold,\n",
    "    class_features_dict=class_features_dict,\n",
    "    related_labels=related_labels,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ==== Cleanup ====\n",
    "del X_train, y_train, X_val, y_val\n",
    "del model, teacher_model, checkpoint, model_dict, prev_state_dict, filtered_dict\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad6ab5d",
   "metadata": {},
   "source": [
    "### Period 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a1ba1e",
   "metadata": {},
   "source": [
    "#### v10 no distillation, all freeze, th=0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0624396a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Automatically selected GPU:\n",
      "    - CUDA Device ID : 1\n",
      "    - Memory Used    : 759 MiB\n",
      "    - Device Name    : NVIDIA RTX A6000\n",
      "‚úÖ Loaded class features from: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_2/class_features.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1777561/2845636887.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Number of LoRA groups: 1\n",
      "‚úÖ Added new LoRA adapters to 8 BasicBlocks\n",
      "üîç Not loaded: fc.weight, shape=torch.Size([6, 1024])\n",
      "üîç Not loaded: fc.bias, shape=torch.Size([6])\n",
      "‚úÖ Loaded shared weights from Period 2 (excluding FC only)\n",
      "\n",
      "üöÄ 'train_with_dynex_clora_ecg' started for Period 3\n",
      "\n",
      "‚úÖ Removed existing folder: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3\n",
      "\n",
      "‚úÖ Data Overview:\n",
      "X_train: torch.Size([5120, 5000, 12]), y_train: torch.Size([5120])\n",
      "X_val: torch.Size([1281, 5000, 12]), y_val: torch.Size([1281])\n",
      "\n",
      "üîé Similarity Analysis:\n",
      "  Similarity threshold: 0.8500\n",
      "  Existing classes: [0, 1, 2, 3]\n",
      "  Current classes: [0, 1, 2, 3, 4, 5]\n",
      "  New classes: [4, 5]\n",
      "\n",
      "üìä Similarity Scores:\n",
      "  New Class 4:\n",
      "    - Existing Class 2: 0.8920\n",
      "    - Existing Class 0: 0.8823\n",
      "    - Existing Class 3: 0.8816\n",
      "    - Existing Class 1: 0.8797\n",
      "  New Class 5:\n",
      "    - Existing Class 1: 0.8797\n",
      "    - Existing Class 3: 0.8523\n",
      "    - Existing Class 2: 0.8347\n",
      "    - Existing Class 0: 0.7913\n",
      "\n",
      "  Average similarity: 0.8663, Std: 0.0298\n",
      "\n",
      "üß© Managing LoRA adapters for 2 new classes...\n",
      "üîÑ New Class 4 is similar to Class 2 (sim=0.8920) ‚Üí Added to adapter '0'\n",
      "üîÑ New Class 4 is similar to Class 0 (sim=0.8823) ‚Üí Added to adapter 'base'\n",
      "üîÑ New Class 5 is similar to Class 1 (sim=0.8797) ‚Üí Added to adapter 'base'\n",
      "\n",
      "üîç Checking stability of existing classes...\n",
      "  Class 0 similarity with itself: 0.8989\n",
      "  Class 1 similarity with itself: 0.8778\n",
      "  Class 2 similarity with itself: 0.8756\n",
      "  Class 3 similarity with itself: 0.8944\n",
      "\n",
      "üîí Default: Freezing ALL model parameters\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚ùå layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚ùå layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚ùå layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚ùå layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚ùå fc.weight                                          | shape=[6, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[6]\n",
      "\n",
      "üîì Unfreezing selected adapters (by group):\n",
      "  - Adapter Group #0 (all blocks) (classes: [2, 4])\n",
      "  ‚õî Base layers (classes: [0, 1, 3, 4, 5]) are frozen and will NOT be updated.\n",
      "\n",
      "üîç Frozen Parameters: (After Unfreeze specific adapter groups)\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå fc.weight                                          | shape=[6, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[6]\n",
      "\n",
      "üîç Frozen Parameters (After Unfreeze newly added adapter groups):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå fc.weight                                          | shape=[6, 1024]\n",
      "  ‚ùå fc.bias                                            | shape=[6]\n",
      "\n",
      "üîç Frozen Parameters (After unfreeze the final layer):\n",
      "  ‚ùå conv1.weight                                       | shape=[64, 12, 15]\n",
      "  ‚ùå bn1.weight                                         | shape=[64]\n",
      "  ‚ùå bn1.bias                                           | shape=[64]\n",
      "  ‚ùå layer1.0.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.0.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.0.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.0.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv1.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn1.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn1.bias                                  | shape=[64]\n",
      "  ‚ùå layer1.1.conv2.weight                              | shape=[64, 64, 3]\n",
      "  ‚ùå layer1.1.bn2.weight                                | shape=[64]\n",
      "  ‚ùå layer1.1.bn2.bias                                  | shape=[64]\n",
      "  ‚ùå layer2.0.conv1.weight                              | shape=[128, 64, 3]\n",
      "  ‚ùå layer2.0.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.0.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.0.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.0.weight                       | shape=[128, 64, 1]\n",
      "  ‚ùå layer2.0.downsample.1.weight                       | shape=[128]\n",
      "  ‚ùå layer2.0.downsample.1.bias                         | shape=[128]\n",
      "  ‚ùå layer2.1.conv1.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn1.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn1.bias                                  | shape=[128]\n",
      "  ‚ùå layer2.1.conv2.weight                              | shape=[128, 128, 3]\n",
      "  ‚ùå layer2.1.bn2.weight                                | shape=[128]\n",
      "  ‚ùå layer2.1.bn2.bias                                  | shape=[128]\n",
      "  ‚ùå layer3.0.conv1.weight                              | shape=[256, 128, 3]\n",
      "  ‚ùå layer3.0.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.0.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.0.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.0.weight                       | shape=[256, 128, 1]\n",
      "  ‚ùå layer3.0.downsample.1.weight                       | shape=[256]\n",
      "  ‚ùå layer3.0.downsample.1.bias                         | shape=[256]\n",
      "  ‚ùå layer3.1.conv1.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn1.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn1.bias                                  | shape=[256]\n",
      "  ‚ùå layer3.1.conv2.weight                              | shape=[256, 256, 3]\n",
      "  ‚ùå layer3.1.bn2.weight                                | shape=[256]\n",
      "  ‚ùå layer3.1.bn2.bias                                  | shape=[256]\n",
      "  ‚ùå layer4.0.conv1.weight                              | shape=[512, 256, 3]\n",
      "  ‚ùå layer4.0.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.0.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.0.bn2.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.0.weight                       | shape=[512, 256, 1]\n",
      "  ‚ùå layer4.0.downsample.1.weight                       | shape=[512]\n",
      "  ‚ùå layer4.0.downsample.1.bias                         | shape=[512]\n",
      "  ‚ùå layer4.1.conv1.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn1.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn1.bias                                  | shape=[512]\n",
      "  ‚ùå layer4.1.conv2.weight                              | shape=[512, 512, 3]\n",
      "  ‚ùå layer4.1.bn2.weight                                | shape=[512]\n",
      "  ‚ùå layer4.1.bn2.bias                                  | shape=[512]\n",
      "\n",
      "üìã Related Labels Summary:\n",
      "  - Base network: Classes [0, 1, 3, 4, 5]\n",
      "  - Adapter #0: Classes [2, 4]\n",
      "\n",
      "üîß Trainable Parameter Status:\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A                    | shape=[64, 4]\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B                    | shape=[4, 192]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A                    | shape=[128, 4]\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B                    | shape=[4, 384]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A                    | shape=[256, 4]\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B                    | shape=[4, 768]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A                    | shape=[512, 4]\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B                    | shape=[4, 1536]\n",
      "  ‚úÖ fc.weight                                          | shape=[6, 1024]\n",
      "  ‚úÖ fc.bias                                            | shape=[6]\n",
      "trainable_count: 18\n",
      "\n",
      "üìä Parameter Statistics:\n",
      "  - Total parameters: 3,891,846\n",
      "  - Trainable parameters: 36,870 (0.95%)\n",
      "  - Frozen parameters: 3,854,976 (99.05%)\n",
      "\n",
      "üîç Frozen Parameters:\n",
      "\n",
      "üß† Parameters currently controlled by the optimizer: (78)\n",
      "  ‚õî conv1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.weight (NOT included in optimizer)\n",
      "  ‚õî bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer1.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer1.0.conv2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer1.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer1.1.conv2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer1.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer1.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer2.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer2.0.conv2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer2.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer2.1.conv2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer2.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer2.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer3.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer3.0.conv2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer3.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer3.1.conv2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer3.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer3.1.lora_adapters.0.lora_B\n",
      "  ‚õî layer4.0.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer4.0.conv2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.bn2.bias (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.0.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.0.downsample.1.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.0.lora_adapters.0.lora_B\n",
      "  ‚õî layer4.1.conv1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn1.bias (NOT included in optimizer)\n",
      "  ‚õî layer4.1.conv2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn2.weight (NOT included in optimizer)\n",
      "  ‚õî layer4.1.bn2.bias (NOT included in optimizer)\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_A\n",
      "  ‚úÖ layer4.1.lora_adapters.0.lora_B\n",
      "  ‚úÖ fc.weight\n",
      "  ‚úÖ fc.bias\n",
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/200, Train Loss: 9.590875, Train-Class-Acc: {0: '42.51%', 1: '37.70%', 2: '26.69%', 3: '49.69%', 4: '4.43%', 5: '73.92%'}\n",
      "Val Loss: 3.035913, Val Acc: 69.56%, Val-Class-Acc: {0: '65.76%', 1: '69.25%', 2: '60.42%', 3: '77.05%', 4: '0.00%', 5: '78.74%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_1.pth\n",
      "Epoch 2/200, Train Loss: 4.560272, Train-Class-Acc: {0: '58.99%', 1: '51.61%', 2: '57.89%', 3: '70.59%', 4: '13.92%', 5: '78.25%'}\n",
      "Val Loss: 2.549965, Val Acc: 76.58%, Val-Class-Acc: {0: '83.70%', 1: '68.06%', 2: '85.42%', 3: '86.07%', 4: '5.00%', 5: '79.04%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_2.pth\n",
      "Epoch 3/200, Train Loss: 3.805337, Train-Class-Acc: {0: '63.08%', 1: '55.12%', 2: '66.03%', 3: '74.90%', 4: '18.99%', 5: '81.46%'}\n",
      "Val Loss: 2.326142, Val Acc: 77.60%, Val-Class-Acc: {0: '71.74%', 1: '73.73%', 2: '89.58%', 3: '90.57%', 4: '22.50%', 5: '76.65%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_3.pth\n",
      "Epoch 4/200, Train Loss: 3.187440, Train-Class-Acc: {0: '66.08%', 1: '59.91%', 2: '73.31%', 3: '76.33%', 4: '29.75%', 5: '82.21%'}\n",
      "Val Loss: 2.117982, Val Acc: 79.86%, Val-Class-Acc: {0: '80.98%', 1: '71.04%', 2: '79.86%', 3: '92.62%', 4: '40.00%', 5: '83.53%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_4.pth\n",
      "Epoch 5/200, Train Loss: 2.826668, Train-Class-Acc: {0: '67.98%', 1: '61.48%', 2: '73.66%', 3: '80.33%', 5: '83.48%', 4: '33.54%'}\n",
      "Val Loss: 2.723814, Val Acc: 79.55%, Val-Class-Acc: {0: '79.35%', 1: '77.31%', 2: '81.94%', 3: '79.51%', 4: '35.00%', 5: '86.23%'}, LR: 0.001000\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_5.pth\n",
      "Epoch 6/200, Train Loss: 2.580482, Train-Class-Acc: {0: '69.62%', 1: '63.80%', 2: '75.91%', 3: '81.97%', 4: '40.51%', 5: '85.43%'}\n",
      "Val Loss: 2.020943, Val Acc: 80.56%, Val-Class-Acc: {0: '79.35%', 1: '73.73%', 2: '84.03%', 3: '87.30%', 4: '57.50%', 5: '84.43%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_1.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_6.pth\n",
      "Epoch 7/200, Train Loss: 2.534732, Train-Class-Acc: {0: '71.80%', 1: '66.27%', 2: '79.20%', 3: '82.07%', 4: '46.84%', 5: '83.86%'}\n",
      "Val Loss: 2.284431, Val Acc: 80.87%, Val-Class-Acc: {0: '69.57%', 1: '80.60%', 2: '82.64%', 3: '87.30%', 4: '47.50%', 5: '85.93%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_2.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_7.pth\n",
      "Epoch 8/200, Train Loss: 2.171370, Train-Class-Acc: {0: '74.66%', 1: '68.06%', 2: '79.72%', 3: '83.20%', 4: '48.10%', 5: '84.75%'}\n",
      "Val Loss: 2.587859, Val Acc: 82.28%, Val-Class-Acc: {0: '82.07%', 1: '73.73%', 2: '81.94%', 3: '89.34%', 4: '52.50%', 5: '89.52%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_3.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_8.pth\n",
      "Epoch 9/200, Train Loss: 2.352575, Train-Class-Acc: {0: '74.11%', 1: '66.87%', 2: '80.07%', 3: '82.89%', 4: '50.63%', 5: '84.90%'}\n",
      "Val Loss: 1.723636, Val Acc: 81.73%, Val-Class-Acc: {0: '77.72%', 1: '79.10%', 2: '85.42%', 3: '90.16%', 4: '62.50%', 5: '81.14%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_5.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_9.pth\n",
      "Epoch 10/200, Train Loss: 2.049449, Train-Class-Acc: {0: '74.66%', 1: '67.02%', 2: '77.99%', 3: '82.58%', 4: '53.16%', 5: '85.05%'}\n",
      "Val Loss: 1.818911, Val Acc: 81.73%, Val-Class-Acc: {0: '75.54%', 1: '77.31%', 2: '79.86%', 3: '90.16%', 4: '60.00%', 5: '86.83%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_4.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_10.pth\n",
      "Epoch 11/200, Train Loss: 2.018799, Train-Class-Acc: {0: '72.62%', 1: '70.01%', 2: '78.68%', 3: '85.45%', 4: '51.27%', 5: '86.47%'}\n",
      "Val Loss: 1.786000, Val Acc: 82.28%, Val-Class-Acc: {0: '75.54%', 1: '80.90%', 2: '81.25%', 3: '88.52%', 4: '72.50%', 5: '84.43%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_6.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_11.pth\n",
      "Epoch 12/200, Train Loss: 1.815483, Train-Class-Acc: {0: '76.70%', 1: '70.01%', 2: '80.76%', 3: '85.76%', 4: '55.70%', 5: '87.52%'}\n",
      "Val Loss: 1.742421, Val Acc: 82.28%, Val-Class-Acc: {0: '82.61%', 1: '77.61%', 2: '86.11%', 3: '88.52%', 4: '72.50%', 5: '81.74%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_7.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_12.pth\n",
      "Epoch 13/200, Train Loss: 1.697552, Train-Class-Acc: {0: '75.61%', 1: '71.80%', 2: '82.15%', 3: '86.68%', 4: '55.06%', 5: '87.14%'}\n",
      "Val Loss: 1.881002, Val Acc: 82.83%, Val-Class-Acc: {0: '80.43%', 1: '77.61%', 2: '81.94%', 3: '89.34%', 4: '67.50%', 5: '86.83%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_9.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_13.pth\n",
      "Epoch 14/200, Train Loss: 1.669823, Train-Class-Acc: {0: '76.16%', 1: '71.95%', 2: '81.63%', 3: '86.27%', 4: '63.92%', 5: '87.82%'}\n",
      "Val Loss: 1.958441, Val Acc: 82.59%, Val-Class-Acc: {0: '82.07%', 1: '79.40%', 2: '88.19%', 3: '92.21%', 4: '67.50%', 5: '78.44%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_10.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_14.pth\n",
      "Epoch 15/200, Train Loss: 1.458485, Train-Class-Acc: {0: '76.98%', 1: '73.97%', 2: '83.71%', 3: '87.19%', 5: '86.10%', 4: '60.13%'}\n",
      "Val Loss: 1.762790, Val Acc: 82.75%, Val-Class-Acc: {0: '79.89%', 1: '79.10%', 2: '84.72%', 3: '88.52%', 4: '70.00%', 5: '84.43%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_8.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_15.pth\n",
      "Epoch 16/200, Train Loss: 1.529238, Train-Class-Acc: {0: '76.84%', 1: '73.45%', 2: '83.19%', 3: '86.58%', 4: '62.66%', 5: '87.22%'}\n",
      "Val Loss: 1.906408, Val Acc: 83.22%, Val-Class-Acc: {0: '77.72%', 1: '74.63%', 2: '88.19%', 3: '91.80%', 4: '77.50%', 5: '87.13%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_11.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_16.pth\n",
      "Epoch 17/200, Train Loss: 1.501537, Train-Class-Acc: {0: '75.20%', 1: '72.48%', 2: '83.71%', 3: '88.42%', 5: '88.94%', 4: '63.29%'}\n",
      "Val Loss: 1.695093, Val Acc: 81.89%, Val-Class-Acc: {0: '70.11%', 1: '81.79%', 2: '82.64%', 3: '92.21%', 4: '72.50%', 5: '81.74%'}, LR: 0.001000\n",
      "Epoch 18/200, Train Loss: 1.423184, Train-Class-Acc: {0: '78.61%', 1: '73.75%', 2: '83.54%', 3: '87.70%', 4: '61.39%', 5: '87.74%'}\n",
      "Val Loss: 1.688263, Val Acc: 81.26%, Val-Class-Acc: {0: '76.09%', 1: '76.12%', 2: '81.25%', 3: '91.80%', 4: '65.00%', 5: '83.53%'}, LR: 0.001000\n",
      "Epoch 19/200, Train Loss: 1.339366, Train-Class-Acc: {0: '81.88%', 1: '74.87%', 2: '83.19%', 3: '88.63%', 4: '68.99%', 5: '88.94%'}\n",
      "Val Loss: 2.614259, Val Acc: 81.34%, Val-Class-Acc: {0: '79.35%', 1: '72.24%', 2: '88.19%', 3: '80.33%', 4: '82.50%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 20/200, Train Loss: 1.604057, Train-Class-Acc: {0: '77.66%', 1: '74.50%', 2: '83.54%', 3: '88.52%', 4: '60.76%', 5: '87.67%'}\n",
      "Val Loss: 2.297128, Val Acc: 82.12%, Val-Class-Acc: {0: '82.61%', 1: '77.01%', 2: '75.69%', 3: '83.61%', 4: '70.00%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 21/200, Train Loss: 1.346142, Train-Class-Acc: {0: '81.34%', 1: '77.49%', 2: '85.44%', 3: '87.81%', 5: '88.49%', 4: '67.72%'}\n",
      "Val Loss: 1.989019, Val Acc: 83.06%, Val-Class-Acc: {0: '76.63%', 1: '79.70%', 2: '85.42%', 3: '85.25%', 4: '77.50%', 5: '88.02%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_12.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_21.pth\n",
      "Epoch 22/200, Train Loss: 1.223282, Train-Class-Acc: {0: '78.20%', 1: '74.57%', 2: '82.67%', 3: '87.40%', 4: '65.82%', 5: '89.99%'}\n",
      "Val Loss: 1.884718, Val Acc: 81.03%, Val-Class-Acc: {0: '70.65%', 1: '82.99%', 2: '77.78%', 3: '82.79%', 4: '70.00%', 5: '86.23%'}, LR: 0.001000\n",
      "Epoch 23/200, Train Loss: 1.090156, Train-Class-Acc: {0: '79.70%', 1: '76.44%', 2: '85.79%', 3: '88.83%', 4: '65.82%', 5: '89.46%'}\n",
      "Val Loss: 2.074357, Val Acc: 83.14%, Val-Class-Acc: {0: '88.04%', 1: '72.54%', 2: '82.64%', 3: '90.16%', 4: '75.00%', 5: '87.13%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_14.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_23.pth\n",
      "Epoch 24/200, Train Loss: 1.185533, Train-Class-Acc: {0: '80.38%', 1: '76.59%', 2: '84.40%', 3: '87.50%', 4: '69.62%', 5: '89.24%'}\n",
      "Val Loss: 1.804127, Val Acc: 82.28%, Val-Class-Acc: {0: '69.57%', 1: '81.49%', 2: '84.03%', 3: '86.07%', 4: '75.00%', 5: '87.43%'}, LR: 0.001000\n",
      "Epoch 25/200, Train Loss: 1.065363, Train-Class-Acc: {0: '81.47%', 1: '78.53%', 2: '87.00%', 3: '91.09%', 5: '89.99%', 4: '66.46%'}\n",
      "Val Loss: 2.360956, Val Acc: 82.44%, Val-Class-Acc: {0: '75.54%', 1: '74.33%', 2: '84.03%', 3: '87.30%', 4: '77.50%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 26/200, Train Loss: 0.944307, Train-Class-Acc: {0: '81.47%', 1: '77.41%', 2: '85.79%', 3: '89.55%', 4: '72.78%', 5: '91.03%'}\n",
      "Val Loss: 2.253131, Val Acc: 82.51%, Val-Class-Acc: {0: '76.09%', 1: '76.72%', 2: '85.42%', 3: '85.25%', 4: '77.50%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 27/200, Train Loss: 0.898554, Train-Class-Acc: {0: '81.61%', 1: '79.21%', 2: '88.21%', 3: '89.96%', 4: '70.25%', 5: '91.41%'}\n",
      "Val Loss: 2.055504, Val Acc: 83.37%, Val-Class-Acc: {0: '80.98%', 1: '75.22%', 2: '82.64%', 3: '89.75%', 4: '80.00%', 5: '88.92%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_15.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_27.pth\n",
      "Epoch 28/200, Train Loss: 0.940188, Train-Class-Acc: {0: '82.56%', 1: '78.61%', 2: '86.31%', 3: '90.68%', 4: '71.52%', 5: '90.43%'}\n",
      "Val Loss: 2.498896, Val Acc: 82.36%, Val-Class-Acc: {0: '82.07%', 1: '68.66%', 2: '87.50%', 3: '86.89%', 4: '72.50%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 29/200, Train Loss: 1.159021, Train-Class-Acc: {0: '79.97%', 1: '79.88%', 2: '85.62%', 3: '89.75%', 4: '67.72%', 5: '89.99%'}\n",
      "Val Loss: 1.897905, Val Acc: 82.51%, Val-Class-Acc: {0: '73.37%', 1: '82.99%', 2: '82.64%', 3: '90.98%', 4: '70.00%', 5: '82.34%'}, LR: 0.001000\n",
      "Epoch 30/200, Train Loss: 0.889030, Train-Class-Acc: {0: '83.79%', 1: '79.13%', 2: '87.35%', 3: '90.68%', 4: '76.58%', 5: '90.88%'}\n",
      "Val Loss: 1.909526, Val Acc: 82.51%, Val-Class-Acc: {0: '76.09%', 1: '70.15%', 2: '87.50%', 3: '91.80%', 4: '82.50%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 31/200, Train Loss: 1.174521, Train-Class-Acc: {0: '83.38%', 1: '78.61%', 2: '87.18%', 3: '91.09%', 4: '68.99%', 5: '89.46%'}\n",
      "Val Loss: 2.005757, Val Acc: 82.83%, Val-Class-Acc: {0: '71.20%', 1: '79.70%', 2: '83.33%', 3: '90.98%', 4: '80.00%', 5: '86.53%'}, LR: 0.001000\n",
      "Epoch 32/200, Train Loss: 0.909122, Train-Class-Acc: {0: '82.70%', 1: '80.85%', 2: '85.10%', 3: '92.42%', 4: '69.62%', 5: '91.26%'}\n",
      "Val Loss: 2.435151, Val Acc: 82.59%, Val-Class-Acc: {0: '88.04%', 1: '68.66%', 2: '82.64%', 3: '88.11%', 4: '75.00%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 33/200, Train Loss: 0.812783, Train-Class-Acc: {0: '84.60%', 1: '81.23%', 2: '89.95%', 3: '92.21%', 4: '78.48%', 5: '90.88%'}\n",
      "Val Loss: 1.955334, Val Acc: 80.95%, Val-Class-Acc: {0: '68.48%', 1: '78.81%', 2: '81.25%', 3: '87.70%', 4: '77.50%', 5: '85.33%'}, LR: 0.001000\n",
      "Epoch 34/200, Train Loss: 0.806599, Train-Class-Acc: {0: '84.74%', 1: '81.30%', 2: '87.18%', 3: '92.01%', 4: '73.42%', 5: '91.03%'}\n",
      "Val Loss: 1.933096, Val Acc: 82.20%, Val-Class-Acc: {0: '78.80%', 1: '75.52%', 2: '83.33%', 3: '88.52%', 4: '82.50%', 5: '85.63%'}, LR: 0.001000\n",
      "Epoch 35/200, Train Loss: 0.835273, Train-Class-Acc: {0: '85.42%', 1: '81.15%', 2: '89.25%', 3: '91.29%', 4: '74.05%', 5: '91.18%'}\n",
      "Val Loss: 2.120824, Val Acc: 82.67%, Val-Class-Acc: {0: '78.26%', 1: '74.33%', 2: '82.64%', 3: '86.48%', 4: '82.50%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 36/200, Train Loss: 0.848612, Train-Class-Acc: {0: '86.51%', 1: '81.68%', 2: '89.77%', 3: '92.32%', 4: '77.85%', 5: '90.88%'}\n",
      "Val Loss: 1.900201, Val Acc: 81.42%, Val-Class-Acc: {0: '73.91%', 1: '74.33%', 2: '85.42%', 3: '91.39%', 4: '70.00%', 5: '85.03%'}, LR: 0.001000\n",
      "Epoch 37/200, Train Loss: 0.782417, Train-Class-Acc: {0: '85.83%', 1: '81.00%', 2: '88.39%', 3: '91.70%', 4: '75.32%', 5: '90.51%'}\n",
      "Val Loss: 1.768098, Val Acc: 83.14%, Val-Class-Acc: {0: '81.52%', 1: '77.01%', 2: '81.94%', 3: '92.21%', 4: '75.00%', 5: '85.03%'}, LR: 0.001000\n",
      "üóë Removed: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_13.pth\n",
      "‚úÖ Saved model: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_37.pth\n",
      "Epoch 38/200, Train Loss: 0.867596, Train-Class-Acc: {0: '84.06%', 1: '80.40%', 2: '87.35%', 3: '92.52%', 5: '90.88%', 4: '74.68%'}\n",
      "Val Loss: 2.087409, Val Acc: 82.59%, Val-Class-Acc: {0: '72.28%', 1: '80.30%', 2: '79.86%', 3: '89.75%', 4: '72.50%', 5: '87.72%'}, LR: 0.001000\n",
      "Epoch 39/200, Train Loss: 0.735064, Train-Class-Acc: {0: '87.19%', 1: '82.87%', 2: '89.95%', 3: '92.32%', 4: '74.05%', 5: '92.53%'}\n",
      "Val Loss: 2.729532, Val Acc: 80.87%, Val-Class-Acc: {0: '67.39%', 1: '75.82%', 2: '77.78%', 3: '89.75%', 4: '67.50%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 40/200, Train Loss: 0.713953, Train-Class-Acc: {0: '86.24%', 1: '82.87%', 2: '87.87%', 3: '91.80%', 4: '75.95%', 5: '92.15%'}\n",
      "Val Loss: 1.908296, Val Acc: 81.81%, Val-Class-Acc: {0: '78.80%', 1: '77.01%', 2: '79.17%', 3: '87.70%', 4: '80.00%', 5: '85.33%'}, LR: 0.001000\n",
      "Epoch 41/200, Train Loss: 0.754880, Train-Class-Acc: {0: '84.88%', 1: '83.40%', 2: '90.29%', 3: '94.36%', 4: '75.95%', 5: '91.33%'}\n",
      "Val Loss: 1.756319, Val Acc: 81.11%, Val-Class-Acc: {0: '72.83%', 1: '77.31%', 2: '82.64%', 3: '92.62%', 4: '77.50%', 5: '80.84%'}, LR: 0.001000\n",
      "Epoch 42/200, Train Loss: 0.773128, Train-Class-Acc: {0: '87.74%', 1: '81.97%', 2: '88.39%', 3: '91.19%', 4: '78.48%', 5: '92.53%'}\n",
      "Val Loss: 2.432543, Val Acc: 81.81%, Val-Class-Acc: {0: '75.00%', 1: '77.31%', 2: '77.78%', 3: '83.20%', 4: '72.50%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 43/200, Train Loss: 0.649192, Train-Class-Acc: {0: '87.33%', 1: '84.89%', 2: '91.51%', 3: '94.57%', 4: '75.32%', 5: '93.50%'}\n",
      "Val Loss: 2.155208, Val Acc: 82.28%, Val-Class-Acc: {0: '76.09%', 1: '76.72%', 2: '81.94%', 3: '90.57%', 4: '80.00%', 5: '85.63%'}, LR: 0.001000\n",
      "Epoch 44/200, Train Loss: 0.598061, Train-Class-Acc: {0: '88.01%', 1: '85.12%', 2: '91.33%', 3: '94.88%', 4: '76.58%', 5: '92.15%'}\n",
      "Val Loss: 1.873687, Val Acc: 80.72%, Val-Class-Acc: {0: '73.37%', 1: '78.51%', 2: '77.78%', 3: '84.43%', 4: '77.50%', 5: '85.93%'}, LR: 0.001000\n",
      "Epoch 45/200, Train Loss: 0.627039, Train-Class-Acc: {0: '89.65%', 1: '83.69%', 2: '89.60%', 3: '91.70%', 4: '70.25%', 5: '92.38%'}\n",
      "Val Loss: 2.052036, Val Acc: 81.58%, Val-Class-Acc: {0: '77.72%', 1: '74.93%', 2: '84.72%', 3: '89.75%', 4: '80.00%', 5: '83.23%'}, LR: 0.001000\n",
      "Epoch 46/200, Train Loss: 0.651831, Train-Class-Acc: {0: '87.19%', 1: '84.74%', 2: '89.08%', 3: '92.73%', 4: '79.75%', 5: '92.45%'}\n",
      "Val Loss: 2.025025, Val Acc: 81.97%, Val-Class-Acc: {0: '77.17%', 1: '76.72%', 2: '80.56%', 3: '88.52%', 4: '77.50%', 5: '86.23%'}, LR: 0.001000\n",
      "Epoch 47/200, Train Loss: 0.591158, Train-Class-Acc: {0: '88.83%', 1: '84.59%', 2: '90.64%', 3: '94.47%', 4: '81.01%', 5: '93.35%'}\n",
      "Val Loss: 2.058798, Val Acc: 82.05%, Val-Class-Acc: {0: '79.35%', 1: '76.72%', 2: '79.17%', 3: '84.02%', 4: '77.50%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 48/200, Train Loss: 0.552379, Train-Class-Acc: {0: '88.42%', 1: '85.27%', 2: '91.16%', 3: '93.75%', 4: '79.11%', 5: '92.60%'}\n",
      "Val Loss: 1.695269, Val Acc: 81.26%, Val-Class-Acc: {0: '74.46%', 1: '74.63%', 2: '85.42%', 3: '87.30%', 4: '80.00%', 5: '85.63%'}, LR: 0.001000\n",
      "Epoch 49/200, Train Loss: 0.623503, Train-Class-Acc: {0: '89.24%', 1: '83.69%', 2: '90.12%', 3: '93.14%', 4: '79.75%', 5: '92.60%'}\n",
      "Val Loss: 2.069881, Val Acc: 81.11%, Val-Class-Acc: {0: '81.52%', 1: '71.94%', 2: '78.47%', 3: '89.75%', 4: '80.00%', 5: '85.03%'}, LR: 0.001000\n",
      "Epoch 50/200, Train Loss: 0.569831, Train-Class-Acc: {0: '88.28%', 1: '85.64%', 2: '91.33%', 3: '93.44%', 4: '78.48%', 5: '92.75%'}\n",
      "Val Loss: 2.319747, Val Acc: 78.77%, Val-Class-Acc: {0: '72.28%', 1: '75.22%', 2: '79.86%', 3: '93.85%', 4: '72.50%', 5: '75.15%'}, LR: 0.001000\n",
      "Epoch 51/200, Train Loss: 0.584767, Train-Class-Acc: {0: '88.42%', 1: '85.71%', 2: '91.68%', 3: '94.47%', 4: '79.75%', 5: '93.57%'}\n",
      "Val Loss: 2.657175, Val Acc: 81.11%, Val-Class-Acc: {0: '69.57%', 1: '76.72%', 2: '85.42%', 3: '87.70%', 4: '77.50%', 5: '85.63%'}, LR: 0.001000\n",
      "Epoch 52/200, Train Loss: 0.582303, Train-Class-Acc: {0: '87.74%', 1: '85.64%', 2: '91.16%', 3: '92.32%', 4: '75.95%', 5: '92.83%'}\n",
      "Val Loss: 1.980636, Val Acc: 81.58%, Val-Class-Acc: {0: '74.46%', 1: '73.13%', 2: '78.47%', 3: '90.98%', 4: '82.50%', 5: '88.32%'}, LR: 0.001000\n",
      "Epoch 53/200, Train Loss: 0.514658, Train-Class-Acc: {0: '90.74%', 1: '86.09%', 2: '91.16%', 3: '94.16%', 4: '79.75%', 5: '94.10%'}\n",
      "Val Loss: 1.962196, Val Acc: 82.05%, Val-Class-Acc: {0: '77.17%', 1: '76.42%', 2: '84.03%', 3: '87.30%', 4: '82.50%', 5: '85.63%'}, LR: 0.001000\n",
      "Epoch 54/200, Train Loss: 0.491779, Train-Class-Acc: {0: '90.46%', 1: '87.28%', 2: '91.51%', 3: '93.65%', 4: '81.01%', 5: '92.90%'}\n",
      "Val Loss: 2.337898, Val Acc: 81.50%, Val-Class-Acc: {0: '71.20%', 1: '78.21%', 2: '81.25%', 3: '86.89%', 4: '82.50%', 5: '86.53%'}, LR: 0.001000\n",
      "Epoch 55/200, Train Loss: 0.480935, Train-Class-Acc: {0: '89.24%', 1: '86.91%', 2: '93.24%', 3: '93.65%', 4: '79.75%', 5: '93.35%'}\n",
      "Val Loss: 2.423850, Val Acc: 82.20%, Val-Class-Acc: {0: '76.63%', 1: '77.31%', 2: '79.17%', 3: '88.11%', 4: '82.50%', 5: '87.13%'}, LR: 0.001000\n",
      "Epoch 56/200, Train Loss: 0.575144, Train-Class-Acc: {0: '91.55%', 1: '85.94%', 2: '90.64%', 3: '93.95%', 4: '81.65%', 5: '92.38%'}\n",
      "Val Loss: 2.505541, Val Acc: 81.81%, Val-Class-Acc: {0: '83.70%', 1: '67.46%', 2: '79.86%', 3: '89.75%', 4: '82.50%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 57/200, Train Loss: 0.459508, Train-Class-Acc: {0: '91.14%', 1: '87.73%', 2: '92.55%', 3: '95.90%', 4: '82.28%', 5: '93.80%'}\n",
      "Val Loss: 2.628744, Val Acc: 81.73%, Val-Class-Acc: {0: '78.80%', 1: '72.54%', 2: '81.94%', 3: '86.48%', 4: '85.00%', 5: '88.62%'}, LR: 0.001000\n",
      "Epoch 58/200, Train Loss: 0.515519, Train-Class-Acc: {0: '90.33%', 1: '87.51%', 2: '91.33%', 3: '94.16%', 5: '93.87%', 4: '81.01%'}\n",
      "Val Loss: 2.294333, Val Acc: 81.97%, Val-Class-Acc: {0: '72.83%', 1: '77.91%', 2: '80.56%', 3: '87.30%', 4: '85.00%', 5: '87.43%'}, LR: 0.001000\n",
      "Epoch 59/200, Train Loss: 0.446368, Train-Class-Acc: {0: '91.01%', 1: '88.33%', 2: '92.20%', 3: '94.16%', 4: '81.65%', 5: '94.32%'}\n",
      "Val Loss: 2.254671, Val Acc: 82.28%, Val-Class-Acc: {0: '70.65%', 1: '80.30%', 2: '78.47%', 3: '88.52%', 4: '82.50%', 5: '87.72%'}, LR: 0.001000\n",
      "Epoch 60/200, Train Loss: 0.390022, Train-Class-Acc: {0: '91.28%', 1: '88.18%', 2: '93.07%', 3: '95.49%', 4: '84.81%', 5: '94.39%'}\n",
      "Val Loss: 2.733082, Val Acc: 80.95%, Val-Class-Acc: {0: '84.78%', 1: '71.04%', 2: '77.08%', 3: '84.43%', 4: '82.50%', 5: '87.72%'}, LR: 0.001000\n",
      "Epoch 61/200, Train Loss: 0.306414, Train-Class-Acc: {0: '90.87%', 1: '89.75%', 2: '94.11%', 3: '95.59%', 4: '81.65%', 5: '94.99%'}\n",
      "Val Loss: 2.416984, Val Acc: 81.65%, Val-Class-Acc: {0: '77.17%', 1: '77.01%', 2: '79.86%', 3: '86.07%', 4: '80.00%', 5: '86.53%'}, LR: 0.001000\n",
      "Epoch 62/200, Train Loss: 0.853171, Train-Class-Acc: {0: '91.96%', 1: '87.21%', 2: '89.43%', 3: '92.73%', 5: '91.93%', 4: '79.11%'}\n",
      "Val Loss: 2.770409, Val Acc: 80.95%, Val-Class-Acc: {0: '78.26%', 1: '73.73%', 2: '72.22%', 3: '84.02%', 4: '82.50%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 63/200, Train Loss: 0.707606, Train-Class-Acc: {0: '90.19%', 1: '84.67%', 2: '89.25%', 3: '93.44%', 4: '78.48%', 5: '93.05%'}\n",
      "Val Loss: 2.828736, Val Acc: 80.56%, Val-Class-Acc: {0: '78.80%', 1: '67.76%', 2: '88.19%', 3: '86.07%', 4: '82.50%', 5: '86.83%'}, LR: 0.001000\n",
      "Epoch 64/200, Train Loss: 0.513654, Train-Class-Acc: {0: '92.51%', 1: '88.93%', 2: '94.45%', 3: '95.49%', 5: '93.65%', 4: '77.85%'}\n",
      "Val Loss: 2.258592, Val Acc: 81.65%, Val-Class-Acc: {0: '75.00%', 1: '75.52%', 2: '86.81%', 3: '89.75%', 4: '82.50%', 5: '83.23%'}, LR: 0.001000\n",
      "Epoch 65/200, Train Loss: 0.458489, Train-Class-Acc: {0: '90.87%', 1: '88.03%', 2: '93.07%', 3: '94.57%', 5: '94.47%', 4: '82.91%'}\n",
      "Val Loss: 2.397114, Val Acc: 82.28%, Val-Class-Acc: {0: '79.89%', 1: '70.75%', 2: '81.94%', 3: '90.16%', 4: '85.00%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 66/200, Train Loss: 0.399874, Train-Class-Acc: {0: '91.69%', 1: '90.05%', 2: '94.11%', 3: '95.80%', 4: '82.28%', 5: '94.39%'}\n",
      "Val Loss: 2.121839, Val Acc: 81.03%, Val-Class-Acc: {0: '74.46%', 1: '74.03%', 2: '79.86%', 3: '87.70%', 4: '82.50%', 5: '87.13%'}, LR: 0.001000\n",
      "Epoch 67/200, Train Loss: 0.407570, Train-Class-Acc: {0: '92.10%', 1: '89.08%', 2: '93.76%', 3: '95.59%', 4: '86.08%', 5: '93.95%'}\n",
      "Val Loss: 3.275975, Val Acc: 80.41%, Val-Class-Acc: {0: '80.43%', 1: '73.73%', 2: '75.00%', 3: '79.10%', 4: '82.50%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 68/200, Train Loss: 0.382061, Train-Class-Acc: {0: '92.10%', 1: '89.90%', 2: '94.11%', 3: '94.77%', 4: '84.18%', 5: '94.25%'}\n",
      "Val Loss: 2.385221, Val Acc: 81.34%, Val-Class-Acc: {0: '79.89%', 1: '72.84%', 2: '79.17%', 3: '88.93%', 4: '82.50%', 5: '85.93%'}, LR: 0.001000\n",
      "Epoch 69/200, Train Loss: 0.556964, Train-Class-Acc: {0: '90.74%', 1: '87.96%', 2: '90.99%', 3: '94.16%', 4: '85.44%', 5: '94.54%'}\n",
      "Val Loss: 2.313854, Val Acc: 80.17%, Val-Class-Acc: {0: '81.52%', 1: '66.27%', 2: '83.33%', 3: '90.57%', 4: '77.50%', 5: '84.73%'}, LR: 0.001000\n",
      "Epoch 70/200, Train Loss: 0.410331, Train-Class-Acc: {0: '92.64%', 1: '88.71%', 2: '93.93%', 3: '95.59%', 4: '84.18%', 5: '94.62%'}\n",
      "Val Loss: 2.522348, Val Acc: 81.26%, Val-Class-Acc: {0: '79.89%', 1: '74.63%', 2: '79.86%', 3: '87.30%', 4: '80.00%', 5: '85.03%'}, LR: 0.001000\n",
      "Epoch 71/200, Train Loss: 0.321139, Train-Class-Acc: {0: '90.19%', 1: '90.13%', 2: '93.59%', 3: '96.21%', 5: '94.92%', 4: '84.18%'}\n",
      "Val Loss: 2.182560, Val Acc: 80.09%, Val-Class-Acc: {0: '64.67%', 1: '76.42%', 2: '84.03%', 3: '90.57%', 4: '75.00%', 5: '83.53%'}, LR: 0.001000\n",
      "Epoch 72/200, Train Loss: 0.411942, Train-Class-Acc: {0: '91.96%', 1: '88.26%', 2: '93.76%', 3: '95.59%', 4: '84.81%', 5: '94.69%'}\n",
      "Val Loss: 2.362631, Val Acc: 79.55%, Val-Class-Acc: {0: '75.00%', 1: '74.93%', 2: '79.17%', 3: '84.43%', 4: '80.00%', 5: '83.23%'}, LR: 0.001000\n",
      "Epoch 73/200, Train Loss: 0.349220, Train-Class-Acc: {0: '92.10%', 1: '90.05%', 2: '94.11%', 3: '96.31%', 4: '81.65%', 5: '94.62%'}\n",
      "Val Loss: 2.585209, Val Acc: 80.72%, Val-Class-Acc: {0: '78.80%', 1: '73.13%', 2: '82.64%', 3: '87.70%', 4: '77.50%', 5: '83.83%'}, LR: 0.001000\n",
      "Epoch 74/200, Train Loss: 0.345500, Train-Class-Acc: {0: '92.23%', 1: '89.83%', 2: '92.72%', 3: '95.70%', 4: '85.44%', 5: '94.99%'}\n",
      "Val Loss: 2.450067, Val Acc: 81.42%, Val-Class-Acc: {0: '75.54%', 1: '72.54%', 2: '82.64%', 3: '90.16%', 4: '82.50%', 5: '86.53%'}, LR: 0.001000\n",
      "Epoch 75/200, Train Loss: 0.338496, Train-Class-Acc: {0: '93.32%', 1: '90.05%', 2: '94.28%', 3: '95.49%', 4: '85.44%', 5: '94.62%'}\n",
      "Val Loss: 3.234991, Val Acc: 81.11%, Val-Class-Acc: {0: '75.54%', 1: '72.54%', 2: '81.25%', 3: '86.89%', 4: '80.00%', 5: '88.62%'}, LR: 0.001000\n",
      "Epoch 76/200, Train Loss: 0.346528, Train-Class-Acc: {0: '91.83%', 1: '90.43%', 2: '94.80%', 3: '95.59%', 4: '82.28%', 5: '95.67%'}\n",
      "Val Loss: 2.630957, Val Acc: 81.50%, Val-Class-Acc: {0: '75.00%', 1: '75.82%', 2: '84.03%', 3: '90.16%', 4: '82.50%', 5: '83.23%'}, LR: 0.001000\n",
      "Epoch 77/200, Train Loss: 0.300557, Train-Class-Acc: {0: '93.73%', 1: '90.35%', 2: '94.45%', 3: '97.03%', 4: '83.54%', 5: '94.17%'}\n",
      "Val Loss: 2.517406, Val Acc: 80.48%, Val-Class-Acc: {0: '73.91%', 1: '73.43%', 2: '84.03%', 3: '84.43%', 4: '82.50%', 5: '86.53%'}, LR: 0.001000\n",
      "Epoch 78/200, Train Loss: 0.373365, Train-Class-Acc: {0: '92.64%', 1: '90.73%', 2: '93.59%', 3: '95.49%', 4: '84.81%', 5: '94.99%'}\n",
      "Val Loss: 2.232186, Val Acc: 81.50%, Val-Class-Acc: {0: '79.89%', 1: '71.94%', 2: '81.94%', 3: '86.89%', 4: '80.00%', 5: '88.02%'}, LR: 0.001000\n",
      "Epoch 79/200, Train Loss: 0.361779, Train-Class-Acc: {0: '92.92%', 1: '91.17%', 2: '93.76%', 3: '96.00%', 4: '86.08%', 5: '95.74%'}\n",
      "Val Loss: 2.427512, Val Acc: 82.05%, Val-Class-Acc: {0: '74.46%', 1: '78.21%', 2: '79.86%', 3: '86.07%', 4: '82.50%', 5: '88.02%'}, LR: 0.001000\n",
      "Epoch 80/200, Train Loss: 0.373902, Train-Class-Acc: {0: '93.32%', 1: '89.98%', 2: '93.93%', 3: '95.90%', 4: '86.08%', 5: '94.84%'}\n",
      "Val Loss: 2.958713, Val Acc: 80.48%, Val-Class-Acc: {0: '81.52%', 1: '65.07%', 2: '79.86%', 3: '86.07%', 4: '80.00%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 81/200, Train Loss: 0.328231, Train-Class-Acc: {0: '92.78%', 1: '90.35%', 2: '94.28%', 3: '95.80%', 4: '83.54%', 5: '95.22%'}\n",
      "Val Loss: 2.362156, Val Acc: 81.58%, Val-Class-Acc: {0: '77.72%', 1: '71.64%', 2: '84.72%', 3: '86.48%', 4: '82.50%', 5: '88.62%'}, LR: 0.001000\n",
      "Epoch 82/200, Train Loss: 0.252165, Train-Class-Acc: {0: '95.10%', 1: '91.92%', 2: '94.63%', 3: '95.80%', 4: '83.54%', 5: '95.29%'}\n",
      "Val Loss: 2.621364, Val Acc: 82.12%, Val-Class-Acc: {0: '78.80%', 1: '74.93%', 2: '80.56%', 3: '85.25%', 4: '82.50%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 83/200, Train Loss: 0.310749, Train-Class-Acc: {0: '93.46%', 1: '92.45%', 2: '95.49%', 3: '96.52%', 4: '88.61%', 5: '96.19%'}\n",
      "Val Loss: 1.817438, Val Acc: 80.33%, Val-Class-Acc: {0: '76.09%', 1: '73.73%', 2: '82.64%', 3: '86.07%', 4: '85.00%', 5: '83.53%'}, LR: 0.001000\n",
      "Epoch 84/200, Train Loss: 0.238566, Train-Class-Acc: {0: '94.01%', 1: '91.70%', 2: '94.80%', 3: '96.72%', 4: '87.97%', 5: '95.22%'}\n",
      "Val Loss: 2.805793, Val Acc: 81.73%, Val-Class-Acc: {0: '79.35%', 1: '72.84%', 2: '78.47%', 3: '85.25%', 4: '80.00%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 85/200, Train Loss: 0.339824, Train-Class-Acc: {0: '94.14%', 1: '91.70%', 2: '96.01%', 3: '97.03%', 4: '82.91%', 5: '95.74%'}\n",
      "Val Loss: 1.874604, Val Acc: 80.72%, Val-Class-Acc: {0: '74.46%', 1: '72.84%', 2: '78.47%', 3: '89.75%', 4: '82.50%', 5: '86.23%'}, LR: 0.001000\n",
      "Epoch 86/200, Train Loss: 0.239446, Train-Class-Acc: {0: '94.55%', 1: '93.49%', 2: '95.84%', 3: '96.41%', 4: '91.14%', 5: '96.19%'}\n",
      "Val Loss: 1.844596, Val Acc: 81.11%, Val-Class-Acc: {0: '72.28%', 1: '76.12%', 2: '83.33%', 3: '87.70%', 4: '77.50%', 5: '85.63%'}, LR: 0.001000\n",
      "Epoch 87/200, Train Loss: 0.370476, Train-Class-Acc: {0: '93.46%', 1: '90.58%', 2: '94.80%', 3: '96.31%', 4: '86.71%', 5: '94.84%'}\n",
      "Val Loss: 2.422395, Val Acc: 80.80%, Val-Class-Acc: {0: '77.72%', 1: '73.73%', 2: '80.56%', 3: '88.52%', 4: '82.50%', 5: '83.83%'}, LR: 0.001000\n",
      "Epoch 88/200, Train Loss: 0.338564, Train-Class-Acc: {0: '93.46%', 1: '90.95%', 2: '94.63%', 3: '95.70%', 4: '88.61%', 5: '95.74%'}\n",
      "Val Loss: 2.623540, Val Acc: 80.95%, Val-Class-Acc: {0: '80.43%', 1: '73.13%', 2: '81.25%', 3: '85.66%', 4: '77.50%', 5: '85.93%'}, LR: 0.001000\n",
      "Epoch 89/200, Train Loss: 0.307677, Train-Class-Acc: {0: '93.05%', 1: '91.32%', 2: '96.01%', 3: '96.31%', 4: '89.87%', 5: '95.37%'}\n",
      "Val Loss: 3.529612, Val Acc: 80.48%, Val-Class-Acc: {0: '60.33%', 1: '81.49%', 2: '77.08%', 3: '81.97%', 4: '80.00%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 90/200, Train Loss: 0.387755, Train-Class-Acc: {0: '92.78%', 1: '90.80%', 2: '94.97%', 3: '96.82%', 4: '82.91%', 5: '95.14%'}\n",
      "Val Loss: 3.376316, Val Acc: 81.58%, Val-Class-Acc: {0: '72.83%', 1: '73.43%', 2: '80.56%', 3: '85.25%', 4: '82.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 91/200, Train Loss: 0.308557, Train-Class-Acc: {0: '94.28%', 1: '91.77%', 2: '94.97%', 3: '96.00%', 4: '83.54%', 5: '95.07%'}\n",
      "Val Loss: 2.876233, Val Acc: 80.72%, Val-Class-Acc: {0: '81.52%', 1: '68.96%', 2: '81.94%', 3: '86.07%', 4: '85.00%', 5: '87.13%'}, LR: 0.001000\n",
      "Epoch 92/200, Train Loss: 0.298747, Train-Class-Acc: {0: '93.32%', 1: '91.62%', 2: '95.84%', 3: '96.82%', 4: '87.34%', 5: '95.14%'}\n",
      "Val Loss: 2.279746, Val Acc: 80.87%, Val-Class-Acc: {0: '76.09%', 1: '74.03%', 2: '79.17%', 3: '90.98%', 4: '80.00%', 5: '83.83%'}, LR: 0.001000\n",
      "Epoch 93/200, Train Loss: 0.297227, Train-Class-Acc: {0: '93.60%', 1: '91.70%', 2: '94.97%', 3: '96.31%', 4: '87.34%', 5: '95.22%'}\n",
      "Val Loss: 2.521049, Val Acc: 81.34%, Val-Class-Acc: {0: '73.37%', 1: '75.22%', 2: '81.94%', 3: '87.70%', 4: '85.00%', 5: '86.53%'}, LR: 0.001000\n",
      "Epoch 94/200, Train Loss: 0.224951, Train-Class-Acc: {0: '95.37%', 1: '93.34%', 2: '95.84%', 3: '97.54%', 4: '87.34%', 5: '96.64%'}\n",
      "Val Loss: 2.457720, Val Acc: 81.50%, Val-Class-Acc: {0: '80.98%', 1: '73.43%', 2: '80.56%', 3: '85.66%', 4: '82.50%', 5: '87.13%'}, LR: 0.001000\n",
      "Epoch 95/200, Train Loss: 0.381647, Train-Class-Acc: {0: '94.01%', 1: '92.00%', 2: '95.67%', 3: '95.29%', 4: '89.87%', 5: '94.69%'}\n",
      "Val Loss: 2.303624, Val Acc: 80.87%, Val-Class-Acc: {0: '73.37%', 1: '73.73%', 2: '82.64%', 3: '81.56%', 4: '85.00%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 96/200, Train Loss: 0.225062, Train-Class-Acc: {0: '95.23%', 1: '92.52%', 2: '94.63%', 3: '97.03%', 4: '88.61%', 5: '96.34%'}\n",
      "Val Loss: 2.819886, Val Acc: 81.58%, Val-Class-Acc: {0: '80.43%', 1: '69.25%', 2: '83.33%', 3: '87.70%', 4: '80.00%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 97/200, Train Loss: 0.230519, Train-Class-Acc: {0: '95.37%', 1: '92.67%', 2: '95.49%', 3: '96.72%', 4: '91.77%', 5: '96.19%'}\n",
      "Val Loss: 2.396188, Val Acc: 80.56%, Val-Class-Acc: {0: '72.83%', 1: '73.43%', 2: '81.94%', 3: '88.11%', 4: '80.00%', 5: '85.93%'}, LR: 0.001000\n",
      "Epoch 98/200, Train Loss: 0.309149, Train-Class-Acc: {0: '94.69%', 1: '92.74%', 2: '94.97%', 3: '95.90%', 4: '89.24%', 5: '95.44%'}\n",
      "Val Loss: 3.250693, Val Acc: 81.73%, Val-Class-Acc: {0: '77.72%', 1: '74.33%', 2: '81.94%', 3: '88.52%', 4: '77.50%', 5: '86.83%'}, LR: 0.001000\n",
      "Epoch 99/200, Train Loss: 0.372599, Train-Class-Acc: {0: '95.50%', 1: '93.27%', 2: '94.63%', 3: '97.03%', 4: '86.08%', 5: '96.19%'}\n",
      "Val Loss: 2.197701, Val Acc: 80.17%, Val-Class-Acc: {0: '69.02%', 1: '82.69%', 2: '81.25%', 3: '81.97%', 4: '82.50%', 5: '81.74%'}, LR: 0.001000\n",
      "Epoch 100/200, Train Loss: 0.338358, Train-Class-Acc: {0: '94.01%', 1: '91.55%', 2: '93.59%', 3: '96.31%', 4: '88.61%', 5: '95.07%'}\n",
      "Val Loss: 2.530013, Val Acc: 81.89%, Val-Class-Acc: {0: '74.46%', 1: '76.72%', 2: '78.47%', 3: '85.66%', 4: '82.50%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 101/200, Train Loss: 0.228454, Train-Class-Acc: {0: '95.91%', 1: '93.94%', 2: '94.80%', 3: '97.34%', 4: '87.97%', 5: '97.01%'}\n",
      "Val Loss: 2.550573, Val Acc: 80.87%, Val-Class-Acc: {0: '69.57%', 1: '80.00%', 2: '79.86%', 3: '79.10%', 4: '85.00%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 102/200, Train Loss: 0.259755, Train-Class-Acc: {0: '95.10%', 1: '93.04%', 2: '95.49%', 3: '96.93%', 5: '95.59%', 4: '87.97%'}\n",
      "Val Loss: 2.910340, Val Acc: 81.26%, Val-Class-Acc: {0: '65.76%', 1: '75.82%', 2: '84.03%', 3: '89.75%', 4: '77.50%', 5: '88.32%'}, LR: 0.001000\n",
      "Epoch 103/200, Train Loss: 0.217813, Train-Class-Acc: {0: '94.96%', 1: '93.19%', 2: '95.67%', 3: '97.23%', 4: '88.61%', 5: '97.01%'}\n",
      "Val Loss: 2.642168, Val Acc: 80.95%, Val-Class-Acc: {0: '74.46%', 1: '73.73%', 2: '80.56%', 3: '87.30%', 4: '80.00%', 5: '87.43%'}, LR: 0.001000\n",
      "Epoch 104/200, Train Loss: 0.230102, Train-Class-Acc: {0: '95.50%', 1: '93.42%', 2: '95.15%', 3: '97.34%', 4: '90.51%', 5: '96.19%'}\n",
      "Val Loss: 2.452082, Val Acc: 80.87%, Val-Class-Acc: {0: '75.54%', 1: '73.73%', 2: '80.56%', 3: '88.11%', 4: '85.00%', 5: '85.33%'}, LR: 0.001000\n",
      "Epoch 105/200, Train Loss: 0.227340, Train-Class-Acc: {0: '94.96%', 1: '93.04%', 2: '96.53%', 3: '96.93%', 4: '90.51%', 5: '96.86%'}\n",
      "Val Loss: 2.483693, Val Acc: 80.64%, Val-Class-Acc: {0: '78.26%', 1: '72.54%', 2: '79.86%', 3: '88.11%', 4: '80.00%', 5: '85.03%'}, LR: 0.001000\n",
      "Epoch 106/200, Train Loss: 0.224648, Train-Class-Acc: {0: '94.96%', 1: '93.49%', 2: '96.36%', 3: '97.54%', 4: '89.24%', 5: '96.71%'}\n",
      "Val Loss: 2.879780, Val Acc: 81.03%, Val-Class-Acc: {0: '69.57%', 1: '73.13%', 2: '80.56%', 3: '90.98%', 4: '82.50%', 5: '88.02%'}, LR: 0.001000\n",
      "Epoch 107/200, Train Loss: 0.227566, Train-Class-Acc: {0: '95.78%', 1: '93.79%', 2: '95.67%', 3: '97.54%', 4: '91.77%', 5: '96.56%'}\n",
      "Val Loss: 3.213348, Val Acc: 80.09%, Val-Class-Acc: {0: '64.67%', 1: '77.61%', 2: '78.47%', 3: '78.69%', 4: '80.00%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 108/200, Train Loss: 0.245607, Train-Class-Acc: {0: '94.01%', 1: '92.15%', 2: '94.80%', 3: '96.11%', 4: '91.77%', 5: '96.19%'}\n",
      "Val Loss: 2.801608, Val Acc: 81.11%, Val-Class-Acc: {0: '67.39%', 1: '75.52%', 2: '77.08%', 3: '90.16%', 4: '80.00%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 109/200, Train Loss: 0.194825, Train-Class-Acc: {0: '94.96%', 1: '93.79%', 2: '97.23%', 3: '97.23%', 4: '89.24%', 5: '97.09%'}\n",
      "Val Loss: 2.575708, Val Acc: 82.05%, Val-Class-Acc: {0: '71.74%', 1: '77.61%', 2: '80.56%', 3: '88.11%', 4: '85.00%', 5: '88.02%'}, LR: 0.001000\n",
      "Epoch 110/200, Train Loss: 0.201621, Train-Class-Acc: {0: '96.46%', 1: '94.39%', 2: '94.45%', 3: '97.54%', 4: '90.51%', 5: '97.01%'}\n",
      "Val Loss: 3.086713, Val Acc: 80.72%, Val-Class-Acc: {0: '69.57%', 1: '74.03%', 2: '81.25%', 3: '87.30%', 4: '80.00%', 5: '88.62%'}, LR: 0.001000\n",
      "Epoch 111/200, Train Loss: 0.183810, Train-Class-Acc: {0: '94.55%', 1: '93.72%', 2: '96.53%', 3: '97.34%', 4: '89.24%', 5: '97.16%'}\n",
      "Val Loss: 2.912972, Val Acc: 80.95%, Val-Class-Acc: {0: '82.07%', 1: '71.64%', 2: '79.86%', 3: '88.11%', 4: '80.00%', 5: '85.03%'}, LR: 0.001000\n",
      "Epoch 112/200, Train Loss: 0.266285, Train-Class-Acc: {0: '95.50%', 1: '92.89%', 2: '95.84%', 3: '96.31%', 4: '86.71%', 5: '96.64%'}\n",
      "Val Loss: 2.788704, Val Acc: 80.80%, Val-Class-Acc: {0: '69.02%', 1: '76.42%', 2: '79.86%', 3: '86.89%', 4: '80.00%', 5: '87.72%'}, LR: 0.001000\n",
      "Epoch 113/200, Train Loss: 0.237014, Train-Class-Acc: {0: '94.82%', 1: '92.37%', 2: '96.19%', 3: '97.03%', 4: '91.14%', 5: '95.96%'}\n",
      "Val Loss: 2.621820, Val Acc: 80.64%, Val-Class-Acc: {0: '68.48%', 1: '72.84%', 2: '81.94%', 3: '87.30%', 4: '85.00%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 114/200, Train Loss: 0.192204, Train-Class-Acc: {0: '95.50%', 1: '94.54%', 2: '96.36%', 3: '97.54%', 4: '87.97%', 5: '97.31%'}\n",
      "Val Loss: 2.646546, Val Acc: 81.58%, Val-Class-Acc: {0: '70.65%', 1: '80.90%', 2: '80.56%', 3: '84.84%', 4: '80.00%', 5: '86.53%'}, LR: 0.001000\n",
      "Epoch 115/200, Train Loss: 0.137441, Train-Class-Acc: {0: '96.59%', 1: '95.14%', 2: '95.84%', 3: '97.85%', 4: '91.14%', 5: '97.53%'}\n",
      "Val Loss: 2.745768, Val Acc: 81.11%, Val-Class-Acc: {0: '70.11%', 1: '75.52%', 2: '79.86%', 3: '86.07%', 4: '80.00%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 116/200, Train Loss: 0.148837, Train-Class-Acc: {0: '94.69%', 1: '93.49%', 2: '97.05%', 3: '97.54%', 4: '91.14%', 5: '97.38%'}\n",
      "Val Loss: 2.053285, Val Acc: 80.80%, Val-Class-Acc: {0: '71.74%', 1: '77.91%', 2: '83.33%', 3: '88.93%', 4: '80.00%', 5: '81.74%'}, LR: 0.001000\n",
      "Epoch 117/200, Train Loss: 0.182781, Train-Class-Acc: {0: '96.59%', 1: '94.32%', 2: '96.19%', 3: '97.34%', 4: '88.61%', 5: '96.94%'}\n",
      "Val Loss: 2.341814, Val Acc: 81.03%, Val-Class-Acc: {0: '74.46%', 1: '75.52%', 2: '79.86%', 3: '87.30%', 4: '82.50%', 5: '85.93%'}, LR: 0.001000\n",
      "Epoch 118/200, Train Loss: 0.229852, Train-Class-Acc: {0: '96.05%', 1: '93.79%', 2: '96.36%', 3: '97.54%', 4: '91.14%', 5: '96.41%'}\n",
      "Val Loss: 2.842670, Val Acc: 80.64%, Val-Class-Acc: {0: '63.59%', 1: '75.82%', 2: '85.42%', 3: '88.52%', 4: '80.00%', 5: '87.13%'}, LR: 0.001000\n",
      "Epoch 119/200, Train Loss: 0.157344, Train-Class-Acc: {0: '95.78%', 1: '94.91%', 2: '96.36%', 3: '97.54%', 4: '89.24%', 5: '96.86%'}\n",
      "Val Loss: 2.693353, Val Acc: 80.80%, Val-Class-Acc: {0: '77.17%', 1: '71.04%', 2: '80.56%', 3: '86.48%', 4: '82.50%', 5: '88.32%'}, LR: 0.001000\n",
      "Epoch 120/200, Train Loss: 0.189307, Train-Class-Acc: {0: '95.37%', 1: '94.32%', 2: '94.97%', 3: '97.03%', 4: '89.24%', 5: '97.31%'}\n",
      "Val Loss: 2.634991, Val Acc: 81.11%, Val-Class-Acc: {0: '73.37%', 1: '74.03%', 2: '86.81%', 3: '86.48%', 4: '82.50%', 5: '85.93%'}, LR: 0.001000\n",
      "Epoch 121/200, Train Loss: 0.189134, Train-Class-Acc: {0: '96.32%', 1: '94.02%', 2: '96.19%', 3: '98.16%', 4: '85.44%', 5: '96.86%'}\n",
      "Val Loss: 2.964767, Val Acc: 80.80%, Val-Class-Acc: {0: '61.96%', 1: '78.21%', 2: '79.17%', 3: '86.48%', 4: '77.50%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 122/200, Train Loss: 0.181745, Train-Class-Acc: {0: '96.87%', 1: '93.72%', 2: '98.27%', 3: '97.64%', 4: '91.14%', 5: '97.01%'}\n",
      "Val Loss: 3.123335, Val Acc: 81.26%, Val-Class-Acc: {0: '80.43%', 1: '71.64%', 2: '81.94%', 3: '86.89%', 4: '80.00%', 5: '87.13%'}, LR: 0.001000\n",
      "Epoch 123/200, Train Loss: 0.148141, Train-Class-Acc: {0: '95.50%', 1: '95.81%', 2: '97.23%', 3: '97.54%', 4: '90.51%', 5: '97.53%'}\n",
      "Val Loss: 2.912829, Val Acc: 81.26%, Val-Class-Acc: {0: '71.20%', 1: '76.12%', 2: '81.25%', 3: '87.70%', 4: '85.00%', 5: '86.83%'}, LR: 0.001000\n",
      "Epoch 124/200, Train Loss: 0.107377, Train-Class-Acc: {0: '97.00%', 1: '95.74%', 2: '96.88%', 3: '98.05%', 4: '93.67%', 5: '97.61%'}\n",
      "Val Loss: 2.834876, Val Acc: 80.64%, Val-Class-Acc: {0: '73.91%', 1: '74.33%', 2: '77.78%', 3: '87.30%', 4: '82.50%', 5: '86.83%'}, LR: 0.001000\n",
      "Epoch 125/200, Train Loss: 0.197817, Train-Class-Acc: {0: '96.05%', 1: '93.72%', 2: '95.67%', 3: '97.34%', 4: '88.61%', 5: '97.01%'}\n",
      "Val Loss: 4.393093, Val Acc: 81.26%, Val-Class-Acc: {0: '76.63%', 1: '70.45%', 2: '77.78%', 3: '86.07%', 4: '80.00%', 5: '92.81%'}, LR: 0.001000\n",
      "Epoch 126/200, Train Loss: 0.196427, Train-Class-Acc: {0: '94.82%', 1: '94.76%', 2: '95.32%', 3: '96.82%', 4: '90.51%', 5: '96.94%'}\n",
      "Val Loss: 3.871024, Val Acc: 79.78%, Val-Class-Acc: {0: '72.28%', 1: '73.13%', 2: '80.56%', 3: '80.33%', 4: '80.00%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 127/200, Train Loss: 0.314722, Train-Class-Acc: {0: '94.82%', 1: '92.60%', 2: '94.45%', 3: '97.03%', 4: '87.97%', 5: '96.56%'}\n",
      "Val Loss: 3.637746, Val Acc: 79.70%, Val-Class-Acc: {0: '71.20%', 1: '72.24%', 2: '79.86%', 3: '81.97%', 4: '85.00%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 128/200, Train Loss: 0.255724, Train-Class-Acc: {0: '95.23%', 1: '93.49%', 2: '96.01%', 3: '97.03%', 4: '88.61%', 5: '96.11%'}\n",
      "Val Loss: 2.670281, Val Acc: 80.87%, Val-Class-Acc: {0: '62.50%', 1: '80.00%', 2: '79.86%', 3: '86.07%', 4: '82.50%', 5: '88.32%'}, LR: 0.001000\n",
      "Epoch 129/200, Train Loss: 0.148471, Train-Class-Acc: {0: '96.32%', 1: '94.24%', 2: '97.40%', 3: '97.64%', 4: '93.67%', 5: '96.86%'}\n",
      "Val Loss: 2.932210, Val Acc: 81.73%, Val-Class-Acc: {0: '75.54%', 1: '70.75%', 2: '79.17%', 3: '91.39%', 4: '80.00%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 130/200, Train Loss: 0.129493, Train-Class-Acc: {0: '97.14%', 1: '95.36%', 2: '97.05%', 3: '98.36%', 4: '90.51%', 5: '97.76%'}\n",
      "Val Loss: 2.922425, Val Acc: 81.58%, Val-Class-Acc: {0: '73.37%', 1: '72.24%', 2: '81.94%', 3: '91.39%', 4: '80.00%', 5: '88.32%'}, LR: 0.001000\n",
      "Epoch 131/200, Train Loss: 0.153050, Train-Class-Acc: {0: '96.59%', 1: '94.99%', 2: '96.88%', 3: '98.46%', 4: '93.67%', 5: '98.21%'}\n",
      "Val Loss: 2.654044, Val Acc: 80.41%, Val-Class-Acc: {0: '71.20%', 1: '75.82%', 2: '78.47%', 3: '83.61%', 4: '80.00%', 5: '88.62%'}, LR: 0.001000\n",
      "Epoch 132/200, Train Loss: 0.517908, Train-Class-Acc: {0: '94.28%', 1: '91.70%', 2: '94.63%', 3: '94.88%', 4: '87.34%', 5: '94.92%'}\n",
      "Val Loss: 2.685431, Val Acc: 79.94%, Val-Class-Acc: {0: '70.11%', 1: '74.33%', 2: '78.47%', 3: '85.66%', 4: '80.00%', 5: '87.43%'}, LR: 0.001000\n",
      "Epoch 133/200, Train Loss: 0.254833, Train-Class-Acc: {0: '94.96%', 1: '94.32%', 2: '94.97%', 3: '97.64%', 4: '91.14%', 5: '96.26%'}\n",
      "Val Loss: 2.460119, Val Acc: 81.42%, Val-Class-Acc: {0: '75.00%', 1: '71.04%', 2: '81.94%', 3: '88.52%', 4: '77.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 134/200, Train Loss: 0.190582, Train-Class-Acc: {0: '96.87%', 1: '95.36%', 2: '97.05%', 3: '98.26%', 5: '97.31%', 4: '91.77%'}\n",
      "Val Loss: 2.552419, Val Acc: 80.72%, Val-Class-Acc: {0: '76.09%', 1: '76.72%', 2: '75.00%', 3: '86.07%', 4: '82.50%', 5: '85.63%'}, LR: 0.001000\n",
      "Epoch 135/200, Train Loss: 0.303722, Train-Class-Acc: {0: '95.64%', 1: '93.12%', 2: '95.32%', 3: '97.13%', 4: '91.77%', 5: '96.04%'}\n",
      "Val Loss: 2.981367, Val Acc: 80.33%, Val-Class-Acc: {0: '80.43%', 1: '70.75%', 2: '78.47%', 3: '83.61%', 4: '80.00%', 5: '88.32%'}, LR: 0.001000\n",
      "Epoch 136/200, Train Loss: 0.125626, Train-Class-Acc: {0: '96.32%', 1: '94.99%', 2: '97.57%', 3: '98.26%', 4: '93.04%', 5: '97.46%'}\n",
      "Val Loss: 2.940246, Val Acc: 80.25%, Val-Class-Acc: {0: '70.11%', 1: '73.13%', 2: '79.17%', 3: '86.48%', 4: '80.00%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 137/200, Train Loss: 0.107007, Train-Class-Acc: {0: '97.28%', 1: '96.34%', 2: '97.92%', 3: '98.36%', 4: '91.77%', 5: '97.91%'}\n",
      "Val Loss: 2.392131, Val Acc: 80.80%, Val-Class-Acc: {0: '70.65%', 1: '76.42%', 2: '86.81%', 3: '85.66%', 4: '85.00%', 5: '84.13%'}, LR: 0.001000\n",
      "Epoch 138/200, Train Loss: 0.106178, Train-Class-Acc: {0: '98.09%', 1: '95.66%', 2: '97.23%', 3: '97.85%', 4: '90.51%', 5: '97.83%'}\n",
      "Val Loss: 3.481830, Val Acc: 80.87%, Val-Class-Acc: {0: '72.83%', 1: '74.03%', 2: '79.17%', 3: '88.11%', 4: '80.00%', 5: '87.72%'}, LR: 0.001000\n",
      "Epoch 139/200, Train Loss: 0.091699, Train-Class-Acc: {0: '97.82%', 1: '96.48%', 2: '98.44%', 3: '98.26%', 4: '93.04%', 5: '98.58%'}\n",
      "Val Loss: 2.823878, Val Acc: 81.03%, Val-Class-Acc: {0: '74.46%', 1: '72.24%', 2: '80.56%', 3: '89.75%', 4: '77.50%', 5: '87.72%'}, LR: 0.001000\n",
      "Epoch 140/200, Train Loss: 0.111925, Train-Class-Acc: {0: '96.32%', 1: '95.29%', 2: '98.09%', 3: '98.26%', 4: '91.77%', 5: '97.76%'}\n",
      "Val Loss: 3.560905, Val Acc: 81.11%, Val-Class-Acc: {0: '67.39%', 1: '74.63%', 2: '79.86%', 3: '88.93%', 4: '80.00%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 141/200, Train Loss: 0.141666, Train-Class-Acc: {0: '96.46%', 1: '94.76%', 2: '97.05%', 3: '97.75%', 5: '97.91%', 4: '93.67%'}\n",
      "Val Loss: 3.917903, Val Acc: 79.86%, Val-Class-Acc: {0: '77.17%', 1: '70.45%', 2: '86.81%', 3: '81.56%', 4: '80.00%', 5: '86.53%'}, LR: 0.001000\n",
      "Epoch 142/200, Train Loss: 0.184973, Train-Class-Acc: {0: '95.64%', 1: '94.99%', 2: '97.23%', 3: '98.46%', 4: '89.24%', 5: '96.49%'}\n",
      "Val Loss: 3.226000, Val Acc: 80.48%, Val-Class-Acc: {0: '70.65%', 1: '77.01%', 2: '79.86%', 3: '90.98%', 4: '75.00%', 5: '82.63%'}, LR: 0.001000\n",
      "Epoch 143/200, Train Loss: 0.199802, Train-Class-Acc: {0: '97.14%', 1: '95.29%', 2: '96.71%', 3: '97.85%', 4: '89.24%', 5: '97.16%'}\n",
      "Val Loss: 2.838726, Val Acc: 80.56%, Val-Class-Acc: {0: '79.35%', 1: '71.04%', 2: '82.64%', 3: '84.84%', 4: '77.50%', 5: '87.13%'}, LR: 0.001000\n",
      "Epoch 144/200, Train Loss: 0.135763, Train-Class-Acc: {0: '96.87%', 1: '95.29%', 2: '97.75%', 3: '98.26%', 4: '93.04%', 5: '98.28%'}\n",
      "Val Loss: 3.234591, Val Acc: 80.72%, Val-Class-Acc: {0: '75.54%', 1: '74.33%', 2: '81.94%', 3: '86.07%', 4: '77.50%', 5: '85.93%'}, LR: 0.001000\n",
      "Epoch 145/200, Train Loss: 0.153310, Train-Class-Acc: {0: '97.28%', 1: '95.74%', 2: '98.27%', 3: '97.95%', 4: '93.04%', 5: '97.23%'}\n",
      "Val Loss: 3.583031, Val Acc: 81.11%, Val-Class-Acc: {0: '83.15%', 1: '68.96%', 2: '77.78%', 3: '85.25%', 4: '82.50%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 146/200, Train Loss: 0.156187, Train-Class-Acc: {0: '96.87%', 1: '95.51%', 2: '97.23%', 3: '98.26%', 4: '93.04%', 5: '97.38%'}\n",
      "Val Loss: 4.047378, Val Acc: 80.41%, Val-Class-Acc: {0: '70.11%', 1: '72.24%', 2: '81.94%', 3: '87.70%', 4: '77.50%', 5: '88.62%'}, LR: 0.001000\n",
      "Epoch 147/200, Train Loss: 0.135716, Train-Class-Acc: {0: '97.96%', 1: '96.11%', 2: '97.57%', 3: '98.67%', 4: '92.41%', 5: '97.61%'}\n",
      "Val Loss: 3.132142, Val Acc: 80.95%, Val-Class-Acc: {0: '72.83%', 1: '75.22%', 2: '81.94%', 3: '85.66%', 4: '80.00%', 5: '87.43%'}, LR: 0.001000\n",
      "Epoch 148/200, Train Loss: 0.105734, Train-Class-Acc: {0: '97.68%', 1: '96.34%', 2: '97.57%', 3: '98.57%', 4: '91.77%', 5: '97.91%'}\n",
      "Val Loss: 4.205597, Val Acc: 80.41%, Val-Class-Acc: {0: '73.91%', 1: '73.13%', 2: '77.08%', 3: '83.61%', 4: '80.00%', 5: '90.42%'}, LR: 0.001000\n",
      "Epoch 149/200, Train Loss: 0.118252, Train-Class-Acc: {0: '96.87%', 1: '96.34%', 2: '98.09%', 3: '98.26%', 4: '92.41%', 5: '97.61%'}\n",
      "Val Loss: 2.844621, Val Acc: 80.95%, Val-Class-Acc: {0: '68.48%', 1: '76.72%', 2: '80.56%', 3: '90.16%', 4: '80.00%', 5: '85.63%'}, LR: 0.001000\n",
      "Epoch 150/200, Train Loss: 0.177318, Train-Class-Acc: {0: '97.14%', 1: '95.59%', 2: '97.57%', 3: '97.85%', 4: '91.14%', 5: '97.01%'}\n",
      "Val Loss: 2.624735, Val Acc: 80.48%, Val-Class-Acc: {0: '75.00%', 1: '75.22%', 2: '77.08%', 3: '90.16%', 4: '77.50%', 5: '83.53%'}, LR: 0.001000\n",
      "Epoch 151/200, Train Loss: 0.185415, Train-Class-Acc: {0: '97.14%', 1: '95.36%', 2: '96.36%', 3: '97.44%', 4: '91.77%', 5: '97.09%'}\n",
      "Val Loss: 3.505566, Val Acc: 81.11%, Val-Class-Acc: {0: '76.63%', 1: '75.22%', 2: '81.94%', 3: '88.93%', 4: '75.00%', 5: '84.13%'}, LR: 0.001000\n",
      "Epoch 152/200, Train Loss: 0.209761, Train-Class-Acc: {0: '95.50%', 1: '95.21%', 2: '97.05%', 3: '98.16%', 4: '90.51%', 5: '97.16%'}\n",
      "Val Loss: 3.568265, Val Acc: 80.56%, Val-Class-Acc: {0: '80.43%', 1: '66.27%', 2: '76.39%', 3: '86.89%', 4: '77.50%', 5: '92.51%'}, LR: 0.001000\n",
      "Epoch 153/200, Train Loss: 0.223285, Train-Class-Acc: {0: '95.64%', 1: '94.39%', 2: '97.40%', 3: '97.44%', 4: '90.51%', 5: '97.91%'}\n",
      "Val Loss: 2.523692, Val Acc: 81.03%, Val-Class-Acc: {0: '74.46%', 1: '75.22%', 2: '81.25%', 3: '86.48%', 4: '80.00%', 5: '86.53%'}, LR: 0.001000\n",
      "Epoch 154/200, Train Loss: 0.141471, Train-Class-Acc: {0: '96.87%', 1: '95.36%', 2: '96.71%', 3: '97.75%', 4: '94.30%', 5: '97.91%'}\n",
      "Val Loss: 3.185469, Val Acc: 80.48%, Val-Class-Acc: {0: '73.91%', 1: '73.13%', 2: '80.56%', 3: '84.43%', 4: '82.50%', 5: '88.32%'}, LR: 0.001000\n",
      "Epoch 155/200, Train Loss: 0.090537, Train-Class-Acc: {0: '97.96%', 1: '96.48%', 2: '98.27%', 3: '98.87%', 4: '94.94%', 5: '98.21%'}\n",
      "Val Loss: 2.624755, Val Acc: 80.64%, Val-Class-Acc: {0: '69.57%', 1: '77.91%', 2: '80.56%', 3: '86.48%', 4: '82.50%', 5: '85.03%'}, LR: 0.001000\n",
      "Epoch 156/200, Train Loss: 0.083200, Train-Class-Acc: {0: '97.82%', 1: '97.16%', 2: '98.09%', 3: '98.46%', 4: '94.94%', 5: '98.28%'}\n",
      "Val Loss: 2.479161, Val Acc: 80.56%, Val-Class-Acc: {0: '67.39%', 1: '80.00%', 2: '75.69%', 3: '85.66%', 4: '82.50%', 5: '86.53%'}, LR: 0.001000\n",
      "Epoch 157/200, Train Loss: 0.122117, Train-Class-Acc: {0: '96.59%', 1: '96.26%', 2: '98.27%', 3: '98.36%', 4: '91.14%', 5: '98.36%'}\n",
      "Val Loss: 2.852085, Val Acc: 81.26%, Val-Class-Acc: {0: '69.02%', 1: '76.72%', 2: '75.69%', 3: '88.11%', 4: '82.50%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 158/200, Train Loss: 0.136615, Train-Class-Acc: {0: '98.50%', 1: '97.01%', 2: '97.40%', 3: '98.98%', 4: '93.04%', 5: '97.98%'}\n",
      "Val Loss: 3.159940, Val Acc: 80.95%, Val-Class-Acc: {0: '71.20%', 1: '72.54%', 2: '77.08%', 3: '88.93%', 4: '85.00%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 159/200, Train Loss: 0.114468, Train-Class-Acc: {0: '97.41%', 1: '97.31%', 2: '98.44%', 3: '98.98%', 4: '92.41%', 5: '98.28%'}\n",
      "Val Loss: 3.200675, Val Acc: 80.56%, Val-Class-Acc: {0: '76.63%', 1: '70.45%', 2: '77.78%', 3: '87.30%', 4: '82.50%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 160/200, Train Loss: 0.112368, Train-Class-Acc: {0: '95.91%', 1: '95.06%', 2: '98.44%', 3: '98.46%', 4: '93.67%', 5: '97.83%'}\n",
      "Val Loss: 2.741505, Val Acc: 80.80%, Val-Class-Acc: {0: '69.57%', 1: '72.54%', 2: '78.47%', 3: '86.89%', 4: '82.50%', 5: '91.62%'}, LR: 0.001000\n",
      "Epoch 161/200, Train Loss: 0.096483, Train-Class-Acc: {0: '97.96%', 1: '95.89%', 2: '97.40%', 3: '98.77%', 4: '91.14%', 5: '98.06%'}\n",
      "Val Loss: 3.337332, Val Acc: 79.94%, Val-Class-Acc: {0: '74.46%', 1: '69.85%', 2: '78.47%', 3: '86.48%', 4: '82.50%', 5: '88.62%'}, LR: 0.001000\n",
      "Epoch 162/200, Train Loss: 0.104242, Train-Class-Acc: {0: '98.23%', 1: '96.86%', 2: '98.61%', 3: '98.98%', 4: '94.94%', 5: '98.13%'}\n",
      "Val Loss: 3.368341, Val Acc: 80.33%, Val-Class-Acc: {0: '70.11%', 1: '72.84%', 2: '81.25%', 3: '85.66%', 4: '85.00%', 5: '88.62%'}, LR: 0.001000\n",
      "Epoch 163/200, Train Loss: 0.190041, Train-Class-Acc: {0: '96.32%', 1: '95.36%', 2: '96.88%', 3: '97.23%', 4: '91.77%', 5: '97.23%'}\n",
      "Val Loss: 3.427403, Val Acc: 80.80%, Val-Class-Acc: {0: '70.65%', 1: '73.73%', 2: '79.17%', 3: '86.07%', 4: '85.00%', 5: '89.82%'}, LR: 0.001000\n",
      "Epoch 164/200, Train Loss: 0.194120, Train-Class-Acc: {0: '97.82%', 1: '95.66%', 2: '96.53%', 3: '97.54%', 4: '93.04%', 5: '97.38%'}\n",
      "Val Loss: 3.702713, Val Acc: 81.42%, Val-Class-Acc: {0: '71.20%', 1: '74.93%', 2: '79.86%', 3: '86.07%', 4: '82.50%', 5: '90.72%'}, LR: 0.001000\n",
      "Epoch 165/200, Train Loss: 0.138301, Train-Class-Acc: {0: '97.28%', 1: '95.66%', 2: '97.23%', 3: '97.85%', 4: '93.04%', 5: '97.01%'}\n",
      "Val Loss: 3.780355, Val Acc: 79.70%, Val-Class-Acc: {0: '69.02%', 1: '69.85%', 2: '77.78%', 3: '83.61%', 4: '82.50%', 5: '93.11%'}, LR: 0.001000\n",
      "Epoch 166/200, Train Loss: 0.175581, Train-Class-Acc: {0: '96.87%', 1: '95.59%', 2: '97.23%', 3: '97.64%', 4: '91.14%', 5: '98.13%'}\n",
      "Val Loss: 3.432946, Val Acc: 81.03%, Val-Class-Acc: {0: '64.67%', 1: '77.91%', 2: '78.47%', 3: '87.30%', 4: '82.50%', 5: '89.52%'}, LR: 0.001000\n",
      "Epoch 167/200, Train Loss: 0.119381, Train-Class-Acc: {0: '97.55%', 1: '96.26%', 2: '97.57%', 3: '98.36%', 5: '97.91%', 4: '95.57%'}\n",
      "Val Loss: 2.926702, Val Acc: 81.03%, Val-Class-Acc: {0: '71.74%', 1: '76.42%', 2: '79.17%', 3: '85.25%', 4: '82.50%', 5: '88.32%'}, LR: 0.001000\n",
      "Epoch 168/200, Train Loss: 0.086915, Train-Class-Acc: {0: '97.41%', 1: '96.86%', 2: '98.09%', 3: '98.36%', 4: '93.04%', 5: '98.51%'}\n",
      "Val Loss: 3.311397, Val Acc: 80.95%, Val-Class-Acc: {0: '71.74%', 1: '78.81%', 2: '79.17%', 3: '82.38%', 4: '82.50%', 5: '87.72%'}, LR: 0.001000\n",
      "Epoch 169/200, Train Loss: 0.073532, Train-Class-Acc: {0: '97.96%', 1: '97.31%', 2: '98.09%', 3: '98.57%', 4: '93.67%', 5: '98.73%'}\n",
      "Val Loss: 3.687743, Val Acc: 80.56%, Val-Class-Acc: {0: '71.20%', 1: '72.54%', 2: '83.33%', 3: '86.48%', 4: '85.00%', 5: '87.72%'}, LR: 0.001000\n",
      "Epoch 170/200, Train Loss: 0.068869, Train-Class-Acc: {0: '98.23%', 1: '96.86%', 2: '98.27%', 3: '98.87%', 4: '96.20%', 5: '98.43%'}\n",
      "Val Loss: 3.358218, Val Acc: 81.03%, Val-Class-Acc: {0: '74.46%', 1: '74.33%', 2: '81.94%', 3: '86.07%', 4: '82.50%', 5: '87.13%'}, LR: 0.001000\n",
      "Epoch 171/200, Train Loss: 0.080688, Train-Class-Acc: {0: '97.41%', 1: '96.93%', 2: '98.79%', 3: '98.98%', 5: '98.73%', 4: '93.67%'}\n",
      "Val Loss: 3.270483, Val Acc: 80.41%, Val-Class-Acc: {0: '75.00%', 1: '71.34%', 2: '77.08%', 3: '82.79%', 4: '80.00%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 172/200, Train Loss: 0.145805, Train-Class-Acc: {0: '96.19%', 1: '96.26%', 2: '97.40%', 3: '98.67%', 4: '94.30%', 5: '97.46%'}\n",
      "Val Loss: 3.493006, Val Acc: 80.56%, Val-Class-Acc: {0: '78.26%', 1: '70.75%', 2: '76.39%', 3: '86.48%', 4: '80.00%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 173/200, Train Loss: 0.227828, Train-Class-Acc: {0: '96.59%', 1: '95.96%', 2: '97.23%', 3: '97.64%', 4: '94.30%', 5: '97.46%'}\n",
      "Val Loss: 3.907857, Val Acc: 80.95%, Val-Class-Acc: {0: '73.37%', 1: '74.33%', 2: '79.86%', 3: '88.52%', 4: '72.50%', 5: '87.72%'}, LR: 0.001000\n",
      "Epoch 174/200, Train Loss: 0.419953, Train-Class-Acc: {0: '94.14%', 1: '91.77%', 2: '93.76%', 3: '95.90%', 4: '84.81%', 5: '95.81%'}\n",
      "Val Loss: 2.772292, Val Acc: 81.65%, Val-Class-Acc: {0: '72.28%', 1: '77.91%', 2: '79.86%', 3: '87.30%', 4: '82.50%', 5: '87.13%'}, LR: 0.001000\n",
      "Epoch 175/200, Train Loss: 0.180910, Train-Class-Acc: {0: '95.50%', 1: '94.61%', 2: '97.40%', 3: '97.54%', 4: '89.87%', 5: '97.01%'}\n",
      "Val Loss: 3.647177, Val Acc: 81.34%, Val-Class-Acc: {0: '69.02%', 1: '74.63%', 2: '81.25%', 3: '87.70%', 4: '82.50%', 5: '90.12%'}, LR: 0.001000\n",
      "Epoch 176/200, Train Loss: 0.110606, Train-Class-Acc: {0: '96.59%', 1: '95.51%', 2: '97.40%', 3: '98.67%', 4: '94.94%', 5: '97.76%'}\n",
      "Val Loss: 3.362135, Val Acc: 80.72%, Val-Class-Acc: {0: '72.83%', 1: '74.93%', 2: '79.86%', 3: '86.07%', 4: '80.00%', 5: '87.43%'}, LR: 0.001000\n",
      "Epoch 177/200, Train Loss: 0.077558, Train-Class-Acc: {0: '98.37%', 1: '97.53%', 2: '98.27%', 3: '98.46%', 4: '95.57%', 5: '98.21%'}\n",
      "Val Loss: 2.621803, Val Acc: 80.72%, Val-Class-Acc: {0: '72.28%', 1: '75.22%', 2: '82.64%', 3: '87.70%', 4: '82.50%', 5: '84.73%'}, LR: 0.001000\n",
      "Epoch 178/200, Train Loss: 0.396130, Train-Class-Acc: {0: '97.00%', 1: '95.21%', 2: '97.57%', 3: '97.13%', 4: '91.14%', 5: '98.06%'}\n",
      "Val Loss: 4.483902, Val Acc: 79.16%, Val-Class-Acc: {0: '63.59%', 1: '80.60%', 2: '75.00%', 3: '73.77%', 4: '80.00%', 5: '91.92%'}, LR: 0.001000\n",
      "Epoch 179/200, Train Loss: 0.281366, Train-Class-Acc: {0: '94.41%', 1: '93.72%', 2: '97.23%', 3: '97.85%', 5: '96.56%', 4: '91.77%'}\n",
      "Val Loss: 2.824549, Val Acc: 80.95%, Val-Class-Acc: {0: '75.00%', 1: '75.22%', 2: '79.17%', 3: '86.48%', 4: '82.50%', 5: '86.53%'}, LR: 0.001000\n",
      "Epoch 180/200, Train Loss: 0.114671, Train-Class-Acc: {0: '97.28%', 1: '96.63%', 2: '97.57%', 3: '98.77%', 4: '93.04%', 5: '98.06%'}\n",
      "Val Loss: 3.153984, Val Acc: 80.80%, Val-Class-Acc: {0: '73.37%', 1: '71.34%', 2: '81.25%', 3: '91.80%', 4: '80.00%', 5: '86.23%'}, LR: 0.001000\n",
      "Epoch 181/200, Train Loss: 0.132090, Train-Class-Acc: {0: '97.41%', 1: '95.74%', 2: '98.61%', 3: '98.26%', 4: '92.41%', 5: '97.83%'}\n",
      "Val Loss: 3.373470, Val Acc: 80.80%, Val-Class-Acc: {0: '79.35%', 1: '69.55%', 2: '79.86%', 3: '88.93%', 4: '80.00%', 5: '87.43%'}, LR: 0.001000\n",
      "Epoch 182/200, Train Loss: 0.259186, Train-Class-Acc: {0: '95.50%', 1: '95.14%', 2: '96.88%', 3: '97.23%', 5: '97.09%', 4: '92.41%'}\n",
      "Val Loss: 3.330262, Val Acc: 80.48%, Val-Class-Acc: {0: '75.54%', 1: '71.04%', 2: '79.86%', 3: '86.48%', 4: '80.00%', 5: '88.62%'}, LR: 0.001000\n",
      "Epoch 183/200, Train Loss: 0.079992, Train-Class-Acc: {0: '99.05%', 1: '96.48%', 2: '98.44%', 3: '99.08%', 4: '93.67%', 5: '98.13%'}\n",
      "Val Loss: 2.842927, Val Acc: 81.03%, Val-Class-Acc: {0: '68.48%', 1: '76.12%', 2: '79.86%', 3: '86.89%', 4: '80.00%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 184/200, Train Loss: 0.066715, Train-Class-Acc: {0: '98.09%', 1: '97.46%', 2: '98.79%', 3: '99.18%', 4: '95.57%', 5: '98.65%'}\n",
      "Val Loss: 3.516960, Val Acc: 80.72%, Val-Class-Acc: {0: '72.28%', 1: '74.03%', 2: '79.17%', 3: '86.07%', 4: '80.00%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 185/200, Train Loss: 0.118048, Train-Class-Acc: {0: '97.82%', 1: '96.48%', 2: '98.27%', 3: '98.36%', 4: '91.77%', 5: '98.06%'}\n",
      "Val Loss: 3.319138, Val Acc: 80.64%, Val-Class-Acc: {0: '72.28%', 1: '72.24%', 2: '79.17%', 3: '88.11%', 4: '80.00%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 186/200, Train Loss: 0.109315, Train-Class-Acc: {0: '97.14%', 1: '96.26%', 2: '97.75%', 3: '98.87%', 4: '93.67%', 5: '97.76%'}\n",
      "Val Loss: 4.053595, Val Acc: 81.03%, Val-Class-Acc: {0: '79.89%', 1: '69.55%', 2: '78.47%', 3: '84.43%', 4: '77.50%', 5: '92.22%'}, LR: 0.001000\n",
      "Epoch 187/200, Train Loss: 0.102375, Train-Class-Acc: {0: '97.96%', 1: '97.08%', 2: '97.57%', 3: '98.87%', 4: '91.14%', 5: '98.06%'}\n",
      "Val Loss: 3.080416, Val Acc: 81.03%, Val-Class-Acc: {0: '67.39%', 1: '77.01%', 2: '80.56%', 3: '89.34%', 4: '77.50%', 5: '87.13%'}, LR: 0.001000\n",
      "Epoch 188/200, Train Loss: 0.116394, Train-Class-Acc: {0: '98.64%', 1: '97.46%', 2: '98.79%', 3: '99.08%', 4: '93.67%', 5: '98.65%'}\n",
      "Val Loss: 2.748497, Val Acc: 80.80%, Val-Class-Acc: {0: '64.13%', 1: '79.40%', 2: '78.47%', 3: '86.48%', 4: '77.50%', 5: '88.62%'}, LR: 0.001000\n",
      "Epoch 189/200, Train Loss: 0.080263, Train-Class-Acc: {0: '97.96%', 1: '97.38%', 2: '98.09%', 3: '99.18%', 4: '94.94%', 5: '98.21%'}\n",
      "Val Loss: 3.339738, Val Acc: 80.56%, Val-Class-Acc: {0: '70.11%', 1: '76.42%', 2: '79.17%', 3: '85.25%', 4: '77.50%', 5: '88.02%'}, LR: 0.001000\n",
      "Epoch 190/200, Train Loss: 0.138378, Train-Class-Acc: {0: '98.50%', 1: '97.16%', 2: '97.75%', 3: '98.16%', 5: '98.06%', 4: '95.57%'}\n",
      "Val Loss: 3.082059, Val Acc: 80.02%, Val-Class-Acc: {0: '66.85%', 1: '73.43%', 2: '79.17%', 3: '86.48%', 4: '85.00%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 191/200, Train Loss: 0.099949, Train-Class-Acc: {0: '97.00%', 1: '96.04%', 2: '97.40%', 3: '98.46%', 4: '96.20%', 5: '98.43%'}\n",
      "Val Loss: 3.605656, Val Acc: 80.33%, Val-Class-Acc: {0: '68.48%', 1: '76.12%', 2: '80.56%', 3: '83.61%', 4: '77.50%', 5: '88.92%'}, LR: 0.001000\n",
      "Epoch 192/200, Train Loss: 0.096520, Train-Class-Acc: {0: '98.23%', 1: '97.08%', 2: '98.44%', 3: '98.26%', 4: '94.30%', 5: '98.65%'}\n",
      "Val Loss: 3.004389, Val Acc: 81.42%, Val-Class-Acc: {0: '75.00%', 1: '73.73%', 2: '79.86%', 3: '90.16%', 4: '80.00%', 5: '87.13%'}, LR: 0.001000\n",
      "Epoch 193/200, Train Loss: 0.133650, Train-Class-Acc: {0: '97.68%', 1: '95.89%', 2: '97.40%', 3: '98.16%', 4: '94.94%', 5: '97.23%'}\n",
      "Val Loss: 3.143922, Val Acc: 80.09%, Val-Class-Acc: {0: '69.02%', 1: '78.81%', 2: '77.78%', 3: '79.51%', 4: '82.50%', 5: '88.62%'}, LR: 0.001000\n",
      "Epoch 194/200, Train Loss: 0.170358, Train-Class-Acc: {0: '96.46%', 1: '95.81%', 2: '97.57%', 3: '98.46%', 5: '97.53%', 4: '91.77%'}\n",
      "Val Loss: 3.019838, Val Acc: 81.26%, Val-Class-Acc: {0: '67.39%', 1: '80.00%', 2: '78.47%', 3: '88.11%', 4: '80.00%', 5: '86.53%'}, LR: 0.001000\n",
      "Epoch 195/200, Train Loss: 0.064695, Train-Class-Acc: {0: '98.23%', 1: '97.31%', 2: '99.13%', 3: '98.67%', 4: '93.67%', 5: '98.73%'}\n",
      "Val Loss: 3.097485, Val Acc: 81.73%, Val-Class-Acc: {0: '73.37%', 1: '74.93%', 2: '78.47%', 3: '86.89%', 4: '80.00%', 5: '91.02%'}, LR: 0.001000\n",
      "Epoch 196/200, Train Loss: 0.067131, Train-Class-Acc: {0: '98.09%', 1: '97.53%', 2: '99.13%', 3: '98.77%', 4: '98.10%', 5: '98.65%'}\n",
      "Val Loss: 4.147224, Val Acc: 80.64%, Val-Class-Acc: {0: '69.57%', 1: '72.84%', 2: '78.47%', 3: '84.02%', 4: '77.50%', 5: '93.41%'}, LR: 0.001000\n",
      "Epoch 197/200, Train Loss: 0.111658, Train-Class-Acc: {0: '97.41%', 1: '97.31%', 2: '98.61%', 3: '98.36%', 4: '94.30%', 5: '98.21%'}\n",
      "Val Loss: 3.230297, Val Acc: 80.48%, Val-Class-Acc: {0: '66.30%', 1: '76.42%', 2: '79.86%', 3: '86.07%', 4: '82.50%', 5: '88.32%'}, LR: 0.001000\n",
      "Epoch 198/200, Train Loss: 0.071615, Train-Class-Acc: {0: '98.77%', 1: '96.63%', 2: '98.09%', 3: '98.67%', 4: '96.20%', 5: '98.80%'}\n",
      "Val Loss: 3.298817, Val Acc: 80.87%, Val-Class-Acc: {0: '71.74%', 1: '76.12%', 2: '77.08%', 3: '84.84%', 4: '82.50%', 5: '89.22%'}, LR: 0.001000\n",
      "Epoch 199/200, Train Loss: 0.056840, Train-Class-Acc: {0: '98.50%', 1: '97.83%', 2: '98.09%', 3: '99.18%', 4: '96.20%', 5: '98.65%'}\n",
      "Val Loss: 3.531704, Val Acc: 80.64%, Val-Class-Acc: {0: '68.48%', 1: '74.93%', 2: '81.25%', 3: '85.66%', 4: '85.00%', 5: '88.62%'}, LR: 0.001000\n",
      "Epoch 200/200, Train Loss: 0.054986, Train-Class-Acc: {0: '98.91%', 1: '97.76%', 2: '99.48%', 3: '99.18%', 4: '96.84%', 5: '98.95%'}\n",
      "Val Loss: 3.107032, Val Acc: 79.94%, Val-Class-Acc: {0: '71.74%', 1: '68.36%', 2: '79.86%', 3: '85.66%', 4: '77.50%', 5: '92.22%'}, LR: 0.001000\n",
      "\n",
      "üèÜ Best model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_best.pth (Val Accuracy: 83.37%)\n",
      "\n",
      "üìå Final model saved as: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_final.pth\n",
      "\n",
      "üéØ Top 5 Best Models:\n",
      "Epoch 27, Train Loss: 0.898554, Train-Acc: {0: '81.61%', 1: '79.21%', 2: '88.21%', 3: '89.96%', 4: '70.25%', 5: '91.41%'},\n",
      "Val Loss: 2.055504, Val Acc: 83.37%, Val-Acc: {0: '80.98%', 1: '75.22%', 2: '82.64%', 3: '89.75%', 4: '80.00%', 5: '88.92%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_27.pth\n",
      "Epoch 16, Train Loss: 1.529238, Train-Acc: {0: '76.84%', 1: '73.45%', 2: '83.19%', 3: '86.58%', 4: '62.66%', 5: '87.22%'},\n",
      "Val Loss: 1.906408, Val Acc: 83.22%, Val-Acc: {0: '77.72%', 1: '74.63%', 2: '88.19%', 3: '91.80%', 4: '77.50%', 5: '87.13%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_16.pth\n",
      "Epoch 37, Train Loss: 0.782417, Train-Acc: {0: '85.83%', 1: '81.00%', 2: '88.39%', 3: '91.70%', 4: '75.32%', 5: '90.51%'},\n",
      "Val Loss: 1.768098, Val Acc: 83.14%, Val-Acc: {0: '81.52%', 1: '77.01%', 2: '81.94%', 3: '92.21%', 4: '75.00%', 5: '85.03%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_37.pth\n",
      "Epoch 23, Train Loss: 1.090156, Train-Acc: {0: '79.70%', 1: '76.44%', 2: '85.79%', 3: '88.83%', 4: '65.82%', 5: '89.46%'},\n",
      "Val Loss: 2.074357, Val Acc: 83.14%, Val-Acc: {0: '88.04%', 1: '72.54%', 2: '82.64%', 3: '90.16%', 4: '75.00%', 5: '87.13%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_23.pth\n",
      "Epoch 21, Train Loss: 1.346142, Train-Acc: {0: '81.34%', 1: '77.49%', 2: '85.44%', 3: '87.81%', 5: '88.49%', 4: '67.72%'},\n",
      "Val Loss: 1.989019, Val Acc: 83.06%, Val-Acc: {0: '76.63%', 1: '79.70%', 2: '85.42%', 3: '85.25%', 4: '77.50%', 5: '88.02%'}, Model Path: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/ResNet18_1D_LoRA_epoch_21.pth\n",
      "\n",
      "üß† Model Summary:\n",
      "Total Parameters: 3,891,846\n",
      "Model Size (float32): 14.85 MB\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "Number of LoRA adapters: 8\n",
      "Number of LoRA groups: 1\n",
      "Total Training Time: 508.37 seconds\n",
      "üìà LoRA Adapter Statistics:\n",
      "  - Total LoRA adapters: 8\n",
      "  - BasicBlocks with adapters: 8\n",
      "---\n",
      "### Period 3 (alpha = 0.0, similarity_threshold = 0.85)\n",
      "+ ##### Total training time: 508.37 seconds\n",
      "+ ##### Model: ResNet18_1D_LoRA\n",
      "+ ##### Training and saving in *'/mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3'*\n",
      "+ ##### Best Epoch: 27\n",
      "#### __Val Accuracy: 83.37%__\n",
      "#### __Val-Class-Acc: {0: '80.98%', 1: '75.22%', 2: '82.64%', 3: '89.75%', 4: '80.00%', 5: '88.92%'}__\n",
      "#### __Total Parameters: 3,891,846__\n",
      "#### __Model Size (float32): 14.85 MB__\n",
      "#### __Number of LoRA adapters: 8__\n",
      "#### __Number of LoRA groups: 1__\n",
      "\n",
      "Saved class features to: /mnt/mydisk/Continual_Learning_JL/Continual_Learning/Class_Incremental_CL/CPSC_CIL/Trained_models/DynEx_CLoRA_CIL_v10/Period_3/class_features.pkl\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mÂú®ÁõÆÂâçÂÑ≤Â≠òÊ†ºÊàñ‰∏ä‰∏ÄÂÄãÂÑ≤Â≠òÊ†º‰∏≠Âü∑Ë°åÁ®ãÂºèÁ¢ºÊôÇÔºåKernel Â∑≤ÊêçÊØÄ„ÄÇ\n",
      "\u001b[1;31mË´ãÊ™¢Èñ±ÂÑ≤Â≠òÊ†º‰∏≠ÁöÑÁ®ãÂºèÁ¢ºÔºåÊâæÂá∫Â§±ÊïóÁöÑÂèØËÉΩÂéüÂõ†„ÄÇ\n",
      "\u001b[1;31mÂ¶ÇÈúÄË©≥Á¥∞Ë≥áË®äÔºåË´ãÊåâ‰∏Ä‰∏ã<a href='https://aka.ms/vscodeJupyterKernelCrash'>ÈÄôË£°</a>„ÄÇ\n",
      "\u001b[1;31mÂ¶ÇÈúÄË©≥Á¥∞Ë≥áÊñôÔºåË´ãÊ™¢Ë¶ñ Jupyter <a href='command:jupyter.viewOutput'>Ë®òÈåÑ</a>„ÄÇ"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìå Period 3: DynEx-CLoRA Training (ECG)\n",
    "# ================================\n",
    "period = 3\n",
    "\n",
    "# ==== Paths ====\n",
    "stop_signal_file = os.path.join(BASE_DIR, \"stop_training.txt\")\n",
    "model_saving_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v10\", f\"Period_{period}\")\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# ==== Load Period 3 Data ====\n",
    "X_train = np.load(os.path.join(save_dir, f\"X_train_p{period}.npy\"))\n",
    "y_train = np.load(os.path.join(save_dir, f\"y_train_p{period}.npy\"))\n",
    "X_val   = np.load(os.path.join(save_dir, f\"X_test_p{period}.npy\"))\n",
    "y_val   = np.load(os.path.join(save_dir, f\"y_test_p{period}.npy\"))\n",
    "\n",
    "# ==== Device ====\n",
    "device = auto_select_cuda_device()\n",
    "\n",
    "# ==== Load Period 2 Features ====\n",
    "prev_folder = os.path.join(BASE_DIR, \"Trained_models\", \"DynEx_CLoRA_CIL_v10\", \"Period_2\")\n",
    "class_features_path = os.path.join(prev_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, \"rb\") as f:\n",
    "    class_features_dict = pickle.load(f)\n",
    "print(f\"‚úÖ Loaded class features from: {class_features_path}\")\n",
    "\n",
    "# ==== Load Period 2 Checkpoint ====\n",
    "prev_model_path = os.path.join(prev_folder, \"ResNet18_1D_LoRA_best.pth\")\n",
    "checkpoint = torch.load(prev_model_path, map_location=device)\n",
    "\n",
    "input_channels = X_train.shape[2]\n",
    "output_size = len(np.unique(y_train))\n",
    "\n",
    "# ==== Build Teacher Model ====\n",
    "teacher_model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size - 2).to(device)\n",
    "teacher_model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "teacher_model.eval()\n",
    "\n",
    "# ==== Build Student Model ====\n",
    "model = ResNet18_1D_LoRA(input_channels=input_channels, output_size=output_size).to(device)\n",
    "\n",
    "# ==== Sync Adapter Structure ====\n",
    "num_lora_groups = checkpoint.get(\"num_lora_groups\", 0)\n",
    "print(f\"üîÑ Number of LoRA groups: {num_lora_groups}\")\n",
    "related_labels = checkpoint.get(\"related_labels\", {\"base\": [0, 1]})\n",
    "for _ in range(num_lora_groups):\n",
    "    model.add_lora_adapter()\n",
    "\n",
    "# ==== Load Previous Weights (excluding FC and LoRA) ====\n",
    "model_dict = model.state_dict()\n",
    "prev_state_dict = checkpoint[\"model_state_dict\"]\n",
    "filtered_dict = {\n",
    "    k: v for k, v in prev_state_dict.items()\n",
    "    if k in model_dict and model_dict[k].shape == v.shape and k not in [\"fc.weight\", \"fc.bias\"]\n",
    "}\n",
    "model.load_state_dict(filtered_dict, strict=False)\n",
    "for k in model_dict:\n",
    "    if k not in filtered_dict:\n",
    "        print(f\"üîç Not loaded: {k}, shape={model_dict[k].shape}\")\n",
    "print(\"‚úÖ Loaded shared weights from Period 2 (excluding FC only)\")\n",
    "\n",
    "# ==== Training Configuration ====\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "alpha = 0.0 # No distillation\n",
    "similarity_threshold = 0.85\n",
    "stable_classes = [0, 2, 3]  # Ê†πÊìö Period 2 class ÁµêÊûú\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# ==== Start Training ====\n",
    "train_with_dynex_clora_ecg(\n",
    "    model=model,\n",
    "    teacher_model=None,\n",
    "    output_size=output_size,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    alpha=alpha,\n",
    "    model_saving_folder=model_saving_folder,\n",
    "    model_name=\"ResNet18_1D_LoRA\",\n",
    "    stop_signal_file=stop_signal_file,\n",
    "    scheduler=scheduler,\n",
    "    period=period,\n",
    "    stable_classes=stable_classes,\n",
    "    similarity_threshold=similarity_threshold,\n",
    "    class_features_dict=class_features_dict,\n",
    "    related_labels=related_labels,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ==== Cleanup ====\n",
    "del X_train, y_train, X_val, y_val\n",
    "del model, teacher_model, checkpoint, model_dict, prev_state_dict, filtered_dict\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
