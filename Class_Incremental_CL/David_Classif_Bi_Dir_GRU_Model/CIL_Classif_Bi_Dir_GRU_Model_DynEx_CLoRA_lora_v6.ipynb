{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *__Working on BTCUSD predictions with GRU model(DynEx_CLoRA)__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Check first before starting__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change the working directory to the project root\n",
    "Working_directory = os.path.normpath(\"C:/Users/james/OneDrive/文件/Continual_Learning\")\n",
    "# Working_directory = os.path.normpath(\"/mnt/mydisk/Continual_Learning\")\n",
    "os.chdir(Working_directory)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __All imports__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating system and file management\n",
    "import os\n",
    "import shutil\n",
    "import contextlib\n",
    "import traceback\n",
    "import gc\n",
    "\n",
    "# Jupyter notebook widgets and display\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_interactions import zoom_factory, panhandler\n",
    "\n",
    "# Machine learning and preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from ta import trend, momentum, volatility, volume\n",
    "\n",
    "# Mathematical and scientific computing\n",
    "import math\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Type hinting\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "# Deep learning with PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __All functions (For data processing)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_folder(folder_path: str) -> None:\n",
    "    \"\"\"Ensure the given folder exists, create it if not.\"\"\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "def plot_with_matplotlib(data: pd.DataFrame, \n",
    "                         title: str, \n",
    "                         interactive: bool = False, \n",
    "                         save_path: str = None, \n",
    "                         show_plot: bool = True, \n",
    "                         save_matplotlib_object: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Plot time-series data using Matplotlib with optional trend-based coloring.\n",
    "\n",
    "    Args:\n",
    "        - data (pd.DataFrame): Data containing a 'close' column (required).\n",
    "        - title (str): Plot title.\n",
    "        - interactive (bool): Enable zoom & pan if True.\n",
    "        - save_path (str, optional): Path to save the figure.\n",
    "        - show_plot (bool): Whether to display the plot.\n",
    "        - save_matplotlib_object (str, optional): Path to save the Matplotlib object.\n",
    "\n",
    "    Returns:\n",
    "        - None: Displays or saves the plot as specified.\n",
    "    \"\"\"\n",
    "    # Check if 'close' column exists\n",
    "    if 'close' not in data.columns:\n",
    "        raise ValueError(\"DataFrame must contain a 'close' column.\")\n",
    "\n",
    "    # Set default color from Matplotlib cycle\n",
    "    default_blue = plt.rcParams['axes.prop_cycle'].by_key()['color'][0]\n",
    "    \n",
    "    # Define colors for different trends\n",
    "    trend_colors = {\n",
    "        0: 'black',\n",
    "        1: 'yellow',\n",
    "        2: 'red',\n",
    "        3: 'green',\n",
    "        4: default_blue\n",
    "    }\n",
    "\n",
    "    # Create figure and axis for plotting\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot with trend-based coloring if 'trend' column exists\n",
    "    if 'trend' in data.columns:\n",
    "        legend_added = set()\n",
    "        prev_idx = data.index[0]\n",
    "        for idx, row in data.iterrows():\n",
    "            if idx != prev_idx:\n",
    "                trend_key = int(row['trend'])\n",
    "                label = f'Trend {trend_key}' if trend_key not in legend_added else None\n",
    "                ax.plot([prev_idx, idx], \n",
    "                        [data.loc[prev_idx, 'close'], row['close']],\n",
    "                        color=trend_colors[trend_key], \n",
    "                        linestyle='-', \n",
    "                        linewidth=1,\n",
    "                        label=label)\n",
    "                legend_added.add(trend_key)\n",
    "            prev_idx = idx\n",
    "        ax.set_title(f\"{title} (Connected, Colored by Trend)\")\n",
    "    else:\n",
    "        # Plot default line if no 'trend' column\n",
    "        ax.plot(data.index, data['close'], label='Closing Price', linestyle='-', marker='o', \n",
    "                markersize=2, linewidth=1, color=default_blue, markerfacecolor='green', markeredgecolor='black')\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    # Set axis labels and add legend/grid\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Closing Price (USD)')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    \n",
    "    # Enable interactive features if requested\n",
    "    if interactive:\n",
    "        zoom_factory(ax)\n",
    "        panhandler(fig)\n",
    "\n",
    "    # Save the plot if a path is provided\n",
    "    if save_path:\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # Save the Matplotlib object if requested\n",
    "    if save_matplotlib_object:\n",
    "        with open(save_matplotlib_object, 'wb') as f:\n",
    "            pickle.dump(fig, f)\n",
    "\n",
    "    # Display the plot if requested\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "\n",
    "def load_and_show_pickle(pickle_file_path: str):\n",
    "    \"\"\"\n",
    "    Load a pickled Matplotlib figure object and display it.\n",
    "\n",
    "    Args:\n",
    "        - pickle_file_path (str): Path to the pickled Matplotlib figure file.\n",
    "\n",
    "    Returns:\n",
    "        - None: Displays the loaded figure.\n",
    "    \"\"\"\n",
    "    # Load and display the pickled figure\n",
    "    try:\n",
    "        with open(pickle_file_path, \"rb\") as f:\n",
    "            loaded_fig = pickle.load(f)\n",
    "\n",
    "        print(f\"Figure successfully loaded and displayed from: {pickle_file_path}\")\n",
    "        plt.show(block=True)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {pickle_file_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the pickled figure: {e}\")\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save DataFrame to CSV.\n",
    "    \"\"\"\n",
    "    df.to_csv(file_path)\n",
    "    print(f\"\\nSuccessfully saved data with moving average to CSV: \\n\\t{file_path}\\n\")\n",
    "\n",
    "def read_csv_file(file_path: str, preview_rows: int = 5, \n",
    "                  days_towards_end: int = None, \n",
    "                  days_from_start: int = None, description: str = \"\"):\n",
    "    \"\"\"\n",
    "    Reads a CSV file and returns a pandas DataFrame filtered by date range.\n",
    "\n",
    "    Args:\n",
    "        - file_path (str): The path to the CSV file.\n",
    "        - preview_rows (int): Number of rows to preview (default is 5).\n",
    "        - days_towards_end (int, optional): Number of days from the most recent date.\n",
    "        - days_from_start (int, optional): Number of days from the oldest date of filtered data.\n",
    "        - description (str): A brief description of the dataset.\n",
    "                           Explanation:\n",
    "                           - To retrieve data from the **end**: Use `days_towards_end`.\n",
    "                           - To retrieve data from the **start of the filtered range**: Use `days_from_start`.\n",
    "                           - To retrieve data from the **middle**: Use both:\n",
    "                             For example, if `days_towards_end=100` and `days_from_start=50`,\n",
    "                             the function will first filter the last 100 days of the dataset,\n",
    "                             and then filter the first 50 days from this range.\n",
    "                             This results in data between the last 100th and the last 50th day.\n",
    "\n",
    "    Returns:\n",
    "        - pd.DataFrame: The loaded and filtered data from the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if description:\n",
    "            print(f\"\\nDescription: {description}\")\n",
    "        print(f\"\\nFile path: {file_path}\")\n",
    "        \n",
    "        # Read the CSV file\n",
    "        data = pd.read_csv(file_path, parse_dates=['date'], index_col='date')\n",
    "        \n",
    "        # Filter by days towards the end\n",
    "        if days_towards_end is not None:\n",
    "            # Get the most recent date in the dataset\n",
    "            last_date = data.index.max()\n",
    "            end_cutoff_date = last_date - pd.Timedelta(days=days_towards_end)\n",
    "            data = data[data.index >= end_cutoff_date]\n",
    "            print(f\"\\nRetrieving data from the past {days_towards_end} days (from {end_cutoff_date.date()} onwards):\")\n",
    "        \n",
    "        # Filter by days from the start (from the filtered data)\n",
    "        if days_from_start is not None:\n",
    "            # Get the earliest date in the filtered dataset\n",
    "            first_date = data.index.min()\n",
    "            start_cutoff_date = first_date + pd.Timedelta(days=days_from_start)\n",
    "            data = data[data.index <= start_cutoff_date]\n",
    "            print(f\"\\nRetrieving the first {days_from_start} days from the filtered data (up to {start_cutoff_date.date()}):\")\n",
    "\n",
    "        if preview_rows:\n",
    "            # Print a preview of the data\n",
    "            print(f\"\\nPreview of the first {preview_rows} rows:\")\n",
    "            display(data.head(preview_rows))\n",
    "            print()\n",
    "\n",
    "            print(f\"\\nPreview of the last {preview_rows} rows:\")\n",
    "            display(data.tail(preview_rows))\n",
    "            print()\n",
    "\n",
    "        return data\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File not found.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The file is empty.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error: File parsing failed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "def downsample_minute_data(data: pd.DataFrame, n: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downsample minute data into N-minute intervals by retaining every Nth row.\n",
    "\n",
    "    Args:\n",
    "        - data (pd.DataFrame): The original DataFrame with a datetime index.\n",
    "        - n (int): The number of minutes for the downsampling interval.\n",
    "\n",
    "    Returns:\n",
    "        - pd.DataFrame: Downsampled DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\n========---> Downsampling the data! \\n\")\n",
    "    data = data.copy()\n",
    "\n",
    "    # Ensure index is a DatetimeIndex\n",
    "    if not isinstance(data.index, pd.DatetimeIndex):\n",
    "        try:\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"DataFrame index conversion to DatetimeIndex failed.\") from e\n",
    "\n",
    "    # Downsample by selecting rows where minute % N == 0\n",
    "    return data[data.index.minute % n == 0]\n",
    "\n",
    "def calculate_log_returns_all_columns(data: pd.DataFrame, exclude_columns: list = [], dropna: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate log returns for all numeric columns in a pandas DataFrame,\n",
    "    excluding specified columns, and removing excluded columns from the returned DataFrame.\n",
    "\n",
    "    Args:\n",
    "        - data (pd.DataFrame): Input DataFrame containing numeric data.\n",
    "        - exclude_columns (list): List of columns to exclude from log return calculations and the result.\n",
    "        - dropna (bool): Whether to drop rows with NaN values resulting from the calculation.\n",
    "\n",
    "    Returns:\n",
    "        - pd.DataFrame: DataFrame with log returns for numeric columns, excluding specified columns.\n",
    "    \"\"\"\n",
    "    # Copy data and remove excluded columns\n",
    "    data = data.copy().drop(columns=exclude_columns)\n",
    "    \n",
    "    # Select numeric columns for transformation\n",
    "    columns_to_transform = data.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"columns_to_transform = \\n{columns_to_transform}, \\nlen(columns_to_transform) = {len(columns_to_transform)}\")\n",
    "\n",
    "    # Calculate log returns for each numeric column\n",
    "    for col in columns_to_transform:\n",
    "        if (data[col] <= 0).any():\n",
    "            raise ValueError(f\"Column '{col}' contains non-positive values. Log returns require strictly positive values.\")\n",
    "        data[col] = np.log(data[col] / data[col].shift(1))\n",
    "\n",
    "    # Return data with or without NaN rows based on dropna\n",
    "    return data.dropna() if dropna else data\n",
    "\n",
    "def created_sequences_2(data: pd.DataFrame, sequence_length: int = 60, sliding_interval: int = 60) -> list:\n",
    "    \"\"\"\n",
    "    Divide the dataset into sequences based on the sequence_length.\n",
    "    Each sequence must fully cover the window size.\n",
    "\n",
    "    Args:\n",
    "    - data (pd.DataFrame): The input DataFrame.\n",
    "    - sequence_length (int): The window size for sequences.\n",
    "\n",
    "    Returns:\n",
    "    - sequences (list): A list of sequences (as DataFrames).\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    \n",
    "    # Iterate over the data with a sliding window to create sequences\n",
    "    for i in range(0, len(data) - sequence_length + 1, sliding_interval):\n",
    "        # Extract a sequence of specified length from the DataFrame\n",
    "        seq = data.iloc[i:i + sequence_length].copy()\n",
    "        sequences.append(seq)\n",
    "\n",
    "    return sequences\n",
    "\n",
    "def gaussian_smoothing(data: pd.DataFrame, sigma=2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies Gaussian smoothing to numeric columns in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        - data (pd.DataFrame): Input DataFrame.\n",
    "        - sigma (float): Standard deviation for the Gaussian kernel (default is 2).\n",
    "\n",
    "    Returns:\n",
    "        - pd.DataFrame: Smoothed DataFrame with sorted index.\n",
    "    \"\"\"\n",
    "    # Sort data by index in ascending order and create a copy\n",
    "    data = data.sort_index(ascending=True).copy()\n",
    "    \n",
    "    # Apply Gaussian smoothing to numeric columns\n",
    "    for column in data.columns:\n",
    "        if pd.api.types.is_numeric_dtype(data[column]):\n",
    "            data[column] = gaussian_filter1d(data[column].values, sigma=sigma)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def detect_trends_4(\n",
    "    dataframe: pd.DataFrame, \n",
    "    column: str = 'close', \n",
    "    lower_threshold: float = 0.001, \n",
    "    upper_threshold: float = 0.02,\n",
    "    reverse_steps: int = 7,\n",
    "    trends_to_keep: set = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detects trends based on log return data provided in a specified column and categorizes them into different strength levels.\n",
    "\n",
    "    This function analyzes time-series data by evaluating cumulative trends in log return values provided in the input DataFrame. \n",
    "    It uses three dictionaries (`dic1`, `dic2`, `dic3`) to track different phases of trends, handles multi-step reversals, and \n",
    "    classifies trends dynamically based on cumulative product thresholds and specified thresholds for trend strengths.\n",
    "\n",
    "    Args:\n",
    "        - dataframe (pd.DataFrame): Input DataFrame with log return data.\n",
    "        - column (str): Column name for log returns (default is 'close').\n",
    "        - lower_threshold (float): Threshold for moderate trends (default is 0.001).\n",
    "        - upper_threshold (float): Threshold for strong trends (default is 0.02).\n",
    "        - reverse_steps (int): Steps to confirm trend reversal (default is 7).\n",
    "        - trends_to_keep (set): Trends to retain, others set to 0 (default is {0, 1, 2, 3, 4}).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'trend' column:\n",
    "                        - 0: No trend\n",
    "                        - 1: Moderate negative trend\n",
    "                        - 2: Very strong negative trend\n",
    "                        - 3: Moderate positive trend\n",
    "                        - 4: Very strong positive trend\n",
    "                      Any trends not included in `trends_to_keep` will be reset to 0.\n",
    "\n",
    "    Function Details:\n",
    "    1. **Input Assumption**:\n",
    "    - The input DataFrame already contains log return data in the specified column (`column`).\n",
    "\n",
    "    2. **Trend Tracking**:\n",
    "    - Uses dictionaries to monitor trends:\n",
    "        - `dic1`: Tracks the first phase of the trend.\n",
    "        - `dic2`: Tracks the second phase if a reversal occurs.\n",
    "        - `dic3`: Tracks the third phase if another reversal occurs.\n",
    "\n",
    "    3. **Cumulative Product**:\n",
    "    - Calculates the cumulative product of `(1 + log_return)` from the specified column to evaluate the strength of trends.\n",
    "\n",
    "    4. **Reversal Handling**:\n",
    "    - If a trend reversal persists beyond `reverse_steps`, labels are assigned based on the cumulative product tracked in `dic1`.\n",
    "    - Subsequent reversals are merged or labeled independently if conditions are met.\n",
    "\n",
    "    5. **Label Assignment**:\n",
    "    - Labels are dynamically assigned based on cumulative product thresholds for positive and negative trends:\n",
    "        - Positive trends are categorized as moderate (3) or strong (4).\n",
    "        - Negative trends are categorized as moderate (1) or strong (2).\n",
    "\n",
    "    6. **Trend Filtering**:\n",
    "    - After detecting trends, only those specified in `trends_to_keep` remain unchanged.\n",
    "    - Any trend category not included in `trends_to_keep` is reset to 0 (No Trend).\n",
    "\n",
    "    7. **Edge Cases**:\n",
    "    - Properly handles scenarios where data points are insufficient for trend analysis or when trend phases overlap, ensuring all data points are labeled.\n",
    "    \"\"\"\n",
    "    # Copy to avoid modifying the original DataFrame\n",
    "    df = dataframe.copy()\n",
    "    df['trend'] = None  # Default value \n",
    "\n",
    "    dic1, dic2, dic3 = None, None, None # Initialize trend tracking dictionaries\n",
    "    \n",
    "    def assign_label(dictio_, lower_threshold, upper_threshold):\n",
    "        cumulative = dictio_['cumulative']\n",
    "        if cumulative > (1 + upper_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 4  # Very strong positive\n",
    "        elif (1 + lower_threshold) < cumulative <= (1 + upper_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 3  # Moderate positive\n",
    "        elif (1 - upper_threshold) < cumulative <= (1 - lower_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 1  # Moderate negative\n",
    "        elif cumulative <= (1 - upper_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 2  # Very strong negative\n",
    "        else:\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 0  # No trend\n",
    "    \n",
    "    # Process each log return to detect trends\n",
    "    for idx, log_ret in enumerate(df[column]):\n",
    "        sign = 1 if log_ret > 0 else -1\n",
    "\n",
    "        if dic1 is None:  # Initialize dic1\n",
    "            dic1 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "            continue\n",
    "\n",
    "        last_sign = dic1['last_sign']\n",
    "        if sign == last_sign and dic2 is None:  # Continue same trend\n",
    "            dic1['ids'].append(idx)\n",
    "            dic1['last_sign'] = sign\n",
    "            dic1['cumulative'] *= (1 + log_ret)\n",
    "            continue\n",
    "\n",
    "        # 1st Reversal occuring\n",
    "        if dic2 is None:  # Start dic2\n",
    "            dic2 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "            continue\n",
    "\n",
    "        last_sign = dic2['last_sign']\n",
    "        if sign == last_sign and dic3 is None:  # Continue same trend\n",
    "            dic2['ids'].append(idx)\n",
    "            dic2['last_sign'] = sign\n",
    "            dic2['cumulative'] *= (1 + log_ret)\n",
    "            if len(dic2['ids']) == reverse_steps:\n",
    "                assign_label(dic1, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "                dic1, dic2 = dic2, None\n",
    "            continue\n",
    "\n",
    "        # 2nd Reversal occuring\n",
    "        if dic3 is None:  # Start dic3\n",
    "            dic3 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "            continue\n",
    "\n",
    "        last_sign = dic3['last_sign']\n",
    "        if sign == last_sign: # Continue same trend, there is no dic4 to check if is None\n",
    "            dic3['ids'].append(idx)\n",
    "            dic3['last_sign'] = sign\n",
    "            dic3['cumulative'] *= (1 + log_ret)\n",
    "            dic_prod = dic2['cumulative'] * dic3['cumulative']\n",
    "            if (sign == 1 and dic_prod > 1) or (sign == -1 and dic_prod < 1):\n",
    "                dic1['ids'] += dic2['ids'] + dic3['ids']\n",
    "                dic1['last_sign'] = dic3['last_sign']\n",
    "                dic1['cumulative'] *= dic2['cumulative'] * dic3['cumulative']\n",
    "                dic2, dic3 = None, None\n",
    "                continue\n",
    "\n",
    "            if len(dic3['ids']) == reverse_steps:      \n",
    "                assign_label(dic1, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "                assign_label(dic2, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "                dic1, dic2, dic3 = dic3, None, None\n",
    "            continue\n",
    "            \n",
    "        # 3rd Reversal occuring\n",
    "        assign_label(dic1, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "        dic1, dic2, dic3 = dic2, dic3, {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "\n",
    "    # Assign remaining labels\n",
    "    if dic1:\n",
    "        assign_label(dic1, lower_threshold, upper_threshold)\n",
    "    if dic2:\n",
    "        assign_label(dic2, lower_threshold, upper_threshold)\n",
    "    if dic3:\n",
    "        assign_label(dic3, lower_threshold, upper_threshold)\n",
    "    \n",
    "    # Apply filtering: Keep only selected trends, set others to 0\n",
    "    df['trend'] = df['trend'].where(df['trend'].isin(trends_to_keep), 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def split_X_y(sequences: list[pd.DataFrame], \n",
    "              target_column: str = 'trend',\n",
    "              detect_trends_function: Callable[[pd.DataFrame, str, float, float, int, set], pd.DataFrame] = detect_trends_4, \n",
    "              column: str = 'close', \n",
    "              lower_threshold: float = 0.0009, \n",
    "              upper_threshold: float = 0.015,\n",
    "              reverse_steps: int = 7,\n",
    "              trends_to_keep: set = {0, 1, 2, 3, 4}) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Process sequences to generate features (X) and labels (y) with trend detection.\n",
    "\n",
    "    Args:\n",
    "        - sequences (list[pd.DataFrame]): List of DataFrame sequences.\n",
    "        - target_column (str): Column name for labels (default is 'trend').\n",
    "        - detect_trends_function (Callable): Trend detection function (default is detect_trends_4).\n",
    "        - column (str): Column for trend detection (default is 'close').\n",
    "        - lower_threshold (float): Lower threshold for trends (default is 0.0009).\n",
    "        - upper_threshold (float): Upper threshold for trends (default is 0.015).\n",
    "        - reverse_steps (int): Steps for trend reversal (default is 7).\n",
    "        - trends_to_keep (set): Trends to retain (default is {0, 1, 2, 3, 4}).\n",
    "\n",
    "    Returns:\n",
    "        - Tuple[np.ndarray, np.ndarray]: X (features), y (labels) as NumPy arrays.\n",
    "    \"\"\"\n",
    "    # Initialize lists for features and labels\n",
    "    X, y = [], []\n",
    "    \n",
    "    # Process each sequence\n",
    "    for seq in sequences:\n",
    "        # Apply trend detection\n",
    "        seq = detect_trends_function(seq, column, lower_threshold, upper_threshold, reverse_steps, trends_to_keep)\n",
    "        \n",
    "        # Extract features and labels\n",
    "        X.append(seq.drop(columns=[target_column]).values)\n",
    "        y.append(seq[target_column].values)\n",
    "    \n",
    "    # Convert to NumPy arrays\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def process_and_return_splits(\n",
    "    with_indicators_file_path: str,\n",
    "    downsampled_data_minutes: int,\n",
    "    exclude_columns: list[str],\n",
    "    lower_threshold: float,\n",
    "    upper_threshold: float,\n",
    "    reverse_steps: int,\n",
    "    sequence_length: int,\n",
    "    sliding_interval: int,\n",
    "    trends_to_keep: set = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    ") -> tuple[\n",
    "    list[list[float]],  # X_train: List of sequences, each containing a list of features\n",
    "    list[list[int]],    # y_train: List of sequences, each containing a list of labels\n",
    "    list[list[float]],  # X_val: List of sequences, each containing a list of features\n",
    "    list[list[int]],    # y_val: List of sequences, each containing a list of labels\n",
    "    list[list[float]],  # X_test: List of sequences, each containing a list of features\n",
    "    list[list[int]]     # y_test: List of sequences, each containing a list of labels\n",
    "]:\n",
    "    \"\"\"\n",
    "    Processes time-series data from a CSV file and prepares it for machine learning.\n",
    "\n",
    "    This function performs the following steps:\n",
    "        1. Reads data from the specified CSV file and sorts it by date in descending order.\n",
    "        2. Optionally downsamples the data to a lower frequency (e.g., 5-minute intervals).\n",
    "        3. Applies Gaussian smoothing to reduce noise in the data.\n",
    "        4. Calculates log returns for all numeric columns, excluding specified columns.\n",
    "        5. Detects trends based on defined thresholds (`lower_threshold`, `upper_threshold`, and `reverse_steps`).\n",
    "        6. Filters trends to keep only those specified in `trends_to_keep`, setting others to 0 (No Trend).\n",
    "        7. Converts the processed data into sequences of a fixed length (`sequence_length`) with a sliding interval.\n",
    "        8. Splits the sequences into training (80%), validation (10%), and test (10%) sets.\n",
    "        9. Further splits the sequences into features (`X`) and labels (`y`) for supervised learning.\n",
    "\n",
    "    Args:\n",
    "        - with_indicators_file_path (str): Path to the CSV file with time-series data.\n",
    "        - downsampled_data_minutes (int): Frequency for downsampling (e.g., 1 for no downsampling).\n",
    "        - exclude_columns (list[str]): Columns to exclude from log return calculations.\n",
    "        - lower_threshold (float): Lower threshold for trend detection.\n",
    "        - upper_threshold (float): Upper threshold for trend detection.\n",
    "        - reverse_steps (int): Steps for reversing trends in trend detection.\n",
    "        - sequence_length (int): Length of sequences to create.\n",
    "        - sliding_interval (int): Interval for sliding the window.\n",
    "        - trends_to_keep (set): Trends to retain, others set to 0 (default is {0, 1, 2, 3, 4}).\n",
    "\n",
    "    Returns:\n",
    "        - tuple: X_train, y_train, X_val, y_val, X_test, y_test as lists of sequences.\n",
    "    \"\"\"\n",
    "    def check_missing_timestamps(data: pd.DataFrame, stage: str):\n",
    "        \"\"\"\n",
    "        Checks for missing timestamps and prints diagnostic info.\n",
    "        \"\"\"\n",
    "        missing_timestamps = pd.date_range(\n",
    "            start=data.index.min(),\n",
    "            end=data.index.max(),\n",
    "            freq='1min',  # Checking 1-minute frequency\n",
    "            tz=data.index.tz,\n",
    "        ).difference(data.index)\n",
    "\n",
    "        print(f\"\\n{stage} - Missing timestamps: \\n{missing_timestamps}\")\n",
    "\n",
    "        if not missing_timestamps.empty:\n",
    "            for timestamp in missing_timestamps[:5]:  # Show only first 5 missing timestamps\n",
    "                print(f\"\\nMissing timestamp: {timestamp}\")\n",
    "\n",
    "                before = data[data.index < timestamp].tail(5)  # 5 data points before\n",
    "                after = data[data.index > timestamp].head(5)  # 5 data points after\n",
    "\n",
    "                print(\"\\nData before missing timestamp:\")\n",
    "                display(before) if not before.empty else print(\"No data available before.\")\n",
    "\n",
    "                print(\"\\nData after missing timestamp:\")\n",
    "                display(after) if not after.empty else print(\"No data available after.\")\n",
    "\n",
    "    print(\"\\n======== Processing Time-Series Data ========\")\n",
    "\n",
    "    # Step 1: Read & Sort Data\n",
    "    data = read_csv_file(with_indicators_file_path, preview_rows=0).sort_index(ascending=False)\n",
    "\n",
    "    # Step 2: Downsample Data\n",
    "    if downsampled_data_minutes != 1:\n",
    "        print(\"\\n---> Downsampling Data\")\n",
    "        data = downsample_minute_data(data, downsampled_data_minutes)\n",
    "\n",
    "    check_missing_timestamps(data, \"Data Retrieved\")\n",
    "\n",
    "    # Step 3: Gaussian Smoothing\n",
    "    data = gaussian_smoothing(data, sigma=7)\n",
    "    check_missing_timestamps(data, \"Gaussian Smoothed Data\")\n",
    "\n",
    "    # Step 4: Compute Log Returns\n",
    "    data = calculate_log_returns_all_columns(data, exclude_columns=exclude_columns)\n",
    "    check_missing_timestamps(data, \"Log Returns Computed\")\n",
    "\n",
    "    # Step 5: Create Sequences\n",
    "    sequences = created_sequences_2(data, sequence_length, sliding_interval)\n",
    "\n",
    "    # Step 6: Train / Validation / Test Split\n",
    "    train_size = int(len(sequences) * 0.8)\n",
    "    val_size = int(len(sequences) * 0.1)\n",
    "\n",
    "    train_sequences = sequences[:train_size]\n",
    "    val_sequences = sequences[train_size:train_size + val_size]\n",
    "    test_sequences = sequences[train_size + val_size:]\n",
    "\n",
    "    print(f\"\\nNumber of sequences:\\n\"\n",
    "          f\"  - Total sequences: {len(sequences)}\\n\"\n",
    "          f\"  - Train: {len(train_sequences)}\\n\"\n",
    "          f\"  - Validation: {len(val_sequences)}\\n\"\n",
    "          f\"  - Test: {len(test_sequences)}\\n\")\n",
    "\n",
    "    # Step 7: Convert Sequences to X, y\n",
    "    def split_and_format_data(sequences):\n",
    "        X, y = split_X_y(\n",
    "            sequences, target_column='trend',\n",
    "            detect_trends_function=detect_trends_4,\n",
    "            column='close', lower_threshold=lower_threshold,\n",
    "            upper_threshold=upper_threshold, reverse_steps=reverse_steps,\n",
    "            trends_to_keep=trends_to_keep\n",
    "        )\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    X_train, y_train = split_and_format_data(train_sequences)\n",
    "    X_val, y_val = split_and_format_data(val_sequences)\n",
    "    X_test, y_test = split_and_format_data(test_sequences)\n",
    "\n",
    "    # Step 8: Data Integrity Check (Ensuring Proper Types)\n",
    "    def check_data_types(X: np.ndarray, y: np.ndarray, label: str):\n",
    "        \"\"\"\n",
    "        Checks if all values in X are float and y are integer.\n",
    "        \"\"\"\n",
    "        unexpected_X = [(i, j, k, type(v)) for i, seq in enumerate(X)\n",
    "                        for j, row in enumerate(seq)\n",
    "                        for k, v in enumerate(row) if not isinstance(v, (float, np.float32))]\n",
    "        unexpected_y = [(i, j, type(v)) for i, seq in enumerate(y)\n",
    "                        for j, v in enumerate(seq) if not isinstance(v, (int, np.int64))]\n",
    "\n",
    "        if unexpected_X:\n",
    "            print(f\"\\n⚠️ Unexpected type in {label} X:\")\n",
    "            for i, j, k, t in unexpected_X[:5]:  # Show first 5 errors\n",
    "                print(f\"  Sequence {i}, Row {j}, Feature {k}: {t}\")\n",
    "\n",
    "        if unexpected_y:\n",
    "            print(f\"\\n⚠️ Unexpected type in {label} y:\")\n",
    "            for i, j, t in unexpected_y[:5]:  # Show first 5 errors\n",
    "                print(f\"  Sequence {i}, Label {j}: {t}\")\n",
    "\n",
    "    check_data_types(X_train, y_train, \"Train\")\n",
    "    check_data_types(X_val, y_val, \"Validation\")\n",
    "    check_data_types(X_test, y_test, \"Test\")\n",
    "\n",
    "    # Step 9: Convert y types if needed\n",
    "    def convert_dtype(y: np.ndarray):\n",
    "        return np.array(y, dtype=np.int64) if isinstance(y, np.ndarray) and y.dtype == np.object_ else y\n",
    "\n",
    "    y_train, y_val, y_test = map(convert_dtype, [y_train, y_val, y_test])\n",
    "\n",
    "    # Get feature info\n",
    "    Number_features = X_train.shape[-1]\n",
    "    close_col_index = data.columns.get_loc('close')\n",
    "    \n",
    "    print(f\"\\nFeature Info:\\n  - close_col_index = {close_col_index}\\n  - Number_features = {Number_features}\")\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, Number_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __All (Initial) parameters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'BTC-USD'\n",
    "downsampled_data_minutes = 1 # No downsampling\n",
    "\n",
    "# Step 0 (Again): Identify parameters for trend settings of the loaded data with 1,000 data points\n",
    "lower_threshold = 0.0009 # 較小的價格變動門檻，代表 輕微的趨勢變化 也可能被識別為趨勢。\n",
    "upper_threshold = 0.015  # 較大的價格變動門檻，當變動超過這個值，才會標記為強趨勢。\n",
    "reverse_steps = 13       # 趨勢反轉的步數門檻，當價格變動連續 13 次反向時，才認為趨勢改變。\n",
    "\n",
    "# Features not to be included in the analysis\n",
    "exclude_columns= ['MACD', 'MACD_signal', 'ROC_10', 'OBV', 'AD_Line']\n",
    "\n",
    "# Step 3, under ### Correlation Analysis\n",
    "# Compute correlations with the 'trend' column\n",
    "# corr = data_trends.corr()\n",
    "# trend_corr = corr['trend'].sort_values(ascending=False)\n",
    "strongly_correlated = ['close', 'open', 'SMA_5', 'high', 'low', 'EMA_10', 'SMA_10'] # Strongly correlated (correlation > 0.6)\n",
    "moderately_correlated = ['BB_middle', 'BB_lower', 'BB_upper', 'RSI_14']             # Moderately correlated (correlation between 0.3 and 0.6)\n",
    "weakly_correlated = ['SMA_50', 'volume', 'BBW', 'ATR_14']                           # Weakly correlated or negligible (correlation <~ 0.3)\n",
    "\n",
    "# Add the weakly_correlated and moderately_correlated features to exclude_columns.\n",
    "exclude_columns += weakly_correlated + moderately_correlated\n",
    "\n",
    "sequence_length = 1000\n",
    "sliding_interval = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Check GPU, CUDA, Pytorch__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_config():\n",
    "    \"\"\"\n",
    "    Check GPU availability and display detailed configuration information.\n",
    "    \"\"\"\n",
    "    # Check if GPU is available\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    \n",
    "    # Print header\n",
    "    print(\"=\" * 50)\n",
    "    print(\"GPU Configuration Check\".center(50))\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic GPU availability\n",
    "    print(f\"{'PyTorch Version':<25}: {torch.__version__}\")\n",
    "    print(f\"{'GPU Available':<25}: {'Yes' if gpu_available else 'No'}\")\n",
    "    \n",
    "    # If GPU is available, print detailed info\n",
    "    if gpu_available:\n",
    "        print(\"-\" * 50)\n",
    "        print(\"GPU Details\".center(50))\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Device info\n",
    "        print(f\"{'Device Name':<25}: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"{'Number of GPUs':<25}: {torch.cuda.device_count()}\")\n",
    "        print(f\"{'Current Device Index':<25}: {torch.cuda.current_device()}\")\n",
    "        \n",
    "        # Compute capability and CUDA cores\n",
    "        props = torch.cuda.get_device_properties(0)\n",
    "        print(f\"{'Compute Capability':<25}: {props.major}.{props.minor}\")\n",
    "        print(f\"{'Total CUDA Cores':<25}: {props.multi_processor_count * 128}\")  # Approx. 128 cores per SM\n",
    "        \n",
    "        # Memory info\n",
    "        total_memory = props.total_memory / (1024 ** 3)  # Convert to GB\n",
    "        memory_allocated = torch.cuda.memory_allocated(0) / (1024 ** 3)\n",
    "        memory_reserved = torch.cuda.memory_reserved(0) / (1024 ** 3)\n",
    "        print(f\"{'Total Memory (GB)':<25}: {total_memory:.2f}\")\n",
    "        print(f\"{'Allocated Memory (GB)':<25}: {memory_allocated:.2f}\")\n",
    "        print(f\"{'Reserved Memory (GB)':<25}: {memory_reserved:.2f}\")\n",
    "    else:\n",
    "        print(\"-\" * 50)\n",
    "        print(\"No GPU detected. Running on CPU.\".center(50))\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_gpu_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_torch_config():\n",
    "    \"\"\"Print PyTorch and CUDA configuration in a formatted manner.\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"PyTorch Configuration\".center(50))\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic PyTorch and CUDA info\n",
    "    print(f\"{'PyTorch Version':<25}: {torch.__version__}\")\n",
    "    print(f\"{'CUDA Compiled Version':<25}: {torch.version.cuda}\")\n",
    "    print(f\"{'CUDA Available':<25}: {'Yes' if torch.cuda.is_available() else 'No'}\")\n",
    "    print(f\"{'Number of GPUs':<25}: {torch.cuda.device_count()}\")\n",
    "\n",
    "    # GPU details if available\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"{'GPU Name':<25}: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Seed setting\n",
    "    torch.manual_seed(42)\n",
    "    print(f\"{'Random Seed':<25}: 42 (Seeding successful!)\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print_torch_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Build the GRU Model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 0: Bi-Directional GRU with Attention without LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BiGRUWithAttention(nn.Module):\n",
    "#     def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int, dropout: float = 0.0):\n",
    "#         \"\"\"\n",
    "#         Bi-directional GRU model with attention mechanism for sequence classification.\n",
    "\n",
    "#         Args:\n",
    "#             - input_size (int): Number of input features.\n",
    "#             - hidden_size (int): Number of hidden units in GRU.\n",
    "#             - output_size (int): Number of output classes or values.\n",
    "#             - num_layers (int): Number of GRU layers.\n",
    "#             - dropout (float): Dropout rate (default is 0.0).\n",
    "#         \"\"\"\n",
    "#         super(BiGRUWithAttention, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "        \n",
    "#         # Bi-directional GRU layer\n",
    "#         self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, \n",
    "#                           bidirectional=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "#         # Attention layer (original implementation)\n",
    "#         self.attention_fc = nn.Linear(hidden_size * 2, hidden_size * 2)  # Hidden size * 2 for bi-directional\n",
    "        \n",
    "#         # Fully connected layer\n",
    "#         self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#         # Initialize weights\n",
    "#         self.init_weights()\n",
    "        \n",
    "#     def init_weights(self):\n",
    "#         for name, param in self.named_parameters():\n",
    "#             if 'weight' in name:\n",
    "#                 nn.init.xavier_uniform_(param)  # Xavier initialization for weights\n",
    "#             elif 'bias' in name:\n",
    "#                 nn.init.constant_(param, 0)     # Zero initialization for biases\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         # Initialize hidden state\n",
    "#         batch_size = x.size(0)\n",
    "#         h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size, device=x.device) # Bi-directional: num_layers * 2\n",
    "\n",
    "#         # GRU forward pass\n",
    "#         out, _ = self.gru(x, h0)  # Shape: (batch_size, seq_length, hidden_size * 2)\n",
    "\n",
    "#         # Attention mechanism\n",
    "#         attn_weights = torch.tanh(self.attention_fc(out))  # Shape: (batch_size, seq_length, hidden_size * 2)\n",
    "#         out = attn_weights * out    # Element-wise attention application\n",
    "#         out = self.dropout(out)     # Apply dropout\n",
    "\n",
    "#         # Fully connected layer\n",
    "#         out = self.fc(out)  # Shape: (batch_size, seq_length, output_size)\n",
    "#         return out\n",
    "    \n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Dummy data\n",
    "#     batch_size, seq_length, input_size = 32, 60, 20\n",
    "#     hidden_size, output_size, num_layers = 64, 2, 2\n",
    "#     x = torch.randn(batch_size, seq_length, input_size)\n",
    "    \n",
    "#     # Initialize model\n",
    "#     model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers, dropout=0.2)\n",
    "#     output = model(x)\n",
    "#     print(f\"Output shape: {output.shape}\")  # Expected: (32, 60, 2)\n",
    "\n",
    "#     del batch_size, seq_length, input_size, hidden_size, output_size, num_layers, x, model, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Attention Layer with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self, linear_layer: nn.Linear, rank: int):\n",
    "        \"\"\"\n",
    "        LoRA module applied to a specified linear layer.\n",
    "\n",
    "        Args:\n",
    "            linear_layer (nn.Linear): The linear layer to adapt (e.g., attention_fc or fc).\n",
    "            rank (int): The rank of the LoRA adjustment matrices (e.g., 8).\n",
    "        \"\"\"\n",
    "        super(LoRA, self).__init__()\n",
    "        self.linear = linear_layer  # 保留對 linear_layer 的引用\n",
    "        self.rank = rank\n",
    "        \n",
    "        # Get input and output dimensions from the linear layer\n",
    "        in_features, out_features = linear_layer.weight.shape\n",
    "        \n",
    "        # Create LoRA matrices A and B\n",
    "        self.A = nn.Parameter(torch.zeros(in_features, rank))  # Shape: (in_features, rank)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_features))  # Shape: (rank, out_features)\n",
    "        \n",
    "        # Initialize A with normal distribution, B with zeros\n",
    "        nn.init.normal_(self.A, mean=0, std=1)\n",
    "        nn.init.zeros_(self.B)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with LoRA adjustment applied to the linear layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with LoRA-adapted weights.\n",
    "        \"\"\"\n",
    "        lora_delta = self.A @ self.B\n",
    "        adapted_weight = self.linear.weight + lora_delta\n",
    "        return nn.functional.linear(x, adapted_weight, self.linear.bias)\n",
    "    \n",
    "    def parameters(self, recurse=True):\n",
    "        \"\"\"\n",
    "        Override parameters() to return only LoRA-specific parameters (A and B).\n",
    "\n",
    "        Args:\n",
    "            recurse (bool): Ignored, included for compatibility with nn.Module.\n",
    "\n",
    "        Returns:\n",
    "            list: List of LoRA parameters (self.A and self.B).\n",
    "        \"\"\"\n",
    "        return [self.A, self.B]\n",
    "    \n",
    "\n",
    "class BiGRUWithAttention(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int, dropout: float = 0.0, lora_rank: int = 8):\n",
    "        \"\"\"\n",
    "        BiGRU model with attention mechanism and optional LoRA support.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Number of input features.\n",
    "            hidden_size (int): Number of hidden units in GRU.\n",
    "            output_size (int): Number of output classes or values.\n",
    "            num_layers (int): Number of GRU layers.\n",
    "            dropout (float): Dropout rate (default is 0.0).\n",
    "            lora_rank (int): Rank for LoRA adapters (default is 8).\n",
    "        \"\"\"\n",
    "        super(BiGRUWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lora_rank = lora_rank\n",
    "        \n",
    "        # Bi-directional GRU layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, \n",
    "                          bidirectional=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention_fc = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        \n",
    "        # List to hold multiple LoRA adapters\n",
    "        self.lora_adapters = nn.ModuleList()\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize base model weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights of the base model (GRU, attention_fc, fc).\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "    \n",
    "    def add_lora_adapter(self):\n",
    "        \"\"\"\n",
    "        Add a new LoRA adapter to the attention layer.\n",
    "        During training, only the latest LoRA adapter should be fine-tuned, while others are frozen.\n",
    "        \"\"\"\n",
    "        new_lora = LoRA(self.attention_fc, self.lora_rank)\n",
    "        # Move the new LoRA adapter to the same device as the model\n",
    "        device = next(self.parameters()).device\n",
    "        new_lora.to(device)\n",
    "        self.lora_adapters.append(new_lora)\n",
    "        print(f\"Added LoRA adapter, total adapters: {len(self.lora_adapters)}, on device: {device}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        During inference, all LoRA adapters are applied by summing their adjustments to the attention_fc weights.\n",
    "        During training, only the latest LoRA adapter should be fine-tuned, while others and the base attention_fc\n",
    "        should be frozen.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, input_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_length, output_size).\n",
    "        \"\"\"\n",
    "        # Initialize hidden state for GRU\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size, device=x.device)\n",
    "        \n",
    "        # GRU forward pass\n",
    "        out, _ = self.gru(x, h0)  # Shape: (batch_size, seq_length, hidden_size * 2)\n",
    "        \n",
    "        # Apply attention with all LoRA adapters (if any)\n",
    "        if self.lora_adapters:\n",
    "            # Sum adjustments from all LoRA adapters\n",
    "            lora_delta = sum(lora.A @ lora.B for lora in self.lora_adapters)\n",
    "            adapted_weight = self.attention_fc.weight + lora_delta\n",
    "            attn_out = nn.functional.linear(out, adapted_weight, self.attention_fc.bias)\n",
    "        else:\n",
    "            attn_out = self.attention_fc(out)\n",
    "        \n",
    "        # Apply tanh activation and compute attention-weighted output\n",
    "        attn_weights = torch.tanh(attn_out)\n",
    "        out = attn_weights * out\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)  # Shape: (batch_size, seq_length, output_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Training and validation function__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classwise_accuracy(student_logits_flat, y_batch, class_correct, class_total):\n",
    "    \"\"\"\n",
    "    Computes per-class accuracy by accumulating correct and total samples for each class using vectorized operations.\n",
    "    \n",
    "    Args:\n",
    "        student_logits_flat (torch.Tensor): Model predictions (logits) in shape [batch_size * seq_len, output_size]\n",
    "        y_batch (torch.Tensor): True labels in shape [batch_size * seq_len]\n",
    "        class_correct (dict): Dictionary to store correct predictions per class\n",
    "        class_total (dict): Dictionary to store total samples per class\n",
    "    \"\"\"\n",
    "    # Ensure inputs are on the same device\n",
    "    if student_logits_flat.device != y_batch.device:\n",
    "        raise ValueError(\"student_logits_flat and y_batch must be on the same device\")\n",
    "\n",
    "    # Convert logits to predicted class indices\n",
    "    predictions = torch.argmax(student_logits_flat, dim=-1)  # Shape: [batch_size * seq_len]\n",
    "\n",
    "    # Compute correct predictions mask\n",
    "    correct_mask = (predictions == y_batch)  # Shape: [batch_size * seq_len], boolean\n",
    "\n",
    "    # Get unique labels in this batch\n",
    "    unique_labels = torch.unique(y_batch)\n",
    "\n",
    "    # Update class_total and class_correct using vectorized operations\n",
    "    for label in unique_labels:\n",
    "        label = label.item()  # Convert tensor to scalar\n",
    "        if label not in class_total:\n",
    "            class_total[label] = 0\n",
    "            class_correct[label] = 0\n",
    "        \n",
    "        # Count total samples for this label\n",
    "        label_mask = (y_batch == label)\n",
    "        class_total[label] += label_mask.sum().item()\n",
    "        \n",
    "        # Count correct predictions for this label\n",
    "        class_correct[label] += (label_mask & correct_mask).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation function for Period 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, output_size, criterion, optimizer, \n",
    "                       X_train, y_train, X_val, y_val, scheduler, \n",
    "                       use_scheduler=None, num_epochs=10, batch_size=64, \n",
    "                       model_saving_folder=None, model_name=None, stop_signal_file=None):\n",
    "    \"\"\"\n",
    "    Training and validation function for Period 1.\n",
    "    \n",
    "    This function trains a model, evaluates on validation data, and saves:\n",
    "    1. The top 5 best models based on validation accuracy.\n",
    "    2. The single best model (`best_model.pth`).\n",
    "    3. The final model at the last epoch (`final_model.pth`).\n",
    "    \"\"\"\n",
    "    print(\"\\n🚀 'train_and_validate' function started.\\n\")\n",
    "\n",
    "    # Ensure the model saving folder exists (delete if it already exists)\n",
    "    if model_saving_folder:\n",
    "        if os.path.exists(model_saving_folder):\n",
    "            shutil.rmtree(model_saving_folder)  # Remove old contents\n",
    "            print(f\"✅ Removed existing folder: {model_saving_folder}\")\n",
    "        os.makedirs(model_saving_folder, exist_ok=True)\n",
    "\n",
    "    # Default model saving settings\n",
    "    if not model_saving_folder:\n",
    "        model_saving_folder = './saved_models'\n",
    "    if not model_name:\n",
    "        model_name = 'model'\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Convert data to tensors and move to device\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device) # (seqs, seq_len, features)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)    # (seqs, seq_len)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "    # Create Dataset & DataLoader\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Print dataset information\n",
    "    print(\"\\n✅ Data Overview:\")\n",
    "    print(f\"X_train Shape: {X_train.shape} | y_train Shape: {y_train.shape}\")\n",
    "    print(f\"X_val Shape: {X_val.shape} | y_val Shape: {y_val.shape}\")\n",
    "\n",
    "    # Record best results\n",
    "    global best_results\n",
    "    best_results = []\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        class_correct, class_total = {}, {}\n",
    "\n",
    "        # Stop signal check\n",
    "        if stop_signal_file and os.path.exists(stop_signal_file):\n",
    "            print(\"\\n🛑 Stop signal detected. Exiting training loop safely.\\n\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch).view(-1, output_size)  # seqs * seq_len, output_size\n",
    "            y_batch = y_batch.view(-1)                      # seqs * seq_len\n",
    "\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "            # Compute class-wise accuracy\n",
    "            compute_classwise_accuracy(outputs, y_batch, class_correct, class_total)\n",
    "\n",
    "        train_loss = epoch_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Compute per-class training accuracy\n",
    "        train_classwise_accuracy = {int(c): f\"{(class_correct[c] / class_total[c]) * 100:.2f}%\" if class_total[c] > 0 else \"0.00%\" \n",
    "                                    for c in sorted(class_total.keys())}\n",
    "\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        val_class_correct, val_class_total = {}, {}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                val_outputs = model(X_val_batch).view(-1, output_size)\n",
    "                val_labels = y_val_batch.view(-1)\n",
    "\n",
    "                val_loss += criterion(val_outputs, val_labels).item() * X_val_batch.size(0)\n",
    "                val_predictions = torch.argmax(val_outputs, dim=-1)\n",
    "                val_correct += (val_predictions == val_labels).sum().item()\n",
    "                val_total += val_labels.size(0)\n",
    "\n",
    "                compute_classwise_accuracy(val_outputs, val_labels, val_class_correct, val_class_total)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_classwise_accuracy = {int(c): f\"{(val_class_correct[c] / val_class_total[c]) * 100:.2f}%\" if val_class_total[c] > 0 else \"0.00%\" \n",
    "                                  for c in sorted(val_class_total.keys())}\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {train_loss:.9f}, \"\n",
    "              f\"Train-Class-Acc: {train_classwise_accuracy}, \"\n",
    "              f\"Val Loss: {val_loss:.9f}, \"\n",
    "              f\"Val Accuracy: {val_accuracy * 100:.2f}%, \"\n",
    "              f\"Val-Class-Acc: {val_classwise_accuracy}, \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.9f}\")\n",
    "\n",
    "        # Save current epoch model information\n",
    "        current_epoch_info = {\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_classwise_accuracy\": train_classwise_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_classwise_accuracy\": val_classwise_accuracy,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"model_path\": os.path.join(model_saving_folder, f\"{model_name}_epoch_{epoch+1}.pth\")\n",
    "        }\n",
    "        \n",
    "        # Save top 5 models\n",
    "        if len(best_results) < 5 or val_accuracy > best_results[-1][\"val_accuracy\"]:\n",
    "            if len(best_results) == 5:\n",
    "                worst = best_results.pop()\n",
    "                if os.path.exists(worst[\"model_path\"]):\n",
    "                    os.remove(worst[\"model_path\"])\n",
    "                    print(f\"🗑 Removed old model: {worst['model_path']} (Acc: {worst['val_accuracy']*100:.2f}%)\")\n",
    "\n",
    "            best_results.append(current_epoch_info)\n",
    "            best_results.sort(key=lambda x: (x[\"val_accuracy\"], x[\"epoch\"]), reverse=True)\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'learning_rate': optimizer.param_groups[0]['lr']\n",
    "            }, current_epoch_info[\"model_path\"])\n",
    "            print(f\"✅ Model saved: {current_epoch_info['model_path']}\")\n",
    "\n",
    "        if use_scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "    # Save best model\n",
    "    if best_results:\n",
    "        best_model_info = best_results[0]  \n",
    "        best_model_path = os.path.join(model_saving_folder, f\"{model_name}_best.pth\")\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': best_model_info[\"epoch\"],\n",
    "            'train_loss': best_model_info[\"train_loss\"],\n",
    "            'val_loss': best_model_info[\"val_loss\"],\n",
    "            'model_state_dict': best_model_info[\"model_state_dict\"],\n",
    "            'optimizer_state_dict': best_model_info[\"optimizer_state_dict\"],\n",
    "            'learning_rate': best_model_info[\"learning_rate\"]\n",
    "        }, best_model_path)\n",
    "        print(f\"\\n🏆 Best model saved as: {best_model_path} (Val Accuracy: {best_model_info['val_accuracy'] * 100:.2f}%)\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ No best model saved\")\n",
    "\n",
    "    # Save the final model\n",
    "    if 'current_epoch_info' in locals():\n",
    "        final_model_path = os.path.join(model_saving_folder, f\"{model_name}_final.pth\")\n",
    "        torch.save({ # Save this model\n",
    "            'epoch': epoch+1,  # Save the current epoch\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'model_state_dict': model.state_dict(),  # Model weights\n",
    "            'optimizer_state_dict': optimizer.state_dict(),  # Optimizer state\n",
    "            'learning_rate': optimizer.param_groups[0]['lr'] # Optimizer state\n",
    "        }, final_model_path)\n",
    "        print(f\"\\n📌 Final model saved as: {final_model_path}\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ No final model saved\")\n",
    "\n",
    "    print(\"\\n🎯 Top 5 Best Models by Validation Accuracy:\")\n",
    "    for res in best_results:        \n",
    "        print(f\"Epoch {res['epoch']}/{num_epochs}, \"\n",
    "              f\"Train Loss: {res['train_loss']:.9f}, \"\n",
    "              f\"Train-Class-Acc: {res['train_classwise_accuracy']},\\n\"  # Adjusted newline here\n",
    "              f\"Val Loss: {res['val_loss']:.9f}, \"\n",
    "              f\"Val Accuracy: {res['val_accuracy'] * 100:.2f}%, \"\n",
    "              f\"Val-Class-Acc: {res['val_classwise_accuracy']}, \"\n",
    "              f\"Model Path: {res['model_path']}\")\n",
    "\n",
    "    del X_train, y_train, X_val, y_val, train_loader, val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation function for Period 2 and beyond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old Version\n",
    "# def train_and_validate_lora(student_model, teacher_model, stable_classes, output_size, criterion, optimizer, \n",
    "#                             X_train, y_train, X_val, y_val, scheduler, \n",
    "#                             use_scheduler=None, num_epochs=10, batch_size=64, alpha=0.5,\n",
    "#                             model_saving_folder=None, model_name=None, stop_signal_file=None):\n",
    "#     \"\"\"\n",
    "#     student_model: The new LoRA-based student model (with output size 3).\n",
    "#     teacher_model: Frozen teacher model from period 1 (with output size 2).\n",
    "#     criterion: CrossEntropyLoss function.\n",
    "#     optimizer: Optimizer for student model.\n",
    "#     X_train, y_train, X_val, y_val: Training/validation data (as NumPy arrays or similar).\n",
    "#     num_epochs: Number of epochs to train.\n",
    "#     batch_size: Batch size for DataLoader.\n",
    "#     alpha: Weighting factor for distillation loss (alpha * distill_loss + (1-alpha) * ce_loss).\n",
    "#     \"\"\"\n",
    "#     print(\"\\n🚀 'train_and_validate_lora' function started.\\n\")\n",
    "\n",
    "#     # Ensure the model saving folder exists (delete if it already exists)\n",
    "#     if model_saving_folder:\n",
    "#         if os.path.exists(model_saving_folder):\n",
    "#             shutil.rmtree(model_saving_folder)  # Remove old contents\n",
    "#             print(f\"✅ Removed existing folder: {model_saving_folder}\")\n",
    "#         os.makedirs(model_saving_folder, exist_ok=True)\n",
    "        \n",
    "#     # Default model saving settings\n",
    "#     if not model_saving_folder:\n",
    "#         model_saving_folder = './saved_models'\n",
    "#     if not model_name:\n",
    "#         model_name = 'model'\n",
    "\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     student_model.to(device)\n",
    "#     teacher_model.to(device)\n",
    "\n",
    "#     # Convert data to tensors and move to device\n",
    "#     X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "#     y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "#     X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "#     y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "#     # Create Dataset & DataLoader\n",
    "#     train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     print(\"\\n✅ Data Overview:\")\n",
    "#     print(f\"X_train Shape: {X_train.shape} | y_train Shape: {y_train.shape}\")\n",
    "#     print(f\"X_val Shape: {X_val.shape} | y_val Shape: {y_val.shape}\")\n",
    "\n",
    "#     # Record best results\n",
    "#     global best_results  # Ensure we can modify the external variable if defined outside.\n",
    "#     best_results = []    # Start empty each training run\n",
    "#     teacher_model.eval() # Ensure teacher is frozen\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         epoch_loss = 0.0\n",
    "#         class_correct, class_total = {}, {}\n",
    "        \n",
    "#         # Stop signal check\n",
    "#         if stop_signal_file and os.path.exists(stop_signal_file):\n",
    "#             print(\"\\n🛑 Stop signal detected. Exiting training loop safely.\\n\")\n",
    "#             break\n",
    "\n",
    "#         student_model.train()\n",
    "#         i=0\n",
    "#         for X_batch, y_batch in train_loader:\n",
    "#             # Reset gradients before forward pass\n",
    "#             optimizer.zero_grad()  # Best practice\n",
    "\n",
    "#             # Forward pass: student model produces logits for output_size classes.\n",
    "#             student_logits = student_model(X_batch)  # Shape: [batch, seq_len, output_size]\n",
    "            \n",
    "#             # Reshape for CE loss computation.\n",
    "#             student_logits_flat = student_logits.view(-1, output_size)\n",
    "#             y_batch = y_batch.view(-1)\n",
    "\n",
    "#             # Compute Cross-Entropy loss\n",
    "#             ce_loss = criterion(student_logits_flat, y_batch)\n",
    "\n",
    "#             # Compute class-wise accuracy (Accumulates values in dict)\n",
    "#             compute_classwise_accuracy(student_logits_flat, y_batch, class_correct, class_total)\n",
    "\n",
    "#             if epoch == 1 and i < 3:\n",
    "#                 i += 1\n",
    "#                 print(f\"\\nUnique target values: {y_batch.unique()}\")\n",
    "#                 print(f\"Target dtype: {y_batch.dtype}\")\n",
    "#                 print(f\"Min target: {y_batch.min()}, Max target: {y_batch.max()}\")\n",
    "#                 print(\"Unique classes in y_train:\", y_train.unique())\n",
    "#                 print(f\"Unique classes in y_val: {y_val.unique()}\\n\")\n",
    "\n",
    "#             # Knowledge Distillation: Forward pass through teacher (pre-trained on previous period data).\n",
    "#             with torch.no_grad():\n",
    "#                 teacher_logits = teacher_model(X_batch)  # Shape: [batch, seq_len, teacher_output_size]\n",
    "            \n",
    "#             # Select stable classes for distillation\n",
    "#             \"\"\"\n",
    "#             Use stable_classes (a list of indices) to extract the relevant logits.\n",
    "#             We distill only the stable classes (class 1 if teacher is from period 1).\n",
    "#             Teacher's class index 1 corresponds to student's class index 1.\n",
    "#             It's safer to use index_select to ensure the operation works on GPU.\n",
    "#             \"\"\"\n",
    "#             stable_indices = torch.tensor(stable_classes, device=teacher_logits.device)\n",
    "#             teacher_stable = teacher_logits.index_select(dim=2, index=stable_indices)\n",
    "#             student_stable = student_logits.index_select(dim=2, index=stable_indices)\n",
    "\n",
    "#             # Compute KL Distillation Loss\n",
    "#             distill_loss = F.mse_loss(student_stable, teacher_stable)\n",
    "\n",
    "#             # Total loss: weighted sum of CE loss and distillation loss\n",
    "#             total_loss = alpha * distill_loss + (1 - alpha) * ce_loss\n",
    "#             total_loss.backward()\n",
    "#             optimizer.step()\n",
    "#             epoch_loss += total_loss.item() * X_batch.size(0)\n",
    "            \n",
    "#         train_loss = epoch_loss / len(train_loader.dataset)\n",
    "\n",
    "#         # Compute per-class training accuracy\n",
    "#         train_classwise_accuracy = {int(c): f\"{(class_correct[c] / class_total[c]) * 100:.2f}%\" if class_total[c] > 0 else \"0.00%\" \n",
    "#                                     for c in sorted(class_total.keys())}\n",
    "\n",
    "#         # Perform validation at the end of each epoch (only CE loss and accuracy)\n",
    "#         student_model.eval()\n",
    "#         val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "#         val_class_correct, val_class_total = {}, {}\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for X_val_batch, y_val_batch in val_loader:\n",
    "#                 val_outputs = student_model(X_val_batch).view(-1, output_size)\n",
    "#                 val_labels = y_val_batch.view(-1)\n",
    "\n",
    "#                 val_loss += criterion(val_outputs, val_labels).item() * X_val_batch.size(0)  # Scale to total loss\n",
    "#                 val_predictions = torch.argmax(val_outputs, dim=-1)\n",
    "#                 val_correct += (val_predictions == val_labels).sum().item()\n",
    "#                 val_total += val_labels.size(0)\n",
    "\n",
    "#                 # Compute per-class validation accuracy\n",
    "#                 compute_classwise_accuracy(val_outputs, val_labels, val_class_correct, val_class_total)\n",
    "\n",
    "#         val_loss /= len(val_loader.dataset)\n",
    "#         val_accuracy = val_correct / val_total\n",
    "\n",
    "#         # Compute per-class validation accuracy\n",
    "#         val_classwise_accuracy = {int(c): f\"{(val_class_correct[c] / val_class_total[c]) * 100:.2f}%\" if val_class_total[c] > 0 else \"0.00%\" \n",
    "#                                   for c in sorted(val_class_total.keys())}\n",
    "\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "#               f\"Train Loss: {train_loss:.9f}, \"\n",
    "#               f\"Train-Class-Acc: {train_classwise_accuracy}, \"\n",
    "#               f\"Val Loss: {val_loss:.9f}, \"\n",
    "#               f\"Val Accuracy: {val_accuracy * 100:.2f}%, \"\n",
    "#               f\"Val-Class-Acc: {val_classwise_accuracy}, \"\n",
    "#               f\"LR: {optimizer.param_groups[0]['lr']:.9f}\")\n",
    "\n",
    "#         # Save current model and update best results if applicable\n",
    "#         current_epoch_info = {\n",
    "#             \"epoch\": epoch+1,\n",
    "#             \"train_loss\": train_loss,\n",
    "#             \"train_classwise_accuracy\": train_classwise_accuracy,\n",
    "#             \"val_loss\": val_loss,\n",
    "#             \"val_accuracy\": val_accuracy,\n",
    "#             \"val_classwise_accuracy\": val_classwise_accuracy,\n",
    "#             \"model_state_dict\": student_model.state_dict(),\n",
    "#             \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "#             \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "#             \"model_path\": os.path.join(model_saving_folder, f\"{model_name}_epoch_{epoch+1}.pth\")\n",
    "#         }\n",
    "\n",
    "#         # Save top 5 models\n",
    "#         if len(best_results) < 5 or val_accuracy > best_results[-1][\"val_accuracy\"]:\n",
    "#             if len(best_results) == 5:\n",
    "#                 # Remove the worst model from the list, the last (lowest accuracy)\n",
    "#                 worst = best_results.pop() \n",
    "#                 if os.path.exists(worst[\"model_path\"]):\n",
    "#                     os.remove(worst[\"model_path\"])\n",
    "#                     print(f\"🗑 Removed old model: {worst['model_path']} (Acc: {worst['val_accuracy']*100:.2f}%)\")\n",
    "\n",
    "#             # Just insert and sort by val_accuracy descending\n",
    "#             best_results.append(current_epoch_info) \n",
    "#             best_results.sort(key=lambda x: (x[\"val_accuracy\"], x[\"epoch\"]), reverse=True)\n",
    "\n",
    "#             torch.save({ # Save this model\n",
    "#                 'epoch': epoch+1,  # Save the current epoch\n",
    "#                 'train_loss': train_loss,\n",
    "#                 'val_loss': val_loss,\n",
    "#                 'model_state_dict': student_model.state_dict(),  # Model weights\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),  # Optimizer state\n",
    "#                 'learning_rate': optimizer.param_groups[0]['lr'] # Optimizer state\n",
    "#             }, current_epoch_info[\"model_path\"])\n",
    "#             print(f\"✅ Model saved: {current_epoch_info['model_path']}\")\n",
    "\n",
    "#         if use_scheduler == True:\n",
    "#             # Scheduler step should follow after considering the results (placed after otallher losses)\n",
    "#             scheduler.step(val_loss)\n",
    "\n",
    "#     # Save best model\n",
    "#     if best_results:\n",
    "#         best_model_info = best_results[0]  \n",
    "#         best_model_path = os.path.join(model_saving_folder, f\"{model_name}_best.pth\")\n",
    "\n",
    "#         torch.save({\n",
    "#             'epoch': best_model_info[\"epoch\"],\n",
    "#             'train_loss': best_model_info[\"train_loss\"],\n",
    "#             'val_loss': best_model_info[\"val_loss\"],\n",
    "#             'model_state_dict': best_model_info[\"model_state_dict\"],\n",
    "#             'optimizer_state_dict': best_model_info[\"optimizer_state_dict\"],\n",
    "#             'learning_rate': best_model_info[\"learning_rate\"]\n",
    "#         }, best_model_path)\n",
    "#         print(f\"\\n🏆 Best model saved as: {best_model_path} (Val Accuracy: {best_model_info['val_accuracy'] * 100:.2f}%)\")\n",
    "#     else:\n",
    "#         print(\"\\n⚠️ No best model saved\")\n",
    "\n",
    "#     # Save the final model\n",
    "#     if 'current_epoch_info' in locals():\n",
    "#         final_model_path = os.path.join(model_saving_folder, f\"{model_name}_final.pth\")\n",
    "#         torch.save({ # Save this model\n",
    "#             'epoch': epoch+1,  # Save the current epoch\n",
    "#             'train_loss': train_loss,\n",
    "#             'val_loss': val_loss,\n",
    "#             'model_state_dict': student_model.state_dict(),  # Model weights\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),  # Optimizer state\n",
    "#             'learning_rate': optimizer.param_groups[0]['lr'] # Optimizer state\n",
    "#         }, final_model_path)\n",
    "#         print(f\"\\n📌 Final model saved as: {final_model_path}\")\n",
    "#     else:\n",
    "#         print(\"\\n⚠️ No final model saved\")\n",
    "\n",
    "#     print(\"\\n🎯 Top 5 Best Models by Validation Accuracy:\")\n",
    "#     for res in best_results:        \n",
    "#         print(f\"Epoch {res['epoch']}/{num_epochs}, \"\n",
    "#               f\"Train Loss: {res['train_loss']:.9f}, \"\n",
    "#               f\"Train-Class-Acc: {res['train_classwise_accuracy']},\\n\"\n",
    "#               f\"Val Loss: {res['val_loss']:.9f}, \"\n",
    "#               f\"Val Accuracy: {res['val_accuracy'] * 100:.2f}%, \"\n",
    "#               f\"Val-Class-Acc: {res['val_classwise_accuracy']}, \"\n",
    "#               f\"Model Path: {res['model_path']}\")\n",
    "    \n",
    "#     del X_train, y_train, X_val, y_val, train_loader, val_loader\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Version\n",
    "def train_and_validate_lora(student_model, teacher_model, stable_classes, output_size, criterion, optimizer, \n",
    "                            X_train, y_train, X_val, y_val, scheduler, \n",
    "                            use_scheduler=None, num_epochs=10, batch_size=64, alpha=0.5,\n",
    "                            model_saving_folder=None, model_name=None, stop_signal_file=None,\n",
    "                            class_features_dict=None, tau_high=0.5, tau_low=0.5, related_labels=None):\n",
    "    \"\"\"\n",
    "    Training and validation function for Period 2 and beyond with LoRA adapters and custom strategy.\n",
    "\n",
    "    Args:\n",
    "        student_model: The student model (BiGRUWithAttention) to train with LoRA adapters.\n",
    "        teacher_model: Frozen teacher model from the previous period.\n",
    "        stable_classes (list): List of class indices to use for knowledge distillation.\n",
    "        output_size (int): Number of output classes for the student model.\n",
    "        criterion: Loss function (e.g., CrossEntropyLoss).\n",
    "        optimizer: Optimizer for the student model.\n",
    "        X_train, y_train, X_val, y_val: Training and validation data (as NumPy arrays).\n",
    "        scheduler: Learning rate scheduler.\n",
    "        use_scheduler (bool): Whether to use the scheduler.\n",
    "        num_epochs (int): Number of epochs to train.\n",
    "        batch_size (int): Batch size for DataLoader.\n",
    "        alpha (float): Weighting factor for distillation loss (alpha * distill_loss + (1-alpha) * ce_loss).\n",
    "        model_saving_folder (str): Folder to save models.\n",
    "        model_name (str): Name of the model for saving.\n",
    "        stop_signal_file (str): Path to a stop signal file to interrupt training.\n",
    "        class_features_dict (dict): Dictionary to store class features from previous periods.\n",
    "        tau_high (float): High similarity threshold for DynEx-CLoRA (set to 0.5 as per request).\n",
    "        tau_low (float): Low similarity threshold for DynEx-CLoRA (set to 0.5 as per request).\n",
    "        related_labels (dict): Dictionary recording related labels for attention_fc and LoRA adapters.\n",
    "    \n",
    "    Returns:\n",
    "        class_features_dict: Updated dictionary of class features.\n",
    "    \"\"\"\n",
    "    print(\"\\n🚀 'train_and_validate_lora' function started.\\n\")\n",
    "\n",
    "    # Initialize related_labels if not provided (for Period 2)\n",
    "    if related_labels is None:\n",
    "        related_labels = {'attention_fc': [0, 1]}\n",
    "    print(f\"Initial related_labels: {related_labels}\")\n",
    "\n",
    "    # Ensure the model saving folder exists (delete if it already exists)\n",
    "    if model_saving_folder:\n",
    "        if os.path.exists(model_saving_folder):\n",
    "            shutil.rmtree(model_saving_folder)\n",
    "            print(f\"✅ Removed existing folder: {model_saving_folder}\")\n",
    "        os.makedirs(model_saving_folder, exist_ok=True)\n",
    "        \n",
    "    # Default model saving settings\n",
    "    if not model_saving_folder:\n",
    "        model_saving_folder = './saved_models'\n",
    "    if not model_name:\n",
    "        model_name = 'model'\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    student_model.to(device)\n",
    "    teacher_model.to(device)\n",
    "\n",
    "    # Convert data to tensors and move to device\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"\\n✅ Data Overview:\")\n",
    "    print(f\"X_train Shape: {X_train.shape} | y_train Shape: {y_train.shape}\")\n",
    "    print(f\"X_val Shape: {X_val.shape} | y_val Shape: {y_val.shape}\")\n",
    "\n",
    "    # Record best results\n",
    "    global best_results\n",
    "    best_results = []\n",
    "    teacher_model.eval()\n",
    "\n",
    "    # Extract features for new classes\n",
    "    student_model.eval()\n",
    "    new_class_features = {}\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            features = student_model.gru(X_batch)[0]  # Shape: [batch, seq_len, hidden_size * 2]\n",
    "            features = features.reshape(-1, features.size(-1))  # Flatten\n",
    "            y_batch = y_batch.view(-1)\n",
    "            for label in torch.unique(y_batch):\n",
    "                label = label.item()\n",
    "                if label not in new_class_features:\n",
    "                    new_class_features[label] = []\n",
    "                mask = (y_batch == label)\n",
    "                new_class_features[label].append(features[mask])\n",
    "    for label in new_class_features:\n",
    "        new_class_features[label] = torch.cat(new_class_features[label], dim=0).mean(dim=0)\n",
    "    student_model.train()\n",
    "\n",
    "    # DynEx-CLoRA: Compare with existing class features\n",
    "    if class_features_dict is None:\n",
    "        class_features_dict = {}\n",
    "    existing_class_features = class_features_dict\n",
    "\n",
    "    # Calculate similarity\n",
    "    cosine_sim = nn.CosineSimilarity(dim=0)\n",
    "    similarity_scores = {}\n",
    "    for new_label, new_feature in new_class_features.items():\n",
    "        similarity_scores[new_label] = {}\n",
    "        for existing_label, existing_feature in existing_class_features.items():\n",
    "            s = cosine_sim(new_feature, existing_feature)\n",
    "            similarity_scores[new_label][existing_label] = s.item()\n",
    "\n",
    "    print(\"\\nSimilarity Scores:\")\n",
    "    for new_label, scores in similarity_scores.items():\n",
    "        print(f\"New Class {new_label}:\")\n",
    "        if scores:\n",
    "            for existing_label, s in scores.items():\n",
    "                print(f\"  - Existing Class {existing_label}: {s:.4f}\")\n",
    "        else:\n",
    "            print(\"  - No existing classes to compare\")\n",
    "    \n",
    "    # 計算相似度的統計信息\n",
    "    all_similarities = [s for scores in similarity_scores.values() for s in scores.values()]\n",
    "    if all_similarities:\n",
    "        avg_similarity = np.mean(all_similarities)\n",
    "        std_similarity = np.std(all_similarities)\n",
    "        print(f\"Average similarity: {avg_similarity:.4f}, Std: {std_similarity:.4f}\")\n",
    "\n",
    "    # Define similarity threshold\n",
    "    similarity_threshold = 0.3  ##### Set to 0.3 as per request #####\n",
    "    print(f\"Similarity threshold: {similarity_threshold}\")\n",
    "\n",
    "    # Classify old and new classes\n",
    "    old_classes = [label for label in new_class_features.keys() if label in existing_class_features]\n",
    "    new_classes = [label for label in new_class_features.keys() if label not in existing_class_features]\n",
    "    print(f\"Old classes: {old_classes}\")\n",
    "    print(f\"New classes: {new_classes}\")\n",
    "\n",
    "    # Decision rules: Determine which networks to unfreeze\n",
    "    to_unfreeze = set()\n",
    "\n",
    "    # Handle old classes (except 0)\n",
    "    for label in old_classes:\n",
    "        if label != 0:\n",
    "            s = similarity_scores[label][label]\n",
    "            print(f\"Old Class {label} similarity with itself: {s:.4f}\")\n",
    "            if s >= similarity_threshold:  # 改為 >= similarity_threshold\n",
    "                # Find related networks and add to unfreeze set\n",
    "                for key, labels in related_labels.items():\n",
    "                    if label in labels:\n",
    "                        to_unfreeze.add(key)\n",
    "                        print(f\"Unfreezing {key} due to high similarity of Class {label}\")\n",
    "\n",
    "    # Special case for Period 2: If no existing features, directly add a LoRA adapter\n",
    "    if not existing_class_features:\n",
    "        print(\"No existing class features found (Period 2). Adding a new LoRA adapter for new classes.\")\n",
    "        # Filter out classes already in related_labels['attention_fc']\n",
    "        truly_new_classes = [label for label in new_classes if label not in related_labels['attention_fc']]\n",
    "        if truly_new_classes:\n",
    "            student_model.add_lora_adapter()\n",
    "            new_lora_index = len(student_model.lora_adapters) - 1\n",
    "            related_labels[new_lora_index] = truly_new_classes  # Associate only truly new classes to this LoRA\n",
    "            to_unfreeze.add(new_lora_index)\n",
    "            print(f\"Added new LoRA adapter (index {new_lora_index}) for Classes {truly_new_classes}\")\n",
    "        else:\n",
    "            print(\"No truly new classes to associate with a new LoRA adapter.\")\n",
    "    else:\n",
    "        # Handle old classes (except 0)\n",
    "        for label in old_classes:\n",
    "            if label != 0:\n",
    "                s = similarity_scores[label][label]\n",
    "                print(f\"Old Class {label} similarity with itself: {s:.4f}\")\n",
    "                if s >= similarity_threshold:  # 修改條件：從 s < similarity_threshold 改為 s >= similarity_threshold\n",
    "                    # Find related networks and add to unfreeze set\n",
    "                    for key, labels in related_labels.items():\n",
    "                        if label in labels:\n",
    "                            to_unfreeze.add(key)\n",
    "                            print(f\"Unfreezing {key} due to high similarity of Class {label}\")\n",
    "\n",
    "        # Handle new classes\n",
    "        for label in new_classes:\n",
    "            s_0 = similarity_scores[label][0] if 0 in existing_class_features else -1\n",
    "            print(f\"New Class {label} similarity with Class 0: {s_0:.4f}\")\n",
    "            \n",
    "            if s_0 >= similarity_threshold:\n",
    "                # Add new LoRA and associate new class\n",
    "                student_model.add_lora_adapter()\n",
    "                new_lora_index = len(student_model.lora_adapters) - 1\n",
    "                related_labels[new_lora_index] = [label]\n",
    "                to_unfreeze.add(new_lora_index)\n",
    "                print(f\"Added new LoRA adapter (index {new_lora_index}) for Class {label}\")\n",
    "            \n",
    "            # Check similarity with other Existing Classes (except 0), regardless of s_0\n",
    "            for existing_label in existing_class_features:\n",
    "                if existing_label != 0:\n",
    "                    s = similarity_scores[label][existing_label]\n",
    "                    if s < similarity_threshold:  # 修改條件：從 s >= similarity_threshold 改為 s < similarity_threshold\n",
    "                        for key, labels in related_labels.items():\n",
    "                            if existing_label in labels:\n",
    "                                to_unfreeze.add(key)\n",
    "                                print(f\"Unfreezing {key} due to low similarity between New Class {label} and Existing Class {existing_label}\")\n",
    "                                # If not adding new LoRA, associate new class to this LoRA\n",
    "                                if s_0 < similarity_threshold and isinstance(key, int):\n",
    "                                    related_labels[key].append(label)\n",
    "                                    print(f\"Associated New Class {label} to LoRA adapter {key}\")\n",
    "                                break  # Assume one association per new class\n",
    "\n",
    "    # Default: Freeze all attention_fc and LoRA adapters\n",
    "    for param in student_model.attention_fc.parameters():\n",
    "        param.requires_grad = False\n",
    "    for lora in student_model.lora_adapters:\n",
    "        for param in lora.parameters():\n",
    "            param.requires_grad = False\n",
    "    print(\"\\nDefault: All attention_fc and LoRA adapters are frozen\")\n",
    "\n",
    "    print(\"to_unfreeze:\", to_unfreeze)\n",
    "    \n",
    "    print(\"After freezing attention_fc & LoRA adapter:\")\n",
    "    for param in student_model.attention_fc.parameters():\n",
    "        print(f\"attention_fc param.requires_grad: {param.requires_grad}\")\n",
    "    for lora in student_model.lora_adapters:\n",
    "        for param in lora.parameters():\n",
    "           print(f\"lora param.requires_grad: {lora}: {param.requires_grad}\")\n",
    "\n",
    "    # Unfreeze networks that need to be updated\n",
    "    if 'attention_fc' in to_unfreeze:\n",
    "        for param in student_model.attention_fc.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Unfroze attention_fc\")\n",
    "\n",
    "    print(\"After recognizing attention_fc (default: False):\")\n",
    "    for param in student_model.attention_fc.parameters():\n",
    "        print(f\"param.requires_grad: {param.requires_grad}\")\n",
    "    for lora in student_model.lora_adapters:\n",
    "        for param in lora.parameters():\n",
    "           print(f\"lora param.requires_grad: {lora}: {param.requires_grad}\")\n",
    "        \n",
    "    for lora_index in to_unfreeze:\n",
    "        if isinstance(lora_index, int):\n",
    "            for param in student_model.lora_adapters[lora_index].parameters():\n",
    "                param.requires_grad = True\n",
    "            print(f\"Unfroze LoRA adapter {lora_index}\")\n",
    "\n",
    "    print(\"After recognizing LoRA adapters (default: False):\")\n",
    "    for param in student_model.attention_fc.parameters():\n",
    "        print(f\"attention_fc param.requires_grad: {param.requires_grad}\")\n",
    "    for lora in student_model.lora_adapters:\n",
    "        for param in lora.parameters():\n",
    "           print(f\"lora param.requires_grad: {lora}: {param.requires_grad}\")\n",
    "\n",
    "    # FC layer is always trainable\n",
    "    for param in student_model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "    print(\"FC layer remains trainable\")\n",
    "\n",
    "    # Print freeze status before training\n",
    "    print(\"\\nFreeze Status Before Training:\")\n",
    "\n",
    "    # Check attention_fc\n",
    "    for param in student_model.attention_fc.parameters():\n",
    "        print(f\"attention_fc param.requires_grad: {param.requires_grad}\")\n",
    "    for lora in student_model.lora_adapters:\n",
    "        for param in lora.parameters():\n",
    "           print(f\"lora param.requires_grad: {lora}: {param.requires_grad}\")\n",
    "        \n",
    "    # Check each LoRA adapter\n",
    "    for i, lora in enumerate(student_model.lora_adapters):\n",
    "        lora_frozen = all(not param.requires_grad for param in lora.parameters())\n",
    "        print(f\"LoRA adapter {i}: {'Frozen' if lora_frozen else 'Unfrozen'}\")\n",
    "        \n",
    "    # Check fc layer\n",
    "    fc_frozen = all(not param.requires_grad for param in student_model.fc.parameters())\n",
    "    print(f\"fc layer: {'Frozen' if fc_frozen else 'Unfrozen'}\")\n",
    "\n",
    "    # Update optimizer to only optimize trainable parameters\n",
    "    optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, student_model.parameters()),\n",
    "        lr=optimizer.param_groups[0]['lr']\n",
    "    )\n",
    "\n",
    "    print(f\"Current Related_labels: {related_labels}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        class_correct, class_total = {}, {}\n",
    "\n",
    "        if stop_signal_file and os.path.exists(stop_signal_file):\n",
    "            print(\"\\n🛑 Stop signal detected. Exiting training loop safely.\\n\")\n",
    "            break\n",
    "\n",
    "        student_model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            student_logits = student_model(X_batch)\n",
    "            student_logits_flat = student_logits.view(-1, output_size)\n",
    "            y_batch = y_batch.view(-1)\n",
    "            ce_loss = criterion(student_logits_flat, y_batch)\n",
    "            \n",
    "            # Compute class-wise accuracy\n",
    "            compute_classwise_accuracy(student_logits_flat, y_batch, class_correct, class_total)\n",
    "\n",
    "            # Knowledge Distillation\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher_model(X_batch)\n",
    "            \n",
    "            # Select stable classes for distillation\n",
    "            stable_indices = torch.tensor(stable_classes, device=teacher_logits.device)\n",
    "            teacher_stable = teacher_logits.index_select(dim=2, index=stable_indices)\n",
    "            student_stable = student_logits.index_select(dim=2, index=stable_indices)\n",
    "\n",
    "            # Compute KL Distillation Loss\n",
    "            distill_loss = F.mse_loss(student_stable, teacher_stable)\n",
    "\n",
    "            # Total loss: weighted sum of CE loss and distillation loss\n",
    "            total_loss = alpha * distill_loss + (1 - alpha) * ce_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += total_loss.item() * X_batch.size(0)\n",
    "\n",
    "        train_loss = epoch_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Compute per-class training accuracy\n",
    "        train_classwise_accuracy = {int(c): f\"{(class_correct[c] / class_total[c]) * 100:.2f}%\" if class_total[c] > 0 else \"0.00%\" \n",
    "                                    for c in sorted(class_total.keys())}\n",
    "        \n",
    "        # Perform validation at the end of each epoch (only CE loss and accuracy)\n",
    "        student_model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        val_class_correct, val_class_total = {}, {}\n",
    "        with torch.no_grad():\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                val_outputs = student_model(X_val_batch).view(-1, output_size)\n",
    "                val_labels = y_val_batch.view(-1)\n",
    "                val_loss += criterion(val_outputs, val_labels).item() * X_val_batch.size(0)\n",
    "                val_predictions = torch.argmax(val_outputs, dim=-1)\n",
    "                val_correct += (val_predictions == val_labels).sum().item()\n",
    "                val_total += val_labels.size(0)\n",
    "\n",
    "                # Compute per-class validation accuracy\n",
    "                compute_classwise_accuracy(val_outputs, val_labels, val_class_correct, val_class_total)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = val_correct / val_total\n",
    "\n",
    "        # Compute per-class validation accuracy\n",
    "        val_classwise_accuracy = {int(c): f\"{(val_class_correct[c] / val_class_total[c]) * 100:.2f}%\" if val_class_total[c] > 0 else \"0.00%\" \n",
    "                                  for c in sorted(val_class_total.keys())}\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {train_loss:.9f}, \"\n",
    "              f\"Train-Class-Acc: {train_classwise_accuracy}, \"\n",
    "              f\"Val Loss: {val_loss:.9f}, \"\n",
    "              f\"Val Accuracy: {val_accuracy * 100:.2f}%, \"\n",
    "              f\"Val-Class-Acc: {val_classwise_accuracy}, \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.9f}\")\n",
    "        \n",
    "        # Save current model and update best results if applicable\n",
    "        current_epoch_info = {\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_classwise_accuracy\": train_classwise_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_classwise_accuracy\": val_classwise_accuracy,\n",
    "            \"model_state_dict\": student_model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"num_lora_adapters\": len(student_model.lora_adapters),\n",
    "            \"related_labels\": related_labels,  # Save related_labels\n",
    "            \"model_path\": os.path.join(model_saving_folder, f\"{model_name}_epoch_{epoch+1}.pth\")\n",
    "        }\n",
    "\n",
    "        # Save top 5 models\n",
    "        if len(best_results) < 5 or val_accuracy > best_results[-1][\"val_accuracy\"]:\n",
    "            if len(best_results) == 5:\n",
    "                # Remove the worst model from the list (lowest accuracy)\n",
    "                worst = best_results.pop()\n",
    "                if os.path.exists(worst[\"model_path\"]):\n",
    "                    os.remove(worst[\"model_path\"])\n",
    "                    print(f\"🗑 Removed old model: {worst['model_path']} (Acc: {worst['val_accuracy']*100:.2f}%)\")\n",
    "            \n",
    "            # Insert and sort by val_accuracy descending\n",
    "            best_results.append(current_epoch_info)\n",
    "            best_results.sort(key=lambda x: (x[\"val_accuracy\"], x[\"epoch\"]), reverse=True)\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'model_state_dict': student_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "                'num_lora_adapters': len(student_model.lora_adapters),\n",
    "                'related_labels': related_labels  # Save related_labels\n",
    "            }, current_epoch_info[\"model_path\"])\n",
    "            print(f\"✅ Model saved: {current_epoch_info['model_path']}\")\n",
    "\n",
    "        if use_scheduler == True:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "    # Save best model\n",
    "    if best_results:\n",
    "        best_model_info = best_results[0]  \n",
    "        best_model_path = os.path.join(model_saving_folder, f\"{model_name}_best.pth\")\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': best_model_info[\"epoch\"],\n",
    "            'train_loss': best_model_info[\"train_loss\"],\n",
    "            'val_loss': best_model_info[\"val_loss\"],\n",
    "            'model_state_dict': best_model_info[\"model_state_dict\"],\n",
    "            'optimizer_state_dict': best_model_info[\"optimizer_state_dict\"],\n",
    "            'learning_rate': best_model_info[\"learning_rate\"],\n",
    "            'num_lora_adapters': best_model_info[\"num_lora_adapters\"],\n",
    "            'related_labels': related_labels  # Save related_labels\n",
    "        }, best_model_path)\n",
    "        print(f\"\\n🏆 Best model saved as: {best_model_path} (Val Accuracy: {best_model_info['val_accuracy'] * 100:.2f}%)\")\n",
    "\n",
    "    # Save the final model\n",
    "    if 'current_epoch_info' in locals():\n",
    "        final_model_path = os.path.join(model_saving_folder, f\"{model_name}_final.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'model_state_dict': student_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "            'num_lora_adapters': len(student_model.lora_adapters),\n",
    "            'related_labels': related_labels  # Save related_labels\n",
    "        }, final_model_path)\n",
    "        print(f\"\\n📌 Final model saved as: {final_model_path}\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ No final model saved\")\n",
    "\n",
    "    print(\"\\n🎯 Top 5 Best Models by Validation Accuracy:\")\n",
    "    for res in best_results:        \n",
    "        print(f\"Epoch {res['epoch']}/{num_epochs}, \"\n",
    "              f\"Train Loss: {res['train_loss']:.9f}, \"\n",
    "              f\"Train-Class-Acc: {res['train_classwise_accuracy']},\\n\"\n",
    "              f\"Val Loss: {res['val_loss']:.9f}, \"\n",
    "              f\"Val Accuracy: {res['val_accuracy'] * 100:.2f}%, \"\n",
    "              f\"Val-Class-Acc: {res['val_classwise_accuracy']}, \"\n",
    "              f\"Model Path: {res['model_path']}\")\n",
    "    \n",
    "    # Save class features\n",
    "    for label, feature in new_class_features.items():\n",
    "        class_features_dict[label] = feature\n",
    "    \n",
    "    del X_train, y_train, X_val, y_val, train_loader, val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return class_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training and validation function for Period 2 and beyond \n",
    "# def train_and_validate_KL_Div(student_model, teacher_model, stable_classes, output_size, criterion, optimizer, \n",
    "#                               X_train, y_train, X_val, y_val, scheduler, use_scheduler=None, \n",
    "#                               num_epochs=10, batch_size=64, alpha=0.5, temperature=2.0,\n",
    "#                               model_saving_folder=None, model_name=None, stop_signal_file=None):\n",
    "#     \"\"\"\n",
    "#     Training and validation function for Period 2+ with KL Divergence distillation.\n",
    "\n",
    "#     Args:\n",
    "#         student_model: The new LoRA-based student model (with output size 3).\n",
    "#         teacher_model: Frozen teacher model from period 1 (with output size 2).\n",
    "#         stable_classes: List of class indices to distill (e.g., [1]).\n",
    "#         output_size: Number of output classes for student model.\n",
    "#         criterion: CrossEntropyLoss function.\n",
    "#         optimizer: Optimizer for student model.\n",
    "#         X_train, y_train, X_val, y_val: Training/validation data (NumPy arrays or similar).\n",
    "#         scheduler: Learning rate scheduler.\n",
    "#         use_scheduler: Boolean to enable scheduler (default None).\n",
    "#         num_epochs: Number of epochs to train (default 10).\n",
    "#         batch_size: Batch size for DataLoader (default 64).\n",
    "#         alpha: Weighting factor for distillation loss (default 0.5).\n",
    "#         temperature: Temperature for softening probabilities in distillation (default 2.0).\n",
    "#         model_saving_folder: Folder to save models (default None).\n",
    "#         model_name: Base name for saved models (default None).\n",
    "#         stop_signal_file: File path to check for early stopping (default None).\n",
    "#     \"\"\"\n",
    "#     print(\"\\n🚀 'train_and_validate_KL_Div' function started.\\n\")\n",
    "\n",
    "#     # Ensure the model saving folder exists (delete if it already exists)\n",
    "#     if model_saving_folder:\n",
    "#         if os.path.exists(model_saving_folder):\n",
    "#             shutil.rmtree(model_saving_folder)\n",
    "#             print(f\"✅ Removed existing folder: {model_saving_folder}\")\n",
    "#         os.makedirs(model_saving_folder, exist_ok=True)\n",
    "        \n",
    "#     # Default model saving settings\n",
    "#     if not model_saving_folder:\n",
    "#         model_saving_folder = './saved_models'\n",
    "#     if not model_name:\n",
    "#         model_name = 'model'\n",
    "\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     student_model.to(device)\n",
    "#     teacher_model.to(device)\n",
    "\n",
    "#     # Convert data to tensors and move to device\n",
    "#     X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "#     y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "#     X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "#     y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "#     # Create Dataset & DataLoader\n",
    "#     train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     # Print dataset information\n",
    "#     print(\"\\n✅ Data Overview:\")\n",
    "#     print(f\"X_train Shape: {X_train.shape} | y_train Shape: {y_train.shape}\")\n",
    "#     print(f\"X_val Shape: {X_val.shape} | y_val Shape: {y_val.shape}\")\n",
    "#     print(f\"Unique values in y_train: {y_train.unique()}\")\n",
    "#     print(f\"Unique values in y_val: {y_val.unique()}\")\n",
    "\n",
    "#     global best_results  # Ensure we can modify the external variable if defined outside.\n",
    "#     best_results = []    # Start empty each training run\n",
    "#     stable_indices = torch.tensor(stable_classes, device=device)\n",
    "#     teacher_model.eval()  # Ensure teacher is frozen\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         epoch_loss = 0.0\n",
    "#         class_correct, class_total = {}, {}\n",
    "        \n",
    "#         if stop_signal_file and os.path.exists(stop_signal_file):\n",
    "#             print(\"\\n🛑 Stop signal detected. Exiting training loop safely.\\n\")\n",
    "#             break\n",
    "\n",
    "#         student_model.train()\n",
    "#         i = 0\n",
    "#         for X_batch, y_batch in train_loader:\n",
    "#             # Reset gradients before forward pass\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # Forward pass: student model produces logits for output_size classes.\n",
    "#             student_logits = student_model(X_batch)  # Shape: [batch, seq_len, output_size]\n",
    "\n",
    "#             # Reshape for CE loss computation.\n",
    "#             student_logits_flat = student_logits.view(-1, output_size)\n",
    "#             y_batch_flat = y_batch.view(-1)\n",
    "\n",
    "#             # Compute Cross-Entropy loss\n",
    "#             ce_loss = criterion(student_logits_flat, y_batch_flat)\n",
    "\n",
    "#             # Compute class-wise accuracy (Accumulates values in dict)\n",
    "#             compute_classwise_accuracy(student_logits_flat, y_batch_flat, class_correct, class_total)\n",
    "            \n",
    "#             if epoch == 1 and i < 3:\n",
    "#                 i += 1\n",
    "#                 print(f\"\\nBatch {i} Debug Info:\")\n",
    "#                 print(f\"Unique target values: {y_batch_flat.unique()}\")\n",
    "#                 print(f\"Target dtype: {y_batch_flat.dtype}\")\n",
    "#                 print(f\"Min target: {y_batch_flat.min()}, Max target: {y_batch_flat.max()}\")\n",
    "\n",
    "#             # Knowledge Distillation: Forward pass through teacher (pre-trained on previous period data).\n",
    "#             with torch.no_grad():\n",
    "#                 teacher_logits = teacher_model(X_batch)  # Shape: [batch, seq_len, teacher_output_size]\n",
    "\n",
    "#             # Select stable classes for distillation\n",
    "#             \"\"\"\n",
    "#             Use stable_classes (a list of indices) to extract the relevant logits.\n",
    "#             We distill only the stable classes (class 1 if teacher is from period 1).\n",
    "#             Teacher's class index 1 corresponds to student's class index 1.\n",
    "#             It's safer to use index_select to ensure the operation works on GPU.\n",
    "#             \"\"\"\n",
    "#             teacher_stable = teacher_logits.index_select(dim=2, index=stable_indices)\n",
    "#             student_stable = student_logits.index_select(dim=2, index=stable_indices)\n",
    "\n",
    "#             # Compute softened probabilities for KL divergence\n",
    "#             teacher_probs = F.softmax(teacher_stable / temperature, dim=2)\n",
    "#             student_log_probs = F.log_softmax(student_stable / temperature, dim=2)\n",
    "            \n",
    "#             # Compute the KL divergence loss; note: multiplying by temperature^2 as in common distillation practice.\n",
    "#             distill_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
    "                        \n",
    "#             # Total loss: balance between cross-entropy and distillation loss.\n",
    "#             total_loss = alpha * distill_loss + (1 - alpha) * ce_loss\n",
    "#             total_loss.backward()\n",
    "#             optimizer.step()\n",
    "#             epoch_loss += total_loss.item() * X_batch.size(0)\n",
    "            \n",
    "#         train_loss = epoch_loss / len(train_loader.dataset)\n",
    "#         train_classwise_accuracy = {int(c): f\"{(class_correct[c] / class_total[c]) * 100:.2f}%\" if class_total[c] > 0 else \"0.00%\" \n",
    "#                                     for c in sorted(class_total.keys())}\n",
    "\n",
    "#         # Perform validation at the end of each epoch (only CE loss and accuracy)\n",
    "#         student_model.eval()\n",
    "#         val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "#         val_class_correct, val_class_total = {}, {}\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for X_val_batch, y_val_batch in val_loader:\n",
    "#                 val_outputs = student_model(X_val_batch).view(-1, output_size)\n",
    "#                 val_labels = y_val_batch.view(-1)\n",
    "#                 val_loss += criterion(val_outputs, val_labels).item() * X_val_batch.size(0)\n",
    "#                 val_predictions = torch.argmax(val_outputs, dim=-1)\n",
    "#                 val_correct += (val_predictions == val_labels).sum().item()\n",
    "#                 val_total += val_labels.size(0)\n",
    "                \n",
    "#                 # Compute per-class validation accuracy\n",
    "#                 compute_classwise_accuracy(val_outputs, val_labels, val_class_correct, val_class_total)\n",
    "\n",
    "#         val_loss /= len(val_loader.dataset)\n",
    "#         val_accuracy = val_correct / val_total\n",
    "\n",
    "#         # Compute per-class validation accuracy\n",
    "#         val_classwise_accuracy = {int(c): f\"{(val_class_correct[c] / val_class_total[c]) * 100:.2f}%\" if val_class_total[c] > 0 else \"0.00%\" \n",
    "#                                   for c in sorted(val_class_total.keys())}\n",
    "\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "#               f\"Train Loss: {train_loss:.9f}, \"\n",
    "#               f\"Train-Class-Acc: {train_classwise_accuracy}, \"\n",
    "#               f\"Val Loss: {val_loss:.9f}, \"\n",
    "#               f\"Val Accuracy: {val_accuracy * 100:.2f}%, \"\n",
    "#               f\"Val-Class-Acc: {val_classwise_accuracy}, \"\n",
    "#               f\"LR: {optimizer.param_groups[0]['lr']:.9f}\")\n",
    "\n",
    "#         # Save current model and update best results if applicable\n",
    "#         current_epoch_info = {\n",
    "#             \"epoch\": epoch+1,\n",
    "#             \"train_loss\": train_loss,\n",
    "#             \"train_classwise_accuracy\": train_classwise_accuracy,\n",
    "#             \"val_loss\": val_loss,\n",
    "#             \"val_accuracy\": val_accuracy,\n",
    "#             \"val_classwise_accuracy\": val_classwise_accuracy,\n",
    "#             \"model_state_dict\": student_model.state_dict(),\n",
    "#             \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "#             \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "#             \"model_path\": os.path.join(model_saving_folder, f\"{model_name}_epoch_{epoch+1}.pth\")\n",
    "#         }\n",
    "\n",
    "#         # Save top 5 models\n",
    "#         if len(best_results) < 5 or val_accuracy > best_results[-1][\"val_accuracy\"]:\n",
    "#             if len(best_results) == 5:\n",
    "#                 # Remove the worst model from the list, the last (lowest accuracy)\n",
    "#                 worst = best_results.pop()\n",
    "#                 if os.path.exists(worst[\"model_path\"]):\n",
    "#                     os.remove(worst[\"model_path\"])\n",
    "#                     print(f\"🗑 Removed old model: {worst['model_path']} (Acc: {worst['val_accuracy']*100:.2f}%)\")\n",
    "\n",
    "#             # Just insert and sort by val_accuracy descending\n",
    "#             best_results.append(current_epoch_info)\n",
    "#             best_results.sort(key=lambda x: (x[\"val_accuracy\"], x[\"epoch\"]), reverse=True)\n",
    "\n",
    "#             try:\n",
    "#                 torch.save({\n",
    "#                     'epoch': epoch+1,\n",
    "#                     'train_loss': train_loss,\n",
    "#                     'val_loss': val_loss,\n",
    "#                     'model_state_dict': student_model.state_dict(),\n",
    "#                     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                     'learning_rate': optimizer.param_groups[0]['lr']\n",
    "#                 }, current_epoch_info[\"model_path\"])\n",
    "#                 print(f\"✅ Model saved: {current_epoch_info['model_path']}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"❌ Failed to save model {current_epoch_info['model_path']}: {e}\")\n",
    "\n",
    "#         if use_scheduler:\n",
    "#             scheduler.step(val_loss)\n",
    "\n",
    "#     # Save best model\n",
    "#     if best_results:\n",
    "#         best_model_info = best_results[0]\n",
    "#         best_model_path = os.path.join(model_saving_folder, f\"{model_name}_best.pth\")\n",
    "#         try:\n",
    "#             torch.save({\n",
    "#                 'epoch': best_model_info[\"epoch\"],\n",
    "#                 'train_loss': best_model_info[\"train_loss\"],\n",
    "#                 'val_loss': best_model_info[\"val_loss\"],\n",
    "#                 'model_state_dict': best_model_info[\"model_state_dict\"],\n",
    "#                 'optimizer_state_dict': best_model_info[\"optimizer_state_dict\"],\n",
    "#                 'learning_rate': best_model_info[\"learning_rate\"]\n",
    "#             }, best_model_path)\n",
    "#             print(f\"\\n🏆 Best model saved as: {best_model_path} (Val Accuracy: {best_model_info['val_accuracy'] * 100:.2f}%)\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Failed to save best model {best_model_path}: {e}\")\n",
    "#     else:\n",
    "#         print(\"\\n⚠️ No best model saved due to early termination or no epochs completed.\")\n",
    "\n",
    "#     # Save the final model\n",
    "#     if 'current_epoch_info' in locals():\n",
    "#         final_model_path = os.path.join(model_saving_folder, f\"{model_name}_final.pth\")\n",
    "#         try:\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch+1,\n",
    "#                 'train_loss': train_loss,\n",
    "#                 'val_loss': val_loss,\n",
    "#                 'model_state_dict': student_model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'learning_rate': optimizer.param_groups[0]['lr']\n",
    "#             }, final_model_path)\n",
    "#             print(f\"\\n📌 Final model saved as: {final_model_path}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Failed to save final model {final_model_path}: {e}\")\n",
    "#     else:\n",
    "#         print(\"\\n⚠️ No final model saved due to early termination before first epoch.\")\n",
    "\n",
    "#     print(\"\\n🎯 Top 5 Best Models by Validation Accuracy:\")\n",
    "#     for res in best_results:\n",
    "#         print(f\"Epoch {res['epoch']}/{num_epochs}, \"\n",
    "#               f\"Train Loss: {res['train_loss']:.9f}, \"\n",
    "#               f\"Train-Class-Acc: {res['train_classwise_accuracy']},\\n\"\n",
    "#               f\"Val Loss: {res['val_loss']:.9f}, \"\n",
    "#               f\"Val Accuracy: {res['val_accuracy'] * 100:.2f}%, \"\n",
    "#               f\"Val-Class-Acc: {res['val_classwise_accuracy']}, \"\n",
    "#               f\"Model Path: {res['model_path']}\")\n",
    "\n",
    "#     del X_train, y_train, X_val, y_val, train_loader, val_loader\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Setup before training__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define list_period_files_full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_file_paths(pair='BTCUSD', base_dir='Data', days=190):\n",
    "    \"\"\"\n",
    "    Set up file paths for cryptocurrency data across multiple periods.\n",
    "\n",
    "    Args:\n",
    "        pair (str): Trading pair (e.g., 'BTCUSD').\n",
    "        base_dir (str): Base directory for data storage (default 'Data').\n",
    "        days (int): Number of days for each period (default 190).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (base_folder_path, with_indicators_file_path, list_period_files_full_path)\n",
    "    \"\"\"\n",
    "    # Define base file name and folder structure\n",
    "    file_name = f\"Polygon_{pair}_4Y_1min\"\n",
    "    base_folder_path = os.path.normpath(os.path.join(base_dir, file_name))\n",
    "    \n",
    "    # Check if folder exists\n",
    "    if not os.path.isdir(base_folder_path):\n",
    "        raise FileNotFoundError(f\"Directory '{base_folder_path}' does not exist.\")\n",
    "\n",
    "    # Define file path with indicators for Period 1\n",
    "    with_indicators_file_path = os.path.normpath(\n",
    "        os.path.join(base_folder_path, f\"_{file_name}_{days}_days_with_indicators.csv\")\n",
    "    )\n",
    "\n",
    "    # Define file paths for all periods\n",
    "    list_period_files_full_path = [\n",
    "        # Period 1\n",
    "        with_indicators_file_path,\n",
    "        # Period 2: 2020-11-11 to 2021-05-20\n",
    "        os.path.normpath(os.path.join(\n",
    "            base_folder_path, f\"{file_name}_{days}_days__2020-11-11__2021-05-20__with_indicators.csv\"\n",
    "        )),\n",
    "        # Period 3: 2021-05-20 to 2021-11-26\n",
    "        os.path.normpath(os.path.join(\n",
    "            base_folder_path, f\"{file_name}_{days}_days__2021-05-20__2021-11-26__with_indicators.csv\"\n",
    "        )),\n",
    "        # Period 4: 2021-11-26 to 2022-06-04\n",
    "        os.path.normpath(os.path.join(\n",
    "            base_folder_path, f\"{file_name}_{days}_days__2021-11-26__2022-06-04__with_indicators.csv\"\n",
    "        )),\n",
    "        # Period 5: 2022-06-04 to 2022-12-11\n",
    "        os.path.normpath(os.path.join(\n",
    "            base_folder_path, f\"{file_name}_{days}_days__2022-06-04__2022-12-11__with_indicators.csv\"\n",
    "        )),\n",
    "    ]\n",
    "\n",
    "    return base_folder_path, with_indicators_file_path, list_period_files_full_path\n",
    "\n",
    "def print_folder_contents(folder_path):\n",
    "    \"\"\"Print all files in the specified folder.\"\"\"\n",
    "    print(\"\\n📂 Folder Contents:\")\n",
    "    for file in os.listdir(folder_path):\n",
    "        print(f\"Found file: {file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up paths\n",
    "    base_folder_path, with_indicators_file_path, list_period_files_full_path = setup_file_paths()\n",
    "\n",
    "    # Print results\n",
    "    print(\"=\" * 70)\n",
    "    print(\"File Path Configuration\".center(70))\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"{'Base Folder Path':<25}: {base_folder_path}\")\n",
    "    print(f\"{'Period 1 File Path':<25}: {with_indicators_file_path}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    print(\"List of Period Files:\")\n",
    "    for i, path in enumerate(list_period_files_full_path, 1):\n",
    "        print(f\"{'Period ' + str(i):<25}: {path}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Print folder contents\n",
    "    print_folder_contents(base_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __All periods data__\n",
    "'trend': Categorized trend values based on the detected phases:\n",
    "- 0: No trend\n",
    "- 1: Moderate negative trend\n",
    "- 2: Very strong negative trend\n",
    "- 3: Moderate positive trend\n",
    "- 4: Very strong positive trend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Train the Model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Period 1 (num_layers = 4, lora_r=4)\n",
    "+ ##### BiGRUWithAttentionLoRA\n",
    "+ ##### Training and saving in *'LoRA_v1/Rank_4_Period_1/1st_try'*\n",
    "#### __Val Accuracy: 98.80%__\n",
    "#### __Val-Class-Acc: {0: '99.15%', 1: '98.25%'}__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "#     X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "#         with_indicators_file_path = list_period_files_full_path[0], # Change \n",
    "#         downsampled_data_minutes = downsampled_data_minutes,\n",
    "#         exclude_columns = exclude_columns,\n",
    "#         lower_threshold = lower_threshold,\n",
    "#         upper_threshold = upper_threshold,\n",
    "#         reverse_steps = reverse_steps,\n",
    "#         sequence_length = sequence_length,\n",
    "#         sliding_interval = sliding_interval,\n",
    "#         trends_to_keep = {0, 1}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "#     )\n",
    "\n",
    "# print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "# unique_classes = np.unique(y_val)\n",
    "# num_classes = len(unique_classes)\n",
    "# print(f\"unique_classes = {unique_classes}\")\n",
    "# print(f\"num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model hyperparameters\n",
    "# input_size = Number_features  # Number of input features\n",
    "# hidden_size = 64  # Number of GRU units\n",
    "# output_size = num_classes  # Number of trend classes (2 for Period 1: {0, 1})\n",
    "# num_layers = 4  # Number of GRU layers\n",
    "# dropout = 0.0  # Dropout rate\n",
    "# lora_r = 4  # Rank of LoRA matrices (not used in Period 1, but defined for consistency)\n",
    "# num_epochs = 2000  # Number of training epochs\n",
    "# batch_size = 64  # Batch size for DataLoader\n",
    "# model_name = 'BiGRUWithAttention'  # Model name for saving\n",
    "# best_results = []  # List to store best results\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Define paths for stop signal and model saving\n",
    "# stop_signal_file = os.path.normpath(os.path.join('Class_Incremental_CL', 'David_Classif_Bi_Dir_GRU_Model/stop_training.txt'))\n",
    "# model_saving_folder = os.path.normpath(os.path.join('Class_Incremental_CL', \"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v2/Rank_4_Period_1/1st_try\"))\n",
    "# ensure_folder(model_saving_folder)\n",
    "\n",
    "# # Instantiate the model (no LoRA adapters added for Period 1)\n",
    "# class_gru_model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers, dropout, lora_r).to(device)\n",
    "\n",
    "# # Define loss function, optimizer, and scheduler\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(class_gru_model.parameters(), lr=0.0001)  # Optimize all parameters for Period 1\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# # Start training\n",
    "# train_and_validate(class_gru_model, output_size, criterion, optimizer, X_train, y_train, X_val, y_val, scheduler, \n",
    "#                    use_scheduler=False, num_epochs=num_epochs, batch_size=batch_size, \n",
    "#                    model_saving_folder=model_saving_folder, model_name=model_name, stop_signal_file=stop_signal_file)\n",
    "\n",
    "# # Print model and class information after training\n",
    "# print(f\"\\nclass_gru_model: \\n{class_gru_model}\")\n",
    "# print(f\"\\nunique_classes = {unique_classes}\")\n",
    "# print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "# # Clean up memory\n",
    "# for var in [\"X_train\", \"y_train\", \"X_val\", \"y_val\", \"X_test\", \"y_test\", \"Number_features\", \"unique_classes\", \"num_classes\"]:\n",
    "#     if var in locals():\n",
    "#         del locals()[var]\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Period 2 (num_layers = 4, lora_r=4, alpha = 0.5)\n",
    "+ ##### BiGRUWithAttention\n",
    "+ ##### Training and saving in *'LoRA_v6/Rank_4_Period_2/alpha_0.5'*\n",
    "#### __Val Accuracy: 97.88%__\n",
    "#### __Val-Class-Acc: {0: '99.10%', 1: '97.37%', 2: '93.64%'}__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "        with_indicators_file_path = list_period_files_full_path[1], # Change \n",
    "        downsampled_data_minutes = downsampled_data_minutes,\n",
    "        exclude_columns = exclude_columns,\n",
    "        lower_threshold = lower_threshold,\n",
    "        upper_threshold = upper_threshold,\n",
    "        reverse_steps = reverse_steps,\n",
    "        sequence_length = sequence_length,\n",
    "        sliding_interval = sliding_interval,\n",
    "        trends_to_keep = {0, 1, 2}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "    )\n",
    "\n",
    "print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "stable_classes = [1]    # From Period 1: Exclude class 0 because it changes\n",
    "input_size = Number_features    # Number of input features\n",
    "hidden_size = 64    # Number of GRU units\n",
    "output_size = num_classes   # Number of trend classes (3 for Period 2: {0, 1, 2})\n",
    "num_layers = 4  # Number of GRU layers\n",
    "dropout = 0.0   # Dropout rate\n",
    "lora_r = 4      # Rank of LoRA matrices\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "alpha = 0.5  # Weighting factor for distillation loss\n",
    "num_epochs = 2000   # Number of training epochs\n",
    "batch_size = 64     # Batch size for DataLoader\n",
    "model_name = 'BiGRUWithAttention'   # Model name for saving\n",
    "best_results = []   # List to store best results\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define paths\n",
    "stop_signal_file = os.path.normpath(os.path.join('Class_Incremental_CL', 'David_Classif_Bi_Dir_GRU_Model/stop_training.txt'))\n",
    "model_saving_folder = os.path.normpath(os.path.join('Class_Incremental_CL', \"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_2/alpha_0.5\"))\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# Load class features from Period 1\n",
    "class_features_path = os.path.normpath(os.path.join('Class_Incremental_CL', \"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_1/alpha_0.5/class_features.pkl\"))\n",
    "if os.path.exists(class_features_path):\n",
    "    with open(class_features_path, 'rb') as f:\n",
    "        class_features_dict = pickle.load(f)\n",
    "    print(f\"Loaded class features from: {class_features_path}\")\n",
    "else:\n",
    "    class_features_dict = {}\n",
    "    print(\"No previous class features found.\")\n",
    "\n",
    "# Instantiate models\n",
    "student_model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers, dropout, lora_r).to(device)\n",
    "teacher_model = BiGRUWithAttention(input_size, hidden_size, output_size - 1, num_layers, dropout, lora_r).to(device)\n",
    "\n",
    "# Load teacher model from Period 1\n",
    "teacher_checkpoint_path = \"Class_Incremental_CL/David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v2/Rank_4_Period_1/1st_try/BiGRUWithAttention_best.pth\"\n",
    "teacher_checkpoint = torch.load(teacher_checkpoint_path, map_location=device, weights_only=True)\n",
    "num_lora_adapters = teacher_checkpoint.get('num_lora_adapters', 0)  # Period 1 may not have LoRA adapters\n",
    "related_labels = {'attention_fc': [0, 1]}  # Initialize related_labels for Period 2\n",
    "print(f\"Teacher model checkpoint has {num_lora_adapters} LoRA adapters.\")\n",
    "print(f\"Initialized related_labels: {related_labels}\")\n",
    "\n",
    "# Add LoRA adapters to teacher_model before loading state_dict\n",
    "for _ in range(num_lora_adapters):\n",
    "    teacher_model.add_lora_adapter()\n",
    "teacher_model.load_state_dict(teacher_checkpoint['model_state_dict'])\n",
    "print(f\"Loaded teacher model from: \\n\\t{teacher_checkpoint_path}\")\n",
    "print(f\"Teacher model now has {len(teacher_model.lora_adapters)} LoRA adapters.\")\n",
    "print(f\"\\n{teacher_model}\\n\")\n",
    "del teacher_checkpoint\n",
    "gc.collect()\n",
    "\n",
    "# Add the same number of LoRA adapters to student_model\n",
    "for _ in range(num_lora_adapters):\n",
    "    student_model.add_lora_adapter()\n",
    "print(f\"Student model now has {len(student_model.lora_adapters)} LoRA adapters.\")\n",
    "\n",
    "# Copy teacher weights to student (excluding FC layer)\n",
    "state_dict_teacher = teacher_model.state_dict()\n",
    "state_dict_student = student_model.state_dict()\n",
    "for name, param in state_dict_teacher.items():\n",
    "    if 'fc' not in name:\n",
    "        state_dict_student[name].copy_(param)\n",
    "student_model.load_state_dict(state_dict_student)\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "class_features_dict = train_and_validate_lora(\n",
    "    student_model, \n",
    "    teacher_model, \n",
    "    stable_classes, \n",
    "    output_size, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    scheduler, \n",
    "    use_scheduler=False, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    model_saving_folder=model_saving_folder, \n",
    "    model_name=model_name, \n",
    "    stop_signal_file=stop_signal_file,\n",
    "    class_features_dict=class_features_dict,\n",
    "    tau_high=0.5,  # Adjusted to 0.5\n",
    "    tau_low=0.5,   # Adjusted to 0.5\n",
    "    related_labels=related_labels  # Pass initialized related_labels\n",
    ")\n",
    "\n",
    "# Save class features\n",
    "class_features_path = os.path.join(model_saving_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, 'wb') as f:\n",
    "    pickle.dump(class_features_dict, f)\n",
    "print(f\"Saved class features to: {class_features_path}\")\n",
    "\n",
    "print(f\"\\nstudent_model: \\n{student_model}\\n\")\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "# Clean up\n",
    "for var in [\"X_train\", \"y_train\", \"X_val\", \"y_val\", \"X_test\", \"y_test\", \"Number_features\", \"unique_classes\", \"num_classes\"]:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Period 3 (num_layers = 4, lora_r=4, alpha = 0.5)\n",
    "+ ##### BiGRUWithAttention\n",
    "+ ##### Training and saving in *'LoRA_v6/Rank_4_Period_3/alpha_0.5'*\n",
    "#### __Val Accuracy: 96.66%__\n",
    "#### __Val-Class-Acc: {0: '86.80%', 1: '97.96%', 2: '94.42%', 3: '97.95%'}__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "        with_indicators_file_path = list_period_files_full_path[2], # Change \n",
    "        downsampled_data_minutes = downsampled_data_minutes,\n",
    "        exclude_columns = exclude_columns,\n",
    "        lower_threshold = lower_threshold,\n",
    "        upper_threshold = upper_threshold,\n",
    "        reverse_steps = reverse_steps,\n",
    "        sequence_length = sequence_length,\n",
    "        sliding_interval = sliding_interval,\n",
    "        trends_to_keep = {0, 1, 2, 3}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "    )\n",
    "\n",
    "print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "stable_classes = [1, 2]  # From Period 2: Exclude class 0 because it changes\n",
    "input_size = Number_features    # Number of input features\n",
    "hidden_size = 64    # Number of GRU units\n",
    "output_size = num_classes   # Number of trend classes\n",
    "num_layers = 4  # Number of GRU layers\n",
    "dropout = 0.0   # Dropout rate\n",
    "lora_r = 4      # Rank of LoRA matrices\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "alpha = 0.5  # Weighting factor for distillation loss\n",
    "num_epochs = 2000   # Number of training epochs\n",
    "batch_size = 64     # Batch size for DataLoader\n",
    "model_name = 'BiGRUWithAttention'   # Model name for saving\n",
    "best_results = []   # List to store best results\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define paths\n",
    "stop_signal_file = os.path.normpath(os.path.join('Class_Incremental_CL', 'David_Classif_Bi_Dir_GRU_Model/stop_training.txt'))\n",
    "model_saving_folder = os.path.normpath(os.path.join('Class_Incremental_CL', \"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_3/alpha_0.5\"))\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# Load class features from Period 2\n",
    "class_features_path = os.path.normpath(os.path.join('Class_Incremental_CL', \"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_2/alpha_0.5/class_features.pkl\"))\n",
    "if os.path.exists(class_features_path):\n",
    "    with open(class_features_path, 'rb') as f:\n",
    "        class_features_dict = pickle.load(f)\n",
    "    print(f\"Loaded class features from: {class_features_path}\")\n",
    "else:\n",
    "    class_features_dict = {}\n",
    "    print(\"No previous class features found.\")\n",
    "\n",
    "# Instantiate models\n",
    "student_model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers, dropout, lora_r).to(device)\n",
    "teacher_model = BiGRUWithAttention(input_size, hidden_size, output_size - 1, num_layers, dropout, lora_r).to(device)\n",
    "\n",
    "# Load teacher model from Period 2\n",
    "teacher_checkpoint_path = \"Class_Incremental_CL/David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_2/alpha_0.5/BiGRUWithAttention_best.pth\"\n",
    "teacher_checkpoint = torch.load(teacher_checkpoint_path, map_location=device, weights_only=True)\n",
    "num_lora_adapters = teacher_checkpoint.get('num_lora_adapters', 0)\n",
    "related_labels = teacher_checkpoint.get('related_labels', {'attention_fc': [0, 1]})  # Load related_labels, default if not found\n",
    "print(f\"Teacher model checkpoint has {num_lora_adapters} LoRA adapters.\")\n",
    "print(f\"Loaded related_labels: {related_labels}\")\n",
    "\n",
    "# Add LoRA adapters to teacher_model before loading state_dict\n",
    "for _ in range(num_lora_adapters):\n",
    "    teacher_model.add_lora_adapter()\n",
    "teacher_model.load_state_dict(teacher_checkpoint['model_state_dict'])\n",
    "print(f\"Loaded teacher model from: \\n\\t{teacher_checkpoint_path}\")\n",
    "print(f\"Teacher model now has {len(teacher_model.lora_adapters)} LoRA adapters.\")\n",
    "print(f\"\\n{teacher_model}\\n\")\n",
    "del teacher_checkpoint\n",
    "gc.collect()\n",
    "\n",
    "# Add the same number of LoRA adapters to student_model\n",
    "for _ in range(num_lora_adapters):\n",
    "    student_model.add_lora_adapter()\n",
    "print(f\"Student model now has {len(student_model.lora_adapters)} LoRA adapters.\")\n",
    "\n",
    "# Copy teacher weights to student (excluding FC layer)\n",
    "state_dict_teacher = teacher_model.state_dict()\n",
    "state_dict_student = student_model.state_dict()\n",
    "for name, param in state_dict_teacher.items():\n",
    "    if 'fc' not in name:\n",
    "        state_dict_student[name].copy_(param)\n",
    "student_model.load_state_dict(state_dict_student)\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# Train and validate with related_labels\n",
    "class_features_dict = train_and_validate_lora(\n",
    "    student_model, \n",
    "    teacher_model, \n",
    "    stable_classes, \n",
    "    output_size, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    scheduler, \n",
    "    use_scheduler=False, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    model_saving_folder=model_saving_folder, \n",
    "    model_name=model_name, \n",
    "    stop_signal_file=stop_signal_file,\n",
    "    class_features_dict=class_features_dict,\n",
    "    tau_high=0.5,  # Adjusted to 0.5\n",
    "    tau_low=0.5,   # Adjusted to 0.5\n",
    "    related_labels=related_labels  # Pass related_labels\n",
    ")\n",
    "\n",
    "# Save class features\n",
    "class_features_path = os.path.join(model_saving_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, 'wb') as f:\n",
    "    pickle.dump(class_features_dict, f)\n",
    "print(f\"Saved class features to: {class_features_path}\")\n",
    "\n",
    "print(f\"\\nstudent_model: \\n{student_model}\\n\")\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "# Clean up\n",
    "for var in [\"X_train\", \"y_train\", \"X_val\", \"y_val\", \"X_test\", \"y_test\", \"Number_features\", \"unique_classes\", \"num_classes\"]:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Period 4 (num_layers = 4, lora_r=4, alpha = 0.5)\n",
    "+ ##### BiGRUWithAttention\n",
    "+ ##### Training and saving in *'LoRA_v6/Rank_4_Period_4/alpha_0.5'*\n",
    "#### __Val Accuracy: 95.72%__\n",
    "#### __Val-Class-Acc: {0: '91.05%', 1: '96.64%', 2: '88.68%', 3: '98.32%', 4: '96.46%'}__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "        with_indicators_file_path = list_period_files_full_path[3], # Change \n",
    "        downsampled_data_minutes = downsampled_data_minutes,\n",
    "        exclude_columns = exclude_columns,\n",
    "        lower_threshold = lower_threshold,\n",
    "        upper_threshold = upper_threshold,\n",
    "        reverse_steps = reverse_steps,\n",
    "        sequence_length = sequence_length,\n",
    "        sliding_interval = sliding_interval,\n",
    "        trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "    )\n",
    "\n",
    "print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "stable_classes = [1, 2, 3]  # From Period 3: Exclude class 0 because it changes\n",
    "input_size = Number_features    # Number of input features\n",
    "hidden_size = 64    # Number of GRU units\n",
    "output_size = num_classes   # Number of trend classes (5 for Period 4: {0, 1, 2, 3, 4})\n",
    "num_layers = 4  # Number of GRU layers\n",
    "dropout = 0.0   # Dropout rate\n",
    "lora_r = 4      # Rank of LoRA matrices\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "alpha = 0.5  # Weighting factor for distillation loss\n",
    "num_epochs = 2000   # Number of training epochs\n",
    "batch_size = 64     # Batch size for DataLoader\n",
    "model_name = 'BiGRUWithAttention'   # Model name for saving\n",
    "best_results = []   # List to store best results\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define paths\n",
    "stop_signal_file = os.path.normpath(os.path.join('Class_Incremental_CL', 'David_Classif_Bi_Dir_GRU_Model/stop_training.txt'))\n",
    "model_saving_folder = os.path.normpath(os.path.join('Class_Incremental_CL', \"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_4/alpha_0.5\"))\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# Load class features from Period 3\n",
    "class_features_path = os.path.normpath(os.path.join('Class_Incremental_CL', \"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_3/alpha_0.5/class_features.pkl\"))\n",
    "if os.path.exists(class_features_path):\n",
    "    with open(class_features_path, 'rb') as f:\n",
    "        class_features_dict = pickle.load(f)\n",
    "    print(f\"Loaded class features from: {class_features_path}\")\n",
    "else:\n",
    "    class_features_dict = {}\n",
    "    print(\"No previous class features found.\")\n",
    "    \n",
    "# Instantiate models\n",
    "student_model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers, dropout, lora_r).to(device)\n",
    "teacher_model = BiGRUWithAttention(input_size, hidden_size, output_size - 1, num_layers, dropout, lora_r).to(device)\n",
    "\n",
    "# Load teacher model from Period 3\n",
    "teacher_checkpoint_path = \"Class_Incremental_CL/David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_3/alpha_0.5/BiGRUWithAttention_best.pth\"\n",
    "teacher_checkpoint = torch.load(teacher_checkpoint_path, map_location=device, weights_only=True)\n",
    "num_lora_adapters = teacher_checkpoint.get('num_lora_adapters', 0)\n",
    "related_labels = teacher_checkpoint.get('related_labels', {'attention_fc': [0, 1]})  # Load related_labels, default if not found\n",
    "print(f\"Teacher model checkpoint has {num_lora_adapters} LoRA adapters.\")\n",
    "print(f\"Loaded related_labels: {related_labels}\")\n",
    "\n",
    "# Add LoRA adapters to teacher_model before loading state_dict\n",
    "for _ in range(num_lora_adapters):\n",
    "    teacher_model.add_lora_adapter()\n",
    "teacher_model.load_state_dict(teacher_checkpoint['model_state_dict'])\n",
    "print(f\"Loaded teacher model from: \\n\\t{teacher_checkpoint_path}\")\n",
    "print(f\"Teacher model now has {len(teacher_model.lora_adapters)} LoRA adapters.\")\n",
    "print(f\"\\n{teacher_model}\\n\")\n",
    "del teacher_checkpoint\n",
    "gc.collect()\n",
    "\n",
    "# Add the same number of LoRA adapters to student_model\n",
    "for _ in range(num_lora_adapters):\n",
    "    student_model.add_lora_adapter()\n",
    "print(f\"Student model now has {len(student_model.lora_adapters)} LoRA adapters.\")\n",
    "\n",
    "# Copy teacher weights to student (excluding FC layer)\n",
    "state_dict_teacher = teacher_model.state_dict()\n",
    "state_dict_student = student_model.state_dict()\n",
    "for name, param in state_dict_teacher.items():\n",
    "    if 'fc' not in name:\n",
    "        state_dict_student[name].copy_(param)\n",
    "student_model.load_state_dict(state_dict_student)\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# Train and validate with related_labels\n",
    "class_features_dict = train_and_validate_lora(\n",
    "    student_model, \n",
    "    teacher_model, \n",
    "    stable_classes, \n",
    "    output_size, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    scheduler, \n",
    "    use_scheduler=False, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    model_saving_folder=model_saving_folder, \n",
    "    model_name=model_name, \n",
    "    stop_signal_file=stop_signal_file,\n",
    "    class_features_dict=class_features_dict,\n",
    "    tau_high=0.5,  # Adjusted to 0.5\n",
    "    tau_low=0.5,   # Adjusted to 0.5\n",
    "    related_labels=related_labels  # Pass related_labels\n",
    ")\n",
    "\n",
    "# Save class features\n",
    "class_features_path = os.path.join(model_saving_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, 'wb') as f:\n",
    "    pickle.dump(class_features_dict, f)\n",
    "print(f\"Saved class features to: {class_features_path}\")\n",
    "\n",
    "print(f\"\\nstudent_model: \\n{student_model}\\n\")\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "# Clean up\n",
    "for var in [\"X_train\", \"y_train\", \"X_val\", \"y_val\", \"X_test\", \"y_test\", \"Number_features\", \"unique_classes\", \"num_classes\"]:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Period 2 (num_layers = 4, lora_r=4, alpha = 0.4)\n",
    "+ ##### BiGRUWithAttention\n",
    "+ ##### Training and saving in *'LoRA_v6/Rank_4_Period_2/alpha_0.4'*\n",
    "#### __Val Accuracy: 97.23%__\n",
    "#### __Val-Class-Acc: {0: '99.30%', 1: '96.83%', 2: '88.55%'}__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "        with_indicators_file_path = list_period_files_full_path[1], # Change \n",
    "        downsampled_data_minutes = downsampled_data_minutes,\n",
    "        exclude_columns = exclude_columns,\n",
    "        lower_threshold = lower_threshold,\n",
    "        upper_threshold = upper_threshold,\n",
    "        reverse_steps = reverse_steps,\n",
    "        sequence_length = sequence_length,\n",
    "        sliding_interval = sliding_interval,\n",
    "        trends_to_keep = {0, 1, 2}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "    )\n",
    "\n",
    "print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "stable_classes = [1]    # From Period 1: Exclude class 0 because it changes\n",
    "input_size = Number_features    # Number of input features\n",
    "hidden_size = 64    # Number of GRU units\n",
    "output_size = num_classes   # Number of trend classes (3 for Period 2: {0, 1, 2})\n",
    "num_layers = 4  # Number of GRU layers\n",
    "dropout = 0.0   # Dropout rate\n",
    "lora_r = 4      # Rank of LoRA matrices\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "alpha = 0.4  # Weighting factor for distillation loss\n",
    "num_epochs = 2000   # Number of training epochs\n",
    "batch_size = 64     # Batch size for DataLoader\n",
    "model_name = 'BiGRUWithAttention'   # Model name for saving\n",
    "best_results = []   # List to store best results\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define paths\n",
    "stop_signal_file = os.path.normpath(os.path.join('Class_Incremental_CL', 'David_Classif_Bi_Dir_GRU_Model/stop_training.txt'))\n",
    "model_saving_folder = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_2/alpha_{alpha}\"))\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# Load class features from Period 1\n",
    "class_features_path = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_1/alpha_{alpha}/class_features.pkl\"))\n",
    "if os.path.exists(class_features_path):\n",
    "    with open(class_features_path, 'rb') as f:\n",
    "        class_features_dict = pickle.load(f)\n",
    "    print(f\"Loaded class features from: {class_features_path}\")\n",
    "else:\n",
    "    class_features_dict = {}\n",
    "    print(\"No previous class features found.\")\n",
    "\n",
    "# Instantiate models\n",
    "student_model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers, dropout, lora_r).to(device)\n",
    "teacher_model = BiGRUWithAttention(input_size, hidden_size, output_size - 1, num_layers, dropout, lora_r).to(device)\n",
    "\n",
    "# Load teacher model from Period 1\n",
    "teacher_checkpoint_path = \"Class_Incremental_CL/David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v2/Rank_4_Period_1/1st_try/BiGRUWithAttention_best.pth\"\n",
    "teacher_checkpoint = torch.load(teacher_checkpoint_path, map_location=device, weights_only=True)\n",
    "num_lora_adapters = teacher_checkpoint.get('num_lora_adapters', 0)  # Period 1 may not have LoRA adapters\n",
    "related_labels = {'attention_fc': [0, 1]}  # Initialize related_labels for Period 2\n",
    "print(f\"Teacher model checkpoint has {num_lora_adapters} LoRA adapters.\")\n",
    "print(f\"Initialized related_labels: {related_labels}\")\n",
    "\n",
    "# Add LoRA adapters to teacher_model before loading state_dict\n",
    "for _ in range(num_lora_adapters):\n",
    "    teacher_model.add_lora_adapter()\n",
    "teacher_model.load_state_dict(teacher_checkpoint['model_state_dict'])\n",
    "print(f\"Loaded teacher model from: \\n\\t{teacher_checkpoint_path}\")\n",
    "print(f\"Teacher model now has {len(teacher_model.lora_adapters)} LoRA adapters.\")\n",
    "print(f\"\\n{teacher_model}\\n\")\n",
    "del teacher_checkpoint\n",
    "gc.collect()\n",
    "\n",
    "# Add the same number of LoRA adapters to student_model\n",
    "for _ in range(num_lora_adapters):\n",
    "    student_model.add_lora_adapter()\n",
    "print(f\"Student model now has {len(student_model.lora_adapters)} LoRA adapters.\")\n",
    "\n",
    "# Copy teacher weights to student (excluding FC layer)\n",
    "state_dict_teacher = teacher_model.state_dict()\n",
    "state_dict_student = student_model.state_dict()\n",
    "for name, param in state_dict_teacher.items():\n",
    "    if 'fc' not in name:\n",
    "        state_dict_student[name].copy_(param)\n",
    "student_model.load_state_dict(state_dict_student)\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "class_features_dict = train_and_validate_lora(\n",
    "    student_model, \n",
    "    teacher_model, \n",
    "    stable_classes, \n",
    "    output_size, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    scheduler, \n",
    "    use_scheduler=False, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    model_saving_folder=model_saving_folder, \n",
    "    model_name=model_name, \n",
    "    stop_signal_file=stop_signal_file,\n",
    "    class_features_dict=class_features_dict,\n",
    "    tau_high=0.5,  # Adjusted to 0.5\n",
    "    tau_low=0.5,   # Adjusted to 0.5\n",
    "    related_labels=related_labels  # Pass initialized related_labels\n",
    ")\n",
    "\n",
    "# Save class features\n",
    "class_features_path = os.path.join(model_saving_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, 'wb') as f:\n",
    "    pickle.dump(class_features_dict, f)\n",
    "print(f\"Saved class features to: {class_features_path}\")\n",
    "\n",
    "print(f\"\\nstudent_model: \\n{student_model}\\n\")\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "# Clean up\n",
    "for var in [\"X_train\", \"y_train\", \"X_val\", \"y_val\", \"X_test\", \"y_test\", \"Number_features\", \"unique_classes\", \"num_classes\"]:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Period 3 (num_layers = 4, lora_r=4, alpha = 0.4)\n",
    "+ ##### BiGRUWithAttention\n",
    "+ ##### Training and saving in *'LoRA_v6/Rank_4_Period_3/alpha_0.4'*\n",
    "#### __Val Accuracy: 97.49%__\n",
    "#### __Val-Class-Acc: {0: '89.02%', 1: '98.69%', 2: '99.01%', 3: '98.12%'}__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "        with_indicators_file_path = list_period_files_full_path[2], # Change \n",
    "        downsampled_data_minutes = downsampled_data_minutes,\n",
    "        exclude_columns = exclude_columns,\n",
    "        lower_threshold = lower_threshold,\n",
    "        upper_threshold = upper_threshold,\n",
    "        reverse_steps = reverse_steps,\n",
    "        sequence_length = sequence_length,\n",
    "        sliding_interval = sliding_interval,\n",
    "        trends_to_keep = {0, 1, 2, 3}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "    )\n",
    "\n",
    "print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "stable_classes = [1, 2]  # From Period 2: Exclude class 0 because it changes\n",
    "input_size = Number_features    # Number of input features\n",
    "hidden_size = 64    # Number of GRU units\n",
    "output_size = num_classes   # Number of trend classes\n",
    "num_layers = 4  # Number of GRU layers\n",
    "dropout = 0.0   # Dropout rate\n",
    "lora_r = 4      # Rank of LoRA matrices\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "alpha = 0.4 # Weighting factor for distillation loss\n",
    "num_epochs = 2000   # Number of training epochs\n",
    "batch_size = 64     # Batch size for DataLoader\n",
    "model_name = 'BiGRUWithAttention'   # Model name for saving\n",
    "best_results = []   # List to store best results\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define paths\n",
    "stop_signal_file = os.path.normpath(os.path.join('Class_Incremental_CL', 'David_Classif_Bi_Dir_GRU_Model/stop_training.txt'))\n",
    "model_saving_folder = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_3/alpha_{alpha}\"))\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# Load class features from Period 2\n",
    "class_features_path = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_2/alpha_{alpha}/class_features.pkl\"))\n",
    "if os.path.exists(class_features_path):\n",
    "    with open(class_features_path, 'rb') as f:\n",
    "        class_features_dict = pickle.load(f)\n",
    "    print(f\"Loaded class features from: {class_features_path}\")\n",
    "else:\n",
    "    class_features_dict = {}\n",
    "    print(\"No previous class features found.\")\n",
    "\n",
    "# Instantiate models\n",
    "student_model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers, dropout, lora_r).to(device)\n",
    "teacher_model = BiGRUWithAttention(input_size, hidden_size, output_size - 1, num_layers, dropout, lora_r).to(device)\n",
    "\n",
    "# Load teacher model from Period 2\n",
    "teacher_checkpoint_path = f\"Class_Incremental_CL/David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_2/alpha_{alpha}/BiGRUWithAttention_best.pth\"\n",
    "teacher_checkpoint = torch.load(teacher_checkpoint_path, map_location=device, weights_only=True)\n",
    "num_lora_adapters = teacher_checkpoint.get('num_lora_adapters', 0)\n",
    "related_labels = teacher_checkpoint.get('related_labels', {'attention_fc': [0, 1]})  # Load related_labels, default if not found\n",
    "print(f\"Teacher model checkpoint has {num_lora_adapters} LoRA adapters.\")\n",
    "print(f\"Loaded related_labels: {related_labels}\")\n",
    "\n",
    "# Add LoRA adapters to teacher_model before loading state_dict\n",
    "for _ in range(num_lora_adapters):\n",
    "    teacher_model.add_lora_adapter()\n",
    "teacher_model.load_state_dict(teacher_checkpoint['model_state_dict'])\n",
    "print(f\"Loaded teacher model from: \\n\\t{teacher_checkpoint_path}\")\n",
    "print(f\"Teacher model now has {len(teacher_model.lora_adapters)} LoRA adapters.\")\n",
    "print(f\"\\n{teacher_model}\\n\")\n",
    "del teacher_checkpoint\n",
    "gc.collect()\n",
    "\n",
    "# Add the same number of LoRA adapters to student_model\n",
    "for _ in range(num_lora_adapters):\n",
    "    student_model.add_lora_adapter()\n",
    "print(f\"Student model now has {len(student_model.lora_adapters)} LoRA adapters.\")\n",
    "\n",
    "# Copy teacher weights to student (excluding FC layer)\n",
    "state_dict_teacher = teacher_model.state_dict()\n",
    "state_dict_student = student_model.state_dict()\n",
    "for name, param in state_dict_teacher.items():\n",
    "    if 'fc' not in name:\n",
    "        state_dict_student[name].copy_(param)\n",
    "student_model.load_state_dict(state_dict_student)\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# Train and validate with related_labels\n",
    "class_features_dict = train_and_validate_lora(\n",
    "    student_model, \n",
    "    teacher_model, \n",
    "    stable_classes, \n",
    "    output_size, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    scheduler, \n",
    "    use_scheduler=False, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    model_saving_folder=model_saving_folder, \n",
    "    model_name=model_name, \n",
    "    stop_signal_file=stop_signal_file,\n",
    "    class_features_dict=class_features_dict,\n",
    "    tau_high=0.5,  # Adjusted to 0.5\n",
    "    tau_low=0.5,   # Adjusted to 0.5\n",
    "    related_labels=related_labels  # Pass related_labels\n",
    ")\n",
    "\n",
    "# Save class features\n",
    "class_features_path = os.path.join(model_saving_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, 'wb') as f:\n",
    "    pickle.dump(class_features_dict, f)\n",
    "print(f\"Saved class features to: {class_features_path}\")\n",
    "\n",
    "print(f\"\\nstudent_model: \\n{student_model}\\n\")\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "# Clean up\n",
    "for var in [\"X_train\", \"y_train\", \"X_val\", \"y_val\", \"X_test\", \"y_test\", \"Number_features\", \"unique_classes\", \"num_classes\"]:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Period 4 (num_layers = 4, lora_r=4, alpha = 0.4)\n",
    "+ ##### BiGRUWithAttention\n",
    "+ ##### Training and saving in *'LoRA_v6/Rank_4_Period_4/alpha_0.4*\n",
    "#### __Val Accuracy: 95.81%__\n",
    "#### __Val-Class-Acc: {0: '88.85%', 1: '98.32%', 2: '90.77%', 3: '97.29%', 4: '92.80%'}__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "        with_indicators_file_path = list_period_files_full_path[3], # Change \n",
    "        downsampled_data_minutes = downsampled_data_minutes,\n",
    "        exclude_columns = exclude_columns,\n",
    "        lower_threshold = lower_threshold,\n",
    "        upper_threshold = upper_threshold,\n",
    "        reverse_steps = reverse_steps,\n",
    "        sequence_length = sequence_length,\n",
    "        sliding_interval = sliding_interval,\n",
    "        trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "    )\n",
    "\n",
    "print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "stable_classes = [1, 2, 3]  # From Period 3: Exclude class 0 because it changes\n",
    "input_size = Number_features    # Number of input features\n",
    "hidden_size = 64    # Number of GRU units\n",
    "output_size = num_classes   # Number of trend classes (5 for Period 4: {0, 1, 2, 3, 4})\n",
    "num_layers = 4  # Number of GRU layers\n",
    "dropout = 0.0   # Dropout rate\n",
    "lora_r = 4      # Rank of LoRA matrices\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "alpha = 0.4 # Weighting factor for distillation loss\n",
    "num_epochs = 2000   # Number of training epochs\n",
    "batch_size = 64     # Batch size for DataLoader\n",
    "model_name = 'BiGRUWithAttention'   # Model name for saving\n",
    "best_results = []   # List to store best results\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define paths\n",
    "stop_signal_file = os.path.normpath(os.path.join('Class_Incremental_CL', 'David_Classif_Bi_Dir_GRU_Model/stop_training.txt'))\n",
    "model_saving_folder = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_4/alpha_{alpha}\"))\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# Load class features from Period 3\n",
    "class_features_path = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_3/alpha_{alpha}/class_features.pkl\"))\n",
    "if os.path.exists(class_features_path):\n",
    "    with open(class_features_path, 'rb') as f:\n",
    "        class_features_dict = pickle.load(f)\n",
    "    print(f\"Loaded class features from: {class_features_path}\")\n",
    "else:\n",
    "    class_features_dict = {}\n",
    "    print(\"No previous class features found.\")\n",
    "    \n",
    "# Instantiate models\n",
    "student_model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers, dropout, lora_r).to(device)\n",
    "teacher_model = BiGRUWithAttention(input_size, hidden_size, output_size - 1, num_layers, dropout, lora_r).to(device)\n",
    "\n",
    "# Load teacher model from Period 3\n",
    "teacher_checkpoint_path = f\"Class_Incremental_CL/David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_3/alpha_{alpha}/BiGRUWithAttention_best.pth\"\n",
    "teacher_checkpoint = torch.load(teacher_checkpoint_path, map_location=device, weights_only=True)\n",
    "num_lora_adapters = teacher_checkpoint.get('num_lora_adapters', 0)\n",
    "related_labels = teacher_checkpoint.get('related_labels', {'attention_fc': [0, 1]})  # Load related_labels, default if not found\n",
    "print(f\"Teacher model checkpoint has {num_lora_adapters} LoRA adapters.\")\n",
    "print(f\"Loaded related_labels: {related_labels}\")\n",
    "\n",
    "# Add LoRA adapters to teacher_model before loading state_dict\n",
    "for _ in range(num_lora_adapters):\n",
    "    teacher_model.add_lora_adapter()\n",
    "teacher_model.load_state_dict(teacher_checkpoint['model_state_dict'])\n",
    "print(f\"Loaded teacher model from: \\n\\t{teacher_checkpoint_path}\")\n",
    "print(f\"Teacher model now has {len(teacher_model.lora_adapters)} LoRA adapters.\")\n",
    "print(f\"\\n{teacher_model}\\n\")\n",
    "del teacher_checkpoint\n",
    "gc.collect()\n",
    "\n",
    "# Add the same number of LoRA adapters to student_model\n",
    "for _ in range(num_lora_adapters):\n",
    "    student_model.add_lora_adapter()\n",
    "print(f\"Student model now has {len(student_model.lora_adapters)} LoRA adapters.\")\n",
    "\n",
    "# Copy teacher weights to student (excluding FC layer)\n",
    "state_dict_teacher = teacher_model.state_dict()\n",
    "state_dict_student = student_model.state_dict()\n",
    "for name, param in state_dict_teacher.items():\n",
    "    if 'fc' not in name:\n",
    "        state_dict_student[name].copy_(param)\n",
    "student_model.load_state_dict(state_dict_student)\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# Train and validate with related_labels\n",
    "class_features_dict = train_and_validate_lora(\n",
    "    student_model, \n",
    "    teacher_model, \n",
    "    stable_classes, \n",
    "    output_size, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    scheduler, \n",
    "    use_scheduler=False, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    model_saving_folder=model_saving_folder, \n",
    "    model_name=model_name, \n",
    "    stop_signal_file=stop_signal_file,\n",
    "    class_features_dict=class_features_dict,\n",
    "    tau_high=0.5,  # Adjusted to 0.5\n",
    "    tau_low=0.5,   # Adjusted to 0.5\n",
    "    related_labels=related_labels  # Pass related_labels\n",
    ")\n",
    "\n",
    "# Save class features\n",
    "class_features_path = os.path.join(model_saving_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, 'wb') as f:\n",
    "    pickle.dump(class_features_dict, f)\n",
    "print(f\"Saved class features to: {class_features_path}\")\n",
    "\n",
    "print(f\"\\nstudent_model: \\n{student_model}\\n\")\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "# Clean up\n",
    "for var in [\"X_train\", \"y_train\", \"X_val\", \"y_val\", \"X_test\", \"y_test\", \"Number_features\", \"unique_classes\", \"num_classes\"]:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Period 2 (num_layers = 4, lora_r=4, alpha = 0.3)\n",
    "+ ##### BiGRUWithAttention\n",
    "+ ##### Training and saving in *'LoRA_v6/Rank_4_Period_2/alpha_0.3'*\n",
    "#### __Val Accuracy: 97.54%__\n",
    "#### __Val-Class-Acc: {0: '98.79%', 1: '97.73%', 2: '90.88%'}__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "        with_indicators_file_path = list_period_files_full_path[1], # Change \n",
    "        downsampled_data_minutes = downsampled_data_minutes,\n",
    "        exclude_columns = exclude_columns,\n",
    "        lower_threshold = lower_threshold,\n",
    "        upper_threshold = upper_threshold,\n",
    "        reverse_steps = reverse_steps,\n",
    "        sequence_length = sequence_length,\n",
    "        sliding_interval = sliding_interval,\n",
    "        trends_to_keep = {0, 1, 2}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "    )\n",
    "\n",
    "print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "stable_classes = [1]    # From Period 1: Exclude class 0 because it changes\n",
    "input_size = Number_features    # Number of input features\n",
    "hidden_size = 64    # Number of GRU units\n",
    "output_size = num_classes   # Number of trend classes (3 for Period 2: {0, 1, 2})\n",
    "num_layers = 4  # Number of GRU layers\n",
    "dropout = 0.0   # Dropout rate\n",
    "lora_r = 4      # Rank of LoRA matrices\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "alpha = 0.3  # Weighting factor for distillation loss\n",
    "num_epochs = 2000   # Number of training epochs\n",
    "batch_size = 64     # Batch size for DataLoader\n",
    "model_name = 'BiGRUWithAttention'   # Model name for saving\n",
    "best_results = []   # List to store best results\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define paths\n",
    "stop_signal_file = os.path.normpath(os.path.join('Class_Incremental_CL', 'David_Classif_Bi_Dir_GRU_Model/stop_training.txt'))\n",
    "model_saving_folder = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_2/alpha_{alpha}\"))\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# Load class features from Period 1\n",
    "class_features_path = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_1/alpha_{alpha}/class_features.pkl\"))\n",
    "if os.path.exists(class_features_path):\n",
    "    with open(class_features_path, 'rb') as f:\n",
    "        class_features_dict = pickle.load(f)\n",
    "    print(f\"Loaded class features from: {class_features_path}\")\n",
    "else:\n",
    "    class_features_dict = {}\n",
    "    print(\"No previous class features found.\")\n",
    "\n",
    "# Instantiate models\n",
    "student_model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers, dropout, lora_r).to(device)\n",
    "teacher_model = BiGRUWithAttention(input_size, hidden_size, output_size - 1, num_layers, dropout, lora_r).to(device)\n",
    "\n",
    "# Load teacher model from Period 1\n",
    "teacher_checkpoint_path = \"Class_Incremental_CL/David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v2/Rank_4_Period_1/1st_try/BiGRUWithAttention_best.pth\"\n",
    "teacher_checkpoint = torch.load(teacher_checkpoint_path, map_location=device, weights_only=True)\n",
    "num_lora_adapters = teacher_checkpoint.get('num_lora_adapters', 0)  # Period 1 may not have LoRA adapters\n",
    "related_labels = {'attention_fc': [0, 1]}  # Initialize related_labels for Period 2\n",
    "print(f\"Teacher model checkpoint has {num_lora_adapters} LoRA adapters.\")\n",
    "print(f\"Initialized related_labels: {related_labels}\")\n",
    "\n",
    "# Add LoRA adapters to teacher_model before loading state_dict\n",
    "for _ in range(num_lora_adapters):\n",
    "    teacher_model.add_lora_adapter()\n",
    "teacher_model.load_state_dict(teacher_checkpoint['model_state_dict'])\n",
    "print(f\"Loaded teacher model from: \\n\\t{teacher_checkpoint_path}\")\n",
    "print(f\"Teacher model now has {len(teacher_model.lora_adapters)} LoRA adapters.\")\n",
    "print(f\"\\n{teacher_model}\\n\")\n",
    "del teacher_checkpoint\n",
    "gc.collect()\n",
    "\n",
    "# Add the same number of LoRA adapters to student_model\n",
    "for _ in range(num_lora_adapters):\n",
    "    student_model.add_lora_adapter()\n",
    "print(f\"Student model now has {len(student_model.lora_adapters)} LoRA adapters.\")\n",
    "\n",
    "# Copy teacher weights to student (excluding FC layer)\n",
    "state_dict_teacher = teacher_model.state_dict()\n",
    "state_dict_student = student_model.state_dict()\n",
    "for name, param in state_dict_teacher.items():\n",
    "    if 'fc' not in name:\n",
    "        state_dict_student[name].copy_(param)\n",
    "student_model.load_state_dict(state_dict_student)\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "class_features_dict = train_and_validate_lora(\n",
    "    student_model, \n",
    "    teacher_model, \n",
    "    stable_classes, \n",
    "    output_size, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    scheduler, \n",
    "    use_scheduler=False, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    model_saving_folder=model_saving_folder, \n",
    "    model_name=model_name, \n",
    "    stop_signal_file=stop_signal_file,\n",
    "    class_features_dict=class_features_dict,\n",
    "    tau_high=0.5,  # Adjusted to 0.5\n",
    "    tau_low=0.5,   # Adjusted to 0.5\n",
    "    related_labels=related_labels  # Pass initialized related_labels\n",
    ")\n",
    "\n",
    "# Save class features\n",
    "class_features_path = os.path.join(model_saving_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, 'wb') as f:\n",
    "    pickle.dump(class_features_dict, f)\n",
    "print(f\"Saved class features to: {class_features_path}\")\n",
    "\n",
    "print(f\"\\nstudent_model: \\n{student_model}\\n\")\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "# Clean up\n",
    "for var in [\"X_train\", \"y_train\", \"X_val\", \"y_val\", \"X_test\", \"y_test\", \"Number_features\", \"unique_classes\", \"num_classes\"]:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Period 3 (num_layers = 4, lora_r=4, alpha = 0.3)\n",
    "+ ##### BiGRUWithAttention\n",
    "+ ##### Training and saving in *'LoRA_v6/Rank_4_Period_3/alpha_0.3'*\n",
    "#### __Val Accuracy: 97.30%__\n",
    "#### __Val-Class-Acc: {0: '89.89%', 1: '98.58%', 2: '96.34%', 3: '97.83%'}__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "        with_indicators_file_path = list_period_files_full_path[2], # Change \n",
    "        downsampled_data_minutes = downsampled_data_minutes,\n",
    "        exclude_columns = exclude_columns,\n",
    "        lower_threshold = lower_threshold,\n",
    "        upper_threshold = upper_threshold,\n",
    "        reverse_steps = reverse_steps,\n",
    "        sequence_length = sequence_length,\n",
    "        sliding_interval = sliding_interval,\n",
    "        trends_to_keep = {0, 1, 2, 3}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "    )\n",
    "\n",
    "print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "stable_classes = [1, 2]  # From Period 2: Exclude class 0 because it changes\n",
    "input_size = Number_features    # Number of input features\n",
    "hidden_size = 64    # Number of GRU units\n",
    "output_size = num_classes   # Number of trend classes\n",
    "num_layers = 4  # Number of GRU layers\n",
    "dropout = 0.0   # Dropout rate\n",
    "lora_r = 4      # Rank of LoRA matrices\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "alpha = 0.3  # Weighting factor for distillation loss\n",
    "num_epochs = 2000   # Number of training epochs\n",
    "batch_size = 64     # Batch size for DataLoader\n",
    "model_name = 'BiGRUWithAttention'   # Model name for saving\n",
    "best_results = []   # List to store best results\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define paths\n",
    "stop_signal_file = os.path.normpath(os.path.join('Class_Incremental_CL', 'David_Classif_Bi_Dir_GRU_Model/stop_training.txt'))\n",
    "model_saving_folder = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_3/alpha_{alpha}\"))\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# Load class features from Period 2\n",
    "class_features_path = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_2/alpha_{alpha}/class_features.pkl\"))\n",
    "if os.path.exists(class_features_path):\n",
    "    with open(class_features_path, 'rb') as f:\n",
    "        class_features_dict = pickle.load(f)\n",
    "    print(f\"Loaded class features from: {class_features_path}\")\n",
    "else:\n",
    "    class_features_dict = {}\n",
    "    print(\"No previous class features found.\")\n",
    "\n",
    "# Instantiate models\n",
    "student_model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers, dropout, lora_r).to(device)\n",
    "teacher_model = BiGRUWithAttention(input_size, hidden_size, output_size - 1, num_layers, dropout, lora_r).to(device)\n",
    "\n",
    "# Load teacher model from Period 2\n",
    "teacher_checkpoint_path = f\"Class_Incremental_CL/David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_2/alpha_{alpha}/BiGRUWithAttention_best.pth\"\n",
    "teacher_checkpoint = torch.load(teacher_checkpoint_path, map_location=device, weights_only=True)\n",
    "num_lora_adapters = teacher_checkpoint.get('num_lora_adapters', 0)\n",
    "related_labels = teacher_checkpoint.get('related_labels', {'attention_fc': [0, 1]})  # Load related_labels, default if not found\n",
    "print(f\"Teacher model checkpoint has {num_lora_adapters} LoRA adapters.\")\n",
    "print(f\"Loaded related_labels: {related_labels}\")\n",
    "\n",
    "# Add LoRA adapters to teacher_model before loading state_dict\n",
    "for _ in range(num_lora_adapters):\n",
    "    teacher_model.add_lora_adapter()\n",
    "teacher_model.load_state_dict(teacher_checkpoint['model_state_dict'])\n",
    "print(f\"Loaded teacher model from: \\n\\t{teacher_checkpoint_path}\")\n",
    "print(f\"Teacher model now has {len(teacher_model.lora_adapters)} LoRA adapters.\")\n",
    "print(f\"\\n{teacher_model}\\n\")\n",
    "del teacher_checkpoint\n",
    "gc.collect()\n",
    "\n",
    "# Add the same number of LoRA adapters to student_model\n",
    "for _ in range(num_lora_adapters):\n",
    "    student_model.add_lora_adapter()\n",
    "print(f\"Student model now has {len(student_model.lora_adapters)} LoRA adapters.\")\n",
    "\n",
    "# Copy teacher weights to student (excluding FC layer)\n",
    "state_dict_teacher = teacher_model.state_dict()\n",
    "state_dict_student = student_model.state_dict()\n",
    "for name, param in state_dict_teacher.items():\n",
    "    if 'fc' not in name:\n",
    "        state_dict_student[name].copy_(param)\n",
    "student_model.load_state_dict(state_dict_student)\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# Train and validate with related_labels\n",
    "class_features_dict = train_and_validate_lora(\n",
    "    student_model, \n",
    "    teacher_model, \n",
    "    stable_classes, \n",
    "    output_size, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    scheduler, \n",
    "    use_scheduler=False, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    model_saving_folder=model_saving_folder, \n",
    "    model_name=model_name, \n",
    "    stop_signal_file=stop_signal_file,\n",
    "    class_features_dict=class_features_dict,\n",
    "    tau_high=0.5,  # Adjusted to 0.5\n",
    "    tau_low=0.5,   # Adjusted to 0.5\n",
    "    related_labels=related_labels  # Pass related_labels\n",
    ")\n",
    "\n",
    "# Save class features\n",
    "class_features_path = os.path.join(model_saving_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, 'wb') as f:\n",
    "    pickle.dump(class_features_dict, f)\n",
    "print(f\"Saved class features to: {class_features_path}\")\n",
    "\n",
    "print(f\"\\nstudent_model: \\n{student_model}\\n\")\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "# Clean up\n",
    "for var in [\"X_train\", \"y_train\", \"X_val\", \"y_val\", \"X_test\", \"y_test\", \"Number_features\", \"unique_classes\", \"num_classes\"]:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Period 4 (num_layers = 4, lora_r=4, alpha = 0.3)\n",
    "+ ##### BiGRUWithAttention\n",
    "+ ##### Training and saving in *'LoRA_v6/Rank_4_Period_4/alpha_0.3'*\n",
    "#### __Val Accuracy: 96.27%__\n",
    "#### __Val-Class-Acc: {0: '90.94%', 1: '97.57%', 2: '88.41%', 3: '98.56%', 4: '98.31%'}__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "        with_indicators_file_path = list_period_files_full_path[3], # Change \n",
    "        downsampled_data_minutes = downsampled_data_minutes,\n",
    "        exclude_columns = exclude_columns,\n",
    "        lower_threshold = lower_threshold,\n",
    "        upper_threshold = upper_threshold,\n",
    "        reverse_steps = reverse_steps,\n",
    "        sequence_length = sequence_length,\n",
    "        sliding_interval = sliding_interval,\n",
    "        trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "    )\n",
    "\n",
    "print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "stable_classes = [1, 2, 3]  # From Period 3: Exclude class 0 because it changes\n",
    "input_size = Number_features    # Number of input features\n",
    "hidden_size = 64    # Number of GRU units\n",
    "output_size = num_classes   # Number of trend classes (5 for Period 4: {0, 1, 2, 3, 4})\n",
    "num_layers = 4  # Number of GRU layers\n",
    "dropout = 0.0   # Dropout rate\n",
    "lora_r = 4      # Rank of LoRA matrices\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "alpha = 0.3  # Weighting factor for distillation loss\n",
    "num_epochs = 2000   # Number of training epochs\n",
    "batch_size = 64     # Batch size for DataLoader\n",
    "model_name = 'BiGRUWithAttention'   # Model name for saving\n",
    "best_results = []   # List to store best results\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define paths\n",
    "stop_signal_file = os.path.normpath(os.path.join('Class_Incremental_CL', 'David_Classif_Bi_Dir_GRU_Model/stop_training.txt'))\n",
    "model_saving_folder = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_4/alpha_{alpha}\"))\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# Load class features from Period 3\n",
    "class_features_path = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_3/alpha_{alpha}/class_features.pkl\"))\n",
    "if os.path.exists(class_features_path):\n",
    "    with open(class_features_path, 'rb') as f:\n",
    "        class_features_dict = pickle.load(f)\n",
    "    print(f\"Loaded class features from: {class_features_path}\")\n",
    "else:\n",
    "    class_features_dict = {}\n",
    "    print(\"No previous class features found.\")\n",
    "    \n",
    "# Instantiate models\n",
    "student_model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers, dropout, lora_r).to(device)\n",
    "teacher_model = BiGRUWithAttention(input_size, hidden_size, output_size - 1, num_layers, dropout, lora_r).to(device)\n",
    "\n",
    "# Load teacher model from Period 3\n",
    "teacher_checkpoint_path = f\"Class_Incremental_CL/David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_3/alpha_{alpha}/BiGRUWithAttention_best.pth\"\n",
    "teacher_checkpoint = torch.load(teacher_checkpoint_path, map_location=device, weights_only=True)\n",
    "num_lora_adapters = teacher_checkpoint.get('num_lora_adapters', 0)\n",
    "related_labels = teacher_checkpoint.get('related_labels', {'attention_fc': [0, 1]})  # Load related_labels, default if not found\n",
    "print(f\"Teacher model checkpoint has {num_lora_adapters} LoRA adapters.\")\n",
    "print(f\"Loaded related_labels: {related_labels}\")\n",
    "\n",
    "# Add LoRA adapters to teacher_model before loading state_dict\n",
    "for _ in range(num_lora_adapters):\n",
    "    teacher_model.add_lora_adapter()\n",
    "teacher_model.load_state_dict(teacher_checkpoint['model_state_dict'])\n",
    "print(f\"Loaded teacher model from: \\n\\t{teacher_checkpoint_path}\")\n",
    "print(f\"Teacher model now has {len(teacher_model.lora_adapters)} LoRA adapters.\")\n",
    "print(f\"\\n{teacher_model}\\n\")\n",
    "del teacher_checkpoint\n",
    "gc.collect()\n",
    "\n",
    "# Add the same number of LoRA adapters to student_model\n",
    "for _ in range(num_lora_adapters):\n",
    "    student_model.add_lora_adapter()\n",
    "print(f\"Student model now has {len(student_model.lora_adapters)} LoRA adapters.\")\n",
    "\n",
    "# Copy teacher weights to student (excluding FC layer)\n",
    "state_dict_teacher = teacher_model.state_dict()\n",
    "state_dict_student = student_model.state_dict()\n",
    "for name, param in state_dict_teacher.items():\n",
    "    if 'fc' not in name:\n",
    "        state_dict_student[name].copy_(param)\n",
    "student_model.load_state_dict(state_dict_student)\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# Train and validate with related_labels\n",
    "class_features_dict = train_and_validate_lora(\n",
    "    student_model, \n",
    "    teacher_model, \n",
    "    stable_classes, \n",
    "    output_size, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    scheduler, \n",
    "    use_scheduler=False, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    model_saving_folder=model_saving_folder, \n",
    "    model_name=model_name, \n",
    "    stop_signal_file=stop_signal_file,\n",
    "    class_features_dict=class_features_dict,\n",
    "    tau_high=0.5,  # Adjusted to 0.5\n",
    "    tau_low=0.5,   # Adjusted to 0.5\n",
    "    related_labels=related_labels  # Pass related_labels\n",
    ")\n",
    "\n",
    "# Save class features\n",
    "class_features_path = os.path.join(model_saving_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, 'wb') as f:\n",
    "    pickle.dump(class_features_dict, f)\n",
    "print(f\"Saved class features to: {class_features_path}\")\n",
    "\n",
    "print(f\"\\nstudent_model: \\n{student_model}\\n\")\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "# Clean up\n",
    "for var in [\"X_train\", \"y_train\", \"X_val\", \"y_val\", \"X_test\", \"y_test\", \"Number_features\", \"unique_classes\", \"num_classes\"]:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Period 2 (num_layers = 4, lora_r=4, alpha = 0.2)\n",
    "+ ##### BiGRUWithAttention\n",
    "+ ##### Training and saving in *'LoRA_v6/Rank_4_Period_2/alpha_0.2'*\n",
    "#### __Val Accuracy: 97.65%__\n",
    "#### __Val-Class-Acc: {0: '99.44%', 1: '97.22%', 2: '90.42%'}__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "        with_indicators_file_path = list_period_files_full_path[1], # Change \n",
    "        downsampled_data_minutes = downsampled_data_minutes,\n",
    "        exclude_columns = exclude_columns,\n",
    "        lower_threshold = lower_threshold,\n",
    "        upper_threshold = upper_threshold,\n",
    "        reverse_steps = reverse_steps,\n",
    "        sequence_length = sequence_length,\n",
    "        sliding_interval = sliding_interval,\n",
    "        trends_to_keep = {0, 1, 2}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "    )\n",
    "\n",
    "print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "stable_classes = [1]    # From Period 1: Exclude class 0 because it changes\n",
    "input_size = Number_features    # Number of input features\n",
    "hidden_size = 64    # Number of GRU units\n",
    "output_size = num_classes   # Number of trend classes (3 for Period 2: {0, 1, 2})\n",
    "num_layers = 4  # Number of GRU layers\n",
    "dropout = 0.0   # Dropout rate\n",
    "lora_r = 4      # Rank of LoRA matrices\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "alpha = 0.2  # Weighting factor for distillation loss\n",
    "num_epochs = 2000   # Number of training epochs\n",
    "batch_size = 64     # Batch size for DataLoader\n",
    "model_name = 'BiGRUWithAttention'   # Model name for saving\n",
    "best_results = []   # List to store best results\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define paths\n",
    "stop_signal_file = os.path.normpath(os.path.join('Class_Incremental_CL', 'David_Classif_Bi_Dir_GRU_Model/stop_training.txt'))\n",
    "model_saving_folder = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_2/alpha_{alpha}\"))\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# Load class features from Period 1\n",
    "class_features_path = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_1/alpha_{alpha}/class_features.pkl\"))\n",
    "if os.path.exists(class_features_path):\n",
    "    with open(class_features_path, 'rb') as f:\n",
    "        class_features_dict = pickle.load(f)\n",
    "    print(f\"Loaded class features from: {class_features_path}\")\n",
    "else:\n",
    "    class_features_dict = {}\n",
    "    print(\"No previous class features found.\")\n",
    "\n",
    "# Instantiate models\n",
    "student_model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers, dropout, lora_r).to(device)\n",
    "teacher_model = BiGRUWithAttention(input_size, hidden_size, output_size - 1, num_layers, dropout, lora_r).to(device)\n",
    "\n",
    "# Load teacher model from Period 1\n",
    "teacher_checkpoint_path = \"Class_Incremental_CL/David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v2/Rank_4_Period_1/1st_try/BiGRUWithAttention_best.pth\"\n",
    "teacher_checkpoint = torch.load(teacher_checkpoint_path, map_location=device, weights_only=True)\n",
    "num_lora_adapters = teacher_checkpoint.get('num_lora_adapters', 0)  # Period 1 may not have LoRA adapters\n",
    "related_labels = {'attention_fc': [0, 1]}  # Initialize related_labels for Period 2\n",
    "print(f\"Teacher model checkpoint has {num_lora_adapters} LoRA adapters.\")\n",
    "print(f\"Initialized related_labels: {related_labels}\")\n",
    "\n",
    "# Add LoRA adapters to teacher_model before loading state_dict\n",
    "for _ in range(num_lora_adapters):\n",
    "    teacher_model.add_lora_adapter()\n",
    "teacher_model.load_state_dict(teacher_checkpoint['model_state_dict'])\n",
    "print(f\"Loaded teacher model from: \\n\\t{teacher_checkpoint_path}\")\n",
    "print(f\"Teacher model now has {len(teacher_model.lora_adapters)} LoRA adapters.\")\n",
    "print(f\"\\n{teacher_model}\\n\")\n",
    "del teacher_checkpoint\n",
    "gc.collect()\n",
    "\n",
    "# Add the same number of LoRA adapters to student_model\n",
    "for _ in range(num_lora_adapters):\n",
    "    student_model.add_lora_adapter()\n",
    "print(f\"Student model now has {len(student_model.lora_adapters)} LoRA adapters.\")\n",
    "\n",
    "# Copy teacher weights to student (excluding FC layer)\n",
    "state_dict_teacher = teacher_model.state_dict()\n",
    "state_dict_student = student_model.state_dict()\n",
    "for name, param in state_dict_teacher.items():\n",
    "    if 'fc' not in name:\n",
    "        state_dict_student[name].copy_(param)\n",
    "student_model.load_state_dict(state_dict_student)\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "class_features_dict = train_and_validate_lora(\n",
    "    student_model, \n",
    "    teacher_model, \n",
    "    stable_classes, \n",
    "    output_size, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    scheduler, \n",
    "    use_scheduler=False, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    model_saving_folder=model_saving_folder, \n",
    "    model_name=model_name, \n",
    "    stop_signal_file=stop_signal_file,\n",
    "    class_features_dict=class_features_dict,\n",
    "    tau_high=0.5,  # Adjusted to 0.5\n",
    "    tau_low=0.5,   # Adjusted to 0.5\n",
    "    related_labels=related_labels  # Pass initialized related_labels\n",
    ")\n",
    "\n",
    "# Save class features\n",
    "class_features_path = os.path.join(model_saving_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, 'wb') as f:\n",
    "    pickle.dump(class_features_dict, f)\n",
    "print(f\"Saved class features to: {class_features_path}\")\n",
    "\n",
    "print(f\"\\nstudent_model: \\n{student_model}\\n\")\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "# Clean up\n",
    "for var in [\"X_train\", \"y_train\", \"X_val\", \"y_val\", \"X_test\", \"y_test\", \"Number_features\", \"unique_classes\", \"num_classes\"]:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Period 3 (num_layers = 4, lora_r=4, alpha = 0.2)\n",
    "+ ##### BiGRUWithAttention\n",
    "+ ##### Training and saving in *'LoRA_v6/Rank_4_Period_3/alpha_0.2'*\n",
    "#### __Val Accuracy: 97.96%__\n",
    "#### __Val-Class-Acc: {0: '93.90%', 1: '98.24%', 2: '98.03%', 3: '98.70%'}__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "        with_indicators_file_path = list_period_files_full_path[2], # Change \n",
    "        downsampled_data_minutes = downsampled_data_minutes,\n",
    "        exclude_columns = exclude_columns,\n",
    "        lower_threshold = lower_threshold,\n",
    "        upper_threshold = upper_threshold,\n",
    "        reverse_steps = reverse_steps,\n",
    "        sequence_length = sequence_length,\n",
    "        sliding_interval = sliding_interval,\n",
    "        trends_to_keep = {0, 1, 2, 3}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "    )\n",
    "\n",
    "print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "stable_classes = [1, 2]  # From Period 2: Exclude class 0 because it changes\n",
    "input_size = Number_features    # Number of input features\n",
    "hidden_size = 64    # Number of GRU units\n",
    "output_size = num_classes   # Number of trend classes\n",
    "num_layers = 4  # Number of GRU layers\n",
    "dropout = 0.0   # Dropout rate\n",
    "lora_r = 4      # Rank of LoRA matrices\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "alpha = 0.2 # Weighting factor for distillation loss\n",
    "num_epochs = 2000   # Number of training epochs\n",
    "batch_size = 64     # Batch size for DataLoader\n",
    "model_name = 'BiGRUWithAttention'   # Model name for saving\n",
    "best_results = []   # List to store best results\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define paths\n",
    "stop_signal_file = os.path.normpath(os.path.join('Class_Incremental_CL', 'David_Classif_Bi_Dir_GRU_Model/stop_training.txt'))\n",
    "model_saving_folder = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_3/alpha_{alpha}\"))\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# Load class features from Period 2\n",
    "class_features_path = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_2/alpha_{alpha}/class_features.pkl\"))\n",
    "if os.path.exists(class_features_path):\n",
    "    with open(class_features_path, 'rb') as f:\n",
    "        class_features_dict = pickle.load(f)\n",
    "    print(f\"Loaded class features from: {class_features_path}\")\n",
    "else:\n",
    "    class_features_dict = {}\n",
    "    print(\"No previous class features found.\")\n",
    "\n",
    "# Instantiate models\n",
    "student_model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers, dropout, lora_r).to(device)\n",
    "teacher_model = BiGRUWithAttention(input_size, hidden_size, output_size - 1, num_layers, dropout, lora_r).to(device)\n",
    "\n",
    "# Load teacher model from Period 2\n",
    "teacher_checkpoint_path = f\"Class_Incremental_CL/David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_2/alpha_{alpha}/BiGRUWithAttention_best.pth\"\n",
    "teacher_checkpoint = torch.load(teacher_checkpoint_path, map_location=device, weights_only=True)\n",
    "num_lora_adapters = teacher_checkpoint.get('num_lora_adapters', 0)\n",
    "related_labels = teacher_checkpoint.get('related_labels', {'attention_fc': [0, 1]})  # Load related_labels, default if not found\n",
    "print(f\"Teacher model checkpoint has {num_lora_adapters} LoRA adapters.\")\n",
    "print(f\"Loaded related_labels: {related_labels}\")\n",
    "\n",
    "# Add LoRA adapters to teacher_model before loading state_dict\n",
    "for _ in range(num_lora_adapters):\n",
    "    teacher_model.add_lora_adapter()\n",
    "teacher_model.load_state_dict(teacher_checkpoint['model_state_dict'])\n",
    "print(f\"Loaded teacher model from: \\n\\t{teacher_checkpoint_path}\")\n",
    "print(f\"Teacher model now has {len(teacher_model.lora_adapters)} LoRA adapters.\")\n",
    "print(f\"\\n{teacher_model}\\n\")\n",
    "del teacher_checkpoint\n",
    "gc.collect()\n",
    "\n",
    "# Add the same number of LoRA adapters to student_model\n",
    "for _ in range(num_lora_adapters):\n",
    "    student_model.add_lora_adapter()\n",
    "print(f\"Student model now has {len(student_model.lora_adapters)} LoRA adapters.\")\n",
    "\n",
    "# Copy teacher weights to student (excluding FC layer)\n",
    "state_dict_teacher = teacher_model.state_dict()\n",
    "state_dict_student = student_model.state_dict()\n",
    "for name, param in state_dict_teacher.items():\n",
    "    if 'fc' not in name:\n",
    "        state_dict_student[name].copy_(param)\n",
    "student_model.load_state_dict(state_dict_student)\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# Train and validate with related_labels\n",
    "class_features_dict = train_and_validate_lora(\n",
    "    student_model, \n",
    "    teacher_model, \n",
    "    stable_classes, \n",
    "    output_size, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    scheduler, \n",
    "    use_scheduler=False, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    model_saving_folder=model_saving_folder, \n",
    "    model_name=model_name, \n",
    "    stop_signal_file=stop_signal_file,\n",
    "    class_features_dict=class_features_dict,\n",
    "    tau_high=0.5,  # Adjusted to 0.5\n",
    "    tau_low=0.5,   # Adjusted to 0.5\n",
    "    related_labels=related_labels  # Pass related_labels\n",
    ")\n",
    "\n",
    "# Save class features\n",
    "class_features_path = os.path.join(model_saving_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, 'wb') as f:\n",
    "    pickle.dump(class_features_dict, f)\n",
    "print(f\"Saved class features to: {class_features_path}\")\n",
    "\n",
    "print(f\"\\nstudent_model: \\n{student_model}\\n\")\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "# Clean up\n",
    "for var in [\"X_train\", \"y_train\", \"X_val\", \"y_val\", \"X_test\", \"y_test\", \"Number_features\", \"unique_classes\", \"num_classes\"]:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Period 4 (num_layers = 4, lora_r=4, alpha = 0.2\n",
    "+ ##### BiGRUWithAttention\n",
    "+ ##### Training and saving in *'LoRA_v6/Rank_4_Period_4/alpha_0.2*\n",
    "#### __Val Accuracy: 96.81%__\n",
    "#### __Val-Class-Acc: {0: '94.02%', 1: '97.85%', 2: '91.15%', 3: '98.32%', 4: '97.83%'}__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "        with_indicators_file_path = list_period_files_full_path[3], # Change \n",
    "        downsampled_data_minutes = downsampled_data_minutes,\n",
    "        exclude_columns = exclude_columns,\n",
    "        lower_threshold = lower_threshold,\n",
    "        upper_threshold = upper_threshold,\n",
    "        reverse_steps = reverse_steps,\n",
    "        sequence_length = sequence_length,\n",
    "        sliding_interval = sliding_interval,\n",
    "        trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "    )\n",
    "\n",
    "print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "stable_classes = [1, 2, 3]  # From Period 3: Exclude class 0 because it changes\n",
    "input_size = Number_features    # Number of input features\n",
    "hidden_size = 64    # Number of GRU units\n",
    "output_size = num_classes   # Number of trend classes (5 for Period 4: {0, 1, 2, 3, 4})\n",
    "num_layers = 4  # Number of GRU layers\n",
    "dropout = 0.0   # Dropout rate\n",
    "lora_r = 4      # Rank of LoRA matrices\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "alpha = 0.2 # Weighting factor for distillation loss\n",
    "num_epochs = 2000   # Number of training epochs\n",
    "batch_size = 64     # Batch size for DataLoader\n",
    "model_name = 'BiGRUWithAttention'   # Model name for saving\n",
    "best_results = []   # List to store best results\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define paths\n",
    "stop_signal_file = os.path.normpath(os.path.join('Class_Incremental_CL', 'David_Classif_Bi_Dir_GRU_Model/stop_training.txt'))\n",
    "model_saving_folder = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_4/alpha_{alpha}\"))\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# Load class features from Period 3\n",
    "class_features_path = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_3/alpha_{alpha}/class_features.pkl\"))\n",
    "if os.path.exists(class_features_path):\n",
    "    with open(class_features_path, 'rb') as f:\n",
    "        class_features_dict = pickle.load(f)\n",
    "    print(f\"Loaded class features from: {class_features_path}\")\n",
    "else:\n",
    "    class_features_dict = {}\n",
    "    print(\"No previous class features found.\")\n",
    "    \n",
    "# Instantiate models\n",
    "student_model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers, dropout, lora_r).to(device)\n",
    "teacher_model = BiGRUWithAttention(input_size, hidden_size, output_size - 1, num_layers, dropout, lora_r).to(device)\n",
    "\n",
    "# Load teacher model from Period 3\n",
    "teacher_checkpoint_path = f\"Class_Incremental_CL/David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_3/alpha_{alpha}/BiGRUWithAttention_best.pth\"\n",
    "teacher_checkpoint = torch.load(teacher_checkpoint_path, map_location=device, weights_only=True)\n",
    "num_lora_adapters = teacher_checkpoint.get('num_lora_adapters', 0)\n",
    "related_labels = teacher_checkpoint.get('related_labels', {'attention_fc': [0, 1]})  # Load related_labels, default if not found\n",
    "print(f\"Teacher model checkpoint has {num_lora_adapters} LoRA adapters.\")\n",
    "print(f\"Loaded related_labels: {related_labels}\")\n",
    "\n",
    "# Add LoRA adapters to teacher_model before loading state_dict\n",
    "for _ in range(num_lora_adapters):\n",
    "    teacher_model.add_lora_adapter()\n",
    "teacher_model.load_state_dict(teacher_checkpoint['model_state_dict'])\n",
    "print(f\"Loaded teacher model from: \\n\\t{teacher_checkpoint_path}\")\n",
    "print(f\"Teacher model now has {len(teacher_model.lora_adapters)} LoRA adapters.\")\n",
    "print(f\"\\n{teacher_model}\\n\")\n",
    "del teacher_checkpoint\n",
    "gc.collect()\n",
    "\n",
    "# Add the same number of LoRA adapters to student_model\n",
    "for _ in range(num_lora_adapters):\n",
    "    student_model.add_lora_adapter()\n",
    "print(f\"Student model now has {len(student_model.lora_adapters)} LoRA adapters.\")\n",
    "\n",
    "# Copy teacher weights to student (excluding FC layer)\n",
    "state_dict_teacher = teacher_model.state_dict()\n",
    "state_dict_student = student_model.state_dict()\n",
    "for name, param in state_dict_teacher.items():\n",
    "    if 'fc' not in name:\n",
    "        state_dict_student[name].copy_(param)\n",
    "student_model.load_state_dict(state_dict_student)\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# Train and validate with related_labels\n",
    "class_features_dict = train_and_validate_lora(\n",
    "    student_model, \n",
    "    teacher_model, \n",
    "    stable_classes, \n",
    "    output_size, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    scheduler, \n",
    "    use_scheduler=False, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    model_saving_folder=model_saving_folder, \n",
    "    model_name=model_name, \n",
    "    stop_signal_file=stop_signal_file,\n",
    "    class_features_dict=class_features_dict,\n",
    "    tau_high=0.5,  # Adjusted to 0.5\n",
    "    tau_low=0.5,   # Adjusted to 0.5\n",
    "    related_labels=related_labels  # Pass related_labels\n",
    ")\n",
    "\n",
    "# Save class features\n",
    "class_features_path = os.path.join(model_saving_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, 'wb') as f:\n",
    "    pickle.dump(class_features_dict, f)\n",
    "print(f\"Saved class features to: {class_features_path}\")\n",
    "\n",
    "print(f\"\\nstudent_model: \\n{student_model}\\n\")\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "# Clean up\n",
    "for var in [\"X_train\", \"y_train\", \"X_val\", \"y_val\", \"X_test\", \"y_test\", \"Number_features\", \"unique_classes\", \"num_classes\"]:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Period 2 (num_layers = 4, lora_r=4, alpha = 0.1)\n",
    "+ ##### BiGRUWithAttention\n",
    "+ ##### Training and saving in *'LoRA_v6/Rank_4_Period_2/alpha_0.1'*\n",
    "#### __Val Accuracy: 97.67%__\n",
    "#### __Val-Class-Acc: {0: '99.79%', 1: '97.61%', 2: '87.65%'}__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "        with_indicators_file_path = list_period_files_full_path[1], # Change \n",
    "        downsampled_data_minutes = downsampled_data_minutes,\n",
    "        exclude_columns = exclude_columns,\n",
    "        lower_threshold = lower_threshold,\n",
    "        upper_threshold = upper_threshold,\n",
    "        reverse_steps = reverse_steps,\n",
    "        sequence_length = sequence_length,\n",
    "        sliding_interval = sliding_interval,\n",
    "        trends_to_keep = {0, 1, 2}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "    )\n",
    "\n",
    "print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "stable_classes = [1]    # From Period 1: Exclude class 0 because it changes\n",
    "input_size = Number_features    # Number of input features\n",
    "hidden_size = 64    # Number of GRU units\n",
    "output_size = num_classes   # Number of trend classes (3 for Period 2: {0, 1, 2})\n",
    "num_layers = 4  # Number of GRU layers\n",
    "dropout = 0.0   # Dropout rate\n",
    "lora_r = 4      # Rank of LoRA matrices\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "alpha = 0.1  # Weighting factor for distillation loss\n",
    "num_epochs = 2000   # Number of training epochs\n",
    "batch_size = 64     # Batch size for DataLoader\n",
    "model_name = 'BiGRUWithAttention'   # Model name for saving\n",
    "best_results = []   # List to store best results\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define paths\n",
    "stop_signal_file = os.path.normpath(os.path.join('Class_Incremental_CL', 'David_Classif_Bi_Dir_GRU_Model/stop_training.txt'))\n",
    "model_saving_folder = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_2/alpha_{alpha}\"))\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# Load class features from Period 1\n",
    "class_features_path = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_1/alpha_{alpha}/class_features.pkl\"))\n",
    "if os.path.exists(class_features_path):\n",
    "    with open(class_features_path, 'rb') as f:\n",
    "        class_features_dict = pickle.load(f)\n",
    "    print(f\"Loaded class features from: {class_features_path}\")\n",
    "else:\n",
    "    class_features_dict = {}\n",
    "    print(\"No previous class features found.\")\n",
    "\n",
    "# Instantiate models\n",
    "student_model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers, dropout, lora_r).to(device)\n",
    "teacher_model = BiGRUWithAttention(input_size, hidden_size, output_size - 1, num_layers, dropout, lora_r).to(device)\n",
    "\n",
    "# Load teacher model from Period 1\n",
    "teacher_checkpoint_path = \"Class_Incremental_CL/David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v2/Rank_4_Period_1/1st_try/BiGRUWithAttention_best.pth\"\n",
    "teacher_checkpoint = torch.load(teacher_checkpoint_path, map_location=device, weights_only=True)\n",
    "num_lora_adapters = teacher_checkpoint.get('num_lora_adapters', 0)  # Period 1 may not have LoRA adapters\n",
    "related_labels = {'attention_fc': [0, 1]}  # Initialize related_labels for Period 2\n",
    "print(f\"Teacher model checkpoint has {num_lora_adapters} LoRA adapters.\")\n",
    "print(f\"Initialized related_labels: {related_labels}\")\n",
    "\n",
    "# Add LoRA adapters to teacher_model before loading state_dict\n",
    "for _ in range(num_lora_adapters):\n",
    "    teacher_model.add_lora_adapter()\n",
    "teacher_model.load_state_dict(teacher_checkpoint['model_state_dict'])\n",
    "print(f\"Loaded teacher model from: \\n\\t{teacher_checkpoint_path}\")\n",
    "print(f\"Teacher model now has {len(teacher_model.lora_adapters)} LoRA adapters.\")\n",
    "print(f\"\\n{teacher_model}\\n\")\n",
    "del teacher_checkpoint\n",
    "gc.collect()\n",
    "\n",
    "# Add the same number of LoRA adapters to student_model\n",
    "for _ in range(num_lora_adapters):\n",
    "    student_model.add_lora_adapter()\n",
    "print(f\"Student model now has {len(student_model.lora_adapters)} LoRA adapters.\")\n",
    "\n",
    "# Copy teacher weights to student (excluding FC layer)\n",
    "state_dict_teacher = teacher_model.state_dict()\n",
    "state_dict_student = student_model.state_dict()\n",
    "for name, param in state_dict_teacher.items():\n",
    "    if 'fc' not in name:\n",
    "        state_dict_student[name].copy_(param)\n",
    "student_model.load_state_dict(state_dict_student)\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "class_features_dict = train_and_validate_lora(\n",
    "    student_model, \n",
    "    teacher_model, \n",
    "    stable_classes, \n",
    "    output_size, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    scheduler, \n",
    "    use_scheduler=False, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    model_saving_folder=model_saving_folder, \n",
    "    model_name=model_name, \n",
    "    stop_signal_file=stop_signal_file,\n",
    "    class_features_dict=class_features_dict,\n",
    "    tau_high=0.5,  # Adjusted to 0.5\n",
    "    tau_low=0.5,   # Adjusted to 0.5\n",
    "    related_labels=related_labels  # Pass initialized related_labels\n",
    ")\n",
    "\n",
    "# Save class features\n",
    "class_features_path = os.path.join(model_saving_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, 'wb') as f:\n",
    "    pickle.dump(class_features_dict, f)\n",
    "print(f\"Saved class features to: {class_features_path}\")\n",
    "\n",
    "print(f\"\\nstudent_model: \\n{student_model}\\n\")\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "# Clean up\n",
    "for var in [\"X_train\", \"y_train\", \"X_val\", \"y_val\", \"X_test\", \"y_test\", \"Number_features\", \"unique_classes\", \"num_classes\"]:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Period 3 (num_layers = 4, lora_r=4, alpha = 0.1)\n",
    "+ ##### BiGRUWithAttention\n",
    "+ ##### Training and saving in *'LoRA_v6/Rank_4_Period_3/alpha_0.1'*\n",
    "#### __Val Accuracy: 97.81%__\n",
    "#### __Val-Class-Acc: {0: '94.50%', 1: '98.59%', 2: '90.37%', 3: '98.55%'}__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "        with_indicators_file_path = list_period_files_full_path[2], # Change \n",
    "        downsampled_data_minutes = downsampled_data_minutes,\n",
    "        exclude_columns = exclude_columns,\n",
    "        lower_threshold = lower_threshold,\n",
    "        upper_threshold = upper_threshold,\n",
    "        reverse_steps = reverse_steps,\n",
    "        sequence_length = sequence_length,\n",
    "        sliding_interval = sliding_interval,\n",
    "        trends_to_keep = {0, 1, 2, 3}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "    )\n",
    "\n",
    "print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "stable_classes = [1, 2]  # From Period 2: Exclude class 0 because it changes\n",
    "input_size = Number_features    # Number of input features\n",
    "hidden_size = 64    # Number of GRU units\n",
    "output_size = num_classes   # Number of trend classes\n",
    "num_layers = 4  # Number of GRU layers\n",
    "dropout = 0.0   # Dropout rate\n",
    "lora_r = 4      # Rank of LoRA matrices\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "alpha = 0.1 # Weighting factor for distillation loss\n",
    "num_epochs = 2000   # Number of training epochs\n",
    "batch_size = 64     # Batch size for DataLoader\n",
    "model_name = 'BiGRUWithAttention'   # Model name for saving\n",
    "best_results = []   # List to store best results\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define paths\n",
    "stop_signal_file = os.path.normpath(os.path.join('Class_Incremental_CL', 'David_Classif_Bi_Dir_GRU_Model/stop_training.txt'))\n",
    "model_saving_folder = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_3/alpha_{alpha}\"))\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# Load class features from Period 2\n",
    "class_features_path = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_2/alpha_{alpha}/class_features.pkl\"))\n",
    "if os.path.exists(class_features_path):\n",
    "    with open(class_features_path, 'rb') as f:\n",
    "        class_features_dict = pickle.load(f)\n",
    "    print(f\"Loaded class features from: {class_features_path}\")\n",
    "else:\n",
    "    class_features_dict = {}\n",
    "    print(\"No previous class features found.\")\n",
    "\n",
    "# Instantiate models\n",
    "student_model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers, dropout, lora_r).to(device)\n",
    "teacher_model = BiGRUWithAttention(input_size, hidden_size, output_size - 1, num_layers, dropout, lora_r).to(device)\n",
    "\n",
    "# Load teacher model from Period 2\n",
    "teacher_checkpoint_path = f\"Class_Incremental_CL/David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_2/alpha_{alpha}/BiGRUWithAttention_best.pth\"\n",
    "teacher_checkpoint = torch.load(teacher_checkpoint_path, map_location=device, weights_only=True)\n",
    "num_lora_adapters = teacher_checkpoint.get('num_lora_adapters', 0)\n",
    "related_labels = teacher_checkpoint.get('related_labels', {'attention_fc': [0, 1]})  # Load related_labels, default if not found\n",
    "print(f\"Teacher model checkpoint has {num_lora_adapters} LoRA adapters.\")\n",
    "print(f\"Loaded related_labels: {related_labels}\")\n",
    "\n",
    "# Add LoRA adapters to teacher_model before loading state_dict\n",
    "for _ in range(num_lora_adapters):\n",
    "    teacher_model.add_lora_adapter()\n",
    "teacher_model.load_state_dict(teacher_checkpoint['model_state_dict'])\n",
    "print(f\"Loaded teacher model from: \\n\\t{teacher_checkpoint_path}\")\n",
    "print(f\"Teacher model now has {len(teacher_model.lora_adapters)} LoRA adapters.\")\n",
    "print(f\"\\n{teacher_model}\\n\")\n",
    "del teacher_checkpoint\n",
    "gc.collect()\n",
    "\n",
    "# Add the same number of LoRA adapters to student_model\n",
    "for _ in range(num_lora_adapters):\n",
    "    student_model.add_lora_adapter()\n",
    "print(f\"Student model now has {len(student_model.lora_adapters)} LoRA adapters.\")\n",
    "\n",
    "# Copy teacher weights to student (excluding FC layer)\n",
    "state_dict_teacher = teacher_model.state_dict()\n",
    "state_dict_student = student_model.state_dict()\n",
    "for name, param in state_dict_teacher.items():\n",
    "    if 'fc' not in name:\n",
    "        state_dict_student[name].copy_(param)\n",
    "student_model.load_state_dict(state_dict_student)\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# Train and validate with related_labels\n",
    "class_features_dict = train_and_validate_lora(\n",
    "    student_model, \n",
    "    teacher_model, \n",
    "    stable_classes, \n",
    "    output_size, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    scheduler, \n",
    "    use_scheduler=False, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    model_saving_folder=model_saving_folder, \n",
    "    model_name=model_name, \n",
    "    stop_signal_file=stop_signal_file,\n",
    "    class_features_dict=class_features_dict,\n",
    "    tau_high=0.5,  # Adjusted to 0.5\n",
    "    tau_low=0.5,   # Adjusted to 0.5\n",
    "    related_labels=related_labels  # Pass related_labels\n",
    ")\n",
    "\n",
    "# Save class features\n",
    "class_features_path = os.path.join(model_saving_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, 'wb') as f:\n",
    "    pickle.dump(class_features_dict, f)\n",
    "print(f\"Saved class features to: {class_features_path}\")\n",
    "\n",
    "print(f\"\\nstudent_model: \\n{student_model}\\n\")\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "# Clean up\n",
    "for var in [\"X_train\", \"y_train\", \"X_val\", \"y_val\", \"X_test\", \"y_test\", \"Number_features\", \"unique_classes\", \"num_classes\"]:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Period 4 (num_layers = 4, lora_r=4, alpha = 0.1\n",
    "+ ##### BiGRUWithAttention\n",
    "+ ##### Training and saving in *'LoRA_v6/Rank_4_Period_4/alpha_0.1*\n",
    "#### __Val Accuracy: 97.94%__\n",
    "#### __Val-Class-Acc: {0: '89.79%', 1: '98.84%', 2: '97.67%', 3: '98.44%', 4: '97.73%'}__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "        with_indicators_file_path = list_period_files_full_path[3], # Change \n",
    "        downsampled_data_minutes = downsampled_data_minutes,\n",
    "        exclude_columns = exclude_columns,\n",
    "        lower_threshold = lower_threshold,\n",
    "        upper_threshold = upper_threshold,\n",
    "        reverse_steps = reverse_steps,\n",
    "        sequence_length = sequence_length,\n",
    "        sliding_interval = sliding_interval,\n",
    "        trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "    )\n",
    "\n",
    "print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "stable_classes = [1, 2, 3]  # From Period 3: Exclude class 0 because it changes\n",
    "input_size = Number_features    # Number of input features\n",
    "hidden_size = 64    # Number of GRU units\n",
    "output_size = num_classes   # Number of trend classes (5 for Period 4: {0, 1, 2, 3, 4})\n",
    "num_layers = 4  # Number of GRU layers\n",
    "dropout = 0.0   # Dropout rate\n",
    "lora_r = 4      # Rank of LoRA matrices\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "alpha = 0.1 # Weighting factor for distillation loss\n",
    "num_epochs = 2000   # Number of training epochs\n",
    "batch_size = 64     # Batch size for DataLoader\n",
    "model_name = 'BiGRUWithAttention'   # Model name for saving\n",
    "best_results = []   # List to store best results\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define paths\n",
    "stop_signal_file = os.path.normpath(os.path.join('Class_Incremental_CL', 'David_Classif_Bi_Dir_GRU_Model/stop_training.txt'))\n",
    "model_saving_folder = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_4/alpha_{alpha}\"))\n",
    "ensure_folder(model_saving_folder)\n",
    "\n",
    "# Load class features from Period 3\n",
    "class_features_path = os.path.normpath(os.path.join('Class_Incremental_CL', f\"David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_3/alpha_{alpha}/class_features.pkl\"))\n",
    "if os.path.exists(class_features_path):\n",
    "    with open(class_features_path, 'rb') as f:\n",
    "        class_features_dict = pickle.load(f)\n",
    "    print(f\"Loaded class features from: {class_features_path}\")\n",
    "else:\n",
    "    class_features_dict = {}\n",
    "    print(\"No previous class features found.\")\n",
    "    \n",
    "# Instantiate models\n",
    "student_model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers, dropout, lora_r).to(device)\n",
    "teacher_model = BiGRUWithAttention(input_size, hidden_size, output_size - 1, num_layers, dropout, lora_r).to(device)\n",
    "\n",
    "# Load teacher model from Period 3\n",
    "teacher_checkpoint_path = f\"Class_Incremental_CL/David_Classif_Bi_Dir_GRU_Model/Trained_models/LoRA_v6/Rank_4_Period_3/alpha_{alpha}/BiGRUWithAttention_best.pth\"\n",
    "teacher_checkpoint = torch.load(teacher_checkpoint_path, map_location=device, weights_only=True)\n",
    "num_lora_adapters = teacher_checkpoint.get('num_lora_adapters', 0)\n",
    "related_labels = teacher_checkpoint.get('related_labels', {'attention_fc': [0, 1]})  # Load related_labels, default if not found\n",
    "print(f\"Teacher model checkpoint has {num_lora_adapters} LoRA adapters.\")\n",
    "print(f\"Loaded related_labels: {related_labels}\")\n",
    "\n",
    "# Add LoRA adapters to teacher_model before loading state_dict\n",
    "for _ in range(num_lora_adapters):\n",
    "    teacher_model.add_lora_adapter()\n",
    "teacher_model.load_state_dict(teacher_checkpoint['model_state_dict'])\n",
    "print(f\"Loaded teacher model from: \\n\\t{teacher_checkpoint_path}\")\n",
    "print(f\"Teacher model now has {len(teacher_model.lora_adapters)} LoRA adapters.\")\n",
    "print(f\"\\n{teacher_model}\\n\")\n",
    "del teacher_checkpoint\n",
    "gc.collect()\n",
    "\n",
    "# Add the same number of LoRA adapters to student_model\n",
    "for _ in range(num_lora_adapters):\n",
    "    student_model.add_lora_adapter()\n",
    "print(f\"Student model now has {len(student_model.lora_adapters)} LoRA adapters.\")\n",
    "\n",
    "# Copy teacher weights to student (excluding FC layer)\n",
    "state_dict_teacher = teacher_model.state_dict()\n",
    "state_dict_student = student_model.state_dict()\n",
    "for name, param in state_dict_teacher.items():\n",
    "    if 'fc' not in name:\n",
    "        state_dict_student[name].copy_(param)\n",
    "student_model.load_state_dict(state_dict_student)\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "# Train and validate with related_labels\n",
    "class_features_dict = train_and_validate_lora(\n",
    "    student_model, \n",
    "    teacher_model, \n",
    "    stable_classes, \n",
    "    output_size, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    scheduler, \n",
    "    use_scheduler=False, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    model_saving_folder=model_saving_folder, \n",
    "    model_name=model_name, \n",
    "    stop_signal_file=stop_signal_file,\n",
    "    class_features_dict=class_features_dict,\n",
    "    tau_high=0.5,  # Adjusted to 0.5\n",
    "    tau_low=0.5,   # Adjusted to 0.5\n",
    "    related_labels=related_labels  # Pass related_labels\n",
    ")\n",
    "\n",
    "# Save class features\n",
    "class_features_path = os.path.join(model_saving_folder, \"class_features.pkl\")\n",
    "with open(class_features_path, 'wb') as f:\n",
    "    pickle.dump(class_features_dict, f)\n",
    "print(f\"Saved class features to: {class_features_path}\")\n",
    "\n",
    "print(f\"\\nstudent_model: \\n{student_model}\\n\")\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "# Clean up\n",
    "for var in [\"X_train\", \"y_train\", \"X_val\", \"y_val\", \"X_test\", \"y_test\", \"Number_features\", \"unique_classes\", \"num_classes\"]:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Getting into Transfer Learning for 5 periods__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_evaluattion_function(model, list_period_files_full_path, criterion, output_size, batch_size=64, model_number=100):\n",
    "#     # 1- With the given model, for each period in the list, predict and print accuracy\n",
    "#     # 2- With the given model, predict and print accuracy for all data combined.\n",
    "#     # For (2), you can do it by saving in a dictionary the accuracy and sample number as you go through each period\n",
    "\n",
    "#     print(f\"\\nUsing model {model_number}: \\n{model}\\n\")\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "#     # Dictionary to save predictions and details.\n",
    "#     store_preds = {}\n",
    "\n",
    "#     for i, path_ in enumerate(list_period_files_full_path):\n",
    "#         # Suppress output by redirecting to os.devnull\n",
    "#         with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "#             X_train_, y_train_, X_val_, y_val_, X_test_, y_test_ = process_and_return_splits(\n",
    "#                 with_indicators_file_path = path_,\n",
    "#                 downsampled_data_minutes = downsampled_data_minutes,\n",
    "#                 exclude_columns = exclude_columns,\n",
    "#                 lower_threshold = lower_threshold,\n",
    "#                 upper_threshold = upper_threshold,\n",
    "#                 reverse_steps = reverse_steps,\n",
    "#                 sequence_length = sequence_length,\n",
    "#                 sliding_interval = sliding_interval\n",
    "#             )\n",
    "\n",
    "#         val_loader_ = DataLoader(TensorDataset(torch.tensor(X_val_, dtype=torch.float32).to(device),  # (seqs, seq_len, features),\n",
    "#                                                        torch.tensor(y_val_, dtype=torch.long).to(device)    # (seqs, seq_len)\n",
    "#                                                        ), \n",
    "#                                                        batch_size=batch_size)\n",
    "#         del X_train_, y_train_, X_val_, y_val_, X_test_, y_test_\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#         # Perform validation at the end of each epoch\n",
    "#         val_loss = 0.0\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "#         with torch.no_grad():\n",
    "#             for X_val_batch, y_val_batch in val_loader_:\n",
    "#                 val_outputs = model(X_val_batch).view(-1, output_size)\n",
    "#                 val_labels = y_val_batch.view(-1)\n",
    "#                 val_loss += criterion(val_outputs, val_labels).item()\n",
    "#                 val_predictions = torch.argmax(val_outputs, dim=-1)\n",
    "#                 val_correct += (val_predictions == val_labels).sum().item()\n",
    "#                 val_total += val_labels.size(0)\n",
    "#         val_loss /= len(val_loader_.dataset)\n",
    "#         val_accuracy = val_correct / val_total\n",
    "\n",
    "#         store_preds[i+1] = {'val_loss' : val_loss, \n",
    "#                             'val_accuracy' : val_accuracy,\n",
    "#                             'val_correct' : val_correct,\n",
    "#                             'val_total' : val_total}\n",
    "        \n",
    "#         print(f\"Period {i+1}/{len(list_period_files_full_path)}, \"\n",
    "#               f\"Val Loss: {val_loss:.9f}, \"\n",
    "#               f\"Val Accuracy: {val_accuracy * 100:.2f}%, \")\n",
    "        \n",
    "#         # Clean up DataLoader and clear cache\n",
    "#         del val_loader_\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#     # Iterate through the stored predictions\n",
    "#     print()\n",
    "#     for period_key in sorted(store_preds.keys()):\n",
    "#         print(\"#---------------------------------------------------------#\")\n",
    "#         # Get current period's accuracy and total\n",
    "#         val_correct = store_preds[period_key]['val_correct']\n",
    "#         val_total = store_preds[period_key]['val_total']\n",
    "#         current_accuracy = store_preds[period_key]['val_accuracy']\n",
    "\n",
    "#         # Print accuracy for the current period\n",
    "#         print(f\"Period {period_key}: Accuracy: {current_accuracy * 100:.2f}%\")\n",
    "\n",
    "#         # If not the first period, calculate and print combined accuracy\n",
    "#         if period_key > 1:\n",
    "#             combined_correct = sum(store_preds[key]['val_correct'] for key in range(1, period_key + 1))\n",
    "#             combined_total = sum(store_preds[key]['val_total'] for key in range(1, period_key + 1))\n",
    "#             combined_accuracy = combined_correct / combined_total\n",
    "#             print(f\"Combined Accuracy up to Period {period_key}: {combined_accuracy * 100:.2f}%\")\n",
    "#     print(\"#---------------------------------------------------------#\")\n",
    "#     print()\n",
    "#     return\n",
    "\n",
    "# def periods_evaluation_transfer_learning(model_number, best_epoch_number_dic, list_period_files_full_path, lr=0.00001):\n",
    "#     \"\"\"\n",
    "#     There are many variables explicitely declared in this function, pay attention!\n",
    "#     \"\"\"\n",
    "    \n",
    "#     torch.manual_seed(42)\n",
    "#     print(\"Seeding successful!\\n\")\n",
    "\n",
    "#     # Model parameters\n",
    "#     input_size = Number_features  # Number of features\n",
    "#     hidden_size = 64  # Number of GRU units\n",
    "#     output_size = 5  # Number of trend classes (0, 15, 25, -15, -25)\n",
    "#     num_layers = 4  # Number of GRU layers\n",
    "#     num_epochs= 2000 # Number of epochs/ go through entire data\n",
    "#     batch_size= 64 # How many sequences passed at once to the model\n",
    "#     model_name = 'BiGRUWithAttention' # Name of the model to use for saving\n",
    "#     global best_results\n",
    "#     best_results = [] # Initialize this outside the training function or at the beginning of training\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     # Define a global stop signal\n",
    "#     stop_signal_file = os.path.normpath(os.path.join(Working_directory, 'Classif_Bi_Dir_GRU_Model/stop_training.txt'))  # Create this file to stop training\n",
    "#     model_saving_folder_init = os.path.normpath(os.path.join(Working_directory, \"Classif_Bi_Dir_GRU_Model/Trained_models/2nd_try\"))\n",
    "#     ensure_folder(model_saving_folder_init)\n",
    "\n",
    "#     # Instantiate the model\n",
    "#     class_gru_model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "\n",
    "#     # Define the loss function, optimizer and scheduler\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(class_gru_model.parameters(), lr=lr) # lr=0.00005\n",
    "#     # optimizer = optim.Adam(class_gru_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "#     # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=10)\n",
    "\n",
    "#     #---------------------------------------------------------\n",
    "    \n",
    "#     if model_number == 1:\n",
    "#         # Load the best saved base model parameters\n",
    "#         epoch_number = best_epoch_number_dic[model_number]\n",
    "#         base_model_path = os.path.normpath(\n",
    "#             os.path.join(model_saving_folder_init, f\"{model_name}_epoch_{epoch_number}.pth\"))\n",
    "\n",
    "#         # Copy the file Destination directory for normalization\n",
    "#         destination_directory = os.path.normpath(\n",
    "#             os.path.join(model_saving_folder_init, f\"Small_Final/period_{model_number}\"))\n",
    "#         ensure_folder(destination_directory)\n",
    "#         destination_path = os.path.join(destination_directory, os.path.basename(base_model_path))\n",
    "#         shutil.copy(base_model_path, destination_path)\n",
    "\n",
    "#         #---------------------------------------------------------\n",
    "#         # print(f\"\\n{class_gru_model}\\n\")\n",
    "#         checkpoint = torch.load(destination_path, map_location=device, weights_only=True)\n",
    "#         # print(f\"\\n{checkpoint}\\n\")\n",
    "#         class_gru_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#         del checkpoint\n",
    "#         gc.collect()\n",
    "#         print(f\"Loaded 'base model / model {model_number}' from: \\n\\t{destination_path}\")\n",
    "#         # print(f\"\\n{class_gru_model}\\n\")\n",
    "\n",
    "#     elif model_number > 1:\n",
    "#         # Load the best saved base model parameters\n",
    "#         epoch_number = best_epoch_number_dic[model_number-1]\n",
    "#         previous_model_folder = os.path.normpath(os.path.join(model_saving_folder_init, f'Small_Final/period_{model_number-1}'))\n",
    "#         previous_model_path = os.path.normpath(os.path.join(previous_model_folder, f\"{model_name}_epoch_{epoch_number}.pth\"))\n",
    "#         # print(f\"\\n{class_gru_model}\\n\")\n",
    "#         checkpoint = torch.load(previous_model_path, map_location=device, weights_only=True)\n",
    "#         # print(f\"\\n{checkpoint}\\n\")\n",
    "#         class_gru_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#         del checkpoint\n",
    "#         gc.collect()\n",
    "#         print(f\"Loaded base model from: \\n\\t{previous_model_path}\")\n",
    "#         # print(f\"\\n{class_gru_model}\\n\")\n",
    "\n",
    "#         #---------------------------------------------------------\n",
    "#         # Creating New Saving Folder\n",
    "#         model_saving_folder = os.path.normpath(os.path.join(model_saving_folder_init, f'Small_Final/period_{model_number}'))\n",
    "#         ensure_folder(model_saving_folder)\n",
    "\n",
    "#         #---------------------------------------------------------\n",
    "#         # New dataset to work with\n",
    "#         X_train_, y_train_, X_val_, y_val_, X_test_, y_test_ = process_and_return_splits(\n",
    "#             with_indicators_file_path = list_period_files_full_path[model_number-1], # Period data\n",
    "#             # with_indicators_file_path = list_period_files_full_path[0], # Period data\n",
    "#             downsampled_data_minutes = downsampled_data_minutes,\n",
    "#             exclude_columns = exclude_columns,\n",
    "#             lower_threshold = lower_threshold,\n",
    "#             upper_threshold = upper_threshold,\n",
    "#             reverse_steps = reverse_steps,\n",
    "#             sequence_length = sequence_length,\n",
    "#             sliding_interval = sliding_interval\n",
    "#         )\n",
    "#         del X_test_, y_test_\n",
    "#         #---------------------------------------------------------\n",
    "\n",
    "#         train_and_validate(class_gru_model, output_size, criterion, optimizer, X_train_, y_train_, X_val_, y_val_, scheduler, \n",
    "#                         False, num_epochs, batch_size, model_saving_folder, model_name, stop_signal_file)\n",
    "\n",
    "#         best_epoch_number_dic[model_number] = best_results[0]['epoch']\n",
    "#         del X_train_, y_train_, X_val_, y_val_\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "#         #---------------------------------------------------------\n",
    "\n",
    "#         for res in best_results:        \n",
    "#             print(f\"Epoch {res['epoch']}/{num_epochs}, \"\n",
    "#                     f\"Train Loss: {res['train_loss']:.4f}, \" \n",
    "#                     f\"Val Loss: {res['val_loss']:.4f}, \"\n",
    "#                     f\"Val Accuracy: {res['val_accuracy'] * 100:.2f}%, \"\n",
    "#                     f\"Model Path: {res['model_path']}\")      \n",
    "#         print(f\"\\nclass_gru_model: \\n{class_gru_model}\")\n",
    "#         del class_gru_model\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "#         #---------------------------------------------------------\n",
    "\n",
    "#         # Instantiate the model again\n",
    "#         class_gru_model = BiGRUWithAttention(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "\n",
    "#         #---------------------------------------------------------\n",
    "#         # Load the best saved base model parameters\n",
    "#         epoch_number = best_epoch_number_dic[model_number]\n",
    "#         curr_best_model_path = os.path.normpath(os.path.join(model_saving_folder, f\"{model_name}_epoch_{epoch_number}.pth\")) # File of the best epoch\n",
    "#         # print(f\"\\n{class_gru_model}\\n\")\n",
    "#         checkpoint = torch.load(curr_best_model_path, map_location=device, weights_only=True)\n",
    "#         # print(f\"\\n{checkpoint}\\n\")\n",
    "#         class_gru_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#         del checkpoint\n",
    "#         gc.collect()\n",
    "#         print(f\"Loaded model {model_number} from: \\n\\t{curr_best_model_path}\")\n",
    "#         # print(f\"\\n{class_gru_model}\\n\")\n",
    "\n",
    "#     else:\n",
    "#         print(f\"Give an appropriate model_number (1, 2, ..., 5, ...). Passed model_number = {model_number}\\n\")\n",
    "#         return -1\n",
    "    \n",
    "#     #---------------------------------------------------------\n",
    "#     custom_evaluattion_function(class_gru_model, list_period_files_full_path, criterion, output_size, batch_size, model_number)\n",
    "#     del class_gru_model\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     #---------------------------------------------------------\n",
    "#     return best_epoch_number_dic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Evaluate the Model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_model(model_class, model_path, X_test, y_test, criterion, input_size, hidden_size, output_size, num_layers):\n",
    "#     \"\"\"\n",
    "#     Function to test a saved model on test data.\n",
    "    \n",
    "#     Parameters:\n",
    "#         model_class (nn.Module): The class of the model to instantiate.\n",
    "#         model_path (str): Path to the saved model file.\n",
    "#         X_test (np.ndarray or torch.Tensor): Test features of shape (num_samples, seq_len, num_features).\n",
    "#         y_test (np.ndarray or torch.Tensor): Test labels of shape (num_samples, seq_len).\n",
    "#         output_size (int): Number of output classes.\n",
    "#         criterion: Loss function.\n",
    "        \n",
    "#     Returns:\n",
    "#         np.ndarray: Predicted classes for the test data.\n",
    "#     \"\"\"\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "#     print(f\"Loading model from {model_path}...\")\n",
    "    \n",
    "#     # Load the model\n",
    "#     model = model_class(input_size=X_test.shape[-1], hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\n",
    "    \n",
    "#     checkpoint = torch.load(model_path, map_location=device, weights_only=True)\n",
    "#     print(\"Checkpoint Keys:\", checkpoint.keys() if isinstance(checkpoint, dict) else \"State dict directly stored\", '\\n')\n",
    "    \n",
    "#     if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "#         print(\"Dictionaries stored \\n\")\n",
    "#         model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     else:\n",
    "#         print(\"State dict directly stored \\n\")\n",
    "#         model.load_state_dict(checkpoint)  # Assume it's directly the state dict\n",
    "\n",
    "#     model.to(device)\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "\n",
    "#     # Convert test data to tensors\n",
    "#     X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "#     y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         # Forward pass\n",
    "#         outputs = model(X_test)  # Shape: (batch_size, seq_len, output_size)\n",
    "#         outputs = outputs.view(-1, output_size)  # Flatten for prediction and loss calculation\n",
    "#         y_test_flat = y_test.view(-1)  # Flatten labels\n",
    "\n",
    "#         # Calculate loss\n",
    "#         test_loss = criterion(outputs, y_test_flat).item()\n",
    "        \n",
    "#         # Predictions\n",
    "#         predictions = torch.argmax(outputs, dim=-1).cpu().numpy()  # Convert to NumPy array\n",
    "\n",
    "#         # Calculate accuracy\n",
    "#         test_accuracy = (predictions == y_test_flat.cpu().numpy()).mean() * 100\n",
    "\n",
    "#     print(f\"Test Loss: {test_loss:.4f}\")\n",
    "#     print(f\"Test Accuracy: {test_accuracy:.2f}% \\n\")\n",
    "\n",
    "#     return predictions.reshape(y_test.shape)  # Reshape to match the original test data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Medical_AI_LoRA_py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
